{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"fenic: The dataframe (re)built for LLM inference","text":"<p>fenic is an opinionated, PySpark-inspired DataFrame framework for building AI and agentic applications. Transform unstructured and structured data into insights using familiar DataFrame operations enhanced with semantic intelligence. With first-class support for markdown, transcripts, and semantic operators, plus efficient batch inference across any model provider.</p>"},{"location":"#install","title":"Install","text":"<p>fenic requires Python 3.9 or later.</p> <pre><code>pip install fenic\n</code></pre>"},{"location":"#llm-provider-setup","title":"LLM Provider Setup","text":"<p>fenic requires an API key from at least one LLM provider. Set the appropriate environment variable for your chosen provider:</p> <pre><code># For OpenAI\nexport OPENAI_API_KEY=\"your-openai-api-key\"\n\n# For Anthropic\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n\n# For Google\nexport GEMINI_API_KEY=\"your-google-api-key\"\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":"<p>The fastest way to learn about fenic is by checking the examples.</p> <p>Below is a quick list of the examples in this repo:</p> Example Description Hello World! Introduction to semantic extraction and classification using fenic's core operators through error log analysis. Enrichment Multi-stage DataFrames with template-based text extraction, joins, and LLM-powered transformations demonstrated via log enrichment. Meeting Transcript Processing Native transcript parsing, Pydantic schema integration, and complex aggregations shown through meeting analysis. News Analysis Analyze and extract insights from news articles using semantic operators and structured data processing. Podcast Summarization Process and summarize podcast transcripts with speaker-aware analysis and key point extraction. Semantic Join Instead of simple fuzzy matching, use fenic's powerful semantic join functionality to match data across tables. Named Entity Recognition Extract and classify named entities from text using semantic extraction and classification. Markdown Processing Process and transform markdown documents with structured data extraction and formatting. JSON Processing Handle complex JSON data structures with semantic operations and schema validation. Feedback Clustering Group and analyze feedback using semantic similarity and clustering operations. Document Extraction Extract structured information from various document formats using semantic operators. <p>(Feel free to click any example above to jump right to its folder.)</p>"},{"location":"#why-use-fenic","title":"Why use fenic?","text":"<p>fenic is an opinionated, PySpark-inspired DataFrame framework for building production AI and agentic applications.</p> <p>Unlike traditional data tools retrofitted for LLMs, fenic's query engine is built from the ground up with inference in mind.</p> <p>Transform structured and unstructured data into insights using familiar DataFrame operations enhanced with semantic intelligence. With first-class support for markdown, transcripts, and semantic operators, plus efficient batch inference across any model provider.</p> <p>fenic brings the reliability of traditional data pipelines to AI workloads.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#purpose-built-for-llm-inference","title":"Purpose-Built for LLM Inference","text":"<ul> <li>Query engine designed from scratch for AI workloads, not retrofitted</li> <li>Automatic batch optimization for API calls</li> <li>Built-in retry logic and rate limiting</li> <li>Token counting and cost tracking</li> </ul>"},{"location":"#semantic-operators-as-first-class-citizens","title":"Semantic Operators as First-Class Citizens","text":"<ul> <li><code>semantic.analyze_sentiment</code> - Built-in sentiment analysis</li> <li><code>semantic.classify</code> - Categorize text with few-shot examples</li> <li><code>semantic.extract</code> - Transform unstructured text into structured data with schemas</li> <li><code>semantic.group_by</code> - Group data by semantic similarity</li> <li><code>semantic.join</code> - Join DataFrames on meaning, not just values</li> <li><code>semantic.map</code> - Apply natural language transformations</li> <li><code>semantic.predicate</code> - Create predicates using natural language to filter rows</li> <li><code>semantic.reduce</code> - Aggregate grouped data with LLM operations</li> </ul>"},{"location":"#native-unstructured-data-support","title":"Native Unstructured Data Support","text":"<p>Goes beyond typical multimodal data types (audio, images) by creating specialized types for text-heavy workloads:</p> <ul> <li>Markdown parsing and extraction as a first-class data type</li> <li>Transcript processing (SRT, generic formats) with speaker and timestamp awareness</li> <li>JSON manipulation with JQ expressions for nested data</li> <li>Automatic text chunking with configurable overlap for long documents</li> </ul>"},{"location":"#production-ready-infrastructure","title":"Production-Ready Infrastructure","text":"<ul> <li>Multi-provider support (OpenAI, Anthropic, Gemini)</li> <li>Local and cloud execution backends</li> <li>Comprehensive error handling and logging</li> <li>Pydantic integration for type safety</li> </ul>"},{"location":"#familiar-dataframe-api","title":"Familiar DataFrame API","text":"<ul> <li>PySpark-compatible operations</li> <li>Lazy evaluation and query optimization</li> <li>SQL support for complex queries</li> <li>Seamless integration with existing data pipelines</li> </ul>"},{"location":"#why-dataframes-for-llm-and-agentic-applications","title":"Why DataFrames for LLM and Agentic Applications?","text":"<p>AI and agentic applications are fundamentally pipelines and workflows - exactly what DataFrame APIs were designed to handle. Rather than reinventing patterns for data transformation, filtering, and aggregation, fenic leverages decades of proven engineering practices.</p>"},{"location":"#decoupled-architecture-for-better-agents","title":"Decoupled Architecture for Better Agents","text":"<p>fenic creates a clear separation between heavy inference tasks and real-time agent interactions. By moving batch processing out of the agent runtime, you get:</p> <ul> <li>More predictable and responsive agents</li> <li>Better resource utilization with batched LLM calls</li> <li>Cleaner separation between planning/orchestration and execution</li> </ul>"},{"location":"#built-for-all-engineers","title":"Built for All Engineers","text":"<p>DataFrames aren't just for data practitioners. The fluent, composable API design makes it accessible to any engineer:</p> <ul> <li>Chain operations naturally: <code>df.filter(...).semantic.group_by(...)</code></li> <li>Mix imperative and declarative styles seamlessly</li> <li>Get started quickly with familiar patterns from pandas/PySpark or SQL</li> </ul>"},{"location":"#support","title":"Support","text":"<p>Join our community on Discord where you can connect with other users, ask questions, and get help with your fenic projects. Our community is always happy to welcome newcomers!</p> <p>If you find fenic useful, consider giving us a \u2b50 at the top of this repository. Your support helps us grow and improve the framework for everyone!</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions of all kinds! Whether you're interested in writing code, improving documentation, testing features, or proposing new ideas, your help is valuable to us.</p> <p>For developers planning to submit code changes, we encourage you to first open an issue to discuss your ideas before creating a Pull Request. This helps ensure alignment with the project's direction and prevents duplicate efforts.</p> <p>Please refer to our contribution guidelines for detailed information about the development process and project setup.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#directory-structure","title":"\ud83d\udcc1 Directory Structure","text":"<p>The repository is organized as follows:</p> <pre><code>fenic/\n\u251c\u2500\u2500 src/fenic/            # Core library\n\u2502   \u251c\u2500\u2500 api/                  # Public API (DataFrame, Column, functions, session)\n\u2502   \u2502   \u251c\u2500\u2500 dataframe/        # DataFrame implementation and extensions\n\u2502   \u2502   \u251c\u2500\u2500 functions/        # Built-in and semantic functions\n\u2502   \u2502   \u251c\u2500\u2500 session/          # Session management and configuration\n\u2502   \u2502   \u2514\u2500\u2500 types/            # Schema definitions and data types\n\u2502   \u251c\u2500\u2500 core/                 # Core framework components\n\u2502   \u2502   \u2514\u2500\u2500 _logical_plan/    # Logical plan representation for operators\n\u2502   \u2502   \u251c\u2500\u2500 types/            # Core types (DataType, Schema, etc)\n\u2502   \u251c\u2500\u2500 _backends/            # Execution backends\n\u2502   \u2502   \u251c\u2500\u2500 local/            # Local execution (Polars/DuckDB)\n\u2502   \u2502   \u2514\u2500\u2500 cloud/            # Cloud execution (Typedef)\n\u2502   \u2514\u2500\u2500 _inference/           # LLM inference layer\n\u251c\u2500\u2500 rust/                     # Rust crates for performance-critical operations\n\u251c\u2500\u2500 tests/                    # Test suite mirroring source structure\n\u2514\u2500\u2500 examples/                 # Usage examples and demos\n</code></pre>"},{"location":"CONTRIBUTING/#development-setup","title":"\ud83d\udee0\ufe0f Development Setup","text":"<p>Local development requires uv and a Rust toolchain.</p> <p>[!OPTIONAL] Not required, but recommended: just</p>"},{"location":"CONTRIBUTING/#first-time-setup","title":"First-Time Setup","text":"<p>From the root of the repo:</p> <pre><code>just setup\n# without just\nuv sync\nuv run maturin develop --uv\n</code></pre> <p>This will:</p> <ul> <li>Create a virtual environment</li> <li>Build the Rust crate</li> <li>Install Python dependencies</li> <li>Set up the package in editable mode</li> </ul> <p>This command also places the built dynamic Rust library inside <code>src/fenic</code>.</p>"},{"location":"CONTRIBUTING/#making-changes","title":"Making Changes","text":"<ul> <li>To apply changes made to Python code:</li> </ul> <p><code>bash   just sync   # without just   uv sync</code></p> <ul> <li>To apply changes made to Rust code:</li> </ul> <p><code>bash   just sync-rust   # without just   uv run maturin develop --uv</code></p> <p>Add <code>--release</code> or <code>-r</code> to build the Rust crate in release mode (better performance).</p> <ul> <li>To preview changes to the documentation from docstring or other changes:</li> </ul> <p><code>bash   just preview-docs   # without just   uv run --group docs mkdocs serve</code></p>"},{"location":"CONTRIBUTING/#running-tests","title":"\u2705 Running Tests","text":"<p>Run an individual test file:</p> <pre><code>uv run pytest tests/path/to/test_foo.py\n</code></pre> <p>Run all tests for the local backend:</p> <pre><code>just test\n# or without just\nuv run pytest -m \"not cloud\" tests\n</code></pre> <p>Run all tests against a different model provider/model name:</p> <ul> <li>OpenAI/gpt-4.1-nano (Default)</li> </ul> <pre><code>uv run pytest --model-provider=openai --model-name='gpt-4.1-nano'\n</code></pre> <ul> <li>Anthropic/claude-3-5-haiku-latest</li> </ul> <pre><code>uv sync --extra=anthropic\nuv run pytest --model-provider=anthropic --model-name='claude-3-5-haiku-latest'\n</code></pre> <p>Run all tests for the cloud backend:</p> <pre><code>just test-cloud\n</code></pre> <p>\u26a0\ufe0f Note: All tests require a valid OpenAI/Anthropic API key set in the environment variables.</p>"},{"location":"CONTRIBUTING/#running-notebooks-vscode-cursor","title":"\ud83d\udcd3 Running Notebooks (VSCode / Cursor)","text":"<p>To run demo notebooks:</p> <ol> <li>Install the Jupyter extension.</li> <li>Add the <code>.venv</code> path to Python: Venv Folders in VSCode settings:</li> <li>Open settings: <code>Preferences: Open User Settings</code></li> <li>Go to Extensions \u2192 Python \u2192 Python: Venv Folders</li> <li>Open a notebook, select the correct kernel from your virtual environment, and run cells.</li> </ol> <p>Restart the kernel to reflect any code changes made to the <code>fenic</code> source.</p> <p>Have questions or want to contribute? Let us know in the Discord!</p>"},{"location":"examples/document_extraction/","title":"Document Metadata Extraction with Fenic","text":"<p>This example demonstrates how to extract structured metadata from unstructured text data, using Fenic's semantic extraction capabilities. It compares two different approaches for defining extraction schemas.</p>"},{"location":"examples/document_extraction/#overview","title":"Overview","text":"<p>Document metadata extraction is a common use case for LLMs, allowing you to automatically parse and structure information from various document types including research papers, product announcements, meeting notes, news articles, and technical documentation.</p> <p>This example showcases:</p> <ul> <li>Two Schema Approaches: ExtractSchema vs Pydantic models</li> <li>Structured Extraction: Converting unstructured text to structured metadata</li> <li>Zero-Shot Extraction: No examples required</li> </ul>"},{"location":"examples/document_extraction/#schema-approaches-compared","title":"Schema Approaches Compared","text":""},{"location":"examples/document_extraction/#extractschema-approach","title":"ExtractSchema Approach","text":"<pre><code>doc_metadata_schema = fc.ExtractSchema([\n    fc.ExtractSchemaField(\n        name=\"keywords\",\n        data_type=fc.ExtractSchemaList(element_type=fc.StringType),\n        description=\"List of key topics and terms\"\n    )\n])\n</code></pre> <p>Advantages:</p> <ul> <li>\u2705 Supports complex data types (lists, nested structures)</li> <li>\u2705 Native Fenic schema definition</li> <li>\u2705 Type-safe with proper list handling</li> </ul>"},{"location":"examples/document_extraction/#pydantic-model-approach","title":"Pydantic Model Approach","text":"<pre><code>from typing import Literal\n\nclass DocumentMetadata(BaseModel):\n    keywords: str = Field(..., description=\"Comma-separated list of key topics and terms\")\n    document_type: Literal[\"research paper\", \"product announcement\", \"meeting notes\", \"news article\", \"technical documentation\"] = Field(..., description=\"Type of document\")\n</code></pre> <p>Advantages:</p> <ul> <li>\u2705 Familiar Python class syntax</li> <li>\u2705 Leverages existing Pydantic knowledge</li> <li>\u2705 Supports Literal strings for constraining output values</li> <li>\u274c Limited to simple data types (str, int, float, bool, Literal)</li> </ul>"},{"location":"examples/document_extraction/#sample-data","title":"Sample Data","text":"<p>The example processes 5 diverse document types:</p> <ol> <li>Research Paper - Academic abstract with technical terms</li> <li>Product Announcement - Marketing content with features and pricing</li> <li>Meeting Notes - Internal documentation with decisions and action items</li> <li>News Article - Breaking news with facts and impact</li> <li>Technical Documentation - API reference with specifications</li> </ol>"},{"location":"examples/document_extraction/#extracted-metadata-fields","title":"Extracted Metadata Fields","text":"<ul> <li>Title: Main subject or heading of the document</li> <li>Document Type: Classification (research paper, product announcement, etc.)</li> <li>Date: Any relevant date mentioned (publication, meeting, etc.)</li> <li>Keywords: Key topics and terms (list vs comma-separated string)</li> <li>Summary: One-sentence overview of the document's purpose</li> </ul>"},{"location":"examples/document_extraction/#key-differences-in-output","title":"Key Differences in Output","text":"<p>ExtractSchema Keywords:</p> <pre><code>[\"machine learning\", \"climate modeling\", \"neural networks\"]\n</code></pre> <p>Pydantic Keywords:</p> <pre><code>\"machine learning, climate modeling, neural networks\"\n</code></pre>"},{"location":"examples/document_extraction/#usage","title":"Usage","text":"<pre><code># Ensure you have OpenAI API key configured\nexport OPENAI_API_KEY=\"your-api-key\"\n\n# Run the extraction example\npython document_extraction.py\n</code></pre>"},{"location":"examples/document_extraction/#expected-output","title":"Expected Output","text":"<p>The script will display:</p> <ol> <li>Sample Documents - Overview of loaded documents with text lengths</li> <li>ExtractSchema Results - Structured extraction using native Fenic schema</li> <li>Pydantic Results - Same extraction using Pydantic model</li> <li>Method Comparison - Side-by-side analysis of both approaches</li> </ol>"},{"location":"examples/document_extraction/#when-to-use-each-approach","title":"When to Use Each Approach","text":"<p>Choose ExtractSchema when:</p> <ul> <li>You need complex data types (lists, nested objects)</li> <li>Working primarily within Fenic ecosystem</li> <li>Type safety is important for downstream processing</li> </ul> <p>Choose Pydantic when:</p> <ul> <li>You prefer familiar Python class syntax</li> <li>Your team has existing Pydantic expertise</li> <li>Simple data types meet your requirements</li> </ul>"},{"location":"examples/document_extraction/#learning-outcomes","title":"Learning Outcomes","text":"<p>This example teaches:</p> <ul> <li>How to perform zero-shot semantic extraction</li> <li>Differences between schema definition approaches</li> </ul> <p>Perfect for understanding Fenic's semantic extraction capabilities and choosing the right schema approach for your use case.</p>"},{"location":"examples/enrichment/","title":"Log Enrichment Pipeline","text":"<p>A log processing system using fenic's text extraction and semantic enrichment capabilities to transform unstructured logs into actionable incident response data.</p>"},{"location":"examples/enrichment/#overview","title":"Overview","text":"<p>This pipeline demonstrates log enrichment through multi-stage processing:</p> <ul> <li>Template-based parsing without regex</li> <li>Service metadata enrichment via joins</li> <li>LLM-powered error categorization and remediation</li> <li>Incident severity assessment with business context</li> </ul>"},{"location":"examples/enrichment/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install fenic:</li> </ol> <p><code>bash    pip install fenic</code></p> <ol> <li>Configure OpenAI API key:    <code>bash    export OPENAI_API_KEY=\"your-api-key-here\"</code></li> </ol>"},{"location":"examples/enrichment/#usage","title":"Usage","text":"<pre><code>python enrichment.py\n</code></pre>"},{"location":"examples/enrichment/#implementation","title":"Implementation","text":"<p>The pipeline processes logs through three stages:</p> <ol> <li>Parse: Extract structured fields from syslog-format messages</li> <li>Enrich: Join with service ownership and criticality data</li> <li>Analyze: Apply LLM operations for incident response</li> </ol>"},{"location":"examples/enrichment/#api-structure","title":"API Structure","text":"<pre><code>from fenic.api.session import Session, SessionConfig, SemanticConfig, OpenAIModelConfig\nfrom fenic.api.functions import col, text, semantic\nfrom pydantic import BaseModel, Field\n\n# Configure session\nconfig = SessionConfig(\n    app_name=\"log_enrichment\",\n    semantic=SemanticConfig(\n        language_models= {\n            \"mini\" : OpenAIModelConfig(\n            model_name=\"gpt-4o-mini\",\n            rpm=500,\n            tpm=200_000\n        )\n        }\n    )\n)\n\n# Define extraction schema with Pydantic\nclass ErrorAnalysis(BaseModel):\n    error_category: str = Field(..., description=\"Main category of the error\")\n    affected_component: str = Field(..., description=\"Specific component affected\")\n    potential_cause: str = Field(..., description=\"Most likely root cause\")\n\n# Stage 1: Template extraction\nparsed = logs_df.select(\n    \"raw_message\", text.extract(\"${timestamp:none} [${level:none}] ${service:none}: ${message:none}\")\n)\n\n# Stage 2: Metadata join\nenriched = parsed.join(metadata_df, on=\"service\", how=\"left\")\n\n# Stage 3: Semantic enrichment\nfinal = enriched.select(\n    semantic.extract(\"message\", ErrorAnalysis).alias(\"analysis\"),\n    semantic.classify(\n        text.concat(col(\"message\"), lit(\" (criticality: \"), col(\"criticality\"), lit(\")\")),\n        [\"low\", \"medium\", \"high\", \"critical\"]\n    ).alias(\"incident_severity\"),\n    semantic.map(\n        \"Generate remediation steps for: {message} | Service: {service} | Team: {team_owner}\"\n    ).alias(\"remediation_steps\")\n)\n</code></pre>"},{"location":"examples/enrichment/#output-format","title":"Output Format","text":"<pre><code>\u2705 Pipeline Complete! Final enriched logs:\n----------------------------------------------------------------------\ntimestamp           level  service       message                  team_owner      error_category  incident_severity  remediation_steps\n2024-01-15 14:32:01 ERROR  payment-api   Connection timeout...    payments-team   database        critical          1. Check Database Connectivity...\n2024-01-15 14:32:15 WARN   user-service  Rate limit exceeded...   identity-team   resource        critical          1. Review Rate Limiting Config...\n\n\ud83d\udcc8 Analytics Examples:\n\nError Category Distribution:\nerror_category    count\ndatabase          1\nresource          5\nauthentication    4\nnetwork           5\n\nHigh-Priority Incidents (Critical/High severity):\nservice           team_owner       incident_severity  on_call_channel    remediation_steps\npayment-api       payments-team    critical          #payments-oncall   1. Check Database Connectivity...\nuser-service      identity-team    critical          #identity-alerts   1. Review Rate Limiting Config...\n</code></pre>"},{"location":"examples/enrichment/#configuration","title":"Configuration","text":""},{"location":"examples/enrichment/#custom-log-templates","title":"Custom Log Templates","text":"<p>Parse different log formats:</p> <pre><code># Syslog format\nlog_template = \"${timestamp:none} [${level:none}] ${service:none}: ${message:none}\"\n\n# Custom application format\nlog_template = \"${service:none} | ${timestamp:none} | ${level:none} - ${message:none}\"\n</code></pre>"},{"location":"examples/enrichment/#troubleshooting","title":"Troubleshooting","text":"<p>Issue: Template extraction returns empty fields Solution: Check template format matches log structure exactly, including spaces</p> <p>Issue: Missing service metadata after join Solution: Use left join to preserve all logs; add default values for missing metadata</p> <p>Issue: Generic remediation steps Solution: Include more context in semantic.map prompt (service, team, criticality)</p>"},{"location":"examples/feedback_clustering/","title":"Customer Feedback Clustering &amp; Analysis with Fenic","text":"<p>This example demonstrates how to use Fenic's <code>semantic.group_by()</code> and <code>semantic.reduce()</code> to automatically cluster customer feedback into themes and generate intelligent summaries for each discovered category.</p>"},{"location":"examples/feedback_clustering/#overview","title":"Overview","text":"<p>Customer feedback analysis is a critical business process that traditionally requires manual categorization and analysis. This example shows how semantic clustering can automatically:</p> <ul> <li>Discover hidden themes in unstructured feedback without predefined categories</li> <li>Group similar feedback based on semantic meaning rather than keywords</li> <li>Generate actionable insights for each theme using AI-powered summarization</li> <li>Prioritize issues based on sentiment and frequency</li> </ul>"},{"location":"examples/feedback_clustering/#key-features-demonstrated","title":"Key Features Demonstrated","text":"<ul> <li>Semantic Clustering: Using <code>semantic.group_by()</code> for embedding-based clustering</li> <li>AI Summarization: Using <code>semantic.reduce()</code> for intelligent theme analysis</li> <li>Automatic Theme Discovery: No manual categorization required</li> <li>Sentiment Analysis: Understanding positive vs negative feedback patterns</li> <li>Business Intelligence: Actionable insights for product teams</li> </ul>"},{"location":"examples/feedback_clustering/#how-it-works","title":"How It Works","text":""},{"location":"examples/feedback_clustering/#step-1-data-preparation","title":"Step 1: Data Preparation","text":"<p>Load customer feedback with ratings and metadata:</p> <pre><code>feedback_data = [\n    {\n        \"feedback_id\": \"fb_001\",\n        \"customer_name\": \"Alice Johnson\",\n        \"feedback\": \"The mobile app crashes every time I try to upload a photo. Very frustrating!\",\n        \"rating\": 1,\n        \"timestamp\": \"2024-01-15\"\n    },\n    # ... more feedback\n]\n</code></pre>"},{"location":"examples/feedback_clustering/#step-2-embedding-creation","title":"Step 2: Embedding Creation","text":"<p>Generate semantic embeddings from feedback text:</p> <pre><code>feedback_with_embeddings = feedback_df.select(\n    \"*\",\n    fc.semantic.embed(fc.col(\"feedback\")).alias(\"feedback_embeddings\")\n)\n</code></pre>"},{"location":"examples/feedback_clustering/#step-3-semantic-clustering-summarization","title":"Step 3: Semantic Clustering &amp; Summarization","text":"<p>Use both operations together in a single aggregation:</p> <pre><code>feedback_clusters = feedback_with_embeddings.semantic.group_by(\n    fc.col(\"feedback_embeddings\"),\n    4  # Number of clusters to discover\n).agg(\n    fc.count(\"*\").alias(\"feedback_count\"),\n    fc.avg(\"rating\").alias(\"avg_rating\"),\n    fc.collect_list(\"customer_name\").alias(\"customer_names\"),\n    fc.semantic.reduce(\n        \"Analyze this cluster of customer feedback and provide a concise summary of the main theme, common issues, and sentiment. Feedback: {feedback}\"\n    ).alias(\"theme_summary\")\n)\n</code></pre>"},{"location":"examples/feedback_clustering/#sample-results","title":"Sample Results","text":"<p>The system automatically discovered these themes from 12 feedback entries:</p>"},{"location":"examples/feedback_clustering/#cluster-0-positive-features-support-475","title":"Cluster 0: Positive Features &amp; Support (4.75\u2605)","text":"<ul> <li>Theme: Praise for specific features and excellent customer support</li> <li>Key Points: Dark mode feature, helpful support team, effective search functionality</li> <li>Sentiment: Predominantly positive with some feature enhancement requests</li> </ul>"},{"location":"examples/feedback_clustering/#cluster-1-uiux-design-issues-20","title":"Cluster 1: UI/UX Design Issues (2.0\u2605)","text":"<ul> <li>Theme: Design consistency and professional appearance concerns</li> <li>Key Points: Inconsistent button layouts across screens</li> <li>Sentiment: Negative due to unprofessional user experience</li> </ul>"},{"location":"examples/feedback_clustering/#cluster-2-technical-performance-problems-175","title":"Cluster 2: Technical Performance Problems (1.75\u2605)","text":"<ul> <li>Theme: Critical technical issues affecting core functionality</li> <li>Key Points: App crashes, slow loading times, frequent freezes</li> <li>Sentiment: Very negative with high frustration levels</li> </ul>"},{"location":"examples/feedback_clustering/#cluster-3-usability-feature-gaps-20","title":"Cluster 3: Usability &amp; Feature Gaps (2.0\u2605)","text":"<ul> <li>Theme: Process complexity and missing functionality</li> <li>Key Points: Confusing checkout, need for offline mode</li> <li>Sentiment: Negative about functionality limitations</li> </ul>"},{"location":"examples/feedback_clustering/#value","title":"Value","text":""},{"location":"examples/feedback_clustering/#automated-insights","title":"Automated Insights","text":"<ul> <li>Identifies themes without manual categorization</li> <li>Provides consistent analysis across all feedback</li> <li>Scales to thousands of feedback entries</li> </ul>"},{"location":"examples/feedback_clustering/#actionable-intelligence","title":"Actionable Intelligence","text":"<ul> <li>Priority 1: Fix technical crashes and performance (Cluster 2)</li> <li>Priority 2: Improve design consistency (Cluster 1)</li> <li>Priority 3: Simplify user workflows (Cluster 3)</li> <li>Maintain: Continue excellent support and features (Cluster 0)</li> </ul>"},{"location":"examples/feedback_clustering/#resource-optimization","title":"Resource Optimization","text":"<ul> <li>Reduces manual analysis time from hours to minutes</li> <li>Enables real-time feedback monitoring</li> <li>Focuses development efforts on highest-impact issues</li> </ul>"},{"location":"examples/feedback_clustering/#technical-architecture","title":"Technical Architecture","text":""},{"location":"examples/feedback_clustering/#session-configuration","title":"Session Configuration","text":"<pre><code>config = fc.SessionConfig(\n    app_name=\"feedback_clustering\",\n    semantic=fc.SemanticConfig(\n        language_models={\n            \"mini\": fc.OpenAIModelConfig(\n                model_name=\"gpt-4o-mini\",\n                rpm=500,\n                tpm=200_000,\n            )\n        },\n        embedding_models={\n            \"small\": fc.OpenAIModelConfig(\n                model_name=\"text-embedding-3-small\",\n                rpm=3000,\n                tpm=1_000_000\n            )\n        }\n    ),\n)\n</code></pre>"},{"location":"examples/feedback_clustering/#key-operations","title":"Key Operations","text":"<p><code>semantic.group_by(embedding_column, num_clusters)</code></p> <ul> <li>Uses K-means clustering on embedding vectors</li> <li>Groups semantically similar feedback together</li> <li>Assigns <code>_cluster_id</code> to each group</li> </ul> <p><code>semantic.reduce(instruction)</code></p> <ul> <li>Aggregation function that summarizes multiple texts</li> <li>Uses LLM to analyze and synthesize insights</li> <li>Generates human-readable theme descriptions</li> </ul>"},{"location":"examples/feedback_clustering/#usage","title":"Usage","text":"<pre><code># Ensure you have OpenAI API key configured\nexport OPENAI_API_KEY=\"your-api-key\"\n\n# Run the feedback clustering analysis\npython feedback_clustering.py\n</code></pre>"},{"location":"examples/feedback_clustering/#expected-output","title":"Expected Output","text":"<p>The script shows:</p> <ol> <li>Raw Feedback Data: Customer names, feedback text, and ratings</li> <li>Clustering Progress: Embedding generation and clustering status</li> <li>Theme Analysis: Detailed summaries for each discovered cluster</li> <li>Business Insights: Actionable themes ranked by priority</li> </ol>"},{"location":"examples/feedback_clustering/#use-cases","title":"Use Cases","text":""},{"location":"examples/feedback_clustering/#product-development","title":"Product Development","text":"<ul> <li>Identify most requested features</li> <li>Understand user pain points</li> <li>Prioritize bug fixes and improvements</li> </ul>"},{"location":"examples/feedback_clustering/#customer-success","title":"Customer Success","text":"<ul> <li>Monitor satisfaction trends</li> <li>Identify at-risk customer segments</li> <li>Improve support processes</li> </ul>"},{"location":"examples/feedback_clustering/#marketing-intelligence","title":"Marketing Intelligence","text":"<ul> <li>Understand customer sentiment</li> <li>Identify product strengths for messaging</li> <li>Track competitive advantages</li> </ul>"},{"location":"examples/feedback_clustering/#learning-outcomes","title":"Learning Outcomes","text":"<p>This example teaches:</p> <ul> <li>How to combine embedding-based clustering with AI summarization</li> <li>When to use semantic operations for business intelligence</li> <li>Patterns for automated text analysis and insight generation</li> <li>Integration of multiple semantic operations in data pipelines</li> </ul>"},{"location":"examples/hello_world/","title":"Hello World","text":"<p>An error log analyzer using Fenic's semantic extraction capabilities to parse and analyze application errors without regex patterns.</p>"},{"location":"examples/hello_world/#overview","title":"Overview","text":"<p>This tool demonstrates automated error log analysis through natural language processing, providing:</p> <ul> <li>Root cause identification</li> <li>Automated fix suggestions</li> <li>Severity classification (low/medium/high/critical)</li> <li>Pattern extraction</li> </ul>"},{"location":"examples/hello_world/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Fenic:</li> </ol> <p><code>bash    pip install fenic</code></p> <ol> <li>Configure OpenAI API key:    <code>bash    export OPENAI_API_KEY=\"your-api-key-here\"</code></li> </ol>"},{"location":"examples/hello_world/#usage","title":"Usage","text":"<pre><code>python hello_world.py\n</code></pre>"},{"location":"examples/hello_world/#implementation","title":"Implementation","text":"<p>The analyzer processes various error types including:</p> <ul> <li>Java NullPointerException</li> <li>Node.js connection errors (ECONNREFUSED)</li> <li>Python API timeouts (Stripe APIConnectionError)</li> <li>Database connection failures (Django OperationalError)</li> <li>React TypeError</li> <li>Performance warnings (slow queries)</li> <li>Cache misses and email delivery delays</li> </ul>"},{"location":"examples/hello_world/#troubleshooting","title":"Troubleshooting","text":"<p>Issue: Generic analysis results Solution: Add more descriptive fields to ExtractSchema</p> <p>Issue: Incorrect severity classification Solution: Adjust classification categories or provide examples</p> <p>Issue: Missing error patterns Solution: Modify ExtractSchemaField descriptions for better targeting</p>"},{"location":"examples/json_processing/","title":"JSON Processing with Fenic","text":"<p>This example demonstrates comprehensive JSON processing capabilities using fenic's JSON type system and JQ integration. It processes a complex nested JSON file containing whisper transcription data and transforms it into multiple structured DataFrames for analysis.</p>"},{"location":"examples/json_processing/#what-this-example-shows","title":"What This Example Shows","text":""},{"location":"examples/json_processing/#core-json-processing-features","title":"Core JSON Processing Features","text":"<ul> <li>JSON Type Casting: Loading string data and converting to Fenic's JsonType</li> <li>JQ Integration: Complex queries for nested data extraction and aggregation</li> <li>Array Operations: Processing nested arrays and extracting scalar values</li> <li>Type Conversion: Converting JSON data to appropriate DataFrame types</li> <li>Hybrid Processing: Combining JSON-native operations with traditional DataFrame analytics</li> </ul>"},{"location":"examples/json_processing/#real-world-use-case","title":"Real-World Use Case","text":"<p>The example processes audio transcription data from OpenAI's Whisper, demonstrating how to handle:</p> <ul> <li>Complex nested JSON structures</li> <li>Multiple levels of data granularity</li> <li>Time-series data with speaker attribution</li> <li>Confidence scores and quality metrics</li> </ul>"},{"location":"examples/json_processing/#data-structure","title":"Data Structure","text":"<p>The input JSON contains:</p> <pre><code>{\n  \"language\": \"en\",\n  \"segments\": [\n    {\n      \"text\": \"Let me ask you about AI.\",\n      \"start\": 2.94,\n      \"end\": 4.48,\n      \"words\": [\n        {\n          \"word\": \" Let\",\n          \"start\": 2.94,\n          \"end\": 3.12,\n          \"speaker\": \"SPEAKER_01\",\n          \"probability\": 0.69384765625\n        }\n        // ... more words\n      ]\n    }\n    // ... more segments\n  ]\n}\n</code></pre>"},{"location":"examples/json_processing/#output-dataframes","title":"Output DataFrames","text":"<p>The pipeline creates three complementary DataFrames:</p>"},{"location":"examples/json_processing/#1-words-dataframe-granular-analysis","title":"1. Words DataFrame (Granular Analysis)","text":"<p>Purpose: Individual word-level analysis with timing and confidence</p> <ul> <li><code>word_text</code>: The spoken word</li> <li><code>speaker</code>: Speaker identifier (SPEAKER_00, SPEAKER_01)</li> <li><code>start_time</code>, <code>end_time</code>: Word timing in seconds</li> <li><code>duration</code>: Calculated word duration</li> <li><code>probability</code>: Speech recognition confidence score</li> </ul> <p>Key Techniques:</p> <ul> <li>Nested JQ traversal with variable binding</li> <li>Array explosion from nested structures</li> <li>Type casting for numeric operations</li> </ul>"},{"location":"examples/json_processing/#2-segments-dataframe-content-analysis","title":"2. Segments DataFrame (Content Analysis)","text":"<p>Purpose: Conversation segments with aggregated metrics</p> <ul> <li><code>segment_text</code>: Full text of the conversation segment</li> <li><code>start_time</code>, <code>end_time</code>, <code>duration</code>: Segment timing</li> <li><code>word_count</code>: Number of words in segment</li> <li><code>average_confidence</code>: Average recognition confidence</li> </ul> <p>Key Techniques:</p> <ul> <li>JQ array aggregations (<code>length</code>, <code>add / length</code>)</li> <li>In-query statistical calculations</li> <li>Mixed JSON and DataFrame operations</li> </ul>"},{"location":"examples/json_processing/#3-speaker-summary-dataframe-analytics","title":"3. Speaker Summary DataFrame (Analytics)","text":"<p>Purpose: High-level speaker analytics and patterns</p> <ul> <li><code>speaker</code>: Speaker identifier</li> <li><code>total_words</code>: Total words spoken</li> <li><code>total_speaking_time</code>: Total time speaking</li> <li><code>average_confidence</code>: Overall speech quality</li> <li><code>first_speaking_time</code>, <code>last_speaking_time</code>: Speaking timeframe</li> <li><code>word_rate</code>: Words per minute</li> </ul> <p>Key Techniques:</p> <ul> <li>Traditional DataFrame aggregations (<code>group_by</code>, <code>agg</code>)</li> <li>Multiple aggregation functions (count, avg, min, max, sum)</li> <li>Calculated metrics from aggregated data</li> </ul>"},{"location":"examples/json_processing/#data-extraction-approaches","title":"Data Extraction Approaches","text":""},{"location":"examples/json_processing/#struct-casting-unnest-recommended-for-simple-fields","title":"Struct Casting + Unnest (Recommended for Simple Fields)","text":"<p>For efficient extraction of simple fields, the example uses struct casting with unnest:</p> <pre><code># Define schema for structured extraction\nword_schema = fc.StructType([\n    fc.StructField(\"word\", fc.StringType),\n    fc.StructField(\"speaker\", fc.StringType),\n    fc.StructField(\"start\", fc.FloatType),\n    fc.StructField(\"end\", fc.FloatType),\n    fc.StructField(\"probability\", fc.FloatType)\n])\n\n# Cast and unnest to extract all fields automatically\nwords_df.select(\n    fc.col(\"word_data\").cast(word_schema).alias(\"word_struct\")\n).unnest(\"word_struct\")\n</code></pre> <p>Benefits:</p> <ul> <li>More efficient than multiple JQ queries</li> <li>Automatic type handling through schema</li> <li>Single operation extracts all fields</li> <li>Better performance for simple field extraction</li> </ul>"},{"location":"examples/json_processing/#jq-queries-best-for-complex-operations","title":"JQ Queries (Best for Complex Operations)","text":"<p>Complex array operations still use JQ for maximum power:</p> <pre><code># Nested array traversal with variable binding\n'.segments[] as $seg | $seg.words[] | {...}'\n\n# Array length calculation\n'.words | length'\n\n# Array aggregation for averages\n'[.words[].probability] | add / length'\n\n# Object construction with mixed data levels\n'{word: .word, segment_start: $seg.start, ...}'\n</code></pre> <p>When to Use JQ:</p> <ul> <li>Array aggregations (<code>length</code>, <code>add / length</code>)</li> <li>Complex transformations</li> <li>Variable binding and nested operations</li> <li>Mathematical operations within queries</li> </ul>"},{"location":"examples/json_processing/#running-the-example","title":"Running the Example","text":""},{"location":"examples/json_processing/#prerequisites","title":"Prerequisites","text":"<ol> <li>Set your OpenAI API key:</li> </ol> <p><code>bash    export OPENAI_API_KEY=\"your-api-key-here\"</code></p> <ol> <li>Ensure you have the whisper-transcript.json file in the same directory</li> </ol>"},{"location":"examples/json_processing/#execute","title":"Execute","text":"<pre><code>python json_processing.py\n</code></pre>"},{"location":"examples/json_processing/#expected-output","title":"Expected Output","text":"<p>The script will display:</p> <ol> <li>Raw JSON data loading and casting</li> <li>Word-level extraction (showing ~4000+ individual words)</li> <li>Segment-level analysis (showing conversation segments)</li> <li>Speaker summary analytics (showing 2 speakers with statistics)</li> <li>Comprehensive pipeline summary</li> </ol>"},{"location":"examples/json_processing/#technical-highlights","title":"Technical Highlights","text":""},{"location":"examples/json_processing/#json-type-system","title":"JSON Type System","text":"<ul> <li>Demonstrates proper JSON type casting from strings</li> <li>Shows how to work with Fenic's JsonType for complex operations</li> </ul>"},{"location":"examples/json_processing/#jq-query-complexity","title":"JQ Query Complexity","text":"<ul> <li>Simple field extraction: <code>.field</code></li> <li>Nested traversal: <code>.segments[].words[]</code></li> <li>Variable binding: <code>.segments[] as $seg</code></li> <li>Array operations: <code>length</code>, <code>add / length</code></li> <li>Object construction with mixed data sources</li> </ul>"},{"location":"examples/json_processing/#hybrid-processing-pattern","title":"Hybrid Processing Pattern","text":"<ul> <li>JSON extraction for granular data access</li> <li>DataFrame aggregations for analytical operations</li> <li>Type conversion between JSON and DataFrame types</li> <li>Calculated fields using both JSON and DataFrame operations</li> </ul>"},{"location":"examples/json_processing/#learning-outcomes","title":"Learning Outcomes","text":"<p>After studying this example, you'll understand:</p> <ol> <li>How to load and cast JSON data in Fenic</li> <li>Struct casting + unnest for efficient field extraction</li> <li>Complex JQ queries for advanced data manipulation</li> <li>When to choose struct casting vs JQ for different use cases</li> <li>Array processing and aggregation within JSON</li> <li>Type conversion between JSON and DataFrame types</li> <li>Hybrid workflows combining multiple extraction approaches</li> <li>Performance optimization for JSON processing pipelines</li> <li>Real-world data processing patterns for audio/transcript analysis</li> </ol> <p>This example serves as a comprehensive reference for JSON processing in Fenic, showcasing both the power of JQ integration and the seamless bridge to traditional DataFrame analytics.</p>"},{"location":"examples/markdown_processing/","title":"Markdown Processing with Fenic","text":"<p>This example demonstrates how to process structured markdown documents using fenic's specialized markdown functions, JSON processing, and text extraction capabilities. We use the \"Attention Is All You Need\" paper, the foundational work that introduced the Transformer architecture, which powers modern large language models (LLMs).</p>"},{"location":"examples/markdown_processing/#what-this-example-shows","title":"What This Example Shows","text":"<p>Learn how to process structured markdown documents using fenic's comprehensive capabilities:</p> <ul> <li><code>markdown.generate_toc()</code> - Generate automatic table of contents from document headings</li> <li><code>markdown.extract_header_chunks()</code> - Extract and structure document sections into DataFrame rows</li> <li><code>markdown.to_json()</code> - Convert markdown to structured JSON for complex querying</li> <li><code>json.jq()</code> - Navigate complex document structures with powerful jq queries</li> <li><code>text.extract()</code> - Parse structured text using templates for field extraction</li> <li>DataFrame operations - Filter, explode, unnest, and transform document data</li> <li>Text processing - Split and parse content using fenic's templating functionality</li> </ul>"},{"location":"examples/markdown_processing/#files","title":"Files","text":"<ul> <li><code>attention_is_all_you_need.md</code> - OCR output of the \"Attention Is All You Need\" paper</li> <li><code>markdown_processing.py</code> - Python script demonstrating markdown processing workflow</li> </ul>"},{"location":"examples/markdown_processing/#running-the-example","title":"Running the Example","text":"<pre><code>cd examples/markdown_processing\nuv run python markdown_processing.py\n</code></pre> <p>Make sure you have your OpenAI API key set in your environment:</p> <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre>"},{"location":"examples/markdown_processing/#what-youll-learn","title":"What You'll Learn","text":"<p>This example showcases how fenic transforms unstructured markdown into structured, queryable data:</p>"},{"location":"examples/markdown_processing/#1-document-structure-analysis","title":"1. Document Structure Analysis","text":"<ul> <li>Load markdown documents into fenic DataFrames</li> <li>Cast content to <code>MarkdownType</code> for specialized processing</li> <li>Generate automatic table of contents from document hierarchy</li> </ul>"},{"location":"examples/markdown_processing/#2-section-extraction-and-structuring","title":"2. Section Extraction and Structuring","text":"<ul> <li>Extract document sections using <code>markdown.extract_header_chunks()</code></li> <li>Convert nested arrays to DataFrame rows with <code>explode()</code> and <code>unnest()</code></li> <li>Work with structured section data (headings, content, hierarchy paths)</li> </ul>"},{"location":"examples/markdown_processing/#3-traditional-text-processing","title":"3. Traditional Text Processing","text":"<ul> <li>Filter DataFrames to find specific sections (e.g., References)</li> <li>Parse structured content using text splitting with regex patterns</li> <li>Handle academic citation formats and numbered references</li> </ul>"},{"location":"examples/markdown_processing/#4-json-based-document-processing","title":"4. JSON-Based Document Processing","text":"<ul> <li>Convert markdown to structured JSON with <code>markdown.to_json()</code></li> <li>Navigate complex nested document structures using <code>json.jq()</code></li> <li>Handle hierarchical content with powerful jq query language</li> <li>Extract data from deeply nested JSON structures</li> </ul>"},{"location":"examples/markdown_processing/#5-template-based-text-extraction","title":"5. Template-Based Text Extraction","text":"<ul> <li>Use <code>text.extract()</code> with templates for structured data parsing</li> <li>Extract multiple fields from text in a single operation</li> <li>Parse academic citations into separate reference numbers and content</li> <li>Handle structured text formats with template patterns</li> </ul>"},{"location":"examples/markdown_processing/#6-advanced-dataframe-operations","title":"6. Advanced DataFrame Operations","text":"<ul> <li>Combine multiple markdown functions in a single select statement</li> <li>Chain operations for complex data transformations</li> <li>Process hierarchical document structures efficiently</li> <li>Cast between different data types (JsonType \u2194 StringType)</li> </ul> <p>Perfect for building academic paper analysis pipelines, research document processing, citation extraction systems, or preparing structured content for downstream analysis.</p>"},{"location":"examples/meeting_transcript_processing/","title":"Meeting Transcript Processing with Fenic","text":"<p>This example demonstrates how to use Fenic to automatically extract actionable insights from engineering meeting transcripts using semantic extraction and structured data processing.</p>"},{"location":"examples/meeting_transcript_processing/#overview","title":"Overview","text":"<p>Engineering teams generate valuable knowledge in meetings, but capturing and organizing this information is often manual and error-prone. This pipeline automates the extraction of:</p> <ul> <li>Action Items: Tasks, assignees, and deadlines</li> <li>Decisions: Key decisions and their rationale</li> <li>Technical Entities: Services, technologies, metrics, and incident references</li> <li>Team Analytics: Workload distribution and productivity metrics</li> </ul>"},{"location":"examples/meeting_transcript_processing/#features","title":"Features","text":"<ul> <li>Native transcript parsing with fenic's built-in functions</li> <li>Semantic extraction of technical entities, action items, and decisions</li> <li>Structured data processing on unstructured meeting content</li> <li>Automated knowledge capture for engineering teams</li> <li>Actionable insights for project management and team coordination</li> </ul>"},{"location":"examples/meeting_transcript_processing/#sample-data","title":"Sample Data","text":"<p>The example processes three types of engineering meetings:</p> <ol> <li>Architecture Review - Technical discussions about system design and bottlenecks</li> <li>Incident Post-Mortem - Analysis of outages and mitigation strategies</li> <li>Sprint Planning - Task allocation and project prioritization</li> </ol>"},{"location":"examples/meeting_transcript_processing/#pipeline-steps","title":"Pipeline Steps","text":""},{"location":"examples/meeting_transcript_processing/#step-1-transcript-parsing","title":"Step 1: Transcript Parsing","text":"<pre><code># Parse transcripts into structured segments\nparsed_transcripts_df = transcripts_df.with_column(\n    \"structured_transcript\",\n    fc.text.parse_transcript(fc.col(\"transcript\"), 'generic')\n)\n</code></pre>"},{"location":"examples/meeting_transcript_processing/#step-2-segment-extraction","title":"Step 2: Segment Extraction","text":"<p>Break down transcripts into individual speaking segments with speaker, start_time, and content.</p>"},{"location":"examples/meeting_transcript_processing/#step-3-semantic-schema-definition","title":"Step 3: Semantic Schema Definition","text":"<p>Define extraction schemas using both Fenic's <code>ExtractSchema</code> and Pydantic models:</p> <pre><code># Technical entities using ExtractSchema\ntechnical_entities_schema = fc.ExtractSchema([\n    fc.ExtractSchemaField(\n        name=\"services\",\n        data_type=fc.ExtractSchemaList(element_type=fc.StringType),\n        description=\"Technical services or systems mentioned\"\n    ),\n    # ... more fields\n])\n\n# Action items using Pydantic\nclass ActionItemSchema(BaseModel):\n    has_action_item: str = Field(description=\"Whether this segment contains an action item (yes/no)\")\n    assignee: str = Field(default=None, description=\"Person assigned to the action item\")\n    task_description: str = Field(description=\"Description of the task or action\")\n    deadline: str = Field(default=None, description=\"When the task should be completed\")\n</code></pre>"},{"location":"examples/meeting_transcript_processing/#step-4-semantic-extraction","title":"Step 4: Semantic Extraction","text":"<p>Apply AI-powered extraction to identify structured information from natural language:</p> <pre><code>enriched_df = segments_df.with_column(\n    \"technical_entities\",\n    fc.semantic.extract(fc.col(\"content\"), technical_entities_schema)\n).with_column(\n    \"action_items\",\n    fc.semantic.extract(fc.col(\"content\"), ActionItemSchema)\n).with_column(\n    \"decisions\",\n    fc.semantic.extract(fc.col(\"content\"), DecisionSchema)\n)\n</code></pre>"},{"location":"examples/meeting_transcript_processing/#step-5-analytics-and-aggregation","title":"Step 5: Analytics and Aggregation","text":"<p>Generate meeting-level insights and team analytics:</p> <ul> <li>Action item workload by team member</li> <li>Technology and service mentions across meetings</li> <li>Decision summary and rationale tracking</li> <li>Meeting productivity metrics</li> </ul>"},{"location":"examples/meeting_transcript_processing/#expected-output","title":"Expected Output","text":"<p>The pipeline produces structured insights including:</p> <p>Action Items Summary:</p> meeting_id meeting_type assignee task_description deadline ARCH-2024-1 Architecture Review Mike investigate Redis impl next Friday INC-2024-12 Incident Post-Mortem Sam review batch processing tomorrow EOD <p>Team Workload Distribution:</p> assignee assigned_tasks Mike 2 Sam 1 Lisa 1 <p>Technology Mentions:</p> technologies mention_count Redis 3 PostgreSQL 2 JWT 2"},{"location":"examples/meeting_transcript_processing/#prerequisites","title":"Prerequisites","text":"<ol> <li>OpenAI API Key: Required for semantic extraction</li> </ol> <p><code>bash    export OPENAI_API_KEY=\"your-api-key-here\"</code></p> <ol> <li>Fenic Installation:    <code>bash    uv sync    uv run maturin develop --uv</code></li> </ol>"},{"location":"examples/meeting_transcript_processing/#running-the-example","title":"Running the Example","text":"<pre><code>uv run python examples/meeting_transcript_processing/transcript_processing.py\n</code></pre>"},{"location":"examples/meeting_transcript_processing/#use-cases","title":"Use Cases","text":"<p>This pipeline is valuable for:</p> <ul> <li>Engineering Managers: Track team workload and action item distribution</li> <li>Technical Program Managers: Monitor project decisions and technical debt</li> <li>DevOps Teams: Analyze incident patterns and response procedures</li> <li>Architecture Teams: Identify technology adoption trends and system bottlenecks</li> </ul>"},{"location":"examples/meeting_transcript_processing/#extensions","title":"Extensions","text":"<p>The example can be extended to:</p> <ul> <li>Integrate with calendar systems for automatic transcript ingestion</li> <li>Export to project management tools (Jira, Linear, etc.)</li> <li>Build dashboards for engineering metrics</li> <li>Create automated follow-up reminders</li> <li>Analyze team communication patterns</li> </ul>"},{"location":"examples/meeting_transcript_processing/#technical-notes","title":"Technical Notes","text":"<ul> <li>Uses <code>gpt-4o-mini</code> for fast and cost-effective semantic extraction</li> <li>Handles mixed transcript formats automatically</li> <li>Implements workarounds for current framework limitations</li> <li>Demonstrates both ExtractSchema and Pydantic model approaches</li> </ul>"},{"location":"examples/named_entity_recognition/","title":"Named Entity Recognition for Security Intelligence","text":"<p>A security-focused NER pipeline using Fenic's semantic extraction capabilities to identify and analyze threats, vulnerabilities, and indicators of compromise from unstructured security reports.</p>"},{"location":"examples/named_entity_recognition/#overview","title":"Overview","text":"<p>This pipeline demonstrates automated security entity extraction and risk assessment:</p> <ul> <li>Zero-shot entity extraction (CVEs, IPs, domains, hashes)</li> <li>Enhanced extraction with threat intelligence context</li> <li>Document chunking for comprehensive analysis</li> <li>Risk prioritization and actionable intelligence</li> </ul>"},{"location":"examples/named_entity_recognition/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Fenic:</li> </ol> <p><code>bash    pip install fenic</code></p> <ol> <li>Configure OpenAI API key:    <code>bash    export OPENAI_API_KEY=\"your-api-key-here\"</code></li> </ol>"},{"location":"examples/named_entity_recognition/#usage","title":"Usage","text":"<pre><code>python ner.py\n</code></pre>"},{"location":"examples/named_entity_recognition/#implementation","title":"Implementation","text":"<p>The pipeline processes security reports through five stages:</p> <ol> <li>Basic NER: Extract standard security entities</li> <li>Enhanced NER: Add threat-specific context</li> <li>Chunking: Handle long documents effectively</li> <li>Analytics: Aggregate and analyze extracted entities</li> <li>Risk Assessment: Generate actionable intelligence</li> </ol>"},{"location":"examples/named_entity_recognition/#troubleshooting","title":"Troubleshooting","text":"<p>Issue: Incomplete entity extraction Solution: Increase chunk size or adjust overlap percentage for better context</p> <p>Issue: Missing threat actors or APT groups Solution: Add more specific descriptions in the Pydantic field definitions</p> <p>Issue: Generic risk assessments Solution: Include more context about your organization in the assessment prompt</p>"},{"location":"examples/news_analysis/","title":"News Article Bias Detection","text":"<p>View in Github</p> <p>A comprehensive demonstration of fenic's semantic classification capabilities for detecting editorial bias and analyzing news articles. This example shows how to use semantic operations to identify bias patterns across multiple news sources and generate AI-powered media profiles.</p>"},{"location":"examples/news_analysis/#overview","title":"Overview","text":"<p>This pipeline performs sophisticated news analysis using fenic's semantic operations in a step-by-step educational format:</p> <ul> <li>Language Analysis: Uses <code>semantic.extract()</code> to identify biased, emotional, or sensationalist language patterns</li> <li>Political Bias Classification: Uses <code>semantic.classify()</code> grounded in extracted data for accurate bias detection</li> <li>Topic Classification: Categorizes articles by subject (politics, technology, business, climate, healthcare)</li> <li>AI-Powered Source Profiling: Uses <code>semantic.reduce()</code> to create comprehensive media profiles for each news source</li> </ul> <p>Available in both Python script (<code>news_analysis.py</code>) and Jupyter notebook (<code>news_analysis.ipynb</code>) formats for different learning preferences.</p>"},{"location":"examples/news_analysis/#key-features","title":"Key Features","text":""},{"location":"examples/news_analysis/#two-stage-analysis-pipeline","title":"Two-Stage Analysis Pipeline","text":"<p>Stage 1 - Information Extraction: Uses <code>semantic.extract()</code> with Pydantic models to identify bias indicators, emotional language, and opinion markers from each article.</p> <p>Stage 2 - Grounded Classification: Uses extracted information as context for <code>semantic.classify()</code> to achieve more accurate political bias detection.</p>"},{"location":"examples/news_analysis/#multi-dimensional-classification","title":"Multi-Dimensional Classification","text":"<p>Simultaneously classifies articles across:</p> <ul> <li>Topics: politics, technology, business, climate, healthcare</li> <li>Political Bias: far_left, left_leaning, neutral, right_leaning, far_right</li> <li>Journalistic Style: sensationalist vs informational</li> </ul>"},{"location":"examples/news_analysis/#source-consistency-analysis","title":"Source Consistency Analysis","text":"<p>Analyzes bias patterns across multiple articles per source to identify editorial consistency and detect sources with mixed editorial perspectives.</p>"},{"location":"examples/news_analysis/#ai-generated-media-profiles","title":"AI-Generated Media Profiles","text":"<p>Uses <code>semantic.reduce()</code> to synthesize extracted information into comprehensive, natural language profiles for each news source.</p>"},{"location":"examples/news_analysis/#dataset","title":"Dataset","text":"<p>The example includes 25 news articles from 8 sources covering diverse topics:</p> <ul> <li>Politics: Federal Reserve policy, climate agreements, Supreme Court cases</li> <li>Technology: AI developments, content moderation, privacy concerns</li> <li>Business: Corporate earnings, market analysis, economic trends</li> <li>Healthcare: Medical breakthroughs, drug pricing, treatment access</li> </ul>"},{"location":"examples/news_analysis/#source-types","title":"Source Types","text":"<ul> <li>Neutral Sources: Global Wire Service, National Press Bureau (3 articles each)</li> <li>Left-leaning Sources: Progressive Voice, Social Justice Today (3 articles each)</li> <li>Right-leaning Sources: Liberty Herald, Free Market Weekly (3 articles each)</li> <li>Mixed-bias Sources: Balanced Tribune (4 articles), Independent Monitor (3 articles)</li> </ul> <p>The mixed-bias sources provide realistic examples of sources with inconsistent editorial patterns, demonstrating how fenic handles nuanced content classification across different bias consistency levels.</p>"},{"location":"examples/news_analysis/#technical-implementation","title":"Technical Implementation","text":""},{"location":"examples/news_analysis/#two-stage-pipeline-implementation","title":"Two-Stage Pipeline Implementation","text":"<p>Stage 1 - Information Extraction:</p> <pre><code># Extract bias indicators and language patterns\nenriched_df = df.select(\n    # Topic classification\n    fc.semantic.classify(\n        fc.col(\"combined_content\"),\n        [\"politics\", \"technology\", \"business\", \"climate\", \"healthcare\"]\n    ).alias(\"primary_topic\"),\n    # Extract structured information using Pydantic\n    fc.semantic.extract(\n        fc.col(\"combined_content\"),\n        ArticleAnalysis  # bias_indicators, emotional_language, opinion_markers\n    ).alias(\"analysis_metadata\")\n).unnest(\"analysis_metadata\")\n</code></pre> <p>Stage 2 - Grounded Classification:</p> <pre><code># Combine extracted information for context-aware classification\ncombined_extracts = fc.text.concat(\n    fc.lit(\"Primary Topic: \"), fc.col(\"primary_topic\"),\n    fc.lit(\"Political Bias Indicators: \"), fc.col(\"bias_indicators\"),\n    fc.lit(\"Emotional Language: \"), fc.col(\"emotional_language\"),\n    fc.lit(\"Opinion Markers: \"), fc.col(\"opinion_markers\")\n)\n\n# Classify bias using extracted context\nresults_df = enriched_df.select(\n    \"*\",\n    fc.semantic.classify(\n        col(\"combined_extracts\"),\n        [\"far_left\", \"left_leaning\", \"neutral\", \"right_leaning\", \"far_right\"]\n    ).alias(\"content_bias\"),\n    fc.semantic.classify(\n        col(\"combined_extracts\"),\n        [\"sensationalist\", \"informational\"]\n    ).alias(\"journalistic_style\")\n)\n</code></pre>"},{"location":"examples/news_analysis/#ai-powered-source-profiling","title":"AI-Powered Source Profiling","text":"<pre><code># Generate comprehensive source profiles using semantic.reduce\nsource_profiles = results_df.group_by(\"source\").agg(\n    fc.semantic.reduce(\"\"\"\n        Create a concise media profile for {source} based on:\n        Detected Political Bias: {content_bias}\n        Bias Indicators: {bias_indicators}\n        Opinion Indicators: {opinion_markers}\n        Emotional Language: {emotional_language}\n        Journalistic Style: {journalistic_style}\n    \"\"\").alias(\"source_profile\")\n)\n</code></pre>"},{"location":"examples/news_analysis/#output-analysis","title":"Output Analysis","text":"<p>The pipeline generates comprehensive analysis including:</p> <ul> <li>Multi-dimensional classifications across topic and political bias spectrum</li> <li>Language pattern analysis extracting bias indicators, emotional language, and opinion markers</li> <li>AI-generated source profiles using semantic.reduce to summarize editorial characteristics</li> </ul>"},{"location":"examples/news_analysis/#use-cases","title":"Use Cases","text":""},{"location":"examples/news_analysis/#media-organizations","title":"Media Organizations","text":"<ul> <li>Content quality assessment for editorial guidelines</li> <li>Bias detection in reporter training and content review</li> <li>Audience analytics understanding reader preferences</li> </ul>"},{"location":"examples/news_analysis/#news-aggregators","title":"News Aggregators","text":"<ul> <li>Content categorization for personalized feeds</li> <li>Bias warnings for balanced information consumption</li> <li>Source diversity ensuring multiple perspectives</li> </ul>"},{"location":"examples/news_analysis/#research-applications","title":"Research Applications","text":"<ul> <li>Media bias studies analyzing coverage patterns</li> <li>Information quality research measuring factual content</li> <li>Comparative analysis across different news sources</li> </ul>"},{"location":"examples/news_analysis/#educational-tools","title":"Educational Tools","text":"<ul> <li>Media literacy training identifying bias indicators</li> <li>Critical thinking exercises comparing article perspectives</li> <li>Journalism education understanding editorial techniques</li> </ul>"},{"location":"examples/news_analysis/#running-the-example","title":"Running the Example","text":""},{"location":"examples/news_analysis/#option-1-python-script","title":"Option 1: Python Script","text":"<ol> <li>Setup: Ensure you have fenic installed with Google Gemini API access</li> <li>Environment: Set your <code>GEMINI_API_KEY</code> environment variable.    a. Alternatively, you can run the example with an OpenAI(<code>OPENAI_API_KEY</code>) model by uncommenting the provided additional model configurations.    b. Using an Anthropic model requires installing fenic with the <code>anthropic</code> extra package, and setting the <code>ANTHROPIC_API_KEY</code> environment variable</li> <li>Execute: Run <code>python news_analysis.py</code></li> </ol>"},{"location":"examples/news_analysis/#option-2-jupyter-notebook","title":"Option 2: Jupyter Notebook","text":"<ol> <li>Setup: Same API requirements as above</li> <li>Launch: Open <code>news_analysis.ipynb</code> in Jupyter</li> <li>Learn: Step-by-step educational walkthrough with explanations</li> </ol>"},{"location":"examples/news_analysis/#alternative-models","title":"Alternative Models","text":"<p>The script includes commented configurations for OpenAI and Anthropic models if you prefer different providers.</p> <p>Explore: Modify the dataset or classification categories to test different scenarios</p>"},{"location":"examples/news_analysis/#advanced-features-demonstrated","title":"Advanced Features Demonstrated","text":""},{"location":"examples/news_analysis/#grounded-classification-pipeline","title":"Grounded Classification Pipeline","text":"<p>Shows how to improve classification accuracy by first extracting relevant information with <code>semantic.extract()</code>, then using that context for more informed <code>semantic.classify()</code> operations.</p>"},{"location":"examples/news_analysis/#pydantic-integration","title":"Pydantic Integration","text":"<p>Demonstrates structured data extraction using type-safe Pydantic models with automatic field validation for consistent output formatting.</p>"},{"location":"examples/news_analysis/#multi-model-support","title":"Multi-Model Support","text":"<p>Includes configurations for Google Gemini (default), OpenAI, and Anthropic models, showing fenic's flexibility across different LLM providers.</p>"},{"location":"examples/news_analysis/#semantic-reduction-for-profiling","title":"Semantic Reduction for Profiling","text":"<p>Uses <code>semantic.reduce()</code> to synthesize multiple data points into coherent natural language profiles, demonstrating AI-powered summarization capabilities.</p>"},{"location":"examples/news_analysis/#educational-format","title":"Educational Format","text":"<p>Available in both script and notebook formats, with the notebook providing step-by-step explanations ideal for learning semantic operations.</p>"},{"location":"examples/news_analysis/#expected-results","title":"Expected Results","text":""},{"location":"examples/news_analysis/#generated-source-profile","title":"Generated Source Profile","text":"<p>The Balanced Tribune presents a diverse range of topics, primarily focusing on business, technology, climate, and healthcare. It exhibits a right-leaning bias in its business and technology coverage, emphasizing themes like Wall Street stability and American free enterprise, while adopting a far-left perspective on climate issues, critiquing fossil fuel companies. The publication often employs sensationalist and informational journalistic styles, utilizing emotional language to evoke strong reactions, such as the impact of inflation and the urgency of climate action. Opinion markers frequently reflect a mix of support for innovation and criticism of regulatory frameworks, indicating a complex stance on various issues.</p>"},{"location":"examples/podcast_summarization/","title":"Podcast Summarization with Fenic","text":"<p>This example demonstrates comprehensive podcast transcript summarization using Fenic's semantic operations and unstructured data processing capabilities.</p>"},{"location":"examples/podcast_summarization/#overview","title":"Overview","text":"<p>This pipeline processes a Lex Fridman podcast episode featuring the Cursor team, showcasing:</p> <ul> <li>Extractive &amp; Abstractive Summarization: Multiple summarization techniques</li> <li>Recursive Summarization: Chunked processing for long-form content</li> <li>Role-Specific Analysis: Tailored summaries for host vs guests</li> <li>Unstructured Data Processing: JSON transcript parsing and analysis</li> </ul>"},{"location":"examples/podcast_summarization/#data","title":"Data","text":"<ul> <li>Episode: \"#447 Cursor Team: Future of Programming with AI\"</li> <li>Duration: 2:37:38</li> <li>Participants: Lex Fridman (host), Michael Truell, Arvid Lunnemark, Aman Sanger, Sualeh Asif</li> <li>Format: JSON transcript with word-level timing and speaker diarization</li> </ul>"},{"location":"examples/podcast_summarization/#pipeline-steps","title":"Pipeline Steps","text":""},{"location":"examples/podcast_summarization/#1-data-loading-processing","title":"1. Data Loading &amp; Processing","text":"<ul> <li>Load JSON files as raw text strings (showcasing unstructured data handling)</li> <li>Parse metadata using JSON operations and type casting</li> <li>Extract word-level and segment-level data from transcript</li> </ul>"},{"location":"examples/podcast_summarization/#2-speaker-identification","title":"2. Speaker Identification","text":"<ul> <li>Filter out noise speakers (ads, intro music) using duration thresholds</li> <li>Map anonymous speaker IDs to actual participant names</li> <li>Create speaker statistics and validation</li> </ul>"},{"location":"examples/podcast_summarization/#3-multi-level-summarization","title":"3. Multi-Level Summarization","text":""},{"location":"examples/podcast_summarization/#full-transcript-summary","title":"Full Transcript Summary","text":"<ul> <li>Chunked recursive summarization for long-form content</li> <li>Uses <code>fc.text.recursive_word_chunk()</code> for optimal processing</li> <li>Combines chunk summaries into cohesive final summary</li> </ul>"},{"location":"examples/podcast_summarization/#host-specific-analysis-lex-fridman","title":"Host-Specific Analysis (Lex Fridman)","text":"<ul> <li>Focuses on thought-provoking questions and insights</li> <li>Captures interviewing technique and philosophical depth</li> <li>Ignores basic facilitation to highlight intellectual contributions</li> </ul>"},{"location":"examples/podcast_summarization/#individual-guest-summaries","title":"Individual Guest Summaries","text":"<ul> <li>Technical expertise and product insights</li> <li>Unique contributions to Cursor development</li> <li>Personal experiences and innovation perspectives</li> </ul>"},{"location":"examples/podcast_summarization/#key-fenic-features-demonstrated","title":"Key Fenic Features Demonstrated","text":""},{"location":"examples/podcast_summarization/#unstructured-data-processing","title":"Unstructured Data Processing","text":"<pre><code># JSON type casting and extraction\nfc.col(\"content\").cast(fc.JsonType)\nfc.json.jq(fc.col(\"json_data\"), '.speaker').get_item(0).cast(fc.StringType)\n</code></pre>"},{"location":"examples/podcast_summarization/#text-processing-aggregation","title":"Text Processing &amp; Aggregation","text":"<pre><code># Proper aggregation with array operations\nfc.collect_list(\"segment_text\").alias(\"speech_segments\")\nfc.text.array_join(fc.col(\"speech_segments\"), \" \")\n</code></pre>"},{"location":"examples/podcast_summarization/#semantic-operations","title":"Semantic Operations","text":"<pre><code># Semantic mapping with placeholders\nfc.semantic.map(\n    \"Analyze this guest's contributions... Guest: {guest_name}. Speech: {full_speech}\"\n)\n</code></pre>"},{"location":"examples/podcast_summarization/#advanced-filtering-mapping","title":"Advanced Filtering &amp; Mapping","text":"<pre><code># Complex conditional mapping\nfc.when(fc.col(\"speaker\") == \"SPEAKER_05\", fc.lit(\"Lex Fridman\"))\n.when(fc.col(\"speaker\") == \"SPEAKER_02\", fc.lit(\"Michael Truell\"))\n.otherwise(fc.lit(\"Unknown\"))\n</code></pre>"},{"location":"examples/podcast_summarization/#technical-highlights","title":"Technical Highlights","text":"<ul> <li>Chunked Processing: Handles 2.5+ hour transcript efficiently</li> <li>Speaker Filtering: Removes noise using time-based thresholds</li> <li>JSON Extraction: Processes complex nested transcript data</li> <li>Role-Aware Prompting: Different analysis for host vs guests</li> <li>Type Safety: Proper casting for JSON-extracted fields</li> </ul>"},{"location":"examples/podcast_summarization/#usage","title":"Usage","text":"<pre><code># Ensure you have OpenAI API key configured\nexport OPENAI_API_KEY=\"your-api-key\"\n\n# Run the summarization pipeline\npython podcast_summarization.py\n</code></pre>"},{"location":"examples/podcast_summarization/#output","title":"Output","text":"<p>The pipeline generates:</p> <ol> <li>Full Episode Summary: Comprehensive overview of key themes and insights</li> <li>Host Analysis: Lex Fridman's interviewing mastery and intellectual contributions</li> <li>Guest Summaries: Individual analyses for each Cursor team member</li> <li>Speaker Statistics: Speaking time, segment counts, and participation metrics</li> </ol>"},{"location":"examples/podcast_summarization/#learning-outcomes","title":"Learning Outcomes","text":"<p>This example teaches:</p> <ul> <li>Working with real-world unstructured data (JSON transcripts)</li> <li>Combining multiple summarization approaches</li> <li>Handling long-form content with chunking strategies</li> <li>Creating role-specific semantic operations</li> <li>Building robust data pipelines with filtering and validation</li> </ul> <p>Perfect for understanding how Fenic handles complex text processing workflows in production scenarios.</p>"},{"location":"examples/semantic_search/","title":"Semantic Joins with Fenic","text":"<p>This example demonstrates how to use Fenic's semantic joins to perform LLM-powered data matching based on natural language reasoning rather than exact equality or similarity scores.</p>"},{"location":"examples/semantic_search/#overview","title":"Overview","text":"<p>Semantic joins enable you to join DataFrames using natural language predicates that are evaluated by language models. Unlike traditional joins that require exact matches or embedding-based similarity joins, semantic joins can understand complex relationships and make intelligent connections based on meaning and context.</p> <p>This example showcases two practical use cases:</p> <ul> <li>Content Recommendation: Matching user interests to relevant articles</li> <li>Product Recommendations: Suggesting complementary products based on purchase history</li> </ul>"},{"location":"examples/semantic_search/#key-features-demonstrated","title":"Key Features Demonstrated","text":"<ul> <li>Natural Language Predicates: Using human-readable join conditions</li> <li>LLM-Powered Reasoning: Leveraging GPT models for intelligent matching</li> <li>Cross-Domain Understanding: Connecting concepts across different contexts</li> <li>Zero-Shot Matching: No training data or examples required</li> </ul>"},{"location":"examples/semantic_search/#how-semantic-joins-work","title":"How Semantic Joins Work","text":""},{"location":"examples/semantic_search/#basic-syntax","title":"Basic Syntax","text":"<pre><code>left_df.semantic.join(\n    right_df,\n    join_instruction=\"Natural language predicate with {column:left} and {column:right}\"\n)\n</code></pre>"},{"location":"examples/semantic_search/#join-instruction-format","title":"Join Instruction Format","text":"<ul> <li>Must reference exactly two columns: one from each DataFrame</li> <li>Use <code>:left</code> and <code>:right</code> suffixes to indicate which DataFrame each column comes from</li> <li>Written as a boolean predicate that the LLM evaluates as True/False</li> <li>Should be clear and unambiguous for consistent results</li> </ul>"},{"location":"examples/semantic_search/#example-1-content-recommendation","title":"Example 1: Content Recommendation","text":""},{"location":"examples/semantic_search/#data-setup","title":"Data Setup","text":"<p>User Profiles:</p> <ul> <li>Sarah: \"I love cooking Italian food and trying new pasta recipes\"</li> <li>Mike: \"I enjoy working on cars and fixing engines in my spare time\"</li> <li>Emily: \"Gardening is my passion, especially growing vegetables and flowers\"</li> <li>David: \"I'm interested in learning about car maintenance and automotive repair\"</li> </ul> <p>Articles:</p> <ul> <li>Cooking Pasta Recipes</li> <li>Car Engine Maintenance</li> <li>Gardening for Beginners</li> <li>Advanced Automotive Repair</li> </ul>"},{"location":"examples/semantic_search/#semantic-join-implementation","title":"Semantic Join Implementation","text":"<pre><code>users_df.semantic.join(\n    articles_df,\n    join_instruction=\"A person with interests '{interests:left}' would be interested in reading about '{description:right}'\"\n)\n</code></pre>"},{"location":"examples/semantic_search/#matching-results","title":"Matching Results","text":"<ul> <li>Sarah \u2192 Cooking Pasta Recipes \u2705</li> <li>Mike \u2192 Car Engine Maintenance + Advanced Automotive Repair \u2705</li> <li>Emily \u2192 Gardening for Beginners \u2705</li> <li>David \u2192 Car Engine Maintenance + Advanced Automotive Repair \u2705</li> </ul>"},{"location":"examples/semantic_search/#example-2-product-recommendations","title":"Example 2: Product Recommendations","text":""},{"location":"examples/semantic_search/#sample-data","title":"Sample Data","text":"<p>Customer Purchases:</p> <ul> <li>Alice: Professional DSLR Camera</li> <li>Bob: Gaming Laptop</li> <li>Carol: Yoga Mat</li> <li>Dan: Coffee Maker</li> </ul> <p>Product Catalog:</p> <ul> <li>Camera Lens Kit, Tripod Stand (Photography)</li> <li>Gaming Mouse, Mechanical Keyboard (Gaming)</li> <li>Yoga Blocks, Exercise Resistance Bands (Fitness)</li> <li>Coffee Beans, French Press (Food &amp; Beverage)</li> </ul>"},{"location":"examples/semantic_search/#recommendation-logic","title":"Recommendation Logic","text":"<pre><code>purchases_df.semantic.join(\n    products_df,\n    join_instruction=\"A customer who bought '{purchased_product:left}' would also be interested in '{product_name:right}'\"\n)\n</code></pre>"},{"location":"examples/semantic_search/#recommendation-results","title":"Recommendation Results","text":"<ul> <li>Alice (DSLR Camera) \u2192 Camera Lens Kit + Tripod Stand \u2705</li> <li>Bob (Gaming Laptop) \u2192 Gaming Mouse + Mechanical Keyboard \u2705</li> <li>Carol (Yoga Mat) \u2192 Yoga Blocks + Exercise Resistance Bands \u2705</li> <li>Dan (Coffee Maker) \u2192 Coffee Beans + French Press \u2705</li> </ul>"},{"location":"examples/semantic_search/#technical-details","title":"Technical Details","text":""},{"location":"examples/semantic_search/#session-configuration","title":"Session Configuration","text":"<pre><code>config = fc.SessionConfig(\n    app_name=\"semantic_joins\",\n    semantic=fc.SemanticConfig(\n        language_models={\n            \"mini\": fc.OpenAIModelConfig(\n                model_name=\"gpt-4o-mini\",\n                rpm=500,\n                tpm=200_000,\n            )\n        }\n    ),\n)\n</code></pre>"},{"location":"examples/semantic_search/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>Complexity: O(m \u00d7 n) where m and n are the sizes of the DataFrames</li> <li>LLM Calls: One API call per potential row pair</li> <li>Rate Limiting: Respects RPM/TPM limits configured in session</li> <li>Batching: Efficiently batches requests to optimize API usage</li> </ul>"},{"location":"examples/semantic_search/#when-to-use-semantic-joins","title":"When to Use Semantic Joins","text":""},{"location":"examples/semantic_search/#ideal-use-cases","title":"Ideal Use Cases:","text":"<ul> <li>Content personalization and recommendation systems</li> <li>Product cross-selling and upselling</li> <li>Skill-job matching in recruitment</li> <li>Entity resolution across different data sources</li> <li>Question-answer pairing for knowledge bases</li> <li>Customer-service matching based on needs</li> </ul>"},{"location":"examples/semantic_search/#advantages","title":"Advantages:","text":"<ul> <li>No training data required (zero-shot)</li> <li>Handles complex reasoning and context</li> <li>Understands domain-specific relationships</li> <li>Works with natural language descriptions</li> <li>Flexible and interpretable join conditions</li> </ul>"},{"location":"examples/semantic_search/#considerations","title":"Considerations:","text":"<ul> <li>Higher latency than traditional joins</li> <li>API costs for LLM usage</li> <li>Rate limiting for large datasets</li> <li>Best for moderate-sized datasets (hundreds to low thousands of rows)</li> </ul>"},{"location":"examples/semantic_search/#usage","title":"Usage","text":"<pre><code># Ensure you have OpenAI API key configured\nexport OPENAI_API_KEY=\"your-api-key\"\n\n# Run the semantic joins example\npython semantic_joins.py\n</code></pre>"},{"location":"examples/semantic_search/#expected-output","title":"Expected Output","text":"<p>The script demonstrates both use cases with clear before/after data views:</p> <ol> <li>User-Article Matching: Shows how semantic understanding connects user interests to relevant content</li> <li>Product Recommendations: Demonstrates intelligent product relationship detection for cross-selling</li> </ol>"},{"location":"examples/semantic_search/#learning-outcomes","title":"Learning Outcomes","text":"<p>This example teaches:</p> <ul> <li>How to construct effective natural language join predicates</li> <li>When semantic joins are preferable to traditional or similarity-based joins</li> <li>Practical applications in recommendation systems and personalization</li> <li>Understanding the trade-offs between accuracy, performance, and cost</li> </ul> <p>Perfect for understanding how to leverage LLM reasoning capabilities for intelligent data joining scenarios that go beyond simple keyword matching or embedding similarity.</p>"},{"location":"reference/fenic/","title":"fenic","text":""},{"location":"reference/fenic/#fenic","title":"fenic","text":"<p>Fenic is an opinionated, PySpark-inspired DataFrame framework for building production AI and agentic applications.</p> <p>Classes:</p> <ul> <li> <code>AnthropicModelConfig</code>           \u2013            <p>Configuration for Anthropic models.</p> </li> <li> <code>ArrayType</code>           \u2013            <p>A type representing a homogeneous variable-length array (list) of elements.</p> </li> <li> <code>Catalog</code>           \u2013            <p>Entry point for catalog operations.</p> </li> <li> <code>ClassifyExample</code>           \u2013            <p>A single semantic example for classification operations.</p> </li> <li> <code>ClassifyExampleCollection</code>           \u2013            <p>Collection of examples for semantic classification operations.</p> </li> <li> <code>Column</code>           \u2013            <p>A column expression in a DataFrame.</p> </li> <li> <code>ColumnField</code>           \u2013            <p>Represents a typed column in a DataFrame schema.</p> </li> <li> <code>DataFrame</code>           \u2013            <p>A data collection organized into named columns.</p> </li> <li> <code>DataFrameReader</code>           \u2013            <p>Interface used to load a DataFrame from external storage systems.</p> </li> <li> <code>DataFrameWriter</code>           \u2013            <p>Interface used to write a DataFrame to external storage systems.</p> </li> <li> <code>DataType</code>           \u2013            <p>Base class for all data types.</p> </li> <li> <code>DocumentPathType</code>           \u2013            <p>Represents a string containing a a document's local (file system) or remote (URL) path.</p> </li> <li> <code>EmbeddingType</code>           \u2013            <p>A type representing a fixed-length embedding vector.</p> </li> <li> <code>ExtractSchema</code>           \u2013            <p>Represents a structured extraction schema.</p> </li> <li> <code>ExtractSchemaField</code>           \u2013            <p>Represents a field within an structured extraction schema.</p> </li> <li> <code>ExtractSchemaList</code>           \u2013            <p>Represents a list data type for structured extraction schema definitions.</p> </li> <li> <code>GoogleGLAModelConfig</code>           \u2013            <p>Configuration for Google GenerativeLAnguage (GLA) models.</p> </li> <li> <code>GroupedData</code>           \u2013            <p>Methods for aggregations on a grouped DataFrame.</p> </li> <li> <code>JoinExample</code>           \u2013            <p>A single semantic example for semantic join operations.</p> </li> <li> <code>JoinExampleCollection</code>           \u2013            <p>Collection of examples for semantic join operations.</p> </li> <li> <code>LMMetrics</code>           \u2013            <p>Tracks language model usage metrics including token counts and costs.</p> </li> <li> <code>Lineage</code>           \u2013            <p>Query interface for tracing data lineage through a query plan.</p> </li> <li> <code>MapExample</code>           \u2013            <p>A single semantic example for semantic mapping operations.</p> </li> <li> <code>MapExampleCollection</code>           \u2013            <p>Collection of examples for semantic mapping operations.</p> </li> <li> <code>OpenAIModelConfig</code>           \u2013            <p>Configuration for OpenAI models.</p> </li> <li> <code>OperatorMetrics</code>           \u2013            <p>Metrics for a single operator in the query execution plan.</p> </li> <li> <code>PredicateExample</code>           \u2013            <p>A single semantic example for semantic predicate operations.</p> </li> <li> <code>PredicateExampleCollection</code>           \u2013            <p>Collection of examples for semantic predicate operations.</p> </li> <li> <code>QueryMetrics</code>           \u2013            <p>Comprehensive metrics for an executed query.</p> </li> <li> <code>QueryResult</code>           \u2013            <p>Container for query execution results and associated metadata.</p> </li> <li> <code>RMMetrics</code>           \u2013            <p>Tracks embedding model usage metrics including token counts and costs.</p> </li> <li> <code>Schema</code>           \u2013            <p>Represents the schema of a DataFrame.</p> </li> <li> <code>SemanticConfig</code>           \u2013            <p>Configuration for semantic language and embedding models.</p> </li> <li> <code>SemanticExtensions</code>           \u2013            <p>A namespace for semantic dataframe operators.</p> </li> <li> <code>Session</code>           \u2013            <p>The entry point to programming with the DataFrame API. Similar to PySpark's SparkSession.</p> </li> <li> <code>SessionConfig</code>           \u2013            <p>Configuration for a user session.</p> </li> <li> <code>StructField</code>           \u2013            <p>A field in a StructType. Fields are nullable.</p> </li> <li> <code>StructType</code>           \u2013            <p>A type representing a struct (record) with named fields.</p> </li> <li> <code>TranscriptType</code>           \u2013            <p>Represents a string containing a transcript in a specific format.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>array</code>             \u2013              <p>Creates a new array column from multiple input columns.</p> </li> <li> <code>array_agg</code>             \u2013              <p>Alias for collect_list().</p> </li> <li> <code>array_contains</code>             \u2013              <p>Checks if array column contains a specific value.</p> </li> <li> <code>array_size</code>             \u2013              <p>Returns the number of elements in an array column.</p> </li> <li> <code>asc</code>             \u2013              <p>Creates a Column expression representing an ascending sort order.</p> </li> <li> <code>asc_nulls_first</code>             \u2013              <p>Creates a Column expression representing an ascending sort order with nulls first.</p> </li> <li> <code>asc_nulls_last</code>             \u2013              <p>Creates a Column expression representing an ascending sort order with nulls last.</p> </li> <li> <code>avg</code>             \u2013              <p>Aggregate function: returns the average (mean) of all values in the specified column.</p> </li> <li> <code>coalesce</code>             \u2013              <p>Returns the first non-null value from the given columns for each row.</p> </li> <li> <code>col</code>             \u2013              <p>Creates a Column expression referencing a column in the DataFrame.</p> </li> <li> <code>collect_list</code>             \u2013              <p>Aggregate function: collects all values from the specified column into a list.</p> </li> <li> <code>configure_logging</code>             \u2013              <p>Configure logging for the library and root logger in interactive environments.</p> </li> <li> <code>count</code>             \u2013              <p>Aggregate function: returns the count of non-null values in the specified column.</p> </li> <li> <code>desc</code>             \u2013              <p>Creates a Column expression representing a descending sort order.</p> </li> <li> <code>desc_nulls_first</code>             \u2013              <p>Creates a Column expression representing a descending sort order with nulls first.</p> </li> <li> <code>desc_nulls_last</code>             \u2013              <p>Creates a Column expression representing a descending sort order with nulls last.</p> </li> <li> <code>lit</code>             \u2013              <p>Creates a Column expression representing a literal value.</p> </li> <li> <code>max</code>             \u2013              <p>Aggregate function: returns the maximum value in the specified column.</p> </li> <li> <code>mean</code>             \u2013              <p>Aggregate function: returns the mean (average) of all values in the specified column.</p> </li> <li> <code>min</code>             \u2013              <p>Aggregate function: returns the minimum value in the specified column.</p> </li> <li> <code>struct</code>             \u2013              <p>Creates a new struct column from multiple input columns.</p> </li> <li> <code>sum</code>             \u2013              <p>Aggregate function: returns the sum of all values in the specified column.</p> </li> <li> <code>udf</code>             \u2013              <p>A decorator or function for creating user-defined functions (UDFs) that can be applied to DataFrame rows.</p> </li> <li> <code>when</code>             \u2013              <p>Evaluates a condition and returns a value if true.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>BooleanType</code>           \u2013            <p>Represents a boolean value. (True/False)</p> </li> <li> <code>DataLike</code>           \u2013            <p>Union type representing any supported data format for both input and output operations.</p> </li> <li> <code>DataLikeType</code>           \u2013            <p>String literal type for specifying data output formats.</p> </li> <li> <code>DoubleType</code>           \u2013            <p>Represents a 64-bit floating-point number.</p> </li> <li> <code>FloatType</code>           \u2013            <p>Represents a 32-bit floating-point number.</p> </li> <li> <code>HtmlType</code>           \u2013            <p>Represents a string containing raw HTML markup.</p> </li> <li> <code>IntegerType</code>           \u2013            <p>Represents a signed integer value.</p> </li> <li> <code>JsonType</code>           \u2013            <p>Represents a string containing JSON data.</p> </li> <li> <code>MarkdownType</code>           \u2013            <p>Represents a string containing Markdown-formatted text.</p> </li> <li> <code>SemanticSimilarityMetric</code>           \u2013            <p>Type alias representing supported semantic similarity metrics.</p> </li> <li> <code>StringType</code>           \u2013            <p>Represents a UTF-8 encoded string value.</p> </li> </ul>"},{"location":"reference/fenic/#fenic.BooleanType","title":"BooleanType  <code>module-attribute</code>","text":"<pre><code>BooleanType = _BooleanType()\n</code></pre> <p>Represents a boolean value. (True/False)</p>"},{"location":"reference/fenic/#fenic.DataLike","title":"DataLike  <code>module-attribute</code>","text":"<pre><code>DataLike = Union[DataFrame, DataFrame, Dict[str, List[Any]], List[Dict[str, Any]], Table]\n</code></pre> <p>Union type representing any supported data format for both input and output operations.</p> <p>This type encompasses all possible data structures that can be: 1. Used as input when creating DataFrames 2. Returned as output from query results</p> Supported formats <ul> <li>pl.DataFrame: Native Polars DataFrame with efficient columnar storage</li> <li>pd.DataFrame: Pandas DataFrame, optionally with PyArrow extension arrays</li> <li>Dict[str, List[Any]]: Column-oriented dictionary where:<ul> <li>Keys are column names (str)</li> <li>Values are lists containing all values for that column</li> </ul> </li> <li>List[Dict[str, Any]]: Row-oriented list where:<ul> <li>Each element is a dictionary representing one row</li> <li>Dictionary keys are column names, values are cell values</li> </ul> </li> <li>pa.Table: Apache Arrow Table with columnar memory layout</li> </ul> Usage <ul> <li>Input: Used in create_dataframe() to accept data in various formats</li> <li>Output: Used in QueryResult.data to return results in requested format</li> </ul> <p>The specific type returned depends on the DataLikeType format specified when collecting query results.</p>"},{"location":"reference/fenic/#fenic.DataLikeType","title":"DataLikeType  <code>module-attribute</code>","text":"<pre><code>DataLikeType = Literal['polars', 'pandas', 'pydict', 'pylist', 'arrow']\n</code></pre> <p>String literal type for specifying data output formats.</p> Valid values <ul> <li>\"polars\": Native Polars DataFrame format</li> <li>\"pandas\": Pandas DataFrame with PyArrow extension arrays</li> <li>\"pydict\": Python dictionary with column names as keys, lists as values</li> <li>\"pylist\": Python list of dictionaries, each representing one row</li> <li>\"arrow\": Apache Arrow Table format</li> </ul> <p>Used as input parameter for methods that can return data in multiple formats.</p>"},{"location":"reference/fenic/#fenic.DoubleType","title":"DoubleType  <code>module-attribute</code>","text":"<pre><code>DoubleType = _DoubleType()\n</code></pre> <p>Represents a 64-bit floating-point number.</p>"},{"location":"reference/fenic/#fenic.FloatType","title":"FloatType  <code>module-attribute</code>","text":"<pre><code>FloatType = _FloatType()\n</code></pre> <p>Represents a 32-bit floating-point number.</p>"},{"location":"reference/fenic/#fenic.HtmlType","title":"HtmlType  <code>module-attribute</code>","text":"<pre><code>HtmlType = _HtmlType()\n</code></pre> <p>Represents a string containing raw HTML markup.</p>"},{"location":"reference/fenic/#fenic.IntegerType","title":"IntegerType  <code>module-attribute</code>","text":"<pre><code>IntegerType = _IntegerType()\n</code></pre> <p>Represents a signed integer value.</p>"},{"location":"reference/fenic/#fenic.JsonType","title":"JsonType  <code>module-attribute</code>","text":"<pre><code>JsonType = _JsonType()\n</code></pre> <p>Represents a string containing JSON data.</p>"},{"location":"reference/fenic/#fenic.MarkdownType","title":"MarkdownType  <code>module-attribute</code>","text":"<pre><code>MarkdownType = _MarkdownType()\n</code></pre> <p>Represents a string containing Markdown-formatted text.</p>"},{"location":"reference/fenic/#fenic.SemanticSimilarityMetric","title":"SemanticSimilarityMetric  <code>module-attribute</code>","text":"<pre><code>SemanticSimilarityMetric = Literal['cosine', 'l2', 'dot']\n</code></pre> <p>Type alias representing supported semantic similarity metrics.</p> <p>Valid values:</p> <ul> <li>\"cosine\": Cosine similarity, measures the cosine of the angle between two vectors.</li> <li>\"l2\": Euclidean (L2) distance, measures the straight-line distance between two vectors.</li> <li>\"dot\": Dot product similarity, the raw inner product of two vectors.</li> </ul> <p>These metrics are commonly used for comparing embedding vectors in semantic search and other similarity-based applications.</p>"},{"location":"reference/fenic/#fenic.StringType","title":"StringType  <code>module-attribute</code>","text":"<pre><code>StringType = _StringType()\n</code></pre> <p>Represents a UTF-8 encoded string value.</p>"},{"location":"reference/fenic/#fenic.AnthropicModelConfig","title":"AnthropicModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Anthropic models.</p> <p>This class defines the configuration settings for Anthropic language models, including model selection and separate rate limiting parameters for input and output tokens.</p> <p>Attributes:</p> <ul> <li> <code>model_name</code>               (<code>ANTHROPIC_AVAILABLE_LANGUAGE_MODELS</code>)           \u2013            <p>The name of the Anthropic model to use.</p> </li> <li> <code>rpm</code>               (<code>int</code>)           \u2013            <p>Requests per minute limit; must be greater than 0.</p> </li> <li> <code>input_tpm</code>               (<code>int</code>)           \u2013            <p>Input tokens per minute limit; must be greater than 0.</p> </li> <li> <code>output_tpm</code>               (<code>int</code>)           \u2013            <p>Output tokens per minute limit; must be greater than 0.</p> </li> </ul> <p>Examples:</p> <p>Configuring an Anthropic model with separate input/output rate limits:</p> <pre><code>config = AnthropicModelConfig(\n    model_name=\"claude-3-5-haiku-latest\",\n    rpm=100,\n    input_tpm=100,\n    output_tpm=100\n)\n</code></pre>"},{"location":"reference/fenic/#fenic.ArrayType","title":"ArrayType","text":"<p>               Bases: <code>DataType</code></p> <p>A type representing a homogeneous variable-length array (list) of elements.</p> <p>Attributes:</p> <ul> <li> <code>element_type</code>               (<code>DataType</code>)           \u2013            <p>The data type of each element in the array.</p> </li> </ul> Create an array of strings <pre><code>ArrayType(StringType)\nArrayType(element_type=StringType)\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog","title":"Catalog","text":"<pre><code>Catalog(catalog: BaseCatalog)\n</code></pre> <p>Entry point for catalog operations.</p> <p>The Catalog provides methods to interact with and manage database tables, including listing available tables, describing table schemas, and dropping tables.</p> Basic usage <pre><code># Create a new catalog\nsession.catalog.create_catalog('my_catalog')\n# Returns: True\n\n# Set the current catalog\nsession.catalog.set_current_catalog('my_catalog')\n# Returns: None\n\n# Create a new database\nsession.catalog.create_database('my_database')\n# Returns: True\n\n# Use the new database\nsession.catalog.set_current_database('my_database')\n# Returns: None\n\n# Create a new table\nsession.catalog.create_table('my_table', Schema([\n    ColumnField('id', IntegerType),\n]))\n# Returns: True\n</code></pre> <p>Initialize a Catalog instance.</p> <p>Parameters:</p> <ul> <li> <code>catalog</code>               (<code>BaseCatalog</code>)           \u2013            <p>The underlying catalog implementation.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>create_catalog</code>             \u2013              <p>Creates a new catalog.</p> </li> <li> <code>create_database</code>             \u2013              <p>Creates a new database.</p> </li> <li> <code>create_table</code>             \u2013              <p>Creates a new table.</p> </li> <li> <code>describe_table</code>             \u2013              <p>Returns the schema of the specified table.</p> </li> <li> <code>does_catalog_exist</code>             \u2013              <p>Checks if a catalog with the specified name exists.</p> </li> <li> <code>does_database_exist</code>             \u2013              <p>Checks if a database with the specified name exists.</p> </li> <li> <code>does_table_exist</code>             \u2013              <p>Checks if a table with the specified name exists.</p> </li> <li> <code>drop_catalog</code>             \u2013              <p>Drops a catalog.</p> </li> <li> <code>drop_database</code>             \u2013              <p>Drops a database.</p> </li> <li> <code>drop_table</code>             \u2013              <p>Drops the specified table.</p> </li> <li> <code>get_current_catalog</code>             \u2013              <p>Returns the name of the current catalog.</p> </li> <li> <code>get_current_database</code>             \u2013              <p>Returns the name of the current database in the current catalog.</p> </li> <li> <code>list_catalogs</code>             \u2013              <p>Returns a list of available catalogs.</p> </li> <li> <code>list_databases</code>             \u2013              <p>Returns a list of databases in the current catalog.</p> </li> <li> <code>list_tables</code>             \u2013              <p>Returns a list of tables stored in the current database.</p> </li> <li> <code>set_current_catalog</code>             \u2013              <p>Sets the current catalog.</p> </li> <li> <code>set_current_database</code>             \u2013              <p>Sets the current database.</p> </li> </ul> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def __init__(self, catalog: BaseCatalog):\n    \"\"\"Initialize a Catalog instance.\n\n    Args:\n        catalog: The underlying catalog implementation.\n    \"\"\"\n    self.catalog = catalog\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog.create_catalog","title":"create_catalog","text":"<pre><code>create_catalog(catalog_name: str, ignore_if_exists: bool = True) -&gt; bool\n</code></pre> <p>Creates a new catalog.</p> <p>Parameters:</p> <ul> <li> <code>catalog_name</code>               (<code>str</code>)           \u2013            <p>Name of the catalog to create.</p> </li> <li> <code>ignore_if_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, return False when the catalog already exists. If False, raise an error when the catalog already exists. Defaults to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CatalogAlreadyExistsError</code>             \u2013            <p>If the catalog already exists and ignore_if_exists is False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the catalog was created successfully, False if the catalog</p> </li> <li> <code>bool</code>           \u2013            <p>already exists and ignore_if_exists is True.</p> </li> </ul> Create a new catalog <pre><code># Create a new catalog named 'my_catalog'\nsession.catalog.create_catalog('my_catalog')\n# Returns: True\n</code></pre> Create an existing catalog with ignore_if_exists <pre><code># Try to create an existing catalog with ignore_if_exists=True\nsession.catalog.create_catalog('my_catalog', ignore_if_exists=True)\n# Returns: False\n</code></pre> Create an existing catalog without ignore_if_exists <pre><code># Try to create an existing catalog with ignore_if_exists=False\nsession.catalog.create_catalog('my_catalog', ignore_if_exists=False)\n# Raises: CatalogAlreadyExistsError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef create_catalog(self, catalog_name: str, ignore_if_exists: bool = True) -&gt; bool:\n    \"\"\"Creates a new catalog.\n\n    Args:\n        catalog_name (str): Name of the catalog to create.\n        ignore_if_exists (bool): If True, return False when the catalog already exists.\n            If False, raise an error when the catalog already exists.\n            Defaults to True.\n\n    Raises:\n        CatalogAlreadyExistsError: If the catalog already exists and ignore_if_exists is False.\n\n    Returns:\n        bool: True if the catalog was created successfully, False if the catalog\n        already exists and ignore_if_exists is True.\n\n    Example: Create a new catalog\n        ```python\n        # Create a new catalog named 'my_catalog'\n        session.catalog.create_catalog('my_catalog')\n        # Returns: True\n        ```\n\n    Example: Create an existing catalog with ignore_if_exists\n        ```python\n        # Try to create an existing catalog with ignore_if_exists=True\n        session.catalog.create_catalog('my_catalog', ignore_if_exists=True)\n        # Returns: False\n        ```\n\n    Example: Create an existing catalog without ignore_if_exists\n        ```python\n        # Try to create an existing catalog with ignore_if_exists=False\n        session.catalog.create_catalog('my_catalog', ignore_if_exists=False)\n        # Raises: CatalogAlreadyExistsError\n        ```\n    \"\"\"\n    return self.catalog.create_catalog(catalog_name, ignore_if_exists)\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog.create_database","title":"create_database","text":"<pre><code>create_database(database_name: str, ignore_if_exists: bool = True) -&gt; bool\n</code></pre> <p>Creates a new database.</p> <p>Parameters:</p> <ul> <li> <code>database_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative database name to create.</p> </li> <li> <code>ignore_if_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, return False when the database already exists. If False, raise an error when the database already exists. Defaults to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatabaseAlreadyExistsError</code>             \u2013            <p>If the database already exists and ignore_if_exists is False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the database was created successfully, False if the database</p> </li> <li> <code>bool</code>           \u2013            <p>already exists and ignore_if_exists is True.</p> </li> </ul> Create a new database <pre><code># Create a new database named 'my_database'\nsession.catalog.create_database('my_database')\n# Returns: True\n</code></pre> Create an existing database with ignore_if_exists <pre><code># Try to create an existing database with ignore_if_exists=True\nsession.catalog.create_database('my_database', ignore_if_exists=True)\n# Returns: False\n</code></pre> Create an existing database without ignore_if_exists <pre><code># Try to create an existing database with ignore_if_exists=False\nsession.catalog.create_database('my_database', ignore_if_exists=False)\n# Raises: DatabaseAlreadyExistsError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef create_database(\n    self, database_name: str, ignore_if_exists: bool = True\n) -&gt; bool:\n    \"\"\"Creates a new database.\n\n    Args:\n        database_name (str): Fully qualified or relative database name to create.\n        ignore_if_exists (bool): If True, return False when the database already exists.\n            If False, raise an error when the database already exists.\n            Defaults to True.\n\n    Raises:\n        DatabaseAlreadyExistsError: If the database already exists and ignore_if_exists is False.\n\n    Returns:\n        bool: True if the database was created successfully, False if the database\n        already exists and ignore_if_exists is True.\n\n    Example: Create a new database\n        ```python\n        # Create a new database named 'my_database'\n        session.catalog.create_database('my_database')\n        # Returns: True\n        ```\n\n    Example: Create an existing database with ignore_if_exists\n        ```python\n        # Try to create an existing database with ignore_if_exists=True\n        session.catalog.create_database('my_database', ignore_if_exists=True)\n        # Returns: False\n        ```\n\n    Example: Create an existing database without ignore_if_exists\n        ```python\n        # Try to create an existing database with ignore_if_exists=False\n        session.catalog.create_database('my_database', ignore_if_exists=False)\n        # Raises: DatabaseAlreadyExistsError\n        ```\n    \"\"\"\n    return self.catalog.create_database(database_name, ignore_if_exists)\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog.create_table","title":"create_table","text":"<pre><code>create_table(table_name: str, schema: Schema, ignore_if_exists: bool = True) -&gt; bool\n</code></pre> <p>Creates a new table.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative table name to create.</p> </li> <li> <code>schema</code>               (<code>Schema</code>)           \u2013            <p>Schema of the table to create.</p> </li> <li> <code>ignore_if_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, return False when the table already exists. If False, raise an error when the table already exists. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the table was created successfully, False if the table</p> </li> <li> <code>bool</code>           \u2013            <p>already exists and ignore_if_exists is True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TableAlreadyExistsError</code>             \u2013            <p>If the table already exists and ignore_if_exists is False</p> </li> </ul> Create a new table <pre><code># Create a new table with an integer column\nsession.catalog.create_table('my_table', Schema([\n    ColumnField('id', IntegerType),\n]))\n# Returns: True\n</code></pre> Create an existing table with ignore_if_exists <pre><code># Try to create an existing table with ignore_if_exists=True\nsession.catalog.create_table('my_table', Schema([\n    ColumnField('id', IntegerType),\n]), ignore_if_exists=True)\n# Returns: False\n</code></pre> Create an existing table without ignore_if_exists <pre><code># Try to create an existing table with ignore_if_exists=False\nsession.catalog.create_table('my_table', Schema([\n    ColumnField('id', IntegerType),\n]), ignore_if_exists=False)\n# Raises: TableAlreadyExistsError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef create_table(\n    self, table_name: str, schema: Schema, ignore_if_exists: bool = True\n) -&gt; bool:\n    \"\"\"Creates a new table.\n\n    Args:\n        table_name (str): Fully qualified or relative table name to create.\n        schema (Schema): Schema of the table to create.\n        ignore_if_exists (bool): If True, return False when the table already exists.\n            If False, raise an error when the table already exists.\n            Defaults to True.\n\n    Returns:\n        bool: True if the table was created successfully, False if the table\n        already exists and ignore_if_exists is True.\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists and ignore_if_exists is False\n\n    Example: Create a new table\n        ```python\n        # Create a new table with an integer column\n        session.catalog.create_table('my_table', Schema([\n            ColumnField('id', IntegerType),\n        ]))\n        # Returns: True\n        ```\n\n    Example: Create an existing table with ignore_if_exists\n        ```python\n        # Try to create an existing table with ignore_if_exists=True\n        session.catalog.create_table('my_table', Schema([\n            ColumnField('id', IntegerType),\n        ]), ignore_if_exists=True)\n        # Returns: False\n        ```\n\n    Example: Create an existing table without ignore_if_exists\n        ```python\n        # Try to create an existing table with ignore_if_exists=False\n        session.catalog.create_table('my_table', Schema([\n            ColumnField('id', IntegerType),\n        ]), ignore_if_exists=False)\n        # Raises: TableAlreadyExistsError\n        ```\n    \"\"\"\n    return self.catalog.create_table(table_name, schema, ignore_if_exists)\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog.describe_table","title":"describe_table","text":"<pre><code>describe_table(table_name: str) -&gt; Schema\n</code></pre> <p>Returns the schema of the specified table.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative table name to describe.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Schema</code> (              <code>Schema</code> )          \u2013            <p>A schema object describing the table's structure with field names and types.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TableNotFoundError</code>             \u2013            <p>If the table doesn't exist.</p> </li> </ul> Describe a table's schema <pre><code># For a table created with: CREATE TABLE t1 (id int)\nsession.catalog.describe_table('t1')\n# Returns: Schema([\n#     ColumnField('id', IntegerType),\n# ])\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef describe_table(self, table_name: str) -&gt; Schema:\n    \"\"\"Returns the schema of the specified table.\n\n    Args:\n        table_name (str): Fully qualified or relative table name to describe.\n\n    Returns:\n        Schema: A schema object describing the table's structure with field names and types.\n\n    Raises:\n        TableNotFoundError: If the table doesn't exist.\n\n    Example: Describe a table's schema\n        ```python\n        # For a table created with: CREATE TABLE t1 (id int)\n        session.catalog.describe_table('t1')\n        # Returns: Schema([\n        #     ColumnField('id', IntegerType),\n        # ])\n        ```\n    \"\"\"\n    return self.catalog.describe_table(table_name)\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog.does_catalog_exist","title":"does_catalog_exist","text":"<pre><code>does_catalog_exist(catalog_name: str) -&gt; bool\n</code></pre> <p>Checks if a catalog with the specified name exists.</p> <p>Parameters:</p> <ul> <li> <code>catalog_name</code>               (<code>str</code>)           \u2013            <p>Name of the catalog to check.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the catalog exists, False otherwise.</p> </li> </ul> Check if a catalog exists <pre><code># Check if 'my_catalog' exists\nsession.catalog.does_catalog_exist('my_catalog')\n# Returns: True\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef does_catalog_exist(self, catalog_name: str) -&gt; bool:\n    \"\"\"Checks if a catalog with the specified name exists.\n\n    Args:\n        catalog_name (str): Name of the catalog to check.\n\n    Returns:\n        bool: True if the catalog exists, False otherwise.\n\n    Example: Check if a catalog exists\n        ```python\n        # Check if 'my_catalog' exists\n        session.catalog.does_catalog_exist('my_catalog')\n        # Returns: True\n        ```\n    \"\"\"\n    return self.catalog.does_catalog_exist(catalog_name)\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog.does_database_exist","title":"does_database_exist","text":"<pre><code>does_database_exist(database_name: str) -&gt; bool\n</code></pre> <p>Checks if a database with the specified name exists.</p> <p>Parameters:</p> <ul> <li> <code>database_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative database name to check.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the database exists, False otherwise.</p> </li> </ul> Check if a database exists <pre><code># Check if 'my_database' exists\nsession.catalog.does_database_exist('my_database')\n# Returns: True\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef does_database_exist(self, database_name: str) -&gt; bool:\n    \"\"\"Checks if a database with the specified name exists.\n\n    Args:\n        database_name (str): Fully qualified or relative database name to check.\n\n    Returns:\n        bool: True if the database exists, False otherwise.\n\n    Example: Check if a database exists\n        ```python\n        # Check if 'my_database' exists\n        session.catalog.does_database_exist('my_database')\n        # Returns: True\n        ```\n    \"\"\"\n    return self.catalog.does_database_exist(database_name)\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog.does_table_exist","title":"does_table_exist","text":"<pre><code>does_table_exist(table_name: str) -&gt; bool\n</code></pre> <p>Checks if a table with the specified name exists.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative table name to check.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the table exists, False otherwise.</p> </li> </ul> Check if a table exists <pre><code># Check if 'my_table' exists\nsession.catalog.does_table_exist('my_table')\n# Returns: True\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef does_table_exist(self, table_name: str) -&gt; bool:\n    \"\"\"Checks if a table with the specified name exists.\n\n    Args:\n        table_name (str): Fully qualified or relative table name to check.\n\n    Returns:\n        bool: True if the table exists, False otherwise.\n\n    Example: Check if a table exists\n        ```python\n        # Check if 'my_table' exists\n        session.catalog.does_table_exist('my_table')\n        # Returns: True\n        ```\n    \"\"\"\n    return self.catalog.does_table_exist(table_name)\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog.drop_catalog","title":"drop_catalog","text":"<pre><code>drop_catalog(catalog_name: str, ignore_if_not_exists: bool = True) -&gt; bool\n</code></pre> <p>Drops a catalog.</p> <p>Parameters:</p> <ul> <li> <code>catalog_name</code>               (<code>str</code>)           \u2013            <p>Name of the catalog to drop.</p> </li> <li> <code>ignore_if_not_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, silently return if the catalog doesn't exist. If False, raise an error if the catalog doesn't exist. Defaults to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CatalogNotFoundError</code>             \u2013            <p>If the catalog does not exist and ignore_if_not_exists is False</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the catalog was dropped successfully, False if the catalog</p> </li> <li> <code>bool</code>           \u2013            <p>didn't exist and ignore_if_not_exists is True.</p> </li> </ul> Drop a non-existent catalog <pre><code># Try to drop a non-existent catalog\nsession.catalog.drop_catalog('my_catalog')\n# Returns: False\n</code></pre> Drop a non-existent catalog without ignore_if_not_exists <pre><code># Try to drop a non-existent catalog with ignore_if_not_exists=False\nsession.catalog.drop_catalog('my_catalog', ignore_if_not_exists=False)\n# Raises: CatalogNotFoundError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef drop_catalog(\n    self, catalog_name: str, ignore_if_not_exists: bool = True\n) -&gt; bool:\n    \"\"\"Drops a catalog.\n\n    Args:\n        catalog_name (str): Name of the catalog to drop.\n        ignore_if_not_exists (bool): If True, silently return if the catalog doesn't exist.\n            If False, raise an error if the catalog doesn't exist.\n            Defaults to True.\n\n    Raises:\n        CatalogNotFoundError: If the catalog does not exist and ignore_if_not_exists is False\n\n    Returns:\n        bool: True if the catalog was dropped successfully, False if the catalog\n        didn't exist and ignore_if_not_exists is True.\n\n    Example: Drop a non-existent catalog\n        ```python\n        # Try to drop a non-existent catalog\n        session.catalog.drop_catalog('my_catalog')\n        # Returns: False\n        ```\n\n    Example: Drop a non-existent catalog without ignore_if_not_exists\n        ```python\n        # Try to drop a non-existent catalog with ignore_if_not_exists=False\n        session.catalog.drop_catalog('my_catalog', ignore_if_not_exists=False)\n        # Raises: CatalogNotFoundError\n        ```\n    \"\"\"\n    return self.catalog.drop_catalog(catalog_name, ignore_if_not_exists)\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog.drop_database","title":"drop_database","text":"<pre><code>drop_database(database_name: str, cascade: bool = False, ignore_if_not_exists: bool = True) -&gt; bool\n</code></pre> <p>Drops a database.</p> <p>Parameters:</p> <ul> <li> <code>database_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative database name to drop.</p> </li> <li> <code>cascade</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, drop all tables in the database. Defaults to False.</p> </li> <li> <code>ignore_if_not_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, silently return if the database doesn't exist. If False, raise an error if the database doesn't exist. Defaults to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatabaseNotFoundError</code>             \u2013            <p>If the database does not exist and ignore_if_not_exists is False</p> </li> <li> <code>CatalogError</code>             \u2013            <p>If the current database is being dropped, if the database is not empty and cascade is False</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the database was dropped successfully, False if the database</p> </li> <li> <code>bool</code>           \u2013            <p>didn't exist and ignore_if_not_exists is True.</p> </li> </ul> Drop a non-existent database <pre><code># Try to drop a non-existent database\nsession.catalog.drop_database('my_database')\n# Returns: False\n</code></pre> Drop a non-existent database without ignore_if_not_exists <pre><code># Try to drop a non-existent database with ignore_if_not_exists=False\nsession.catalog.drop_database('my_database', ignore_if_not_exists=False)\n# Raises: DatabaseNotFoundError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef drop_database(\n    self,\n    database_name: str,\n    cascade: bool = False,\n    ignore_if_not_exists: bool = True,\n) -&gt; bool:\n    \"\"\"Drops a database.\n\n    Args:\n        database_name (str): Fully qualified or relative database name to drop.\n        cascade (bool): If True, drop all tables in the database.\n            Defaults to False.\n        ignore_if_not_exists (bool): If True, silently return if the database doesn't exist.\n            If False, raise an error if the database doesn't exist.\n            Defaults to True.\n\n    Raises:\n        DatabaseNotFoundError: If the database does not exist and ignore_if_not_exists is False\n        CatalogError: If the current database is being dropped, if the database is not empty and cascade is False\n\n    Returns:\n        bool: True if the database was dropped successfully, False if the database\n        didn't exist and ignore_if_not_exists is True.\n\n    Example: Drop a non-existent database\n        ```python\n        # Try to drop a non-existent database\n        session.catalog.drop_database('my_database')\n        # Returns: False\n        ```\n\n    Example: Drop a non-existent database without ignore_if_not_exists\n        ```python\n        # Try to drop a non-existent database with ignore_if_not_exists=False\n        session.catalog.drop_database('my_database', ignore_if_not_exists=False)\n        # Raises: DatabaseNotFoundError\n        ```\n    \"\"\"\n    return self.catalog.drop_database(database_name, cascade, ignore_if_not_exists)\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog.drop_table","title":"drop_table","text":"<pre><code>drop_table(table_name: str, ignore_if_not_exists: bool = True) -&gt; bool\n</code></pre> <p>Drops the specified table.</p> <p>By default this method will return False if the table doesn't exist.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative table name to drop.</p> </li> <li> <code>ignore_if_not_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, return False when the table doesn't exist. If False, raise an error when the table doesn't exist. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the table was dropped successfully, False if the table</p> </li> <li> <code>bool</code>           \u2013            <p>didn't exist and ignore_if_not_exist is True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TableNotFoundError</code>             \u2013            <p>If the table doesn't exist and ignore_if_not_exists is False</p> </li> </ul> Drop an existing table <pre><code># Drop an existing table 't1'\nsession.catalog.drop_table('t1')\n# Returns: True\n</code></pre> Drop a non-existent table with ignore_if_not_exists <pre><code># Try to drop a non-existent table with ignore_if_not_exists=True\nsession.catalog.drop_table('t2', ignore_if_not_exists=True)\n# Returns: False\n</code></pre> Drop a non-existent table without ignore_if_not_exists <pre><code># Try to drop a non-existent table with ignore_if_not_exists=False\nsession.catalog.drop_table('t2', ignore_if_not_exists=False)\n# Raises: TableNotFoundError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef drop_table(self, table_name: str, ignore_if_not_exists: bool = True) -&gt; bool:\n    \"\"\"Drops the specified table.\n\n    By default this method will return False if the table doesn't exist.\n\n    Args:\n        table_name (str): Fully qualified or relative table name to drop.\n        ignore_if_not_exists (bool): If True, return False when the table doesn't exist.\n            If False, raise an error when the table doesn't exist.\n            Defaults to True.\n\n    Returns:\n        bool: True if the table was dropped successfully, False if the table\n        didn't exist and ignore_if_not_exist is True.\n\n    Raises:\n        TableNotFoundError: If the table doesn't exist and ignore_if_not_exists is False\n\n    Example: Drop an existing table\n        ```python\n        # Drop an existing table 't1'\n        session.catalog.drop_table('t1')\n        # Returns: True\n        ```\n\n    Example: Drop a non-existent table with ignore_if_not_exists\n        ```python\n        # Try to drop a non-existent table with ignore_if_not_exists=True\n        session.catalog.drop_table('t2', ignore_if_not_exists=True)\n        # Returns: False\n        ```\n\n    Example: Drop a non-existent table without ignore_if_not_exists\n        ```python\n        # Try to drop a non-existent table with ignore_if_not_exists=False\n        session.catalog.drop_table('t2', ignore_if_not_exists=False)\n        # Raises: TableNotFoundError\n        ```\n    \"\"\"\n    return self.catalog.drop_table(table_name, ignore_if_not_exists)\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog.get_current_catalog","title":"get_current_catalog","text":"<pre><code>get_current_catalog() -&gt; str\n</code></pre> <p>Returns the name of the current catalog.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The name of the current catalog.</p> </li> </ul> Get current catalog name <pre><code># Get the name of the current catalog\nsession.catalog.get_current_catalog()\n# Returns: 'default'\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def get_current_catalog(self) -&gt; str:\n    \"\"\"Returns the name of the current catalog.\n\n    Returns:\n        str: The name of the current catalog.\n\n    Example: Get current catalog name\n        ```python\n        # Get the name of the current catalog\n        session.catalog.get_current_catalog()\n        # Returns: 'default'\n        ```\n    \"\"\"\n    return self.catalog.get_current_catalog()\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog.get_current_database","title":"get_current_database","text":"<pre><code>get_current_database() -&gt; str\n</code></pre> <p>Returns the name of the current database in the current catalog.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The name of the current database.</p> </li> </ul> Get current database name <pre><code># Get the name of the current database\nsession.catalog.get_current_database()\n# Returns: 'default'\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def get_current_database(self) -&gt; str:\n    \"\"\"Returns the name of the current database in the current catalog.\n\n    Returns:\n        str: The name of the current database.\n\n    Example: Get current database name\n        ```python\n        # Get the name of the current database\n        session.catalog.get_current_database()\n        # Returns: 'default'\n        ```\n    \"\"\"\n    return self.catalog.get_current_database()\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog.list_catalogs","title":"list_catalogs","text":"<pre><code>list_catalogs() -&gt; List[str]\n</code></pre> <p>Returns a list of available catalogs.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of catalog names available in the system.</p> </li> <li> <code>List[str]</code>           \u2013            <p>Returns an empty list if no catalogs are found.</p> </li> </ul> List all catalogs <pre><code># Get all available catalogs\nsession.catalog.list_catalogs()\n# Returns: ['default', 'my_catalog', 'other_catalog']\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def list_catalogs(self) -&gt; List[str]:\n    \"\"\"Returns a list of available catalogs.\n\n    Returns:\n        List[str]: A list of catalog names available in the system.\n        Returns an empty list if no catalogs are found.\n\n    Example: List all catalogs\n        ```python\n        # Get all available catalogs\n        session.catalog.list_catalogs()\n        # Returns: ['default', 'my_catalog', 'other_catalog']\n        ```\n    \"\"\"\n    return self.catalog.list_catalogs()\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog.list_databases","title":"list_databases","text":"<pre><code>list_databases() -&gt; List[str]\n</code></pre> <p>Returns a list of databases in the current catalog.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of database names in the current catalog.</p> </li> <li> <code>List[str]</code>           \u2013            <p>Returns an empty list if no databases are found.</p> </li> </ul> List all databases <pre><code># Get all databases in the current catalog\nsession.catalog.list_databases()\n# Returns: ['default', 'my_database', 'other_database']\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def list_databases(self) -&gt; List[str]:\n    \"\"\"Returns a list of databases in the current catalog.\n\n    Returns:\n        List[str]: A list of database names in the current catalog.\n        Returns an empty list if no databases are found.\n\n    Example: List all databases\n        ```python\n        # Get all databases in the current catalog\n        session.catalog.list_databases()\n        # Returns: ['default', 'my_database', 'other_database']\n        ```\n    \"\"\"\n    return self.catalog.list_databases()\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog.list_tables","title":"list_tables","text":"<pre><code>list_tables() -&gt; List[str]\n</code></pre> <p>Returns a list of tables stored in the current database.</p> <p>This method queries the current database to retrieve all available table names.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of table names stored in the database.</p> </li> <li> <code>List[str]</code>           \u2013            <p>Returns an empty list if no tables are found.</p> </li> </ul> List all tables <pre><code># Get all tables in the current database\nsession.catalog.list_tables()\n# Returns: ['table1', 'table2', 'table3']\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def list_tables(self) -&gt; List[str]:\n    \"\"\"Returns a list of tables stored in the current database.\n\n    This method queries the current database to retrieve all available table names.\n\n    Returns:\n        List[str]: A list of table names stored in the database.\n        Returns an empty list if no tables are found.\n\n    Example: List all tables\n        ```python\n        # Get all tables in the current database\n        session.catalog.list_tables()\n        # Returns: ['table1', 'table2', 'table3']\n        ```\n    \"\"\"\n    return self.catalog.list_tables()\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog.set_current_catalog","title":"set_current_catalog","text":"<pre><code>set_current_catalog(catalog_name: str) -&gt; None\n</code></pre> <p>Sets the current catalog.</p> <p>Parameters:</p> <ul> <li> <code>catalog_name</code>               (<code>str</code>)           \u2013            <p>Name of the catalog to set as current.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the specified catalog doesn't exist.</p> </li> </ul> Set current catalog <pre><code># Set 'my_catalog' as the current catalog\nsession.catalog.set_current_catalog('my_catalog')\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef set_current_catalog(self, catalog_name: str) -&gt; None:\n    \"\"\"Sets the current catalog.\n\n    Args:\n        catalog_name (str): Name of the catalog to set as current.\n\n    Raises:\n        ValueError: If the specified catalog doesn't exist.\n\n    Example: Set current catalog\n        ```python\n        # Set 'my_catalog' as the current catalog\n        session.catalog.set_current_catalog('my_catalog')\n        ```\n    \"\"\"\n    self.catalog.set_current_catalog(catalog_name)\n</code></pre>"},{"location":"reference/fenic/#fenic.Catalog.set_current_database","title":"set_current_database","text":"<pre><code>set_current_database(database_name: str) -&gt; None\n</code></pre> <p>Sets the current database.</p> <p>Parameters:</p> <ul> <li> <code>database_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative database name to set as current.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatabaseNotFoundError</code>             \u2013            <p>If the specified database doesn't exist.</p> </li> </ul> Set current database <pre><code># Set 'my_database' as the current database\nsession.catalog.set_current_database('my_database')\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef set_current_database(self, database_name: str) -&gt; None:\n    \"\"\"Sets the current database.\n\n    Args:\n        database_name (str): Fully qualified or relative database name to set as current.\n\n    Raises:\n        DatabaseNotFoundError: If the specified database doesn't exist.\n\n    Example: Set current database\n        ```python\n        # Set 'my_database' as the current database\n        session.catalog.set_current_database('my_database')\n        ```\n    \"\"\"\n    self.catalog.set_current_database(database_name)\n</code></pre>"},{"location":"reference/fenic/#fenic.ClassifyExample","title":"ClassifyExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single semantic example for classification operations.</p> <p>Classify examples demonstrate the classification of an input string into a specific category string, used in a semantic.classify operation.</p>"},{"location":"reference/fenic/#fenic.ClassifyExampleCollection","title":"ClassifyExampleCollection","text":"<pre><code>ClassifyExampleCollection(examples: List[ExampleType] = None)\n</code></pre> <p>               Bases: <code>BaseExampleCollection[ClassifyExample]</code></p> <p>Collection of examples for semantic classification operations.</p> <p>Classification operations categorize input text into predefined classes. This collection manages examples that demonstrate the expected classification results for different inputs.</p> <p>Examples in this collection have a single input string and an output string representing the classification result.</p> <p>Methods:</p> <ul> <li> <code>from_polars</code>             \u2013              <p>Create collection from a Polars DataFrame. Must have an 'output' column and an 'input' column.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def __init__(self, examples: List[ExampleType] = None):\n    \"\"\"Initialize a collection of semantic examples.\n\n    Args:\n        examples: Optional list of examples to add to the collection. Each example\n            will be processed through create_example() to ensure proper formatting\n            and validation.\n\n    Note:\n        The examples list is initialized as empty if no examples are provided.\n        Each example in the provided list will be processed through create_example()\n        to ensure proper formatting and validation.\n    \"\"\"\n    self.examples: List[ExampleType] = []\n    if examples:\n        for example in examples:\n            self.create_example(example)\n</code></pre>"},{"location":"reference/fenic/#fenic.ClassifyExampleCollection.from_polars","title":"from_polars  <code>classmethod</code>","text":"<pre><code>from_polars(df: DataFrame) -&gt; ClassifyExampleCollection\n</code></pre> <p>Create collection from a Polars DataFrame. Must have an 'output' column and an 'input' column.</p> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: pl.DataFrame) -&gt; ClassifyExampleCollection:\n    \"\"\"Create collection from a Polars DataFrame. Must have an 'output' column and an 'input' column.\"\"\"\n    collection = cls()\n\n    if EXAMPLE_INPUT_KEY not in df.columns:\n        raise InvalidExampleCollectionError(\n            f\"Classify Examples DataFrame missing required '{EXAMPLE_INPUT_KEY}' column\"\n        )\n    if EXAMPLE_OUTPUT_KEY not in df.columns:\n        raise InvalidExampleCollectionError(\n            f\"Classify Examples DataFrame missing required '{EXAMPLE_OUTPUT_KEY}' column\"\n        )\n\n    for row in df.iter_rows(named=True):\n        if row[EXAMPLE_INPUT_KEY] is None:\n            raise InvalidExampleCollectionError(\n                f\"Classify Examples DataFrame contains null values in '{EXAMPLE_INPUT_KEY}' column\"\n            )\n        if row[EXAMPLE_OUTPUT_KEY] is None:\n            raise InvalidExampleCollectionError(\n                f\"Classify Examples DataFrame contains null values in '{EXAMPLE_OUTPUT_KEY}' column\"\n            )\n\n        example = ClassifyExample(\n            input=row[EXAMPLE_INPUT_KEY],\n            output=row[EXAMPLE_OUTPUT_KEY],\n        )\n        collection.create_example(example)\n\n    return collection\n</code></pre>"},{"location":"reference/fenic/#fenic.Column","title":"Column","text":"<p>A column expression in a DataFrame.</p> <p>This class represents a column expression that can be used in DataFrame operations. It provides methods for accessing, transforming, and combining column data.</p> Create a column reference <pre><code># Reference a column by name using col() function\ncol(\"column_name\")\n</code></pre> Use column in operations <pre><code># Perform arithmetic operations\ndf.select(col(\"price\") * col(\"quantity\"))\n</code></pre> Chain column operations <pre><code># Chain multiple operations\ndf.select(col(\"name\").upper().contains(\"John\"))\n</code></pre> <p>Methods:</p> <ul> <li> <code>alias</code>             \u2013              <p>Create an alias for this column.</p> </li> <li> <code>asc</code>             \u2013              <p>Apply ascending order to this column during a dataframe sort or order_by.</p> </li> <li> <code>asc_nulls_first</code>             \u2013              <p>Apply ascending order putting nulls first to this column during a dataframe sort or order_by.</p> </li> <li> <code>asc_nulls_last</code>             \u2013              <p>Apply ascending order putting nulls last to this column during a dataframe sort or order_by.</p> </li> <li> <code>cast</code>             \u2013              <p>Cast the column to a new data type.</p> </li> <li> <code>contains</code>             \u2013              <p>Check if the column contains a substring.</p> </li> <li> <code>contains_any</code>             \u2013              <p>Check if the column contains any of the specified substrings.</p> </li> <li> <code>desc</code>             \u2013              <p>Apply descending order to this column during a dataframe sort or order_by.</p> </li> <li> <code>desc_nulls_first</code>             \u2013              <p>Apply descending order putting nulls first to this column during a dataframe sort or order_by.</p> </li> <li> <code>desc_nulls_last</code>             \u2013              <p>Apply descending order putting nulls last to this column during a dataframe sort or order_by.</p> </li> <li> <code>ends_with</code>             \u2013              <p>Check if the column ends with a substring.</p> </li> <li> <code>get_item</code>             \u2013              <p>Access an item in a struct or array column.</p> </li> <li> <code>ilike</code>             \u2013              <p>Check if the column matches a SQL LIKE pattern (case-insensitive).</p> </li> <li> <code>is_in</code>             \u2013              <p>Check if the column is in a list of values or a column expression.</p> </li> <li> <code>is_not_null</code>             \u2013              <p>Check if the column contains non-NULL values.</p> </li> <li> <code>is_null</code>             \u2013              <p>Check if the column contains NULL values.</p> </li> <li> <code>like</code>             \u2013              <p>Check if the column matches a SQL LIKE pattern.</p> </li> <li> <code>otherwise</code>             \u2013              <p>Evaluates a list of conditions and returns one of multiple possible result expressions.</p> </li> <li> <code>rlike</code>             \u2013              <p>Check if the column matches a regular expression pattern.</p> </li> <li> <code>starts_with</code>             \u2013              <p>Check if the column starts with a substring.</p> </li> <li> <code>when</code>             \u2013              <p>Evaluates a list of conditions and returns one of multiple possible result expressions.</p> </li> </ul>"},{"location":"reference/fenic/#fenic.Column.alias","title":"alias","text":"<pre><code>alias(name: str) -&gt; Column\n</code></pre> <p>Create an alias for this column.</p> <p>This method assigns a new name to the column expression, which is useful for renaming columns or providing names for complex expressions.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The alias name to assign</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>Column with the specified alias</p> </li> </ul> Rename a column <pre><code># Rename a column to a new name\ndf.select(col(\"original_name\").alias(\"new_name\"))\n</code></pre> Name a complex expression <pre><code># Give a name to a calculated column\ndf.select((col(\"price\") * col(\"quantity\")).alias(\"total_value\"))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def alias(self, name: str) -&gt; Column:\n    \"\"\"Create an alias for this column.\n\n    This method assigns a new name to the column expression, which is useful\n    for renaming columns or providing names for complex expressions.\n\n    Args:\n        name (str): The alias name to assign\n\n    Returns:\n        Column: Column with the specified alias\n\n    Example: Rename a column\n        ```python\n        # Rename a column to a new name\n        df.select(col(\"original_name\").alias(\"new_name\"))\n        ```\n\n    Example: Name a complex expression\n        ```python\n        # Give a name to a calculated column\n        df.select((col(\"price\") * col(\"quantity\")).alias(\"total_value\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(AliasExpr(self._logical_expr, name))\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.asc","title":"asc","text":"<pre><code>asc() -&gt; Column\n</code></pre> <p>Apply ascending order to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in ascending order <pre><code># Sort a dataframe by age in ascending order\ndf.sort(col(\"age\").asc()).show()\n</code></pre> Sort using column reference <pre><code># Sort using column reference with ascending order\ndf.sort(col(\"age\").asc()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def asc(self) -&gt; Column:\n    \"\"\"Apply ascending order to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function.\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in ascending order\n        ```python\n        # Sort a dataframe by age in ascending order\n        df.sort(col(\"age\").asc()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        # Sort using column reference with ascending order\n        df.sort(col(\"age\").asc()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(SortExpr(self._logical_expr, ascending=True))\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.asc_nulls_first","title":"asc_nulls_first","text":"<pre><code>asc_nulls_first() -&gt; Column\n</code></pre> <p>Apply ascending order putting nulls first to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in ascending order with nulls first <pre><code># Sort a dataframe by age in ascending order, with nulls appearing first\ndf.sort(col(\"age\").asc_nulls_first()).show()\n</code></pre> Sort using column reference <pre><code># Sort using column reference with ascending order and nulls first\ndf.sort(col(\"age\").asc_nulls_first()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def asc_nulls_first(self) -&gt; Column:\n    \"\"\"Apply ascending order putting nulls first to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function.\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in ascending order with nulls first\n        ```python\n        # Sort a dataframe by age in ascending order, with nulls appearing first\n        df.sort(col(\"age\").asc_nulls_first()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        # Sort using column reference with ascending order and nulls first\n        df.sort(col(\"age\").asc_nulls_first()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        SortExpr(self._logical_expr, ascending=True, nulls_last=False)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.asc_nulls_last","title":"asc_nulls_last","text":"<pre><code>asc_nulls_last() -&gt; Column\n</code></pre> <p>Apply ascending order putting nulls last to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in ascending order with nulls last <pre><code># Sort a dataframe by age in ascending order, with nulls appearing last\ndf.sort(col(\"age\").asc_nulls_last()).show()\n</code></pre> Sort using column reference <pre><code># Sort using column reference with ascending order and nulls last\ndf.sort(col(\"age\").asc_nulls_last()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def asc_nulls_last(self) -&gt; Column:\n    \"\"\"Apply ascending order putting nulls last to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function.\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in ascending order with nulls last\n        ```python\n        # Sort a dataframe by age in ascending order, with nulls appearing last\n        df.sort(col(\"age\").asc_nulls_last()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        # Sort using column reference with ascending order and nulls last\n        df.sort(col(\"age\").asc_nulls_last()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        SortExpr(self._logical_expr, ascending=True, nulls_last=True)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.cast","title":"cast","text":"<pre><code>cast(data_type: DataType) -&gt; Column\n</code></pre> <p>Cast the column to a new data type.</p> <p>This method creates an expression that casts the column to a specified data type. The casting behavior depends on the source and target types:</p> <p>Primitive type casting:</p> <ul> <li>Numeric types (IntegerType, FloatType, DoubleType) can be cast between each other</li> <li>Numeric types can be cast to/from StringType</li> <li>BooleanType can be cast to/from numeric types and StringType</li> <li>StringType cannot be directly cast to BooleanType (will raise TypeError)</li> </ul> <p>Complex type casting:</p> <ul> <li>ArrayType can only be cast to another ArrayType (with castable element types)</li> <li>StructType can only be cast to another StructType (with matching/castable fields)</li> <li>Primitive types cannot be cast to/from complex types</li> </ul> <p>Parameters:</p> <ul> <li> <code>data_type</code>               (<code>DataType</code>)           \u2013            <p>The target DataType to cast the column to</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column representing the casted expression</p> </li> </ul> Cast integer to string <pre><code># Convert an integer column to string type\ndf.select(col(\"int_col\").cast(StringType))\n</code></pre> Cast array of integers to array of strings <pre><code># Convert an array of integers to an array of strings\ndf.select(col(\"int_array\").cast(ArrayType(element_type=StringType)))\n</code></pre> Cast struct fields to different types <pre><code># Convert struct fields to different types\nnew_type = StructType([\n    StructField(\"id\", StringType),\n    StructField(\"value\", FloatType)\n])\ndf.select(col(\"data_struct\").cast(new_type))\n</code></pre> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the requested cast operation is not supported</p> </li> </ul> Source code in <code>src/fenic/api/column.py</code> <pre><code>def cast(self, data_type: DataType) -&gt; Column:\n    \"\"\"Cast the column to a new data type.\n\n    This method creates an expression that casts the column to a specified data type.\n    The casting behavior depends on the source and target types:\n\n    Primitive type casting:\n\n    - Numeric types (IntegerType, FloatType, DoubleType) can be cast between each other\n    - Numeric types can be cast to/from StringType\n    - BooleanType can be cast to/from numeric types and StringType\n    - StringType cannot be directly cast to BooleanType (will raise TypeError)\n\n    Complex type casting:\n\n    - ArrayType can only be cast to another ArrayType (with castable element types)\n    - StructType can only be cast to another StructType (with matching/castable fields)\n    - Primitive types cannot be cast to/from complex types\n\n    Args:\n        data_type (DataType): The target DataType to cast the column to\n\n    Returns:\n        Column: A Column representing the casted expression\n\n    Example: Cast integer to string\n        ```python\n        # Convert an integer column to string type\n        df.select(col(\"int_col\").cast(StringType))\n        ```\n\n    Example: Cast array of integers to array of strings\n        ```python\n        # Convert an array of integers to an array of strings\n        df.select(col(\"int_array\").cast(ArrayType(element_type=StringType)))\n        ```\n\n    Example: Cast struct fields to different types\n        ```python\n        # Convert struct fields to different types\n        new_type = StructType([\n            StructField(\"id\", StringType),\n            StructField(\"value\", FloatType)\n        ])\n        df.select(col(\"data_struct\").cast(new_type))\n        ```\n\n    Raises:\n        TypeError: If the requested cast operation is not supported\n    \"\"\"\n    return Column._from_logical_expr(CastExpr(self._logical_expr, data_type))\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.contains","title":"contains","text":"<pre><code>contains(other: Union[str, Column]) -&gt; Column\n</code></pre> <p>Check if the column contains a substring.</p> <p>This method creates a boolean expression that checks if each value in the column contains the specified substring. The substring can be either a literal string or a column expression.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[str, Column]</code>)           \u2013            <p>The substring to search for (can be a string or column expression)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value contains the substring</p> </li> </ul> Find rows where name contains \"john\" <pre><code># Filter rows where the name column contains \"john\"\ndf.filter(col(\"name\").contains(\"john\"))\n</code></pre> Find rows where text contains a dynamic pattern <pre><code># Filter rows where text contains a value from another column\ndf.filter(col(\"text\").contains(col(\"pattern\")))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def contains(self, other: Union[str, Column]) -&gt; Column:\n    \"\"\"Check if the column contains a substring.\n\n    This method creates a boolean expression that checks if each value in the column\n    contains the specified substring. The substring can be either a literal string\n    or a column expression.\n\n    Args:\n        other (Union[str, Column]): The substring to search for (can be a string or column expression)\n\n    Returns:\n        Column: A boolean column indicating whether each value contains the substring\n\n    Example: Find rows where name contains \"john\"\n        ```python\n        # Filter rows where the name column contains \"john\"\n        df.filter(col(\"name\").contains(\"john\"))\n        ```\n\n    Example: Find rows where text contains a dynamic pattern\n        ```python\n        # Filter rows where text contains a value from another column\n        df.filter(col(\"text\").contains(col(\"pattern\")))\n        ```\n    \"\"\"\n    if isinstance(other, str):\n        return Column._from_logical_expr(ContainsExpr(self._logical_expr, other))\n    else:\n        return Column._from_logical_expr(\n            ContainsExpr(self._logical_expr, other._logical_expr)\n        )\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.contains_any","title":"contains_any","text":"<pre><code>contains_any(others: List[str], case_insensitive: bool = True) -&gt; Column\n</code></pre> <p>Check if the column contains any of the specified substrings.</p> <p>This method creates a boolean expression that checks if each value in the column contains any of the specified substrings. The matching can be case-sensitive or case-insensitive.</p> <p>Parameters:</p> <ul> <li> <code>others</code>               (<code>List[str]</code>)           \u2013            <p>List of substrings to search for</p> </li> <li> <code>case_insensitive</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to perform case-insensitive matching (default: True)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value contains any substring</p> </li> </ul> Find rows where name contains \"john\" or \"jane\" (case-insensitive) <pre><code># Filter rows where name contains either \"john\" or \"jane\"\ndf.filter(col(\"name\").contains_any([\"john\", \"jane\"]))\n</code></pre> Case-sensitive matching <pre><code># Filter rows with case-sensitive matching\ndf.filter(col(\"name\").contains_any([\"John\", \"Jane\"], case_insensitive=False))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def contains_any(self, others: List[str], case_insensitive: bool = True) -&gt; Column:\n    \"\"\"Check if the column contains any of the specified substrings.\n\n    This method creates a boolean expression that checks if each value in the column\n    contains any of the specified substrings. The matching can be case-sensitive or\n    case-insensitive.\n\n    Args:\n        others (List[str]): List of substrings to search for\n        case_insensitive (bool): Whether to perform case-insensitive matching (default: True)\n\n    Returns:\n        Column: A boolean column indicating whether each value contains any substring\n\n    Example: Find rows where name contains \"john\" or \"jane\" (case-insensitive)\n        ```python\n        # Filter rows where name contains either \"john\" or \"jane\"\n        df.filter(col(\"name\").contains_any([\"john\", \"jane\"]))\n        ```\n\n    Example: Case-sensitive matching\n        ```python\n        # Filter rows with case-sensitive matching\n        df.filter(col(\"name\").contains_any([\"John\", \"Jane\"], case_insensitive=False))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        ContainsAnyExpr(self._logical_expr, others, case_insensitive)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.desc","title":"desc","text":"<pre><code>desc() -&gt; Column\n</code></pre> <p>Apply descending order to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in descending order <pre><code># Sort a dataframe by age in descending order\ndf.sort(col(\"age\").desc()).show()\n</code></pre> Sort using column reference <pre><code># Sort using column reference with descending order\ndf.sort(col(\"age\").desc()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def desc(self) -&gt; Column:\n    \"\"\"Apply descending order to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function.\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in descending order\n        ```python\n        # Sort a dataframe by age in descending order\n        df.sort(col(\"age\").desc()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        # Sort using column reference with descending order\n        df.sort(col(\"age\").desc()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(SortExpr(self._logical_expr, ascending=False))\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.desc_nulls_first","title":"desc_nulls_first","text":"<pre><code>desc_nulls_first() -&gt; Column\n</code></pre> <p>Apply descending order putting nulls first to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in descending order with nulls first <pre><code>df.sort(col(\"age\").desc_nulls_first()).show()\n</code></pre> Sort using column reference <pre><code>df.sort(col(\"age\").desc_nulls_first()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def desc_nulls_first(self) -&gt; Column:\n    \"\"\"Apply descending order putting nulls first to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in descending order with nulls first\n        ```python\n        df.sort(col(\"age\").desc_nulls_first()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        df.sort(col(\"age\").desc_nulls_first()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        SortExpr(self._logical_expr, ascending=False, nulls_last=False)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.desc_nulls_last","title":"desc_nulls_last","text":"<pre><code>desc_nulls_last() -&gt; Column\n</code></pre> <p>Apply descending order putting nulls last to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in descending order with nulls last <pre><code># Sort a dataframe by age in descending order, with nulls appearing last\ndf.sort(col(\"age\").desc_nulls_last()).show()\n</code></pre> Sort using column reference <pre><code># Sort using column reference with descending order and nulls last\ndf.sort(col(\"age\").desc_nulls_last()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def desc_nulls_last(self) -&gt; Column:\n    \"\"\"Apply descending order putting nulls last to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function.\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in descending order with nulls last\n        ```python\n        # Sort a dataframe by age in descending order, with nulls appearing last\n        df.sort(col(\"age\").desc_nulls_last()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        # Sort using column reference with descending order and nulls last\n        df.sort(col(\"age\").desc_nulls_last()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        SortExpr(self._logical_expr, ascending=False, nulls_last=True)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.ends_with","title":"ends_with","text":"<pre><code>ends_with(other: Union[str, Column]) -&gt; Column\n</code></pre> <p>Check if the column ends with a substring.</p> <p>This method creates a boolean expression that checks if each value in the column ends with the specified substring. The substring can be either a literal string or a column expression.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[str, Column]</code>)           \u2013            <p>The substring to check for at the end (can be a string or column expression)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value ends with the substring</p> </li> </ul> Find rows where email ends with \"@gmail.com\" <pre><code>df.filter(col(\"email\").ends_with(\"@gmail.com\"))\n</code></pre> Find rows where text ends with a dynamic pattern <pre><code>df.filter(col(\"text\").ends_with(col(\"suffix\")))\n</code></pre> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the substring ends with a regular expression anchor ($)</p> </li> </ul> Source code in <code>src/fenic/api/column.py</code> <pre><code>def ends_with(self, other: Union[str, Column]) -&gt; Column:\n    \"\"\"Check if the column ends with a substring.\n\n    This method creates a boolean expression that checks if each value in the column\n    ends with the specified substring. The substring can be either a literal string\n    or a column expression.\n\n    Args:\n        other (Union[str, Column]): The substring to check for at the end (can be a string or column expression)\n\n    Returns:\n        Column: A boolean column indicating whether each value ends with the substring\n\n    Example: Find rows where email ends with \"@gmail.com\"\n        ```python\n        df.filter(col(\"email\").ends_with(\"@gmail.com\"))\n        ```\n\n    Example: Find rows where text ends with a dynamic pattern\n        ```python\n        df.filter(col(\"text\").ends_with(col(\"suffix\")))\n        ```\n\n    Raises:\n        ValueError: If the substring ends with a regular expression anchor ($)\n    \"\"\"\n    if isinstance(other, str):\n        return Column._from_logical_expr(EndsWithExpr(self._logical_expr, other))\n    else:\n        return Column._from_logical_expr(\n            EndsWithExpr(self._logical_expr, other._logical_expr)\n        )\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.get_item","title":"get_item","text":"<pre><code>get_item(key: Union[str, int]) -&gt; Column\n</code></pre> <p>Access an item in a struct or array column.</p> <p>This method allows accessing elements in complex data types:</p> <ul> <li>For array columns, the key should be an integer index</li> <li>For struct columns, the key should be a field name</li> </ul> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>Union[str, int]</code>)           \u2013            <p>The index (for arrays) or field name (for structs) to access</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column representing the accessed item</p> </li> </ul> Access an array element <pre><code># Get the first element from an array column\ndf.select(col(\"array_column\").get_item(0))\n</code></pre> Access a struct field <pre><code># Get a field from a struct column\ndf.select(col(\"struct_column\").get_item(\"field_name\"))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def get_item(self, key: Union[str, int]) -&gt; Column:\n    \"\"\"Access an item in a struct or array column.\n\n    This method allows accessing elements in complex data types:\n\n    - For array columns, the key should be an integer index\n    - For struct columns, the key should be a field name\n\n    Args:\n        key (Union[str, int]): The index (for arrays) or field name (for structs) to access\n\n    Returns:\n        Column: A Column representing the accessed item\n\n    Example: Access an array element\n        ```python\n        # Get the first element from an array column\n        df.select(col(\"array_column\").get_item(0))\n        ```\n\n    Example: Access a struct field\n        ```python\n        # Get a field from a struct column\n        df.select(col(\"struct_column\").get_item(\"field_name\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(IndexExpr(self._logical_expr, key))\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.ilike","title":"ilike","text":"<pre><code>ilike(other: str) -&gt; Column\n</code></pre> <p>Check if the column matches a SQL LIKE pattern (case-insensitive).</p> <p>This method creates a boolean expression that checks if each value in the column matches the specified SQL LIKE pattern, ignoring case. The pattern must be a literal string and cannot be a column expression.</p> <p>SQL LIKE pattern syntax:</p> <ul> <li>% matches any sequence of characters</li> <li>_ matches any single character</li> </ul> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>str</code>)           \u2013            <p>The SQL LIKE pattern to match against</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value matches the pattern</p> </li> </ul> Find rows where name starts with \"j\" and ends with \"n\" (case-insensitive) <pre><code># Filter rows where name matches the pattern \"j%n\" (case-insensitive)\ndf.filter(col(\"name\").ilike(\"j%n\"))\n</code></pre> Find rows where code matches pattern (case-insensitive) <pre><code># Filter rows where code matches the pattern \"a_b%\" (case-insensitive)\ndf.filter(col(\"code\").ilike(\"a_b%\"))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def ilike(self, other: str) -&gt; Column:\n    r\"\"\"Check if the column matches a SQL LIKE pattern (case-insensitive).\n\n    This method creates a boolean expression that checks if each value in the column\n    matches the specified SQL LIKE pattern, ignoring case. The pattern must be a literal string\n    and cannot be a column expression.\n\n    SQL LIKE pattern syntax:\n\n    - % matches any sequence of characters\n    - _ matches any single character\n\n    Args:\n        other (str): The SQL LIKE pattern to match against\n\n    Returns:\n        Column: A boolean column indicating whether each value matches the pattern\n\n    Example: Find rows where name starts with \"j\" and ends with \"n\" (case-insensitive)\n        ```python\n        # Filter rows where name matches the pattern \"j%n\" (case-insensitive)\n        df.filter(col(\"name\").ilike(\"j%n\"))\n        ```\n\n    Example: Find rows where code matches pattern (case-insensitive)\n        ```python\n        # Filter rows where code matches the pattern \"a_b%\" (case-insensitive)\n        df.filter(col(\"code\").ilike(\"a_b%\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(ILikeExpr(self._logical_expr, other))\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.is_in","title":"is_in","text":"<pre><code>is_in(other: Union[List[Any], ColumnOrName]) -&gt; Column\n</code></pre> <p>Check if the column is in a list of values or a column expression.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[List[Any], ColumnOrName]</code>)           \u2013            <p>A list of values or a Column expression</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression representing whether each element of Column is in the list</p> </li> </ul> Check if name is in a list of values <pre><code># Filter rows where name is in a list of values\ndf.filter(col(\"name\").is_in([\"Alice\", \"Bob\"]))\n</code></pre> Check if value is in another column <pre><code># Filter rows where name is in another column\ndf.filter(col(\"name\").is_in(col(\"other_column\")))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def is_in(self, other: Union[List[Any], ColumnOrName]) -&gt; Column:\n    \"\"\"Check if the column is in a list of values or a column expression.\n\n    Args:\n        other (Union[List[Any], ColumnOrName]): A list of values or a Column expression\n\n    Returns:\n        Column: A Column expression representing whether each element of Column is in the list\n\n    Example: Check if name is in a list of values\n        ```python\n        # Filter rows where name is in a list of values\n        df.filter(col(\"name\").is_in([\"Alice\", \"Bob\"]))\n        ```\n\n    Example: Check if value is in another column\n        ```python\n        # Filter rows where name is in another column\n        df.filter(col(\"name\").is_in(col(\"other_column\")))\n        ```\n    \"\"\"\n    if isinstance(other, list):\n        try:\n            type_ = infer_dtype_from_pyobj(other)\n            return Column._from_logical_expr(InExpr(self._logical_expr, LiteralExpr(other, type_)))\n        except TypeInferenceError as e:\n            raise ValidationError(f\"Cannot apply IN on {other}. List argument to IN must be be a valid Python List literal.\") from e\n    else:\n        return Column._from_logical_expr(InExpr(self._logical_expr, other._logical_expr))\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.is_not_null","title":"is_not_null","text":"<pre><code>is_not_null() -&gt; Column\n</code></pre> <p>Check if the column contains non-NULL values.</p> <p>This method creates an expression that evaluates to TRUE when the column value is not NULL.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column representing a boolean expression that is TRUE when this column is not NULL</p> </li> </ul> Filter rows where a column is not NULL <pre><code>df.filter(col(\"some_column\").is_not_null())\n</code></pre> Use in a complex condition <pre><code>df.filter(col(\"col1\").is_not_null() &amp; (col(\"col2\") &lt;= 50))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def is_not_null(self) -&gt; Column:\n    \"\"\"Check if the column contains non-NULL values.\n\n    This method creates an expression that evaluates to TRUE when the column value is not NULL.\n\n    Returns:\n        Column: A Column representing a boolean expression that is TRUE when this column is not NULL\n\n    Example: Filter rows where a column is not NULL\n        ```python\n        df.filter(col(\"some_column\").is_not_null())\n        ```\n\n    Example: Use in a complex condition\n        ```python\n        df.filter(col(\"col1\").is_not_null() &amp; (col(\"col2\") &lt;= 50))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(IsNullExpr(self._logical_expr, False))\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.is_null","title":"is_null","text":"<pre><code>is_null() -&gt; Column\n</code></pre> <p>Check if the column contains NULL values.</p> <p>This method creates an expression that evaluates to TRUE when the column value is NULL.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column representing a boolean expression that is TRUE when this column is NULL</p> </li> </ul> Filter rows where a column is NULL <pre><code># Filter rows where some_column is NULL\ndf.filter(col(\"some_column\").is_null())\n</code></pre> Use in a complex condition <pre><code># Filter rows where col1 is NULL or col2 is greater than 100\ndf.filter(col(\"col1\").is_null() | (col(\"col2\") &gt; 100))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def is_null(self) -&gt; Column:\n    \"\"\"Check if the column contains NULL values.\n\n    This method creates an expression that evaluates to TRUE when the column value is NULL.\n\n    Returns:\n        Column: A Column representing a boolean expression that is TRUE when this column is NULL\n\n    Example: Filter rows where a column is NULL\n        ```python\n        # Filter rows where some_column is NULL\n        df.filter(col(\"some_column\").is_null())\n        ```\n\n    Example: Use in a complex condition\n        ```python\n        # Filter rows where col1 is NULL or col2 is greater than 100\n        df.filter(col(\"col1\").is_null() | (col(\"col2\") &gt; 100))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(IsNullExpr(self._logical_expr, True))\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.like","title":"like","text":"<pre><code>like(other: str) -&gt; Column\n</code></pre> <p>Check if the column matches a SQL LIKE pattern.</p> <p>This method creates a boolean expression that checks if each value in the column matches the specified SQL LIKE pattern. The pattern must be a literal string and cannot be a column expression.</p> <p>SQL LIKE pattern syntax:</p> <ul> <li>% matches any sequence of characters</li> <li>_ matches any single character</li> </ul> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>str</code>)           \u2013            <p>The SQL LIKE pattern to match against</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value matches the pattern</p> </li> </ul> Find rows where name starts with \"J\" and ends with \"n\" <pre><code># Filter rows where name matches the pattern \"J%n\"\ndf.filter(col(\"name\").like(\"J%n\"))\n</code></pre> Find rows where code matches specific pattern <pre><code># Filter rows where code matches the pattern \"A_B%\"\ndf.filter(col(\"code\").like(\"A_B%\"))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def like(self, other: str) -&gt; Column:\n    r\"\"\"Check if the column matches a SQL LIKE pattern.\n\n    This method creates a boolean expression that checks if each value in the column\n    matches the specified SQL LIKE pattern. The pattern must be a literal string\n    and cannot be a column expression.\n\n    SQL LIKE pattern syntax:\n\n    - % matches any sequence of characters\n    - _ matches any single character\n\n    Args:\n        other (str): The SQL LIKE pattern to match against\n\n    Returns:\n        Column: A boolean column indicating whether each value matches the pattern\n\n    Example: Find rows where name starts with \"J\" and ends with \"n\"\n        ```python\n        # Filter rows where name matches the pattern \"J%n\"\n        df.filter(col(\"name\").like(\"J%n\"))\n        ```\n\n    Example: Find rows where code matches specific pattern\n        ```python\n        # Filter rows where code matches the pattern \"A_B%\"\n        df.filter(col(\"code\").like(\"A_B%\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(LikeExpr(self._logical_expr, other))\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.otherwise","title":"otherwise","text":"<pre><code>otherwise(value: Column) -&gt; Column\n</code></pre> <p>Evaluates a list of conditions and returns one of multiple possible result expressions.</p> <p>If Column.otherwise() is not invoked, None is returned for unmatched conditions. Otherwise() will return for rows with None inputs.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Column</code>)           \u2013            <p>A literal value or Column expression to return</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression representing whether each element of Column is not matched by any previous conditions</p> </li> </ul> Use when/otherwise for conditional logic <pre><code># Create a DataFrame with age and name columns\ndf = session.createDataFrame(\n    {\"age\": [2, 5]}, {\"name\": [\"Alice\", \"Bob\"]}\n)\n\n# Use when/otherwise to create a case result column\ndf.select(\n    col(\"name\"),\n    when(col(\"age\") &gt; 3, 1).otherwise(0).alias(\"case_result\")\n).show()\n# Output:\n# +-----+-----------+\n# | name|case_result|\n# +-----+-----------+\n# |Alice|          0|\n# |  Bob|          1|\n# +-----+-----------+\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def otherwise(self, value: Column) -&gt; Column:\n    \"\"\"Evaluates a list of conditions and returns one of multiple possible result expressions.\n\n    If Column.otherwise() is not invoked, None is returned for unmatched conditions.\n    Otherwise() will return for rows with None inputs.\n\n    Args:\n        value (Column): A literal value or Column expression to return\n\n    Returns:\n        Column: A Column expression representing whether each element of Column is not matched by any previous conditions\n\n    Example: Use when/otherwise for conditional logic\n        ```python\n        # Create a DataFrame with age and name columns\n        df = session.createDataFrame(\n            {\"age\": [2, 5]}, {\"name\": [\"Alice\", \"Bob\"]}\n        )\n\n        # Use when/otherwise to create a case result column\n        df.select(\n            col(\"name\"),\n            when(col(\"age\") &gt; 3, 1).otherwise(0).alias(\"case_result\")\n        ).show()\n        # Output:\n        # +-----+-----------+\n        # | name|case_result|\n        # +-----+-----------+\n        # |Alice|          0|\n        # |  Bob|          1|\n        # +-----+-----------+\n        ```\n    \"\"\"\n    return Column._from_logical_expr(OtherwiseExpr(self._logical_expr, value._logical_expr))\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.rlike","title":"rlike","text":"<pre><code>rlike(other: str) -&gt; Column\n</code></pre> <p>Check if the column matches a regular expression pattern.</p> <p>This method creates a boolean expression that checks if each value in the column matches the specified regular expression pattern. The pattern must be a literal string and cannot be a column expression.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>str</code>)           \u2013            <p>The regular expression pattern to match against</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value matches the pattern</p> </li> </ul> Find rows where phone number matches pattern <pre><code># Filter rows where phone number matches a specific pattern\ndf.filter(col(\"phone\").rlike(r\"^\\d{3}-\\d{3}-\\d{4}$\"))\n</code></pre> Find rows where text contains word boundaries <pre><code># Filter rows where text contains a word with boundaries\ndf.filter(col(\"text\").rlike(r\"\\bhello\\b\"))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def rlike(self, other: str) -&gt; Column:\n    r\"\"\"Check if the column matches a regular expression pattern.\n\n    This method creates a boolean expression that checks if each value in the column\n    matches the specified regular expression pattern. The pattern must be a literal string\n    and cannot be a column expression.\n\n    Args:\n        other (str): The regular expression pattern to match against\n\n    Returns:\n        Column: A boolean column indicating whether each value matches the pattern\n\n    Example: Find rows where phone number matches pattern\n        ```python\n        # Filter rows where phone number matches a specific pattern\n        df.filter(col(\"phone\").rlike(r\"^\\d{3}-\\d{3}-\\d{4}$\"))\n        ```\n\n    Example: Find rows where text contains word boundaries\n        ```python\n        # Filter rows where text contains a word with boundaries\n        df.filter(col(\"text\").rlike(r\"\\bhello\\b\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(RLikeExpr(self._logical_expr, other))\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.starts_with","title":"starts_with","text":"<pre><code>starts_with(other: Union[str, Column]) -&gt; Column\n</code></pre> <p>Check if the column starts with a substring.</p> <p>This method creates a boolean expression that checks if each value in the column starts with the specified substring. The substring can be either a literal string or a column expression.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[str, Column]</code>)           \u2013            <p>The substring to check for at the start (can be a string or column expression)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value starts with the substring</p> </li> </ul> Find rows where name starts with \"Mr\" <pre><code># Filter rows where name starts with \"Mr\"\ndf.filter(col(\"name\").starts_with(\"Mr\"))\n</code></pre> Find rows where text starts with a dynamic pattern <pre><code># Filter rows where text starts with a value from another column\ndf.filter(col(\"text\").starts_with(col(\"prefix\")))\n</code></pre> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the substring starts with a regular expression anchor (^)</p> </li> </ul> Source code in <code>src/fenic/api/column.py</code> <pre><code>def starts_with(self, other: Union[str, Column]) -&gt; Column:\n    \"\"\"Check if the column starts with a substring.\n\n    This method creates a boolean expression that checks if each value in the column\n    starts with the specified substring. The substring can be either a literal string\n    or a column expression.\n\n    Args:\n        other (Union[str, Column]): The substring to check for at the start (can be a string or column expression)\n\n    Returns:\n        Column: A boolean column indicating whether each value starts with the substring\n\n    Example: Find rows where name starts with \"Mr\"\n        ```python\n        # Filter rows where name starts with \"Mr\"\n        df.filter(col(\"name\").starts_with(\"Mr\"))\n        ```\n\n    Example: Find rows where text starts with a dynamic pattern\n        ```python\n        # Filter rows where text starts with a value from another column\n        df.filter(col(\"text\").starts_with(col(\"prefix\")))\n        ```\n\n    Raises:\n        ValueError: If the substring starts with a regular expression anchor (^)\n    \"\"\"\n    if isinstance(other, str):\n        return Column._from_logical_expr(StartsWithExpr(self._logical_expr, other))\n    else:\n        return Column._from_logical_expr(\n            StartsWithExpr(self._logical_expr, other._logical_expr)\n        )\n</code></pre>"},{"location":"reference/fenic/#fenic.Column.when","title":"when","text":"<pre><code>when(condition: Column, value: Column) -&gt; Column\n</code></pre> <p>Evaluates a list of conditions and returns one of multiple possible result expressions.</p> <p>If Column.otherwise() is not invoked, None is returned for unmatched conditions. Otherwise() will return for rows with None inputs.</p> <p>Parameters:</p> <ul> <li> <code>condition</code>               (<code>Column</code>)           \u2013            <p>A boolean Column expression</p> </li> <li> <code>value</code>               (<code>Column</code>)           \u2013            <p>A literal value or Column expression to return if the condition is true</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression representing whether each element of Column matches the condition</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the condition is not a boolean Column expression</p> </li> </ul> Use when/otherwise for conditional logic <pre><code># Create a DataFrame with age and name columns\ndf = session.createDataFrame(\n    {\"age\": [2, 5]}, {\"name\": [\"Alice\", \"Bob\"]}\n)\n\n# Use when/otherwise to create a case result column\ndf.select(\n    col(\"name\"),\n    when(col(\"age\") &gt; 3, 1).otherwise(0).alias(\"case_result\")\n).show()\n# Output:\n# +-----+-----------+\n# | name|case_result|\n# +-----+-----------+\n# |Alice|          0|\n# |  Bob|          1|\n# +-----+-----------+\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def when(self, condition: Column, value: Column) -&gt; Column:\n    \"\"\"Evaluates a list of conditions and returns one of multiple possible result expressions.\n\n    If Column.otherwise() is not invoked, None is returned for unmatched conditions.\n    Otherwise() will return for rows with None inputs.\n\n    Args:\n        condition (Column): A boolean Column expression\n        value (Column): A literal value or Column expression to return if the condition is true\n\n    Returns:\n        Column: A Column expression representing whether each element of Column matches the condition\n\n    Raises:\n        TypeError: If the condition is not a boolean Column expression\n\n    Example: Use when/otherwise for conditional logic\n        ```python\n        # Create a DataFrame with age and name columns\n        df = session.createDataFrame(\n            {\"age\": [2, 5]}, {\"name\": [\"Alice\", \"Bob\"]}\n        )\n\n        # Use when/otherwise to create a case result column\n        df.select(\n            col(\"name\"),\n            when(col(\"age\") &gt; 3, 1).otherwise(0).alias(\"case_result\")\n        ).show()\n        # Output:\n        # +-----+-----------+\n        # | name|case_result|\n        # +-----+-----------+\n        # |Alice|          0|\n        # |  Bob|          1|\n        # +-----+-----------+\n        ```\n    \"\"\"\n    return Column._from_logical_expr(WhenExpr(self._logical_expr, condition._logical_expr, value._logical_expr))\n</code></pre>"},{"location":"reference/fenic/#fenic.ColumnField","title":"ColumnField","text":"<p>Represents a typed column in a DataFrame schema.</p> <p>A ColumnField defines the structure of a single column by specifying its name and data type. This is used as a building block for DataFrame schemas.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the column.</p> </li> <li> <code>data_type</code>               (<code>DataType</code>)           \u2013            <p>The data type of the column, as a DataType instance.</p> </li> </ul>"},{"location":"reference/fenic/#fenic.DataFrame","title":"DataFrame","text":"<p>A data collection organized into named columns.</p> <p>The DataFrame class represents a lazily evaluated computation on data. Operations on DataFrame build up a logical query plan that is only executed when an action like show(), to_polars(), to_pandas(), to_arrow(), to_pydict(), to_pylist(), or count() is called.</p> <p>The DataFrame supports method chaining for building complex transformations.</p> Create and transform a DataFrame <pre><code># Create a DataFrame from a dictionary\ndf = session.create_dataframe({\"id\": [1, 2, 3], \"value\": [\"a\", \"b\", \"c\"]})\n\n# Chain transformations\nresult = df.filter(col(\"id\") &gt; 1).select(\"id\", \"value\")\n\n# Show results\nresult.show()\n# Output:\n# +---+-----+\n# | id|value|\n# +---+-----+\n# |  2|    b|\n# |  3|    c|\n# +---+-----+\n</code></pre> <p>Methods:</p> <ul> <li> <code>agg</code>             \u2013              <p>Aggregate on the entire DataFrame without groups.</p> </li> <li> <code>cache</code>             \u2013              <p>Alias for persist(). Mark DataFrame for caching after first computation.</p> </li> <li> <code>collect</code>             \u2013              <p>Execute the DataFrame computation and return the result as a QueryResult.</p> </li> <li> <code>count</code>             \u2013              <p>Count the number of rows in the DataFrame.</p> </li> <li> <code>drop</code>             \u2013              <p>Remove one or more columns from this DataFrame.</p> </li> <li> <code>drop_duplicates</code>             \u2013              <p>Return a DataFrame with duplicate rows removed.</p> </li> <li> <code>explain</code>             \u2013              <p>Display the logical plan of the DataFrame.</p> </li> <li> <code>explode</code>             \u2013              <p>Create a new row for each element in an array column.</p> </li> <li> <code>filter</code>             \u2013              <p>Filters rows using the given condition.</p> </li> <li> <code>group_by</code>             \u2013              <p>Groups the DataFrame using the specified columns.</p> </li> <li> <code>join</code>             \u2013              <p>Joins this DataFrame with another DataFrame.</p> </li> <li> <code>limit</code>             \u2013              <p>Limits the number of rows to the specified number.</p> </li> <li> <code>lineage</code>             \u2013              <p>Create a Lineage object to trace data through transformations.</p> </li> <li> <code>order_by</code>             \u2013              <p>Sort the DataFrame by the specified columns. Alias for sort().</p> </li> <li> <code>persist</code>             \u2013              <p>Mark this DataFrame to be persisted after first computation.</p> </li> <li> <code>select</code>             \u2013              <p>Projects a set of Column expressions or column names.</p> </li> <li> <code>show</code>             \u2013              <p>Display the DataFrame content in a tabular form.</p> </li> <li> <code>sort</code>             \u2013              <p>Sort the DataFrame by the specified columns.</p> </li> <li> <code>to_arrow</code>             \u2013              <p>Execute the DataFrame computation and return an Apache Arrow Table.</p> </li> <li> <code>to_pandas</code>             \u2013              <p>Execute the DataFrame computation and return a Pandas DataFrame.</p> </li> <li> <code>to_polars</code>             \u2013              <p>Execute the DataFrame computation and return the result as a Polars DataFrame.</p> </li> <li> <code>to_pydict</code>             \u2013              <p>Execute the DataFrame computation and return a dictionary of column arrays.</p> </li> <li> <code>to_pylist</code>             \u2013              <p>Execute the DataFrame computation and return a list of row dictionaries.</p> </li> <li> <code>union</code>             \u2013              <p>Return a new DataFrame containing the union of rows in this and another DataFrame.</p> </li> <li> <code>unnest</code>             \u2013              <p>Unnest the specified struct columns into separate columns.</p> </li> <li> <code>where</code>             \u2013              <p>Filters rows using the given condition (alias for filter()).</p> </li> <li> <code>with_column</code>             \u2013              <p>Add a new column or replace an existing column.</p> </li> <li> <code>with_column_renamed</code>             \u2013              <p>Rename a column. No-op if the column does not exist.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>columns</code>               (<code>List[str]</code>)           \u2013            <p>Get list of column names.</p> </li> <li> <code>schema</code>               (<code>Schema</code>)           \u2013            <p>Get the schema of this DataFrame.</p> </li> <li> <code>semantic</code>               (<code>SemanticExtensions</code>)           \u2013            <p>Interface for semantic operations on the DataFrame.</p> </li> <li> <code>write</code>               (<code>DataFrameWriter</code>)           \u2013            <p>Interface for saving the content of the DataFrame.</p> </li> </ul>"},{"location":"reference/fenic/#fenic.DataFrame.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: List[str]\n</code></pre> <p>Get list of column names.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: List of all column names in the DataFrame</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df.columns\n['name', 'age', 'city']\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.schema","title":"schema  <code>property</code>","text":"<pre><code>schema: Schema\n</code></pre> <p>Get the schema of this DataFrame.</p> <p>Returns:</p> <ul> <li> <code>Schema</code> (              <code>Schema</code> )          \u2013            <p>Schema containing field names and data types</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df.schema\nSchema([\n    ColumnField('name', StringType),\n    ColumnField('age', IntegerType)\n])\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.semantic","title":"semantic  <code>property</code>","text":"<pre><code>semantic: SemanticExtensions\n</code></pre> <p>Interface for semantic operations on the DataFrame.</p>"},{"location":"reference/fenic/#fenic.DataFrame.write","title":"write  <code>property</code>","text":"<pre><code>write: DataFrameWriter\n</code></pre> <p>Interface for saving the content of the DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrameWriter</code> (              <code>DataFrameWriter</code> )          \u2013            <p>Writer interface to write DataFrame.</p> </li> </ul>"},{"location":"reference/fenic/#fenic.DataFrame.agg","title":"agg","text":"<pre><code>agg(*exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame\n</code></pre> <p>Aggregate on the entire DataFrame without groups.</p> <p>This is equivalent to group_by() without any grouping columns.</p> <p>Parameters:</p> <ul> <li> <code>*exprs</code>               (<code>Union[Column, Dict[str, str]]</code>, default:                   <code>()</code> )           \u2013            <p>Aggregation expressions or dictionary of aggregations.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Aggregation results.</p> </li> </ul> Multiple aggregations <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"salary\": [80000, 70000, 90000, 75000, 85000],\n    \"age\": [25, 30, 35, 28, 32]\n})\n\n# Multiple aggregations\ndf.agg(\n    count().alias(\"total_rows\"),\n    avg(col(\"salary\")).alias(\"avg_salary\")\n).show()\n# Output:\n# +----------+-----------+\n# |total_rows|avg_salary|\n# +----------+-----------+\n# |         5|   80000.0|\n# +----------+-----------+\n</code></pre> Dictionary style <pre><code># Dictionary style\ndf.agg({col(\"salary\"): \"avg\", col(\"age\"): \"max\"}).show()\n# Output:\n# +-----------+--------+\n# |avg(salary)|max(age)|\n# +-----------+--------+\n# |    80000.0|      35|\n# +-----------+--------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def agg(self, *exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame:\n    \"\"\"Aggregate on the entire DataFrame without groups.\n\n    This is equivalent to group_by() without any grouping columns.\n\n    Args:\n        *exprs: Aggregation expressions or dictionary of aggregations.\n\n    Returns:\n        DataFrame: Aggregation results.\n\n    Example: Multiple aggregations\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"salary\": [80000, 70000, 90000, 75000, 85000],\n            \"age\": [25, 30, 35, 28, 32]\n        })\n\n        # Multiple aggregations\n        df.agg(\n            count().alias(\"total_rows\"),\n            avg(col(\"salary\")).alias(\"avg_salary\")\n        ).show()\n        # Output:\n        # +----------+-----------+\n        # |total_rows|avg_salary|\n        # +----------+-----------+\n        # |         5|   80000.0|\n        # +----------+-----------+\n        ```\n\n    Example: Dictionary style\n        ```python\n        # Dictionary style\n        df.agg({col(\"salary\"): \"avg\", col(\"age\"): \"max\"}).show()\n        # Output:\n        # +-----------+--------+\n        # |avg(salary)|max(age)|\n        # +-----------+--------+\n        # |    80000.0|      35|\n        # +-----------+--------+\n        ```\n    \"\"\"\n    return self.group_by().agg(*exprs)\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.cache","title":"cache","text":"<pre><code>cache() -&gt; DataFrame\n</code></pre> <p>Alias for persist(). Mark DataFrame for caching after first computation.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Same DataFrame, but marked for caching</p> </li> </ul> See Also <p>persist(): Full documentation of caching behavior</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def cache(self) -&gt; DataFrame:\n    \"\"\"Alias for persist(). Mark DataFrame for caching after first computation.\n\n    Returns:\n        DataFrame: Same DataFrame, but marked for caching\n\n    See Also:\n        persist(): Full documentation of caching behavior\n    \"\"\"\n    return self.persist()\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.collect","title":"collect","text":"<pre><code>collect(data_type: DataLikeType = 'polars') -&gt; QueryResult\n</code></pre> <p>Execute the DataFrame computation and return the result as a QueryResult.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a QueryResult, which contains both the result data and the query metrics.</p> <p>Parameters:</p> <ul> <li> <code>data_type</code>               (<code>DataLikeType</code>, default:                   <code>'polars'</code> )           \u2013            <p>The type of data to return</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>QueryResult</code> (              <code>QueryResult</code> )          \u2013            <p>A QueryResult with materialized data and query metrics</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def collect(self, data_type: DataLikeType = \"polars\") -&gt; QueryResult:\n    \"\"\"Execute the DataFrame computation and return the result as a QueryResult.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a QueryResult, which contains both the result data and the query metrics.\n\n    Args:\n        data_type: The type of data to return\n\n    Returns:\n        QueryResult: A QueryResult with materialized data and query metrics\n    \"\"\"\n    result: Tuple[pl.DataFrame, QueryMetrics] = self._logical_plan.session_state.execution.collect(self._logical_plan)\n    df, metrics = result\n    logger.info(metrics.get_summary())\n\n    if data_type == \"polars\":\n        return QueryResult(df, metrics)\n    elif data_type == \"pandas\":\n        return QueryResult(df.to_pandas(use_pyarrow_extension_array=True), metrics)\n    elif data_type == \"arrow\":\n        return QueryResult(df.to_arrow(), metrics)\n    elif data_type == \"pydict\":\n        return QueryResult(df.to_dict(as_series=False), metrics)\n    elif data_type == \"pylist\":\n        return QueryResult(df.to_dicts(), metrics)\n    else:\n        raise ValidationError(f\"Invalid data type: {data_type} in collect(). Valid data types are: polars, pandas, arrow, pydict, pylist\")\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.count","title":"count","text":"<pre><code>count() -&gt; int\n</code></pre> <p>Count the number of rows in the DataFrame.</p> <p>This is an action that triggers computation of the DataFrame. The output is an integer representing the number of rows.</p> <p>Returns:</p> <ul> <li> <code>int</code> (              <code>int</code> )          \u2013            <p>The number of rows in the DataFrame</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def count(self) -&gt; int:\n    \"\"\"Count the number of rows in the DataFrame.\n\n    This is an action that triggers computation of the DataFrame.\n    The output is an integer representing the number of rows.\n\n    Returns:\n        int: The number of rows in the DataFrame\n    \"\"\"\n    return self._logical_plan.session_state.execution.count(self._logical_plan)[0]\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.drop","title":"drop","text":"<pre><code>drop(*col_names: str) -&gt; DataFrame\n</code></pre> <p>Remove one or more columns from this DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>*col_names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>Names of columns to drop.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>New DataFrame without specified columns.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If any specified column doesn't exist in the DataFrame.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If dropping the columns would result in an empty DataFrame.</p> </li> </ul> Drop single column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35]\n})\n\n# Drop single column\ndf.drop(\"age\").show()\n# Output:\n# +---+-------+\n# | id|   name|\n# +---+-------+\n# |  1|  Alice|\n# |  2|    Bob|\n# |  3|Charlie|\n# +---+-------+\n</code></pre> Drop multiple columns <pre><code># Drop multiple columns\ndf.drop(col(\"id\"), \"age\").show()\n# Output:\n# +-------+\n# |   name|\n# +-------+\n# |  Alice|\n# |    Bob|\n# |Charlie|\n# +-------+\n</code></pre> Error when dropping non-existent column <pre><code># This will raise a ValueError\ndf.drop(\"non_existent_column\")\n# ValueError: Column 'non_existent_column' not found in DataFrame\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def drop(self, *col_names: str) -&gt; DataFrame:\n    \"\"\"Remove one or more columns from this DataFrame.\n\n    Args:\n        *col_names: Names of columns to drop.\n\n    Returns:\n        DataFrame: New DataFrame without specified columns.\n\n    Raises:\n        ValueError: If any specified column doesn't exist in the DataFrame.\n        ValueError: If dropping the columns would result in an empty DataFrame.\n\n    Example: Drop single column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"id\": [1, 2, 3],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n            \"age\": [25, 30, 35]\n        })\n\n        # Drop single column\n        df.drop(\"age\").show()\n        # Output:\n        # +---+-------+\n        # | id|   name|\n        # +---+-------+\n        # |  1|  Alice|\n        # |  2|    Bob|\n        # |  3|Charlie|\n        # +---+-------+\n        ```\n\n    Example: Drop multiple columns\n        ```python\n        # Drop multiple columns\n        df.drop(col(\"id\"), \"age\").show()\n        # Output:\n        # +-------+\n        # |   name|\n        # +-------+\n        # |  Alice|\n        # |    Bob|\n        # |Charlie|\n        # +-------+\n        ```\n\n    Example: Error when dropping non-existent column\n        ```python\n        # This will raise a ValueError\n        df.drop(\"non_existent_column\")\n        # ValueError: Column 'non_existent_column' not found in DataFrame\n        ```\n    \"\"\"\n    if not col_names:\n        return self\n\n    current_cols = set(self.columns)\n    to_drop = set(col_names)\n    missing = to_drop - current_cols\n\n    if missing:\n        missing_str = (\n            f\"Column '{next(iter(missing))}'\"\n            if len(missing) == 1\n            else f\"Columns {sorted(missing)}\"\n        )\n        raise ValueError(f\"{missing_str} not found in DataFrame\")\n\n    remaining_cols = [\n        col(c)._logical_expr for c in self.columns if c not in to_drop\n    ]\n\n    if not remaining_cols:\n        raise ValueError(\"Cannot drop all columns from DataFrame\")\n\n    return self._from_logical_plan(\n        Projection(self._logical_plan, remaining_cols)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.drop_duplicates","title":"drop_duplicates","text":"<pre><code>drop_duplicates(subset: Optional[List[str]] = None) -&gt; DataFrame\n</code></pre> <p>Return a DataFrame with duplicate rows removed.</p> <p>Parameters:</p> <ul> <li> <code>subset</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Column names to consider when identifying duplicates. If not provided, all columns are considered.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with duplicate rows removed.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a specified column is not present in the current DataFrame schema.</p> </li> </ul> Remove duplicates considering specific columns <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"c1\": [1, 2, 3, 1],\n    \"c2\": [\"a\", \"a\", \"a\", \"a\"],\n    \"c3\": [\"b\", \"b\", \"b\", \"b\"]\n})\n\n# Remove duplicates considering all columns\ndf.drop_duplicates([col(\"c1\"), col(\"c2\"), col(\"c3\")]).show()\n# Output:\n# +---+---+---+\n# | c1| c2| c3|\n# +---+---+---+\n# |  1|  a|  b|\n# |  2|  a|  b|\n# |  3|  a|  b|\n# +---+---+---+\n\n# Remove duplicates considering only c1\ndf.drop_duplicates([col(\"c1\")]).show()\n# Output:\n# +---+---+---+\n# | c1| c2| c3|\n# +---+---+---+\n# |  1|  a|  b|\n# |  2|  a|  b|\n# |  3|  a|  b|\n# +---+---+---+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def drop_duplicates(\n    self,\n    subset: Optional[List[str]] = None,\n) -&gt; DataFrame:\n    \"\"\"Return a DataFrame with duplicate rows removed.\n\n    Args:\n        subset: Column names to consider when identifying duplicates. If not provided, all columns are considered.\n\n    Returns:\n        DataFrame: A new DataFrame with duplicate rows removed.\n\n    Raises:\n        ValueError: If a specified column is not present in the current DataFrame schema.\n\n    Example: Remove duplicates considering specific columns\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"c1\": [1, 2, 3, 1],\n            \"c2\": [\"a\", \"a\", \"a\", \"a\"],\n            \"c3\": [\"b\", \"b\", \"b\", \"b\"]\n        })\n\n        # Remove duplicates considering all columns\n        df.drop_duplicates([col(\"c1\"), col(\"c2\"), col(\"c3\")]).show()\n        # Output:\n        # +---+---+---+\n        # | c1| c2| c3|\n        # +---+---+---+\n        # |  1|  a|  b|\n        # |  2|  a|  b|\n        # |  3|  a|  b|\n        # +---+---+---+\n\n        # Remove duplicates considering only c1\n        df.drop_duplicates([col(\"c1\")]).show()\n        # Output:\n        # +---+---+---+\n        # | c1| c2| c3|\n        # +---+---+---+\n        # |  1|  a|  b|\n        # |  2|  a|  b|\n        # |  3|  a|  b|\n        # +---+---+---+\n        ```\n    \"\"\"\n    exprs = []\n    if subset:\n        for c in subset:\n            if c not in self.columns:\n                raise TypeError(f\"Column {c} not found in DataFrame.\")\n            exprs.append(col(c)._logical_expr)\n\n    return self._from_logical_plan(\n        DropDuplicates(self._logical_plan, exprs),\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.explain","title":"explain","text":"<pre><code>explain() -&gt; None\n</code></pre> <p>Display the logical plan of the DataFrame.</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def explain(self) -&gt; None:\n    \"\"\"Display the logical plan of the DataFrame.\"\"\"\n    print(str(self._logical_plan))\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.explode","title":"explode","text":"<pre><code>explode(column: ColumnOrName) -&gt; DataFrame\n</code></pre> <p>Create a new row for each element in an array column.</p> <p>This operation is useful for flattening nested data structures. For each row in the input DataFrame that contains an array/list in the specified column, this method will: 1. Create N new rows, where N is the length of the array 2. Each new row will be identical to the original row, except the array column will    contain just a single element from the original array 3. Rows with NULL values or empty arrays in the specified column are filtered out</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Name of array column to explode (as string) or Column expression.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>New DataFrame with the array column exploded into multiple rows.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column argument is not a string or Column.</p> </li> </ul> Explode array column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"id\": [1, 2, 3, 4],\n    \"tags\": [[\"red\", \"blue\"], [\"green\"], [], None],\n    \"name\": [\"Alice\", \"Bob\", \"Carol\", \"Dave\"]\n})\n\n# Explode the tags column\ndf.explode(\"tags\").show()\n# Output:\n# +---+-----+-----+\n# | id| tags| name|\n# +---+-----+-----+\n# |  1|  red|Alice|\n# |  1| blue|Alice|\n# |  2|green|  Bob|\n# +---+-----+-----+\n</code></pre> Using column expression <pre><code># Explode using column expression\ndf.explode(col(\"tags\")).show()\n# Output:\n# +---+-----+-----+\n# | id| tags| name|\n# +---+-----+-----+\n# |  1|  red|Alice|\n# |  1| blue|Alice|\n# |  2|green|  Bob|\n# +---+-----+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def explode(self, column: ColumnOrName) -&gt; DataFrame:\n    \"\"\"Create a new row for each element in an array column.\n\n    This operation is useful for flattening nested data structures. For each row in the\n    input DataFrame that contains an array/list in the specified column, this method will:\n    1. Create N new rows, where N is the length of the array\n    2. Each new row will be identical to the original row, except the array column will\n       contain just a single element from the original array\n    3. Rows with NULL values or empty arrays in the specified column are filtered out\n\n    Args:\n        column: Name of array column to explode (as string) or Column expression.\n\n    Returns:\n        DataFrame: New DataFrame with the array column exploded into multiple rows.\n\n    Raises:\n        TypeError: If column argument is not a string or Column.\n\n    Example: Explode array column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"id\": [1, 2, 3, 4],\n            \"tags\": [[\"red\", \"blue\"], [\"green\"], [], None],\n            \"name\": [\"Alice\", \"Bob\", \"Carol\", \"Dave\"]\n        })\n\n        # Explode the tags column\n        df.explode(\"tags\").show()\n        # Output:\n        # +---+-----+-----+\n        # | id| tags| name|\n        # +---+-----+-----+\n        # |  1|  red|Alice|\n        # |  1| blue|Alice|\n        # |  2|green|  Bob|\n        # +---+-----+-----+\n        ```\n\n    Example: Using column expression\n        ```python\n        # Explode using column expression\n        df.explode(col(\"tags\")).show()\n        # Output:\n        # +---+-----+-----+\n        # | id| tags| name|\n        # +---+-----+-----+\n        # |  1|  red|Alice|\n        # |  1| blue|Alice|\n        # |  2|green|  Bob|\n        # +---+-----+-----+\n        ```\n    \"\"\"\n    return self._from_logical_plan(\n        Explode(self._logical_plan, Column._from_col_or_name(column)._logical_expr),\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.filter","title":"filter","text":"<pre><code>filter(condition: Column) -&gt; DataFrame\n</code></pre> <p>Filters rows using the given condition.</p> <p>Parameters:</p> <ul> <li> <code>condition</code>               (<code>Column</code>)           \u2013            <p>A Column expression that evaluates to a boolean</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Filtered DataFrame</p> </li> </ul> Filter with numeric comparison <pre><code># Create a DataFrame\ndf = session.create_dataframe({\"age\": [25, 30, 35], \"name\": [\"Alice\", \"Bob\", \"Charlie\"]})\n\n# Filter with numeric comparison\ndf.filter(col(\"age\") &gt; 25).show()\n# Output:\n# +---+-------+\n# |age|   name|\n# +---+-------+\n# | 30|    Bob|\n# | 35|Charlie|\n# +---+-------+\n</code></pre> Filter with semantic predicate <pre><code># Filter with semantic predicate\ndf.filter((col(\"age\") &gt; 25) &amp; semantic.predicate(\"This {feedback} mentions problems with the user interface or navigation\")).show()\n# Output:\n# +---+-------+\n# |age|   name|\n# +---+-------+\n# | 30|    Bob|\n# | 35|Charlie|\n# +---+-------+\n</code></pre> Filter with multiple conditions <pre><code># Filter with multiple conditions\ndf.filter((col(\"age\") &gt; 25) &amp; (col(\"age\") &lt;= 35)).show()\n# Output:\n# +---+-------+\n# |age|   name|\n# +---+-------+\n# | 30|    Bob|\n# | 35|Charlie|\n# +---+-------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def filter(self, condition: Column) -&gt; DataFrame:\n    \"\"\"Filters rows using the given condition.\n\n    Args:\n        condition: A Column expression that evaluates to a boolean\n\n    Returns:\n        DataFrame: Filtered DataFrame\n\n    Example: Filter with numeric comparison\n        ```python\n        # Create a DataFrame\n        df = session.create_dataframe({\"age\": [25, 30, 35], \"name\": [\"Alice\", \"Bob\", \"Charlie\"]})\n\n        # Filter with numeric comparison\n        df.filter(col(\"age\") &gt; 25).show()\n        # Output:\n        # +---+-------+\n        # |age|   name|\n        # +---+-------+\n        # | 30|    Bob|\n        # | 35|Charlie|\n        # +---+-------+\n        ```\n\n    Example: Filter with semantic predicate\n        ```python\n        # Filter with semantic predicate\n        df.filter((col(\"age\") &gt; 25) &amp; semantic.predicate(\"This {feedback} mentions problems with the user interface or navigation\")).show()\n        # Output:\n        # +---+-------+\n        # |age|   name|\n        # +---+-------+\n        # | 30|    Bob|\n        # | 35|Charlie|\n        # +---+-------+\n        ```\n\n    Example: Filter with multiple conditions\n        ```python\n        # Filter with multiple conditions\n        df.filter((col(\"age\") &gt; 25) &amp; (col(\"age\") &lt;= 35)).show()\n        # Output:\n        # +---+-------+\n        # |age|   name|\n        # +---+-------+\n        # | 30|    Bob|\n        # | 35|Charlie|\n        # +---+-------+\n        ```\n    \"\"\"\n    return self._from_logical_plan(\n        Filter(self._logical_plan, condition._logical_expr),\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.group_by","title":"group_by","text":"<pre><code>group_by(*cols: ColumnOrName) -&gt; GroupedData\n</code></pre> <p>Groups the DataFrame using the specified columns.</p> <p>Parameters:</p> <ul> <li> <code>*cols</code>               (<code>ColumnOrName</code>, default:                   <code>()</code> )           \u2013            <p>Columns to group by. Can be column names as strings or Column expressions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GroupedData</code> (              <code>GroupedData</code> )          \u2013            <p>Object for performing aggregations on the grouped data.</p> </li> </ul> Group by single column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"department\": [\"IT\", \"HR\", \"IT\", \"HR\", \"IT\"],\n    \"salary\": [80000, 70000, 90000, 75000, 85000]\n})\n\n# Group by single column\ndf.group_by(col(\"department\")).count().show()\n# Output:\n# +----------+-----+\n# |department|count|\n# +----------+-----+\n# |        IT|    3|\n# |        HR|    2|\n# +----------+-----+\n</code></pre> Group by multiple columns <pre><code># Group by multiple columns\ndf.group_by(col(\"department\"), col(\"location\")).agg({\"salary\": \"avg\"}).show()\n# Output:\n# +----------+--------+-----------+\n# |department|location|avg(salary)|\n# +----------+--------+-----------+\n# |        IT|    NYC|    85000.0|\n# |        HR|    NYC|    72500.0|\n# +----------+--------+-----------+\n</code></pre> Group by expression <pre><code># Group by expression\ndf.group_by(col(\"age\").cast(\"int\").alias(\"age_group\")).count().show()\n# Output:\n# +---------+-----+\n# |age_group|count|\n# +---------+-----+\n# |       20|    2|\n# |       30|    3|\n# |       40|    1|\n# +---------+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def group_by(self, *cols: ColumnOrName) -&gt; GroupedData:\n    \"\"\"Groups the DataFrame using the specified columns.\n\n    Args:\n        *cols: Columns to group by. Can be column names as strings or Column expressions.\n\n    Returns:\n        GroupedData: Object for performing aggregations on the grouped data.\n\n    Example: Group by single column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"department\": [\"IT\", \"HR\", \"IT\", \"HR\", \"IT\"],\n            \"salary\": [80000, 70000, 90000, 75000, 85000]\n        })\n\n        # Group by single column\n        df.group_by(col(\"department\")).count().show()\n        # Output:\n        # +----------+-----+\n        # |department|count|\n        # +----------+-----+\n        # |        IT|    3|\n        # |        HR|    2|\n        # +----------+-----+\n        ```\n\n    Example: Group by multiple columns\n        ```python\n        # Group by multiple columns\n        df.group_by(col(\"department\"), col(\"location\")).agg({\"salary\": \"avg\"}).show()\n        # Output:\n        # +----------+--------+-----------+\n        # |department|location|avg(salary)|\n        # +----------+--------+-----------+\n        # |        IT|    NYC|    85000.0|\n        # |        HR|    NYC|    72500.0|\n        # +----------+--------+-----------+\n        ```\n\n    Example: Group by expression\n        ```python\n        # Group by expression\n        df.group_by(col(\"age\").cast(\"int\").alias(\"age_group\")).count().show()\n        # Output:\n        # +---------+-----+\n        # |age_group|count|\n        # +---------+-----+\n        # |       20|    2|\n        # |       30|    3|\n        # |       40|    1|\n        # +---------+-----+\n        ```\n    \"\"\"\n    return GroupedData(self, list(cols) if cols else None)\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.join","title":"join","text":"<pre><code>join(other: DataFrame, on: Union[str, List[str]], *, how: JoinType = 'inner') -&gt; DataFrame\n</code></pre><pre><code>join(other: DataFrame, *, left_on: Union[ColumnOrName, List[ColumnOrName]], right_on: Union[ColumnOrName, List[ColumnOrName]], how: JoinType = 'inner') -&gt; DataFrame\n</code></pre> <pre><code>join(other: DataFrame, on: Optional[Union[str, List[str]]] = None, *, left_on: Optional[Union[ColumnOrName, List[ColumnOrName]]] = None, right_on: Optional[Union[ColumnOrName, List[ColumnOrName]]] = None, how: JoinType = 'inner') -&gt; DataFrame\n</code></pre> <p>Joins this DataFrame with another DataFrame.</p> <p>The Dataframes must have no duplicate column names between them. This API only supports equi-joins. For non-equi-joins, use session.sql().</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>DataFrame</code>)           \u2013            <p>DataFrame to join with.</p> </li> <li> <code>on</code>               (<code>Optional[Union[str, List[str]]]</code>, default:                   <code>None</code> )           \u2013            <p>Join condition(s). Can be: - A column name (str) - A list of column names (List[str]) - A Column expression (e.g., col('a')) - A list of Column expressions - <code>None</code> for cross joins</p> </li> <li> <code>left_on</code>               (<code>Optional[Union[ColumnOrName, List[ColumnOrName]]]</code>, default:                   <code>None</code> )           \u2013            <p>Column(s) from the left DataFrame to join on. Can be: - A column name (str) - A Column expression (e.g., col('a'), col('a') + 1) - A list of column names or expressions</p> </li> <li> <code>right_on</code>               (<code>Optional[Union[ColumnOrName, List[ColumnOrName]]]</code>, default:                   <code>None</code> )           \u2013            <p>Column(s) from the right DataFrame to join on. Can be: - A column name (str) - A Column expression (e.g., col('b'), upper(col('b'))) - A list of column names or expressions</p> </li> <li> <code>how</code>               (<code>JoinType</code>, default:                   <code>'inner'</code> )           \u2013            <p>Type of join to perform.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Joined DataFrame.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If cross join is used with an ON clause.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If join condition is invalid.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If both 'on' and 'left_on'/'right_on' parameters are provided.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If only one of 'left_on' or 'right_on' is provided.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If 'left_on' and 'right_on' have different lengths</p> </li> </ul> Inner join on column name <pre><code># Create sample DataFrames\ndf1 = session.create_dataframe({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n})\ndf2 = session.create_dataframe({\n    \"id\": [1, 2, 4],\n    \"age\": [25, 30, 35]\n})\n\n# Join on single column\ndf1.join(df2, on=col(\"id\")).show()\n# Output:\n# +---+-----+---+\n# | id| name|age|\n# +---+-----+---+\n# |  1|Alice| 25|\n# |  2|  Bob| 30|\n# +---+-----+---+\n</code></pre> Join with expression <pre><code># Join with Column expressions\ndf1.join(\n    df2,\n    left_on=col(\"id\"),\n    right_on=col(\"id\"),\n).show()\n# Output:\n# +---+-----+---+\n# | id| name|age|\n# +---+-----+---+\n# |  1|Alice| 25|\n# |  2|  Bob| 30|\n# +---+-----+---+\n</code></pre> Cross join <pre><code># Cross join (cartesian product)\ndf1.join(df2, how=\"cross\").show()\n# Output:\n# +---+-----+---+---+\n# | id| name| id|age|\n# +---+-----+---+---+\n# |  1|Alice|  1| 25|\n# |  1|Alice|  2| 30|\n# |  1|Alice|  4| 35|\n# |  2|  Bob|  1| 25|\n# |  2|  Bob|  2| 30|\n# |  2|  Bob|  4| 35|\n# |  3|Charlie| 1| 25|\n# |  3|Charlie| 2| 30|\n# |  3|Charlie| 4| 35|\n# +---+-----+---+---+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def join(\n    self,\n    other: DataFrame,\n    on: Optional[Union[str, List[str]]] = None,\n    *,\n    left_on: Optional[Union[ColumnOrName, List[ColumnOrName]]] = None,\n    right_on: Optional[Union[ColumnOrName, List[ColumnOrName]]] = None,\n    how: JoinType = \"inner\",\n) -&gt; DataFrame:\n    \"\"\"Joins this DataFrame with another DataFrame.\n\n    The Dataframes must have no duplicate column names between them. This API only supports equi-joins.\n    For non-equi-joins, use session.sql().\n\n    Args:\n        other: DataFrame to join with.\n        on: Join condition(s). Can be:\n            - A column name (str)\n            - A list of column names (List[str])\n            - A Column expression (e.g., col('a'))\n            - A list of Column expressions\n            - `None` for cross joins\n        left_on: Column(s) from the left DataFrame to join on. Can be:\n            - A column name (str)\n            - A Column expression (e.g., col('a'), col('a') + 1)\n            - A list of column names or expressions\n        right_on: Column(s) from the right DataFrame to join on. Can be:\n            - A column name (str)\n            - A Column expression (e.g., col('b'), upper(col('b')))\n            - A list of column names or expressions\n        how: Type of join to perform.\n\n    Returns:\n        Joined DataFrame.\n\n    Raises:\n        ValidationError: If cross join is used with an ON clause.\n        ValidationError: If join condition is invalid.\n        ValidationError: If both 'on' and 'left_on'/'right_on' parameters are provided.\n        ValidationError: If only one of 'left_on' or 'right_on' is provided.\n        ValidationError: If 'left_on' and 'right_on' have different lengths\n\n    Example: Inner join on column name\n        ```python\n        # Create sample DataFrames\n        df1 = session.create_dataframe({\n            \"id\": [1, 2, 3],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n        })\n        df2 = session.create_dataframe({\n            \"id\": [1, 2, 4],\n            \"age\": [25, 30, 35]\n        })\n\n        # Join on single column\n        df1.join(df2, on=col(\"id\")).show()\n        # Output:\n        # +---+-----+---+\n        # | id| name|age|\n        # +---+-----+---+\n        # |  1|Alice| 25|\n        # |  2|  Bob| 30|\n        # +---+-----+---+\n        ```\n\n    Example: Join with expression\n        ```python\n        # Join with Column expressions\n        df1.join(\n            df2,\n            left_on=col(\"id\"),\n            right_on=col(\"id\"),\n        ).show()\n        # Output:\n        # +---+-----+---+\n        # | id| name|age|\n        # +---+-----+---+\n        # |  1|Alice| 25|\n        # |  2|  Bob| 30|\n        # +---+-----+---+\n        ```\n\n    Example: Cross join\n        ```python\n        # Cross join (cartesian product)\n        df1.join(df2, how=\"cross\").show()\n        # Output:\n        # +---+-----+---+---+\n        # | id| name| id|age|\n        # +---+-----+---+---+\n        # |  1|Alice|  1| 25|\n        # |  1|Alice|  2| 30|\n        # |  1|Alice|  4| 35|\n        # |  2|  Bob|  1| 25|\n        # |  2|  Bob|  2| 30|\n        # |  2|  Bob|  4| 35|\n        # |  3|Charlie| 1| 25|\n        # |  3|Charlie| 2| 30|\n        # |  3|Charlie| 4| 35|\n        # +---+-----+---+---+\n        ```\n    \"\"\"\n    validate_join_parameters(self, on, left_on, right_on, how)\n\n    # Build join conditions\n    left_conditions, right_conditions = build_join_conditions(on, left_on, right_on)\n\n    return self._from_logical_plan(\n        Join(self._logical_plan, other._logical_plan, left_conditions, right_conditions, how),\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.limit","title":"limit","text":"<pre><code>limit(n: int) -&gt; DataFrame\n</code></pre> <p>Limits the number of rows to the specified number.</p> <p>Parameters:</p> <ul> <li> <code>n</code>               (<code>int</code>)           \u2013            <p>Maximum number of rows to return.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>DataFrame with at most n rows.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If n is not an integer.</p> </li> </ul> Limit rows <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"id\": [1, 2, 3, 4, 5],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Eve\"]\n})\n\n# Get first 3 rows\ndf.limit(3).show()\n# Output:\n# +---+-------+\n# | id|   name|\n# +---+-------+\n# |  1|  Alice|\n# |  2|    Bob|\n# |  3|Charlie|\n# +---+-------+\n</code></pre> Limit with other operations <pre><code># Limit after filtering\ndf.filter(col(\"id\") &gt; 2).limit(2).show()\n# Output:\n# +---+-------+\n# | id|   name|\n# +---+-------+\n# |  3|Charlie|\n# |  4|   Dave|\n# +---+-------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def limit(self, n: int) -&gt; DataFrame:\n    \"\"\"Limits the number of rows to the specified number.\n\n    Args:\n        n: Maximum number of rows to return.\n\n    Returns:\n        DataFrame: DataFrame with at most n rows.\n\n    Raises:\n        TypeError: If n is not an integer.\n\n    Example: Limit rows\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"id\": [1, 2, 3, 4, 5],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Eve\"]\n        })\n\n        # Get first 3 rows\n        df.limit(3).show()\n        # Output:\n        # +---+-------+\n        # | id|   name|\n        # +---+-------+\n        # |  1|  Alice|\n        # |  2|    Bob|\n        # |  3|Charlie|\n        # +---+-------+\n        ```\n\n    Example: Limit with other operations\n        ```python\n        # Limit after filtering\n        df.filter(col(\"id\") &gt; 2).limit(2).show()\n        # Output:\n        # +---+-------+\n        # | id|   name|\n        # +---+-------+\n        # |  3|Charlie|\n        # |  4|   Dave|\n        # +---+-------+\n        ```\n    \"\"\"\n    return self._from_logical_plan(Limit(self._logical_plan, n))\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.lineage","title":"lineage","text":"<pre><code>lineage() -&gt; Lineage\n</code></pre> <p>Create a Lineage object to trace data through transformations.</p> <p>The Lineage interface allows you to trace how specific rows are transformed through your DataFrame operations, both forwards and backwards through the computation graph.</p> <p>Returns:</p> <ul> <li> <code>Lineage</code> (              <code>Lineage</code> )          \u2013            <p>Interface for querying data lineage</p> </li> </ul> Example <pre><code># Create lineage query\nlineage = df.lineage()\n\n# Trace specific rows backwards through transformations\nsource_rows = lineage.backward([\"result_uuid1\", \"result_uuid2\"])\n\n# Or trace forwards to see outputs\nresult_rows = lineage.forward([\"source_uuid1\"])\n</code></pre> See Also <p>LineageQuery: Full documentation of lineage querying capabilities</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def lineage(self) -&gt; Lineage:\n    \"\"\"Create a Lineage object to trace data through transformations.\n\n    The Lineage interface allows you to trace how specific rows are transformed\n    through your DataFrame operations, both forwards and backwards through the\n    computation graph.\n\n    Returns:\n        Lineage: Interface for querying data lineage\n\n    Example:\n        ```python\n        # Create lineage query\n        lineage = df.lineage()\n\n        # Trace specific rows backwards through transformations\n        source_rows = lineage.backward([\"result_uuid1\", \"result_uuid2\"])\n\n        # Or trace forwards to see outputs\n        result_rows = lineage.forward([\"source_uuid1\"])\n        ```\n\n    See Also:\n        LineageQuery: Full documentation of lineage querying capabilities\n    \"\"\"\n    return Lineage(self._logical_plan.session_state.execution.build_lineage(self._logical_plan))\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.order_by","title":"order_by","text":"<pre><code>order_by(cols: Union[ColumnOrName, List[ColumnOrName], None] = None, ascending: Optional[Union[bool, List[bool]]] = None) -&gt; 'DataFrame'\n</code></pre> <p>Sort the DataFrame by the specified columns. Alias for sort().</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>'DataFrame'</code> )          \u2013            <p>sorted Dataframe.</p> </li> </ul> See Also <p>sort(): Full documentation of sorting behavior and parameters.</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def order_by(\n    self,\n    cols: Union[ColumnOrName, List[ColumnOrName], None] = None,\n    ascending: Optional[Union[bool, List[bool]]] = None,\n) -&gt; \"DataFrame\":\n    \"\"\"Sort the DataFrame by the specified columns. Alias for sort().\n\n    Returns:\n        DataFrame: sorted Dataframe.\n\n    See Also:\n        sort(): Full documentation of sorting behavior and parameters.\n    \"\"\"\n    return self.sort(cols, ascending)\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.persist","title":"persist","text":"<pre><code>persist() -&gt; DataFrame\n</code></pre> <p>Mark this DataFrame to be persisted after first computation.</p> <p>The persisted DataFrame will be cached after its first computation, avoiding recomputation in subsequent operations. This is useful for DataFrames that are reused multiple times in your workflow.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Same DataFrame, but marked for persistence</p> </li> </ul> Example <pre><code># Cache intermediate results for reuse\nfiltered_df = (df\n    .filter(col(\"age\") &gt; 25)\n    .persist()  # Cache these results\n)\n\n# Both operations will use cached results\nresult1 = filtered_df.group_by(\"department\").count()\nresult2 = filtered_df.select(\"name\", \"salary\")\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def persist(self) -&gt; DataFrame:\n    \"\"\"Mark this DataFrame to be persisted after first computation.\n\n    The persisted DataFrame will be cached after its first computation,\n    avoiding recomputation in subsequent operations. This is useful for DataFrames\n    that are reused multiple times in your workflow.\n\n    Returns:\n        DataFrame: Same DataFrame, but marked for persistence\n\n    Example:\n        ```python\n        # Cache intermediate results for reuse\n        filtered_df = (df\n            .filter(col(\"age\") &gt; 25)\n            .persist()  # Cache these results\n        )\n\n        # Both operations will use cached results\n        result1 = filtered_df.group_by(\"department\").count()\n        result2 = filtered_df.select(\"name\", \"salary\")\n        ```\n    \"\"\"\n    table_name = f\"cache_{uuid.uuid4().hex}\"\n    cache_info = CacheInfo(duckdb_table_name=table_name)\n    self._logical_plan.set_cache_info(cache_info)\n    return self._from_logical_plan(self._logical_plan)\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.select","title":"select","text":"<pre><code>select(*cols: ColumnOrName) -&gt; DataFrame\n</code></pre> <p>Projects a set of Column expressions or column names.</p> <p>Parameters:</p> <ul> <li> <code>*cols</code>               (<code>ColumnOrName</code>, default:                   <code>()</code> )           \u2013            <p>Column expressions to select. Can be: - String column names (e.g., \"id\", \"name\") - Column objects (e.g., col(\"id\"), col(\"age\") + 1)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with selected columns</p> </li> </ul> Select by column names <pre><code># Create a DataFrame\ndf = session.create_dataframe({\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]})\n\n# Select by column names\ndf.select(col(\"name\"), col(\"age\")).show()\n# Output:\n# +-----+---+\n# | name|age|\n# +-----+---+\n# |Alice| 25|\n# |  Bob| 30|\n# +-----+---+\n</code></pre> Select with expressions <pre><code># Select with expressions\ndf.select(col(\"name\"), col(\"age\") + 1).show()\n# Output:\n# +-----+-------+\n# | name|age + 1|\n# +-----+-------+\n# |Alice|     26|\n# |  Bob|     31|\n# +-----+-------+\n</code></pre> Mix strings and expressions <pre><code># Mix strings and expressions\ndf.select(col(\"name\"), col(\"age\") * 2).show()\n# Output:\n# +-----+-------+\n# | name|age * 2|\n# +-----+-------+\n# |Alice|     50|\n# |  Bob|     60|\n# +-----+-------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def select(self, *cols: ColumnOrName) -&gt; DataFrame:\n    \"\"\"Projects a set of Column expressions or column names.\n\n    Args:\n        *cols: Column expressions to select. Can be:\n            - String column names (e.g., \"id\", \"name\")\n            - Column objects (e.g., col(\"id\"), col(\"age\") + 1)\n\n    Returns:\n        DataFrame: A new DataFrame with selected columns\n\n    Example: Select by column names\n        ```python\n        # Create a DataFrame\n        df = session.create_dataframe({\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]})\n\n        # Select by column names\n        df.select(col(\"name\"), col(\"age\")).show()\n        # Output:\n        # +-----+---+\n        # | name|age|\n        # +-----+---+\n        # |Alice| 25|\n        # |  Bob| 30|\n        # +-----+---+\n        ```\n\n    Example: Select with expressions\n        ```python\n        # Select with expressions\n        df.select(col(\"name\"), col(\"age\") + 1).show()\n        # Output:\n        # +-----+-------+\n        # | name|age + 1|\n        # +-----+-------+\n        # |Alice|     26|\n        # |  Bob|     31|\n        # +-----+-------+\n        ```\n\n    Example: Mix strings and expressions\n        ```python\n        # Mix strings and expressions\n        df.select(col(\"name\"), col(\"age\") * 2).show()\n        # Output:\n        # +-----+-------+\n        # | name|age * 2|\n        # +-----+-------+\n        # |Alice|     50|\n        # |  Bob|     60|\n        # +-----+-------+\n        ```\n    \"\"\"\n    exprs = []\n    if not cols:\n        return self\n    for c in cols:\n        if isinstance(c, str):\n            if c == \"*\":\n                exprs.extend(col(field)._logical_expr for field in self.columns)\n            else:\n                exprs.append(col(c)._logical_expr)\n        else:\n            exprs.append(c._logical_expr)\n\n    return self._from_logical_plan(\n        Projection(self._logical_plan, exprs)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.show","title":"show","text":"<pre><code>show(n: int = 10, explain_analyze: bool = False) -&gt; None\n</code></pre> <p>Display the DataFrame content in a tabular form.</p> <p>This is an action that triggers computation of the DataFrame. The output is printed to stdout in a formatted table.</p> <p>Parameters:</p> <ul> <li> <code>n</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of rows to display</p> </li> <li> <code>explain_analyze</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to print the explain analyze plan</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def show(self, n: int = 10, explain_analyze: bool = False) -&gt; None:\n    \"\"\"Display the DataFrame content in a tabular form.\n\n    This is an action that triggers computation of the DataFrame.\n    The output is printed to stdout in a formatted table.\n\n    Args:\n        n: Number of rows to display\n        explain_analyze: Whether to print the explain analyze plan\n    \"\"\"\n    output, metrics = self._logical_plan.session_state.execution.show(self._logical_plan, n)\n    logger.info(metrics.get_summary())\n    print(output)\n    if explain_analyze:\n        print(metrics.get_execution_plan_details())\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.sort","title":"sort","text":"<pre><code>sort(cols: Union[ColumnOrName, List[ColumnOrName], None] = None, ascending: Optional[Union[bool, List[bool]]] = None) -&gt; DataFrame\n</code></pre> <p>Sort the DataFrame by the specified columns.</p> <p>Parameters:</p> <ul> <li> <code>cols</code>               (<code>Union[ColumnOrName, List[ColumnOrName], None]</code>, default:                   <code>None</code> )           \u2013            <p>Columns to sort by. This can be: - A single column name (str) - A Column expression (e.g., <code>col(\"name\")</code>) - A list of column names or Column expressions - Column expressions may include sorting directives such as <code>asc(\"col\")</code>, <code>desc(\"col\")</code>, <code>asc_nulls_last(\"col\")</code>, etc. - If no columns are provided, the operation is a no-op.</p> </li> <li> <code>ascending</code>               (<code>Optional[Union[bool, List[bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>A boolean or list of booleans indicating sort order. - If <code>True</code>, sorts in ascending order; if <code>False</code>, descending. - If a list is provided, its length must match the number of columns. - Cannot be used if any of the columns use <code>asc()</code>/<code>desc()</code> expressions. - If not specified and no sort expressions are used, columns will be sorted in ascending order by default.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame sorted by the specified columns.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <ul> <li>If <code>ascending</code> is provided and its length does not match <code>cols</code></li> <li>If both <code>ascending</code> and column expressions like <code>asc()</code>/<code>desc()</code> are used</li> </ul> </li> <li> <code>TypeError</code>             \u2013            <ul> <li>If <code>cols</code> is not a column name, Column, or list of column names/Columns</li> <li>If <code>ascending</code> is not a boolean or list of booleans</li> </ul> </li> </ul> Sort in ascending order <pre><code># Create sample DataFrame\ndf = session.create_dataframe([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n\n# Sort by age in ascending order\ndf.sort(asc(col(\"age\"))).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  2|Alice|\n# |  5|  Bob|\n# +---+-----+\n</code></pre> Sort in descending order <pre><code># Sort by age in descending order\ndf.sort(col(\"age\").desc()).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  5|  Bob|\n# |  2|Alice|\n# +---+-----+\n</code></pre> Sort with boolean ascending parameter <pre><code># Sort by age in descending order using boolean\ndf.sort(col(\"age\"), ascending=False).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  5|  Bob|\n# |  2|Alice|\n# +---+-----+\n</code></pre> Multiple columns with different sort orders <pre><code># Create sample DataFrame\ndf = session.create_dataframe([(2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n\n# Sort by age descending, then name ascending\ndf.sort(desc(col(\"age\")), col(\"name\")).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  5|  Bob|\n# |  2|Alice|\n# |  2|  Bob|\n# +---+-----+\n</code></pre> Multiple columns with list of ascending strategies <pre><code># Sort both columns in descending order\ndf.sort([col(\"age\"), col(\"name\")], ascending=[False, False]).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  5|  Bob|\n# |  2|  Bob|\n# |  2|Alice|\n# +---+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def sort(\n    self,\n    cols: Union[ColumnOrName, List[ColumnOrName], None] = None,\n    ascending: Optional[Union[bool, List[bool]]] = None,\n) -&gt; DataFrame:\n    \"\"\"Sort the DataFrame by the specified columns.\n\n    Args:\n        cols: Columns to sort by. This can be:\n            - A single column name (str)\n            - A Column expression (e.g., `col(\"name\")`)\n            - A list of column names or Column expressions\n            - Column expressions may include sorting directives such as `asc(\"col\")`, `desc(\"col\")`,\n            `asc_nulls_last(\"col\")`, etc.\n            - If no columns are provided, the operation is a no-op.\n\n        ascending: A boolean or list of booleans indicating sort order.\n            - If `True`, sorts in ascending order; if `False`, descending.\n            - If a list is provided, its length must match the number of columns.\n            - Cannot be used if any of the columns use `asc()`/`desc()` expressions.\n            - If not specified and no sort expressions are used, columns will be sorted in ascending order by default.\n\n    Returns:\n        DataFrame: A new DataFrame sorted by the specified columns.\n\n    Raises:\n        ValueError:\n            - If `ascending` is provided and its length does not match `cols`\n            - If both `ascending` and column expressions like `asc()`/`desc()` are used\n        TypeError:\n            - If `cols` is not a column name, Column, or list of column names/Columns\n            - If `ascending` is not a boolean or list of booleans\n\n    Example: Sort in ascending order\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n\n        # Sort by age in ascending order\n        df.sort(asc(col(\"age\"))).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  2|Alice|\n        # |  5|  Bob|\n        # +---+-----+\n        ```\n\n    Example: Sort in descending order\n        ```python\n        # Sort by age in descending order\n        df.sort(col(\"age\").desc()).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  5|  Bob|\n        # |  2|Alice|\n        # +---+-----+\n        ```\n\n    Example: Sort with boolean ascending parameter\n        ```python\n        # Sort by age in descending order using boolean\n        df.sort(col(\"age\"), ascending=False).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  5|  Bob|\n        # |  2|Alice|\n        # +---+-----+\n        ```\n\n    Example: Multiple columns with different sort orders\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe([(2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n\n        # Sort by age descending, then name ascending\n        df.sort(desc(col(\"age\")), col(\"name\")).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  5|  Bob|\n        # |  2|Alice|\n        # |  2|  Bob|\n        # +---+-----+\n        ```\n\n    Example: Multiple columns with list of ascending strategies\n        ```python\n        # Sort both columns in descending order\n        df.sort([col(\"age\"), col(\"name\")], ascending=[False, False]).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  5|  Bob|\n        # |  2|  Bob|\n        # |  2|Alice|\n        # +---+-----+\n        ```\n    \"\"\"\n    col_args = cols\n    if cols is None:\n        return self._from_logical_plan(\n            Sort(self._logical_plan, [])\n        )\n    elif not isinstance(cols, List):\n        col_args = [cols]\n\n    # parse the ascending arguments\n    bool_ascending = []\n    using_default_ascending = False\n    if ascending is None:\n        using_default_ascending = True\n        bool_ascending = [True] * len(col_args)\n    elif isinstance(ascending, bool):\n        bool_ascending = [ascending] * len(col_args)\n    elif isinstance(ascending, List):\n        bool_ascending = ascending\n        if len(bool_ascending) != len(cols):\n            raise ValueError(\n                f\"the list length of ascending sort strategies must match the specified sort columns\"\n                f\"Got {len(cols)} column expressions and {len(bool_ascending)} ascending strategies. \"\n            )\n    else:\n        raise TypeError(\n            f\"Invalid ascending strategy type: {type(ascending)}.  Must be a boolean or list of booleans.\"\n        )\n\n    # create our list of sort expressions, for each column expression\n    # that isn't already provided as a asc()/desc() SortExpr\n    sort_exprs = []\n    for c, asc_bool in zip(col_args, bool_ascending, strict=True):\n        if isinstance(c, ColumnOrName):\n            c_expr = Column._from_col_or_name(c)._logical_expr\n        else:\n            raise TypeError(\n                f\"Invalid column type: {type(c).__name__}.  Must be a string or Column Expression.\"\n            )\n        if not isinstance(asc_bool, bool):\n            raise TypeError(\n                f\"Invalid ascending strategy type: {type(asc_bool).__name__}.  Must be a boolean.\"\n            )\n        if isinstance(c_expr, SortExpr):\n            if not using_default_ascending:\n                raise TypeError(\n                    \"Cannot specify both asc()/desc() expressions and boolean ascending strategies.\"\n                    f\"Got expression: {c_expr} and ascending argument: {bool_ascending}\"\n                )\n            sort_exprs.append(c_expr)\n        else:\n            sort_exprs.append(SortExpr(c_expr, ascending=asc_bool))\n\n    return self._from_logical_plan(\n        Sort(self._logical_plan, sort_exprs),\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.to_arrow","title":"to_arrow","text":"<pre><code>to_arrow() -&gt; pa.Table\n</code></pre> <p>Execute the DataFrame computation and return an Apache Arrow Table.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into an Apache Arrow Table with columnar memory layout optimized for analytics and zero-copy data exchange.</p> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>pa.Table: An Apache Arrow Table containing the computed results</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_arrow(self) -&gt; pa.Table:\n    \"\"\"Execute the DataFrame computation and return an Apache Arrow Table.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into an Apache Arrow Table with columnar memory layout\n    optimized for analytics and zero-copy data exchange.\n\n    Returns:\n        pa.Table: An Apache Arrow Table containing the computed results\n    \"\"\"\n    return self.collect(\"arrow\").data\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; pd.DataFrame\n</code></pre> <p>Execute the DataFrame computation and return a Pandas DataFrame.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a Pandas DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A Pandas DataFrame containing the computed results with</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"Execute the DataFrame computation and return a Pandas DataFrame.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a Pandas DataFrame.\n\n    Returns:\n        pd.DataFrame: A Pandas DataFrame containing the computed results with\n    \"\"\"\n    return self.collect(\"pandas\").data\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.to_polars","title":"to_polars","text":"<pre><code>to_polars() -&gt; pl.DataFrame\n</code></pre> <p>Execute the DataFrame computation and return the result as a Polars DataFrame.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a Polars DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pl.DataFrame: A Polars DataFrame with materialized results</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_polars(self) -&gt; pl.DataFrame:\n    \"\"\"Execute the DataFrame computation and return the result as a Polars DataFrame.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a Polars DataFrame.\n\n    Returns:\n        pl.DataFrame: A Polars DataFrame with materialized results\n    \"\"\"\n    return self.collect(\"polars\").data\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.to_pydict","title":"to_pydict","text":"<pre><code>to_pydict() -&gt; Dict[str, List[Any]]\n</code></pre> <p>Execute the DataFrame computation and return a dictionary of column arrays.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a Python dictionary where each column becomes a list of values.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, List[Any]]</code>           \u2013            <p>Dict[str, List[Any]]: A dictionary containing the computed results with: - Keys: Column names as strings - Values: Lists containing all values for each column</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_pydict(self) -&gt; Dict[str, List[Any]]:\n    \"\"\"Execute the DataFrame computation and return a dictionary of column arrays.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a Python dictionary where each column becomes a list of values.\n\n    Returns:\n        Dict[str, List[Any]]: A dictionary containing the computed results with:\n            - Keys: Column names as strings\n            - Values: Lists containing all values for each column\n    \"\"\"\n    return self.collect(\"pydict\").data\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.to_pylist","title":"to_pylist","text":"<pre><code>to_pylist() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Execute the DataFrame computation and return a list of row dictionaries.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a Python list where each element is a dictionary representing one row with column names as keys.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>List[Dict[str, Any]]: A list containing the computed results with: - Each element: A dictionary representing one row - Dictionary keys: Column names as strings - Dictionary values: Cell values in Python native types - List length equals number of rows in the result</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_pylist(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Execute the DataFrame computation and return a list of row dictionaries.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a Python list where each element is a dictionary\n    representing one row with column names as keys.\n\n    Returns:\n        List[Dict[str, Any]]: A list containing the computed results with:\n            - Each element: A dictionary representing one row\n            - Dictionary keys: Column names as strings\n            - Dictionary values: Cell values in Python native types\n            - List length equals number of rows in the result\n    \"\"\"\n    return self.collect(\"pylist\").data\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.union","title":"union","text":"<pre><code>union(other: DataFrame) -&gt; DataFrame\n</code></pre> <p>Return a new DataFrame containing the union of rows in this and another DataFrame.</p> <p>This is equivalent to UNION ALL in SQL. To remove duplicates, use drop_duplicates() after union().</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>DataFrame</code>)           \u2013            <p>Another DataFrame with the same schema.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame containing rows from both DataFrames.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the DataFrames have different schemas.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If other is not a DataFrame.</p> </li> </ul> Union two DataFrames <pre><code># Create two DataFrames\ndf1 = session.create_dataframe({\n    \"id\": [1, 2],\n    \"value\": [\"a\", \"b\"]\n})\ndf2 = session.create_dataframe({\n    \"id\": [3, 4],\n    \"value\": [\"c\", \"d\"]\n})\n\n# Union the DataFrames\ndf1.union(df2).show()\n# Output:\n# +---+-----+\n# | id|value|\n# +---+-----+\n# |  1|    a|\n# |  2|    b|\n# |  3|    c|\n# |  4|    d|\n# +---+-----+\n</code></pre> Union with duplicates <pre><code># Create DataFrames with overlapping data\ndf1 = session.create_dataframe({\n    \"id\": [1, 2],\n    \"value\": [\"a\", \"b\"]\n})\ndf2 = session.create_dataframe({\n    \"id\": [2, 3],\n    \"value\": [\"b\", \"c\"]\n})\n\n# Union with duplicates\ndf1.union(df2).show()\n# Output:\n# +---+-----+\n# | id|value|\n# +---+-----+\n# |  1|    a|\n# |  2|    b|\n# |  2|    b|\n# |  3|    c|\n# +---+-----+\n\n# Remove duplicates after union\ndf1.union(df2).drop_duplicates().show()\n# Output:\n# +---+-----+\n# | id|value|\n# +---+-----+\n# |  1|    a|\n# |  2|    b|\n# |  3|    c|\n# +---+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def union(self, other: DataFrame) -&gt; DataFrame:\n    \"\"\"Return a new DataFrame containing the union of rows in this and another DataFrame.\n\n    This is equivalent to UNION ALL in SQL. To remove duplicates, use drop_duplicates() after union().\n\n    Args:\n        other: Another DataFrame with the same schema.\n\n    Returns:\n        DataFrame: A new DataFrame containing rows from both DataFrames.\n\n    Raises:\n        ValueError: If the DataFrames have different schemas.\n        TypeError: If other is not a DataFrame.\n\n    Example: Union two DataFrames\n        ```python\n        # Create two DataFrames\n        df1 = session.create_dataframe({\n            \"id\": [1, 2],\n            \"value\": [\"a\", \"b\"]\n        })\n        df2 = session.create_dataframe({\n            \"id\": [3, 4],\n            \"value\": [\"c\", \"d\"]\n        })\n\n        # Union the DataFrames\n        df1.union(df2).show()\n        # Output:\n        # +---+-----+\n        # | id|value|\n        # +---+-----+\n        # |  1|    a|\n        # |  2|    b|\n        # |  3|    c|\n        # |  4|    d|\n        # +---+-----+\n        ```\n\n    Example: Union with duplicates\n        ```python\n        # Create DataFrames with overlapping data\n        df1 = session.create_dataframe({\n            \"id\": [1, 2],\n            \"value\": [\"a\", \"b\"]\n        })\n        df2 = session.create_dataframe({\n            \"id\": [2, 3],\n            \"value\": [\"b\", \"c\"]\n        })\n\n        # Union with duplicates\n        df1.union(df2).show()\n        # Output:\n        # +---+-----+\n        # | id|value|\n        # +---+-----+\n        # |  1|    a|\n        # |  2|    b|\n        # |  2|    b|\n        # |  3|    c|\n        # +---+-----+\n\n        # Remove duplicates after union\n        df1.union(df2).drop_duplicates().show()\n        # Output:\n        # +---+-----+\n        # | id|value|\n        # +---+-----+\n        # |  1|    a|\n        # |  2|    b|\n        # |  3|    c|\n        # +---+-----+\n        ```\n    \"\"\"\n    return self._from_logical_plan(\n        UnionLogicalPlan([self._logical_plan, other._logical_plan]),\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.unnest","title":"unnest","text":"<pre><code>unnest(*col_names: str) -&gt; DataFrame\n</code></pre> <p>Unnest the specified struct columns into separate columns.</p> <p>This operation flattens nested struct data by expanding each field of a struct into its own top-level column.</p> <p>For each specified column containing a struct: 1. Each field in the struct becomes a separate column. 2. New columns are named after the corresponding struct fields. 3. The new columns are inserted into the DataFrame in place of the original struct column. 4. The overall column order is preserved.</p> <p>Parameters:</p> <ul> <li> <code>*col_names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>One or more struct columns to unnest. Each can be a string (column name) or a Column expression.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with the specified struct columns expanded.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If any argument is not a string or Column.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If a specified column does not contain struct data.</p> </li> </ul> Unnest struct column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"id\": [1, 2],\n    \"tags\": [{\"red\": 1, \"blue\": 2}, {\"red\": 3}],\n    \"name\": [\"Alice\", \"Bob\"]\n})\n\n# Unnest the tags column\ndf.unnest(col(\"tags\")).show()\n# Output:\n# +---+---+----+-----+\n# | id| red|blue| name|\n# +---+---+----+-----+\n# |  1|  1|   2|Alice|\n# |  2|  3|null|  Bob|\n# +---+---+----+-----+\n</code></pre> Unnest multiple struct columns <pre><code># Create sample DataFrame with multiple struct columns\ndf = session.create_dataframe({\n    \"id\": [1, 2],\n    \"tags\": [{\"red\": 1, \"blue\": 2}, {\"red\": 3}],\n    \"info\": [{\"age\": 25, \"city\": \"NY\"}, {\"age\": 30, \"city\": \"LA\"}],\n    \"name\": [\"Alice\", \"Bob\"]\n})\n\n# Unnest multiple struct columns\ndf.unnest(col(\"tags\"), col(\"info\")).show()\n# Output:\n# +---+---+----+---+----+-----+\n# | id| red|blue|age|city| name|\n# +---+---+----+---+----+-----+\n# |  1|  1|   2| 25|  NY|Alice|\n# |  2|  3|null| 30|  LA|  Bob|\n# +---+---+----+---+----+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def unnest(self, *col_names: str) -&gt; DataFrame:\n    \"\"\"Unnest the specified struct columns into separate columns.\n\n    This operation flattens nested struct data by expanding each field of a struct\n    into its own top-level column.\n\n    For each specified column containing a struct:\n    1. Each field in the struct becomes a separate column.\n    2. New columns are named after the corresponding struct fields.\n    3. The new columns are inserted into the DataFrame in place of the original struct column.\n    4. The overall column order is preserved.\n\n    Args:\n        *col_names: One or more struct columns to unnest. Each can be a string (column name)\n            or a Column expression.\n\n    Returns:\n        DataFrame: A new DataFrame with the specified struct columns expanded.\n\n    Raises:\n        TypeError: If any argument is not a string or Column.\n        ValueError: If a specified column does not contain struct data.\n\n    Example: Unnest struct column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"id\": [1, 2],\n            \"tags\": [{\"red\": 1, \"blue\": 2}, {\"red\": 3}],\n            \"name\": [\"Alice\", \"Bob\"]\n        })\n\n        # Unnest the tags column\n        df.unnest(col(\"tags\")).show()\n        # Output:\n        # +---+---+----+-----+\n        # | id| red|blue| name|\n        # +---+---+----+-----+\n        # |  1|  1|   2|Alice|\n        # |  2|  3|null|  Bob|\n        # +---+---+----+-----+\n        ```\n\n    Example: Unnest multiple struct columns\n        ```python\n        # Create sample DataFrame with multiple struct columns\n        df = session.create_dataframe({\n            \"id\": [1, 2],\n            \"tags\": [{\"red\": 1, \"blue\": 2}, {\"red\": 3}],\n            \"info\": [{\"age\": 25, \"city\": \"NY\"}, {\"age\": 30, \"city\": \"LA\"}],\n            \"name\": [\"Alice\", \"Bob\"]\n        })\n\n        # Unnest multiple struct columns\n        df.unnest(col(\"tags\"), col(\"info\")).show()\n        # Output:\n        # +---+---+----+---+----+-----+\n        # | id| red|blue|age|city| name|\n        # +---+---+----+---+----+-----+\n        # |  1|  1|   2| 25|  NY|Alice|\n        # |  2|  3|null| 30|  LA|  Bob|\n        # +---+---+----+---+----+-----+\n        ```\n    \"\"\"\n    if not col_names:\n        return self\n    exprs = []\n    for c in col_names:\n        if c not in self.columns:\n            raise TypeError(f\"Column {c} not found in DataFrame.\")\n        exprs.append(col(c)._logical_expr)\n    return self._from_logical_plan(\n        Unnest(self._logical_plan, exprs),\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.where","title":"where","text":"<pre><code>where(condition: Column) -&gt; DataFrame\n</code></pre> <p>Filters rows using the given condition (alias for filter()).</p> <p>Parameters:</p> <ul> <li> <code>condition</code>               (<code>Column</code>)           \u2013            <p>A Column expression that evaluates to a boolean</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Filtered DataFrame</p> </li> </ul> See Also <p>filter(): Full documentation of filtering behavior</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def where(self, condition: Column) -&gt; DataFrame:\n    \"\"\"Filters rows using the given condition (alias for filter()).\n\n    Args:\n        condition: A Column expression that evaluates to a boolean\n\n    Returns:\n        DataFrame: Filtered DataFrame\n\n    See Also:\n        filter(): Full documentation of filtering behavior\n    \"\"\"\n    return self.filter(condition)\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.with_column","title":"with_column","text":"<pre><code>with_column(col_name: str, col: Union[Any, Column]) -&gt; DataFrame\n</code></pre> <p>Add a new column or replace an existing column.</p> <p>Parameters:</p> <ul> <li> <code>col_name</code>               (<code>str</code>)           \u2013            <p>Name of the new column</p> </li> <li> <code>col</code>               (<code>Union[Any, Column]</code>)           \u2013            <p>Column expression or value to assign to the column. If not a Column, it will be treated as a literal value.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>New DataFrame with added/replaced column</p> </li> </ul> Add literal column <pre><code># Create a DataFrame\ndf = session.create_dataframe({\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]})\n\n# Add literal column\ndf.with_column(\"constant\", lit(1)).show()\n# Output:\n# +-----+---+--------+\n# | name|age|constant|\n# +-----+---+--------+\n# |Alice| 25|       1|\n# |  Bob| 30|       1|\n# +-----+---+--------+\n</code></pre> Add computed column <pre><code># Add computed column\ndf.with_column(\"double_age\", col(\"age\") * 2).show()\n# Output:\n# +-----+---+----------+\n# | name|age|double_age|\n# +-----+---+----------+\n# |Alice| 25|        50|\n# |  Bob| 30|        60|\n# +-----+---+----------+\n</code></pre> Replace existing column <pre><code># Replace existing column\ndf.with_column(\"age\", col(\"age\") + 1).show()\n# Output:\n# +-----+---+\n# | name|age|\n# +-----+---+\n# |Alice| 26|\n# |  Bob| 31|\n# +-----+---+\n</code></pre> Add column with complex expression <pre><code># Add column with complex expression\ndf.with_column(\n    \"age_category\",\n    when(col(\"age\") &lt; 30, \"young\")\n    .when(col(\"age\") &lt; 50, \"middle\")\n    .otherwise(\"senior\")\n).show()\n# Output:\n# +-----+---+------------+\n# | name|age|age_category|\n# +-----+---+------------+\n# |Alice| 25|       young|\n# |  Bob| 30|     middle|\n# +-----+---+------------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def with_column(self, col_name: str, col: Union[Any, Column]) -&gt; DataFrame:\n    \"\"\"Add a new column or replace an existing column.\n\n    Args:\n        col_name: Name of the new column\n        col: Column expression or value to assign to the column. If not a Column,\n            it will be treated as a literal value.\n\n    Returns:\n        DataFrame: New DataFrame with added/replaced column\n\n    Example: Add literal column\n        ```python\n        # Create a DataFrame\n        df = session.create_dataframe({\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]})\n\n        # Add literal column\n        df.with_column(\"constant\", lit(1)).show()\n        # Output:\n        # +-----+---+--------+\n        # | name|age|constant|\n        # +-----+---+--------+\n        # |Alice| 25|       1|\n        # |  Bob| 30|       1|\n        # +-----+---+--------+\n        ```\n\n    Example: Add computed column\n        ```python\n        # Add computed column\n        df.with_column(\"double_age\", col(\"age\") * 2).show()\n        # Output:\n        # +-----+---+----------+\n        # | name|age|double_age|\n        # +-----+---+----------+\n        # |Alice| 25|        50|\n        # |  Bob| 30|        60|\n        # +-----+---+----------+\n        ```\n\n    Example: Replace existing column\n        ```python\n        # Replace existing column\n        df.with_column(\"age\", col(\"age\") + 1).show()\n        # Output:\n        # +-----+---+\n        # | name|age|\n        # +-----+---+\n        # |Alice| 26|\n        # |  Bob| 31|\n        # +-----+---+\n        ```\n\n    Example: Add column with complex expression\n        ```python\n        # Add column with complex expression\n        df.with_column(\n            \"age_category\",\n            when(col(\"age\") &lt; 30, \"young\")\n            .when(col(\"age\") &lt; 50, \"middle\")\n            .otherwise(\"senior\")\n        ).show()\n        # Output:\n        # +-----+---+------------+\n        # | name|age|age_category|\n        # +-----+---+------------+\n        # |Alice| 25|       young|\n        # |  Bob| 30|     middle|\n        # +-----+---+------------+\n        ```\n    \"\"\"\n    exprs = []\n    if not isinstance(col, Column):\n        col = lit(col)\n\n    for field in self.columns:\n        if field != col_name:\n            exprs.append(Column._from_column_name(field)._logical_expr)\n\n    # Add the new column with alias\n    exprs.append(col.alias(col_name)._logical_expr)\n\n    return self._from_logical_plan(\n        Projection(self._logical_plan, exprs)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrame.with_column_renamed","title":"with_column_renamed","text":"<pre><code>with_column_renamed(col_name: str, new_col_name: str) -&gt; DataFrame\n</code></pre> <p>Rename a column. No-op if the column does not exist.</p> <p>Parameters:</p> <ul> <li> <code>col_name</code>               (<code>str</code>)           \u2013            <p>Name of the column to rename.</p> </li> <li> <code>new_col_name</code>               (<code>str</code>)           \u2013            <p>New name for the column.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>New DataFrame with the column renamed.</p> </li> </ul> Rename a column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"age\": [25, 30, 35],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n})\n\n# Rename a column\ndf.with_column_renamed(\"age\", \"age_in_years\").show()\n# Output:\n# +------------+-------+\n# |age_in_years|   name|\n# +------------+-------+\n# |         25|  Alice|\n# |         30|    Bob|\n# |         35|Charlie|\n# +------------+-------+\n</code></pre> Rename multiple columns <pre><code># Rename multiple columns\ndf = (df\n    .with_column_renamed(\"age\", \"age_in_years\")\n    .with_column_renamed(\"name\", \"full_name\")\n).show()\n# Output:\n# +------------+----------+\n# |age_in_years|full_name |\n# +------------+----------+\n# |         25|     Alice|\n# |         30|       Bob|\n# |         35|   Charlie|\n# +------------+----------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def with_column_renamed(self, col_name: str, new_col_name: str) -&gt; DataFrame:\n    \"\"\"Rename a column. No-op if the column does not exist.\n\n    Args:\n        col_name: Name of the column to rename.\n        new_col_name: New name for the column.\n\n    Returns:\n        DataFrame: New DataFrame with the column renamed.\n\n    Example: Rename a column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"age\": [25, 30, 35],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n        })\n\n        # Rename a column\n        df.with_column_renamed(\"age\", \"age_in_years\").show()\n        # Output:\n        # +------------+-------+\n        # |age_in_years|   name|\n        # +------------+-------+\n        # |         25|  Alice|\n        # |         30|    Bob|\n        # |         35|Charlie|\n        # +------------+-------+\n        ```\n\n    Example: Rename multiple columns\n        ```python\n        # Rename multiple columns\n        df = (df\n            .with_column_renamed(\"age\", \"age_in_years\")\n            .with_column_renamed(\"name\", \"full_name\")\n        ).show()\n        # Output:\n        # +------------+----------+\n        # |age_in_years|full_name |\n        # +------------+----------+\n        # |         25|     Alice|\n        # |         30|       Bob|\n        # |         35|   Charlie|\n        # +------------+----------+\n        ```\n    \"\"\"\n    exprs = []\n    renamed = False\n\n    for field in self.schema.column_fields:\n        name = field.name\n        if name == col_name:\n            exprs.append(col(name).alias(new_col_name)._logical_expr)\n            renamed = True\n        else:\n            exprs.append(col(name)._logical_expr)\n\n    if not renamed:\n        return self\n\n    return self._from_logical_plan(\n        Projection(self._logical_plan, exprs)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrameReader","title":"DataFrameReader","text":"<pre><code>DataFrameReader(session_state: BaseSessionState)\n</code></pre> <p>Interface used to load a DataFrame from external storage systems.</p> <p>Similar to PySpark's DataFrameReader.</p> <p>Creates a DataFrameReader.</p> <p>Parameters:</p> <ul> <li> <code>session_state</code>               (<code>BaseSessionState</code>)           \u2013            <p>The session state to use for reading</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>csv</code>             \u2013              <p>Load a DataFrame from one or more CSV files.</p> </li> <li> <code>parquet</code>             \u2013              <p>Load a DataFrame from one or more Parquet files.</p> </li> </ul> Source code in <code>src/fenic/api/io/reader.py</code> <pre><code>def __init__(self, session_state: BaseSessionState):\n    \"\"\"Creates a DataFrameReader.\n\n    Args:\n        session_state: The session state to use for reading\n    \"\"\"\n    self._options: Dict[str, Any] = {}\n    self._session_state = session_state\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrameReader.csv","title":"csv","text":"<pre><code>csv(paths: Union[str, Path, list[Union[str, Path]]], schema: Optional[Schema] = None, merge_schemas: bool = False) -&gt; DataFrame\n</code></pre> <p>Load a DataFrame from one or more CSV files.</p> <p>Parameters:</p> <ul> <li> <code>paths</code>               (<code>Union[str, Path, list[Union[str, Path]]]</code>)           \u2013            <p>A single file path, a glob pattern (e.g., \"data/*.csv\"), or a list of paths.</p> </li> <li> <code>schema</code>               (<code>Optional[Schema]</code>, default:                   <code>None</code> )           \u2013            <p>(optional) A complete schema definition of column names and their types. Only primitive types are supported. - For e.g.:     - Schema([ColumnField(name=\"id\", data_type=IntegerType), ColumnField(name=\"name\", data_type=StringType)]) - If provided, all files must match this schema exactly\u2014all column names must be present, and values must be convertible to the specified types. Partial schemas are not allowed.</p> </li> <li> <code>merge_schemas</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to merge schemas across all files. - If True: Column names are unified across files. Missing columns are filled with nulls. Column types are inferred and widened as needed. - If False (default): Only accepts columns from the first file. Column types from the first file are inferred and applied across all files. If subsequent files do not have the same column name and order as the first file, an error is raised. - The \"first file\" is defined as:     - The first file in lexicographic order (for glob patterns), or     - The first file in the provided list (for lists of paths).</p> </li> </ul> Notes <ul> <li>The first row in each file is assumed to be a header row.</li> <li>Delimiters (e.g., comma, tab) are automatically inferred.</li> <li>You may specify either <code>schema</code> or <code>merge_schemas=True</code>, but not both.</li> <li>Any date/datetime columns are cast to strings during ingestion.</li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If both <code>schema</code> and <code>merge_schemas=True</code> are provided.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If any path does not end with <code>.csv</code>.</p> </li> <li> <code>PlanError</code>             \u2013            <p>If schemas cannot be merged or if there's a schema mismatch when merge_schemas=False.</p> </li> </ul> Read a single CSV file <pre><code>df = session.read.csv(\"file.csv\")\n</code></pre> Read multiple CSV files with schema merging <pre><code>df = session.read.csv(\"data/*.csv\", merge_schemas=True)\n</code></pre> Read CSV files with explicit schema <p><code>python df = session.read.csv(     [\"a.csv\", \"b.csv\"],     schema=Schema([         ColumnField(name=\"id\", data_type=IntegerType),         ColumnField(name=\"value\", data_type=FloatType)     ]) )</code></p> Source code in <code>src/fenic/api/io/reader.py</code> <pre><code>def csv(\n    self,\n    paths: Union[str, Path, list[Union[str, Path]]],\n    schema: Optional[Schema] = None,\n    merge_schemas: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Load a DataFrame from one or more CSV files.\n\n    Args:\n        paths: A single file path, a glob pattern (e.g., \"data/*.csv\"), or a list of paths.\n        schema: (optional) A complete schema definition of column names and their types. Only primitive types are supported.\n            - For e.g.:\n                - Schema([ColumnField(name=\"id\", data_type=IntegerType), ColumnField(name=\"name\", data_type=StringType)])\n            - If provided, all files must match this schema exactly\u2014all column names must be present, and values must be\n            convertible to the specified types. Partial schemas are not allowed.\n        merge_schemas: Whether to merge schemas across all files.\n            - If True: Column names are unified across files. Missing columns are filled with nulls. Column types are\n            inferred and widened as needed.\n            - If False (default): Only accepts columns from the first file. Column types from the first file are\n            inferred and applied across all files. If subsequent files do not have the same column name and order as the first file, an error is raised.\n            - The \"first file\" is defined as:\n                - The first file in lexicographic order (for glob patterns), or\n                - The first file in the provided list (for lists of paths).\n\n    Notes:\n        - The first row in each file is assumed to be a header row.\n        - Delimiters (e.g., comma, tab) are automatically inferred.\n        - You may specify either `schema` or `merge_schemas=True`, but not both.\n        - Any date/datetime columns are cast to strings during ingestion.\n\n    Raises:\n        ValidationError: If both `schema` and `merge_schemas=True` are provided.\n        ValidationError: If any path does not end with `.csv`.\n        PlanError: If schemas cannot be merged or if there's a schema mismatch when merge_schemas=False.\n\n    Example: Read a single CSV file\n        ```python\n        df = session.read.csv(\"file.csv\")\n        ```\n\n    Example: Read multiple CSV files with schema merging\n        ```python\n        df = session.read.csv(\"data/*.csv\", merge_schemas=True)\n        ```\n\n    Example: Read CSV files with explicit schema\n        ```python\n        df = session.read.csv(\n            [\"a.csv\", \"b.csv\"],\n            schema=Schema([\n                ColumnField(name=\"id\", data_type=IntegerType),\n                ColumnField(name=\"value\", data_type=FloatType)\n            ])\n        )            ```\n    \"\"\"\n    if schema is not None and merge_schemas:\n        raise ValidationError(\n            \"Cannot specify both 'schema' and 'merge_schemas=True' - these options conflict. \"\n            \"Choose one approach: \"\n            \"1) Use 'schema' to enforce a specific schema: csv(paths, schema=your_schema), \"\n            \"2) Use 'merge_schemas=True' to automatically merge schemas: csv(paths, merge_schemas=True), \"\n            \"3) Use neither to inherit schema from the first file: csv(paths)\"\n        )\n    if schema is not None:\n        for col_field in schema.column_fields:\n            if not isinstance(\n                col_field.data_type,\n                _PrimitiveType,\n            ):\n                raise ValidationError(\n                    f\"CSV files only support primitive data types in schema definitions. \"\n                    f\"Column '{col_field.name}' has type {type(col_field.data_type).__name__}, but CSV schemas must use: \"\n                    f\"IntegerType, FloatType, DoubleType, BooleanType, or StringType. \"\n                    f\"Example: Schema([ColumnField(name='id', data_type=IntegerType), ColumnField(name='name', data_type=StringType)])\"\n                )\n    options = {\n        \"schema\": schema,\n        \"merge_schemas\": merge_schemas,\n    }\n    return self._read_file(\n        paths, file_format=\"csv\", file_extension=\".csv\", **options\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrameReader.parquet","title":"parquet","text":"<pre><code>parquet(paths: Union[str, Path, list[Union[str, Path]]], merge_schemas: bool = False) -&gt; DataFrame\n</code></pre> <p>Load a DataFrame from one or more Parquet files.</p> <p>Parameters:</p> <ul> <li> <code>paths</code>               (<code>Union[str, Path, list[Union[str, Path]]]</code>)           \u2013            <p>A single file path, a glob pattern (e.g., \"data/*.parquet\"), or a list of paths.</p> </li> <li> <code>merge_schemas</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, infers and merges schemas across all files. Missing columns are filled with nulls, and differing types are widened to a common supertype.</p> </li> </ul> Behavior <ul> <li>If <code>merge_schemas=False</code> (default), all files must match the schema of the first file exactly. Subsequent files must contain all columns from the first file with compatible data types. If any column is missing or has incompatible types, an error is raised.</li> <li>If <code>merge_schemas=True</code>, column names are unified across all files, and data types are automatically widened to accommodate all values.</li> <li>The \"first file\" is defined as:<ul> <li>The first file in lexicographic order (for glob patterns), or</li> <li>The first file in the provided list (for lists of paths).</li> </ul> </li> </ul> Notes <ul> <li>Date and datetime columns are cast to strings during ingestion.</li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If any file does not have a <code>.parquet</code> extension.</p> </li> <li> <code>PlanError</code>             \u2013            <p>If schemas cannot be merged or if there's a schema mismatch when merge_schemas=False.</p> </li> </ul> Read a single Parquet file <pre><code>df = session.read.parquet(\"file.parquet\")\n</code></pre> Read multiple Parquet files <pre><code>df = session.read.parquet(\"data/*.parquet\")\n</code></pre> Read Parquet files with schema merging <pre><code>df = session.read.parquet([\"a.parquet\", \"b.parquet\"], merge_schemas=True)\n</code></pre> Source code in <code>src/fenic/api/io/reader.py</code> <pre><code>def parquet(\n    self,\n    paths: Union[str, Path, list[Union[str, Path]]],\n    merge_schemas: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Load a DataFrame from one or more Parquet files.\n\n    Args:\n        paths: A single file path, a glob pattern (e.g., \"data/*.parquet\"), or a list of paths.\n        merge_schemas: If True, infers and merges schemas across all files.\n            Missing columns are filled with nulls, and differing types are widened to a common supertype.\n\n    Behavior:\n        - If `merge_schemas=False` (default), all files must match the schema of the first file exactly.\n        Subsequent files must contain all columns from the first file with compatible data types.\n        If any column is missing or has incompatible types, an error is raised.\n        - If `merge_schemas=True`, column names are unified across all files, and data types are automatically\n        widened to accommodate all values.\n        - The \"first file\" is defined as:\n            - The first file in lexicographic order (for glob patterns), or\n            - The first file in the provided list (for lists of paths).\n\n    Notes:\n        - Date and datetime columns are cast to strings during ingestion.\n\n    Raises:\n        ValidationError: If any file does not have a `.parquet` extension.\n        PlanError: If schemas cannot be merged or if there's a schema mismatch when merge_schemas=False.\n\n    Example: Read a single Parquet file\n        ```python\n        df = session.read.parquet(\"file.parquet\")\n        ```\n\n    Example: Read multiple Parquet files\n        ```python\n        df = session.read.parquet(\"data/*.parquet\")\n        ```\n\n    Example: Read Parquet files with schema merging\n        ```python\n        df = session.read.parquet([\"a.parquet\", \"b.parquet\"], merge_schemas=True)\n        ```\n    \"\"\"\n    options = {\n        \"merge_schemas\": merge_schemas,\n    }\n    return self._read_file(\n        paths, file_format=\"parquet\", file_extension=\".parquet\", **options\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrameWriter","title":"DataFrameWriter","text":"<pre><code>DataFrameWriter(dataframe: DataFrame)\n</code></pre> <p>Interface used to write a DataFrame to external storage systems.</p> <p>Similar to PySpark's DataFrameWriter.</p> <p>Initialize a DataFrameWriter.</p> <p>Parameters:</p> <ul> <li> <code>dataframe</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to write.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>csv</code>             \u2013              <p>Saves the content of the DataFrame as a single CSV file with comma as the delimiter and headers in the first row.</p> </li> <li> <code>parquet</code>             \u2013              <p>Saves the content of the DataFrame as a single Parquet file.</p> </li> <li> <code>save_as_table</code>             \u2013              <p>Saves the content of the DataFrame as the specified table.</p> </li> </ul> Source code in <code>src/fenic/api/io/writer.py</code> <pre><code>def __init__(self, dataframe: DataFrame):\n    \"\"\"Initialize a DataFrameWriter.\n\n    Args:\n        dataframe: The DataFrame to write.\n    \"\"\"\n    self._dataframe = dataframe\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrameWriter.csv","title":"csv","text":"<pre><code>csv(file_path: Union[str, Path], mode: Literal['error', 'overwrite', 'ignore'] = 'overwrite') -&gt; QueryMetrics\n</code></pre> <p>Saves the content of the DataFrame as a single CSV file with comma as the delimiter and headers in the first row.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path to save the CSV file to</p> </li> <li> <code>mode</code>               (<code>Literal['error', 'overwrite', 'ignore']</code>, default:                   <code>'overwrite'</code> )           \u2013            <p>Write mode. Default is \"overwrite\".  - error: Raises an error if file exists  - overwrite: Overwrites the file if it exists  - ignore: Silently ignores operation if file exists</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>QueryMetrics</code> (              <code>QueryMetrics</code> )          \u2013            <p>The query metrics</p> </li> </ul> Save with overwrite mode (default) <pre><code>df.write.csv(\"output.csv\")  # Overwrites if exists\n</code></pre> Save with error mode <pre><code>df.write.csv(\"output.csv\", mode=\"error\")  # Raises error if exists\n</code></pre> Save with ignore mode <pre><code>df.write.csv(\"output.csv\", mode=\"ignore\")  # Skips if exists\n</code></pre> Source code in <code>src/fenic/api/io/writer.py</code> <pre><code>def csv(\n    self,\n    file_path: Union[str, Path],\n    mode: Literal[\"error\", \"overwrite\", \"ignore\"] = \"overwrite\",\n) -&gt; QueryMetrics:\n    \"\"\"Saves the content of the DataFrame as a single CSV file with comma as the delimiter and headers in the first row.\n\n    Args:\n        file_path: Path to save the CSV file to\n        mode: Write mode. Default is \"overwrite\".\n             - error: Raises an error if file exists\n             - overwrite: Overwrites the file if it exists\n             - ignore: Silently ignores operation if file exists\n\n    Returns:\n        QueryMetrics: The query metrics\n\n    Example: Save with overwrite mode (default)\n        ```python\n        df.write.csv(\"output.csv\")  # Overwrites if exists\n        ```\n\n    Example: Save with error mode\n        ```python\n        df.write.csv(\"output.csv\", mode=\"error\")  # Raises error if exists\n        ```\n\n    Example: Save with ignore mode\n        ```python\n        df.write.csv(\"output.csv\", mode=\"ignore\")  # Skips if exists\n        ```\n    \"\"\"\n    file_path = str(file_path)\n    if not file_path.endswith(\".csv\"):\n        raise ValidationError(\n            f\"CSV writer requires a '.csv' file extension. \"\n            f\"Your path '{file_path}' is missing the extension.\"\n        )\n\n    sink_plan = FileSink(\n        child=self._dataframe._logical_plan,\n        sink_type=\"csv\",\n        path=file_path,\n        mode=mode,\n    )\n\n    metrics = self._dataframe._logical_plan.session_state.execution.save_to_file(\n        sink_plan, file_path=file_path, mode=mode\n    )\n    logger.info(metrics.get_summary())\n    return metrics\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrameWriter.parquet","title":"parquet","text":"<pre><code>parquet(file_path: Union[str, Path], mode: Literal['error', 'overwrite', 'ignore'] = 'overwrite') -&gt; QueryMetrics\n</code></pre> <p>Saves the content of the DataFrame as a single Parquet file.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path to save the Parquet file to</p> </li> <li> <code>mode</code>               (<code>Literal['error', 'overwrite', 'ignore']</code>, default:                   <code>'overwrite'</code> )           \u2013            <p>Write mode. Default is \"overwrite\".  - error: Raises an error if file exists  - overwrite: Overwrites the file if it exists  - ignore: Silently ignores operation if file exists</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>QueryMetrics</code> (              <code>QueryMetrics</code> )          \u2013            <p>The query metrics</p> </li> </ul> Save with overwrite mode (default) <pre><code>df.write.parquet(\"output.parquet\")  # Overwrites if exists\n</code></pre> Save with error mode <pre><code>df.write.parquet(\"output.parquet\", mode=\"error\")  # Raises error if exists\n</code></pre> Save with ignore mode <pre><code>df.write.parquet(\"output.parquet\", mode=\"ignore\")  # Skips if exists\n</code></pre> Source code in <code>src/fenic/api/io/writer.py</code> <pre><code>def parquet(\n    self,\n    file_path: Union[str, Path],\n    mode: Literal[\"error\", \"overwrite\", \"ignore\"] = \"overwrite\",\n) -&gt; QueryMetrics:\n    \"\"\"Saves the content of the DataFrame as a single Parquet file.\n\n    Args:\n        file_path: Path to save the Parquet file to\n        mode: Write mode. Default is \"overwrite\".\n             - error: Raises an error if file exists\n             - overwrite: Overwrites the file if it exists\n             - ignore: Silently ignores operation if file exists\n\n    Returns:\n        QueryMetrics: The query metrics\n\n    Example: Save with overwrite mode (default)\n        ```python\n        df.write.parquet(\"output.parquet\")  # Overwrites if exists\n        ```\n\n    Example: Save with error mode\n        ```python\n        df.write.parquet(\"output.parquet\", mode=\"error\")  # Raises error if exists\n        ```\n\n    Example: Save with ignore mode\n        ```python\n        df.write.parquet(\"output.parquet\", mode=\"ignore\")  # Skips if exists\n        ```\n    \"\"\"\n    file_path = str(file_path)\n    if not file_path.endswith(\".parquet\"):\n        raise ValidationError(\n            f\"Parquet writer requires a '.parquet' file extension. \"\n            f\"Your path '{file_path}' is missing the extension.\"\n        )\n\n    sink_plan = FileSink(\n        child=self._dataframe._logical_plan,\n        sink_type=\"parquet\",\n        path=file_path,\n        mode=mode,\n    )\n\n    metrics = self._dataframe._logical_plan.session_state.execution.save_to_file(\n        sink_plan, file_path=file_path, mode=mode\n    )\n    logger.info(metrics.get_summary())\n    return metrics\n</code></pre>"},{"location":"reference/fenic/#fenic.DataFrameWriter.save_as_table","title":"save_as_table","text":"<pre><code>save_as_table(table_name: str, mode: Literal['error', 'append', 'overwrite', 'ignore'] = 'error') -&gt; QueryMetrics\n</code></pre> <p>Saves the content of the DataFrame as the specified table.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Name of the table to save to</p> </li> <li> <code>mode</code>               (<code>Literal['error', 'append', 'overwrite', 'ignore']</code>, default:                   <code>'error'</code> )           \u2013            <p>Write mode. Default is \"error\".  - error: Raises an error if table exists  - append: Appends data to table if it exists  - overwrite: Overwrites existing table  - ignore: Silently ignores operation if table exists</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>QueryMetrics</code> (              <code>QueryMetrics</code> )          \u2013            <p>The query metrics</p> </li> </ul> Save with error mode (default) <pre><code>df.write.save_as_table(\"my_table\")  # Raises error if table exists\n</code></pre> Save with append mode <pre><code>df.write.save_as_table(\"my_table\", mode=\"append\")  # Adds to existing table\n</code></pre> Save with overwrite mode <pre><code>df.write.save_as_table(\"my_table\", mode=\"overwrite\")  # Replaces existing table\n</code></pre> Source code in <code>src/fenic/api/io/writer.py</code> <pre><code>def save_as_table(\n    self,\n    table_name: str,\n    mode: Literal[\"error\", \"append\", \"overwrite\", \"ignore\"] = \"error\",\n) -&gt; QueryMetrics:\n    \"\"\"Saves the content of the DataFrame as the specified table.\n\n    Args:\n        table_name: Name of the table to save to\n        mode: Write mode. Default is \"error\".\n             - error: Raises an error if table exists\n             - append: Appends data to table if it exists\n             - overwrite: Overwrites existing table\n             - ignore: Silently ignores operation if table exists\n\n    Returns:\n        QueryMetrics: The query metrics\n\n    Example: Save with error mode (default)\n        ```python\n        df.write.save_as_table(\"my_table\")  # Raises error if table exists\n        ```\n\n    Example: Save with append mode\n        ```python\n        df.write.save_as_table(\"my_table\", mode=\"append\")  # Adds to existing table\n        ```\n\n    Example: Save with overwrite mode\n        ```python\n        df.write.save_as_table(\"my_table\", mode=\"overwrite\")  # Replaces existing table\n        ```\n    \"\"\"\n    sink_plan = TableSink(\n        child=self._dataframe._logical_plan, table_name=table_name, mode=mode\n    )\n\n    metrics = self._dataframe._logical_plan.session_state.execution.save_as_table(\n        sink_plan, table_name=table_name, mode=mode\n    )\n    logger.info(metrics.get_summary())\n    return metrics\n</code></pre>"},{"location":"reference/fenic/#fenic.DataType","title":"DataType","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all data types.</p> <p>You won't instantiate this class directly. Instead, use one of the concrete types like <code>StringType</code>, <code>ArrayType</code>, or <code>StructType</code>.</p> <p>Used for casting, type validation, and schema inference in the DataFrame API.</p>"},{"location":"reference/fenic/#fenic.DocumentPathType","title":"DocumentPathType","text":"<p>               Bases: <code>_StringBackedType</code></p> <p>Represents a string containing a a document's local (file system) or remote (URL) path.</p>"},{"location":"reference/fenic/#fenic.EmbeddingType","title":"EmbeddingType","text":"<p>               Bases: <code>DataType</code></p> <p>A type representing a fixed-length embedding vector.</p> <p>Attributes:</p> <ul> <li> <code>dimensions</code>               (<code>int</code>)           \u2013            <p>The number of dimensions in the embedding vector.</p> </li> <li> <code>embedding_model</code>               (<code>str</code>)           \u2013            <p>Name of the model used to generate the embedding.</p> </li> </ul> Create an embedding type for text-embedding-3-small <pre><code>EmbeddingType(384, embedding_model=\"text-embedding-3-small\")\n</code></pre>"},{"location":"reference/fenic/#fenic.ExtractSchema","title":"ExtractSchema","text":"<p>Represents a structured extraction schema.</p> <p>An extract schema contains a collection of named fields with descriptions that define what information should be extracted into each field.</p> <p>Methods:</p> <ul> <li> <code>field_names</code>             \u2013              <p>Get a list of all field names in the schema.</p> </li> </ul>"},{"location":"reference/fenic/#fenic.ExtractSchema.field_names","title":"field_names","text":"<pre><code>field_names() -&gt; List[str]\n</code></pre> <p>Get a list of all field names in the schema.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>A list of strings containing the names of all fields in the schema.</p> </li> </ul> Source code in <code>src/fenic/core/types/extract_schema.py</code> <pre><code>def field_names(self) -&gt; List[str]:\n    \"\"\"Get a list of all field names in the schema.\n\n    Returns:\n        A list of strings containing the names of all fields in the schema.\n    \"\"\"\n    return [field.name for field in self.struct_fields]\n</code></pre>"},{"location":"reference/fenic/#fenic.ExtractSchemaField","title":"ExtractSchemaField","text":"<pre><code>ExtractSchemaField(name: str, data_type: Union[DataType, ExtractSchemaList, ExtractSchema], description: str)\n</code></pre> <p>Represents a field within an structured extraction schema.</p> <p>An extract schema field has a name, a data type, and a required description that explains what information should be extracted into this field.</p> <p>Initialize an ExtractSchemaField.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the field.</p> </li> <li> <code>data_type</code>               (<code>Union[DataType, ExtractSchemaList, ExtractSchema]</code>)           \u2013            <p>The data type of the field. Must be either a primitive DataType, ExtractSchemaList, or ExtractSchema.</p> </li> <li> <code>description</code>               (<code>str</code>)           \u2013            <p>A description of what information should be extracted into this field.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If data_type is a non-primitive DataType.</p> </li> </ul> Source code in <code>src/fenic/core/types/extract_schema.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    data_type: Union[DataType, ExtractSchemaList, ExtractSchema],\n    description: str,\n):\n    \"\"\"Initialize an ExtractSchemaField.\n\n    Args:\n        name: The name of the field.\n        data_type: The data type of the field. Must be either a primitive DataType,\n            ExtractSchemaList, or ExtractSchema.\n        description: A description of what information should be extracted into this field.\n\n    Raises:\n        ValueError: If data_type is a non-primitive DataType.\n    \"\"\"\n    self.name = name\n    if isinstance(data_type, DataType) and not isinstance(\n        data_type, _PrimitiveType\n    ):\n        raise ValueError(\n            f\"Invalid data type: {data_type}. Only primitive types are supported directly. \"\n            f\"For complex types, please use ExtractSchemaList or ExtractSchema instead.\"\n        )\n    self.data_type = data_type\n    self.description = description\n</code></pre>"},{"location":"reference/fenic/#fenic.ExtractSchemaList","title":"ExtractSchemaList","text":"<pre><code>ExtractSchemaList(element_type: Union[DataType, ExtractSchema])\n</code></pre> <p>Represents a list data type for structured extraction schema definitions.</p> <p>A schema list contains elements of a specific data type and is used for defining array-like structures in structured extraction schemas.</p> <p>Initialize an ExtractSchemaList.</p> <p>Parameters:</p> <ul> <li> <code>element_type</code>               (<code>Union[DataType, ExtractSchema]</code>)           \u2013            <p>The data type of elements in the list. Must be either a primitive DataType or another ExtractSchema.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If element_type is a non-primitive DataType.</p> </li> </ul> Source code in <code>src/fenic/core/types/extract_schema.py</code> <pre><code>def __init__(\n    self,\n    element_type: Union[DataType, ExtractSchema],\n):\n    \"\"\"Initialize an ExtractSchemaList.\n\n    Args:\n        element_type: The data type of elements in the list. Must be either a primitive\n            DataType or another ExtractSchema.\n\n    Raises:\n        ValueError: If element_type is a non-primitive DataType.\n    \"\"\"\n    if isinstance(element_type, DataType) and not isinstance(\n        element_type, _PrimitiveType\n    ):\n        raise ValueError(\n            f\"Invalid element type: {element_type}. Only primitive types are supported directly. \"\n            f\"For complex types, please use ExtractSchemaList or ExtractSchema instead.\"\n        )\n    self.element_type = element_type\n</code></pre>"},{"location":"reference/fenic/#fenic.GoogleGLAModelConfig","title":"GoogleGLAModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Google GenerativeLAnguage (GLA) models.</p> <p>This class defines the configuration settings for models available in Google Developer AI Studio, including model selection and rate limiting parameters. These models are accessible using a GEMINI_API_KEY environment variable.</p>"},{"location":"reference/fenic/#fenic.GroupedData","title":"GroupedData","text":"<pre><code>GroupedData(df: DataFrame, by: Optional[List[ColumnOrName]] = None)\n</code></pre> <p>               Bases: <code>BaseGroupedData</code></p> <p>Methods for aggregations on a grouped DataFrame.</p> <p>Initialize grouped data.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to group.</p> </li> <li> <code>by</code>               (<code>Optional[List[ColumnOrName]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of columns to group by.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>agg</code>             \u2013              <p>Compute aggregations on grouped data and return the result as a DataFrame.</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/grouped_data.py</code> <pre><code>def __init__(self, df: DataFrame, by: Optional[List[ColumnOrName]] = None):\n    \"\"\"Initialize grouped data.\n\n    Args:\n        df: The DataFrame to group.\n        by: Optional list of columns to group by.\n    \"\"\"\n    super().__init__(df)\n    self._by: List[Column] = []\n    for c in by or []:\n        if isinstance(c, str):\n            self._by.append(col(c))\n        elif isinstance(c, Column):\n            # Allow any expression except literals\n            if isinstance(c._logical_expr, LiteralExpr):\n                raise ValueError(f\"Cannot group by literal value: {c}\")\n            self._by.append(c)\n        else:\n            raise TypeError(\n                f\"Group by expressions must be string or Column, got {type(c)}\"\n            )\n    self._by_exprs = [c._logical_expr for c in self._by]\n</code></pre>"},{"location":"reference/fenic/#fenic.GroupedData.agg","title":"agg","text":"<pre><code>agg(*exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame\n</code></pre> <p>Compute aggregations on grouped data and return the result as a DataFrame.</p> <p>This method applies aggregate functions to the grouped data.</p> <p>Parameters:</p> <ul> <li> <code>*exprs</code>               (<code>Union[Column, Dict[str, str]]</code>, default:                   <code>()</code> )           \u2013            <p>Aggregation expressions. Can be:</p> <ul> <li>Column expressions with aggregate functions (e.g., <code>count(\"*\")</code>, <code>sum(\"amount\")</code>)</li> <li>A dictionary mapping column names to aggregate function names (e.g., <code>{\"amount\": \"sum\", \"age\": \"avg\"}</code>)</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with one row per group and columns for group keys and aggregated values</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If arguments are not Column expressions or a dictionary</p> </li> <li> <code>ValueError</code>             \u2013            <p>If dictionary values are not valid aggregate function names</p> </li> </ul> Count employees by department <pre><code># Group by department and count employees\ndf.group_by(\"department\").agg(count(\"*\").alias(\"employee_count\"))\n</code></pre> Multiple aggregations <pre><code># Multiple aggregations\ndf.group_by(\"department\").agg(\n    count(\"*\").alias(\"employee_count\"),\n    avg(\"salary\").alias(\"avg_salary\"),\n    max(\"age\").alias(\"max_age\")\n)\n</code></pre> Dictionary style aggregations <pre><code># Dictionary style for simple aggregations\ndf.group_by(\"department\", \"location\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n</code></pre> Source code in <code>src/fenic/api/dataframe/grouped_data.py</code> <pre><code>def agg(self, *exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame:\n    \"\"\"Compute aggregations on grouped data and return the result as a DataFrame.\n\n    This method applies aggregate functions to the grouped data.\n\n    Args:\n        *exprs: Aggregation expressions. Can be:\n\n            - Column expressions with aggregate functions (e.g., `count(\"*\")`, `sum(\"amount\")`)\n            - A dictionary mapping column names to aggregate function names (e.g., `{\"amount\": \"sum\", \"age\": \"avg\"}`)\n\n    Returns:\n        DataFrame: A new DataFrame with one row per group and columns for group keys and aggregated values\n\n    Raises:\n        ValueError: If arguments are not Column expressions or a dictionary\n        ValueError: If dictionary values are not valid aggregate function names\n\n    Example: Count employees by department\n        ```python\n        # Group by department and count employees\n        df.group_by(\"department\").agg(count(\"*\").alias(\"employee_count\"))\n        ```\n\n    Example: Multiple aggregations\n        ```python\n        # Multiple aggregations\n        df.group_by(\"department\").agg(\n            count(\"*\").alias(\"employee_count\"),\n            avg(\"salary\").alias(\"avg_salary\"),\n            max(\"age\").alias(\"max_age\")\n        )\n        ```\n\n    Example: Dictionary style aggregations\n        ```python\n        # Dictionary style for simple aggregations\n        df.group_by(\"department\", \"location\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n        ```\n    \"\"\"\n    self._validate_agg_exprs(*exprs)\n    if len(exprs) == 1 and isinstance(exprs[0], dict):\n        agg_dict = exprs[0]\n        return self.agg(*self._process_agg_dict(agg_dict))\n\n    agg_exprs = self._process_agg_exprs(exprs)\n    return self._df._from_logical_plan(\n        Aggregate(self._df._logical_plan, self._by_exprs, agg_exprs),\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.JoinExample","title":"JoinExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single semantic example for semantic join operations.</p> <p>Join examples demonstrate the evaluation of two input strings across different datasets against a specific condition, used in a semantic.join operation.</p>"},{"location":"reference/fenic/#fenic.JoinExampleCollection","title":"JoinExampleCollection","text":"<pre><code>JoinExampleCollection(examples: List[ExampleType] = None)\n</code></pre> <p>               Bases: <code>BaseExampleCollection[JoinExample]</code></p> <p>Collection of examples for semantic join operations.</p> <p>Methods:</p> <ul> <li> <code>from_polars</code>             \u2013              <p>Create collection from a Polars DataFrame. Must have 'left', 'right', and 'output' columns.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def __init__(self, examples: List[ExampleType] = None):\n    \"\"\"Initialize a collection of semantic examples.\n\n    Args:\n        examples: Optional list of examples to add to the collection. Each example\n            will be processed through create_example() to ensure proper formatting\n            and validation.\n\n    Note:\n        The examples list is initialized as empty if no examples are provided.\n        Each example in the provided list will be processed through create_example()\n        to ensure proper formatting and validation.\n    \"\"\"\n    self.examples: List[ExampleType] = []\n    if examples:\n        for example in examples:\n            self.create_example(example)\n</code></pre>"},{"location":"reference/fenic/#fenic.JoinExampleCollection.from_polars","title":"from_polars  <code>classmethod</code>","text":"<pre><code>from_polars(df: DataFrame) -&gt; JoinExampleCollection\n</code></pre> <p>Create collection from a Polars DataFrame. Must have 'left', 'right', and 'output' columns.</p> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: pl.DataFrame) -&gt; JoinExampleCollection:\n    \"\"\"Create collection from a Polars DataFrame. Must have 'left', 'right', and 'output' columns.\"\"\"\n    collection = cls()\n\n    required_columns = [\n        EXAMPLE_LEFT_KEY,\n        EXAMPLE_RIGHT_KEY,\n        EXAMPLE_OUTPUT_KEY,\n    ]\n    for col in required_columns:\n        if col not in df.columns:\n            raise InvalidExampleCollectionError(\n                f\"Join Examples DataFrame missing required '{col}' column\"\n            )\n\n    for row in df.iter_rows(named=True):\n        for col in required_columns:\n            if row[col] is None:\n                raise InvalidExampleCollectionError(\n                    f\"Join Examples DataFrame contains null values in '{col}' column\"\n                )\n\n        example = JoinExample(\n            left=row[EXAMPLE_LEFT_KEY],\n            right=row[EXAMPLE_RIGHT_KEY],\n            output=row[EXAMPLE_OUTPUT_KEY],\n        )\n        collection.create_example(example)\n\n    return collection\n</code></pre>"},{"location":"reference/fenic/#fenic.LMMetrics","title":"LMMetrics  <code>dataclass</code>","text":"<pre><code>LMMetrics(num_uncached_input_tokens: int = 0, num_cached_input_tokens: int = 0, num_output_tokens: int = 0, cost: float = 0.0, num_requests: int = 0)\n</code></pre> <p>Tracks language model usage metrics including token counts and costs.</p> <p>Attributes:</p> <ul> <li> <code>num_uncached_input_tokens</code>               (<code>int</code>)           \u2013            <p>Number of uncached tokens in the prompt/input</p> </li> <li> <code>num_cached_input_tokens</code>               (<code>int</code>)           \u2013            <p>Number of cached tokens in the prompt/input,</p> </li> <li> <code>num_output_tokens</code>               (<code>int</code>)           \u2013            <p>Number of tokens in the completion/output</p> </li> <li> <code>cost</code>               (<code>float</code>)           \u2013            <p>Total cost in USD for the LM API call</p> </li> </ul>"},{"location":"reference/fenic/#fenic.Lineage","title":"Lineage","text":"<pre><code>Lineage(lineage: BaseLineage)\n</code></pre> <p>Query interface for tracing data lineage through a query plan.</p> <p>This class allows you to navigate through the query plan both forwards and backwards, tracing how specific rows are transformed through each operation.</p> Example <pre><code># Create a lineage query starting from the root\nquery = LineageQuery(lineage, session.execution)\n\n# Or start from a specific source\nquery.start_from_source(\"my_table\")\n\n# Trace rows backwards through a transformation\nresult = query.backward([\"uuid1\", \"uuid2\"])\n\n# Trace rows forward to see their outputs\nresult = query.forward([\"uuid3\", \"uuid4\"])\n</code></pre> <p>Initialize a Lineage instance.</p> <p>Parameters:</p> <ul> <li> <code>lineage</code>               (<code>BaseLineage</code>)           \u2013            <p>The underlying lineage implementation.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>backwards</code>             \u2013              <p>Trace rows backwards to see which input rows produced them.</p> </li> <li> <code>forwards</code>             \u2013              <p>Trace rows forward to see how they are transformed by the next operation.</p> </li> <li> <code>get_result_df</code>             \u2013              <p>Get the result of the query as a Polars DataFrame.</p> </li> <li> <code>get_source_df</code>             \u2013              <p>Get a query source by name as a Polars DataFrame.</p> </li> <li> <code>get_source_names</code>             \u2013              <p>Get the names of all sources in the query plan. Used to determine where to start the lineage traversal.</p> </li> <li> <code>show</code>             \u2013              <p>Print the operator tree of the query.</p> </li> <li> <code>skip_backwards</code>             \u2013              <p>[Not Implemented] Trace rows backwards through multiple operations at once.</p> </li> <li> <code>skip_forwards</code>             \u2013              <p>[Not Implemented] Trace rows forward through multiple operations at once.</p> </li> <li> <code>start_from_source</code>             \u2013              <p>Set the current position to a specific source in the query plan.</p> </li> </ul> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>def __init__(self, lineage: BaseLineage):\n    \"\"\"Initialize a Lineage instance.\n\n    Args:\n        lineage: The underlying lineage implementation.\n    \"\"\"\n    self.lineage = lineage\n</code></pre>"},{"location":"reference/fenic/#fenic.Lineage.backwards","title":"backwards","text":"<pre><code>backwards(ids: List[str], branch_side: Optional[BranchSide] = None) -&gt; pl.DataFrame\n</code></pre> <p>Trace rows backwards to see which input rows produced them.</p> <p>Parameters:</p> <ul> <li> <code>ids</code>               (<code>List[str]</code>)           \u2013            <p>List of UUIDs identifying the rows to trace back</p> </li> <li> <code>branch_side</code>               (<code>Optional[BranchSide]</code>, default:                   <code>None</code> )           \u2013            <p>For operators with multiple inputs (like joins), specify which input to trace (\"left\" or \"right\"). Not needed for single-input operations.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame containing the source rows that produced the specified outputs</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If invalid ids format or incorrect branch_side specification</p> </li> </ul> Example <pre><code># Simple backward trace\nsource_rows = query.backward([\"result_uuid1\"])\n\n# Trace back through a join\nleft_rows = query.backward([\"join_uuid1\"], branch_side=\"left\")\nright_rows = query.backward([\"join_uuid1\"], branch_side=\"right\")\n</code></pre> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef backwards(\n    self, ids: List[str], branch_side: Optional[BranchSide] = None\n) -&gt; pl.DataFrame:\n    \"\"\"Trace rows backwards to see which input rows produced them.\n\n    Args:\n        ids: List of UUIDs identifying the rows to trace back\n        branch_side: For operators with multiple inputs (like joins), specify which\n            input to trace (\"left\" or \"right\"). Not needed for single-input operations.\n\n    Returns:\n        DataFrame containing the source rows that produced the specified outputs\n\n    Raises:\n        ValueError: If invalid ids format or incorrect branch_side specification\n\n    Example:\n        ```python\n        # Simple backward trace\n        source_rows = query.backward([\"result_uuid1\"])\n\n        # Trace back through a join\n        left_rows = query.backward([\"join_uuid1\"], branch_side=\"left\")\n        right_rows = query.backward([\"join_uuid1\"], branch_side=\"right\")\n        ```\n    \"\"\"\n    return self.lineage.backwards(ids, branch_side)\n</code></pre>"},{"location":"reference/fenic/#fenic.Lineage.forwards","title":"forwards","text":"<pre><code>forwards(row_ids: List[str]) -&gt; pl.DataFrame\n</code></pre> <p>Trace rows forward to see how they are transformed by the next operation.</p> <p>Parameters:</p> <ul> <li> <code>row_ids</code>               (<code>List[str]</code>)           \u2013            <p>List of UUIDs identifying the rows to trace</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame containing the transformed rows in the next operation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If at root node or if row_ids format is invalid</p> </li> </ul> Example <pre><code># Trace how specific customer rows are transformed\ntransformed = query.forward([\"customer_uuid1\", \"customer_uuid2\"])\n</code></pre> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef forwards(self, row_ids: List[str]) -&gt; pl.DataFrame:\n    \"\"\"Trace rows forward to see how they are transformed by the next operation.\n\n    Args:\n        row_ids: List of UUIDs identifying the rows to trace\n\n    Returns:\n        DataFrame containing the transformed rows in the next operation\n\n    Raises:\n        ValueError: If at root node or if row_ids format is invalid\n\n    Example:\n        ```python\n        # Trace how specific customer rows are transformed\n        transformed = query.forward([\"customer_uuid1\", \"customer_uuid2\"])\n        ```\n    \"\"\"\n    return self.lineage.forwards(row_ids)\n</code></pre>"},{"location":"reference/fenic/#fenic.Lineage.get_result_df","title":"get_result_df","text":"<pre><code>get_result_df() -&gt; pl.DataFrame\n</code></pre> <p>Get the result of the query as a Polars DataFrame.</p> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>def get_result_df(self) -&gt; pl.DataFrame:\n    \"\"\"Get the result of the query as a Polars DataFrame.\"\"\"\n    return self.lineage.get_result_df()\n</code></pre>"},{"location":"reference/fenic/#fenic.Lineage.get_source_df","title":"get_source_df","text":"<pre><code>get_source_df(source_name: str) -&gt; pl.DataFrame\n</code></pre> <p>Get a query source by name as a Polars DataFrame.</p> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef get_source_df(self, source_name: str) -&gt; pl.DataFrame:\n    \"\"\"Get a query source by name as a Polars DataFrame.\"\"\"\n    return self.lineage.get_source_df(source_name)\n</code></pre>"},{"location":"reference/fenic/#fenic.Lineage.get_source_names","title":"get_source_names","text":"<pre><code>get_source_names() -&gt; List[str]\n</code></pre> <p>Get the names of all sources in the query plan. Used to determine where to start the lineage traversal.</p> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef get_source_names(self) -&gt; List[str]:\n    \"\"\"Get the names of all sources in the query plan. Used to determine where to start the lineage traversal.\"\"\"\n    return self.lineage.get_source_names()\n</code></pre>"},{"location":"reference/fenic/#fenic.Lineage.show","title":"show","text":"<pre><code>show() -&gt; None\n</code></pre> <p>Print the operator tree of the query.</p> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>def show(self) -&gt; None:\n    \"\"\"Print the operator tree of the query.\"\"\"\n    print(self.lineage.stringify_graph())\n</code></pre>"},{"location":"reference/fenic/#fenic.Lineage.skip_backwards","title":"skip_backwards","text":"<pre><code>skip_backwards(ids: List[str]) -&gt; Dict[str, pl.DataFrame]\n</code></pre> <p>[Not Implemented] Trace rows backwards through multiple operations at once.</p> <p>This method will allow efficient tracing through multiple operations without intermediate results.</p> <p>Parameters:</p> <ul> <li> <code>ids</code>               (<code>List[str]</code>)           \u2013            <p>List of UUIDs identifying the rows to trace back</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, DataFrame]</code>           \u2013            <p>Dictionary mapping operation names to their source DataFrames</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>This method is not yet implemented</p> </li> </ul> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>def skip_backwards(self, ids: List[str]) -&gt; Dict[str, pl.DataFrame]:\n    \"\"\"[Not Implemented] Trace rows backwards through multiple operations at once.\n\n    This method will allow efficient tracing through multiple operations without\n    intermediate results.\n\n    Args:\n        ids: List of UUIDs identifying the rows to trace back\n\n    Returns:\n        Dictionary mapping operation names to their source DataFrames\n\n    Raises:\n        NotImplementedError: This method is not yet implemented\n    \"\"\"\n    raise NotImplementedError(\"Skip backwards not yet implemented\")\n</code></pre>"},{"location":"reference/fenic/#fenic.Lineage.skip_forwards","title":"skip_forwards","text":"<pre><code>skip_forwards(row_ids: List[str]) -&gt; pl.DataFrame\n</code></pre> <p>[Not Implemented] Trace rows forward through multiple operations at once.</p> <p>This method will allow efficient tracing through multiple operations without intermediate results.</p> <p>Parameters:</p> <ul> <li> <code>row_ids</code>               (<code>List[str]</code>)           \u2013            <p>List of UUIDs identifying the rows to trace</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame containing the final transformed rows</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>This method is not yet implemented</p> </li> </ul> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>def skip_forwards(self, row_ids: List[str]) -&gt; pl.DataFrame:\n    \"\"\"[Not Implemented] Trace rows forward through multiple operations at once.\n\n    This method will allow efficient tracing through multiple operations without\n    intermediate results.\n\n    Args:\n        row_ids: List of UUIDs identifying the rows to trace\n\n    Returns:\n        DataFrame containing the final transformed rows\n\n    Raises:\n        NotImplementedError: This method is not yet implemented\n    \"\"\"\n    raise NotImplementedError(\"Skip forwards not yet implemented\")\n</code></pre>"},{"location":"reference/fenic/#fenic.Lineage.start_from_source","title":"start_from_source","text":"<pre><code>start_from_source(source_name: str) -&gt; None\n</code></pre> <p>Set the current position to a specific source in the query plan.</p> <p>Parameters:</p> <ul> <li> <code>source_name</code>               (<code>str</code>)           \u2013            <p>Name of the source table to start from</p> </li> </ul> Example <pre><code>query.start_from_source(\"customers\")\n# Now you can trace forward from the customers table\n</code></pre> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef start_from_source(self, source_name: str) -&gt; None:\n    \"\"\"Set the current position to a specific source in the query plan.\n\n    Args:\n        source_name: Name of the source table to start from\n\n    Example:\n        ```python\n        query.start_from_source(\"customers\")\n        # Now you can trace forward from the customers table\n        ```\n    \"\"\"\n    self.lineage.start_from_source(source_name)\n</code></pre>"},{"location":"reference/fenic/#fenic.MapExample","title":"MapExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single semantic example for semantic mapping operations.</p> <p>Map examples demonstrate the transformation of input variables to a specific output string used in a semantic.map operation.</p>"},{"location":"reference/fenic/#fenic.MapExampleCollection","title":"MapExampleCollection","text":"<pre><code>MapExampleCollection(examples: List[ExampleType] = None)\n</code></pre> <p>               Bases: <code>BaseExampleCollection[MapExample]</code></p> <p>Collection of examples for semantic mapping operations.</p> <p>Map operations transform input variables into a text output according to specified instructions. This collection manages examples that demonstrate the expected transformations for different inputs.</p> <p>Examples in this collection can have multiple input variables, each mapped to their respective values, with a single output string representing the expected transformation result.</p> <p>Methods:</p> <ul> <li> <code>from_polars</code>             \u2013              <p>Create collection from a Polars DataFrame. Must have an 'output' column and at least one input column.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def __init__(self, examples: List[ExampleType] = None):\n    \"\"\"Initialize a collection of semantic examples.\n\n    Args:\n        examples: Optional list of examples to add to the collection. Each example\n            will be processed through create_example() to ensure proper formatting\n            and validation.\n\n    Note:\n        The examples list is initialized as empty if no examples are provided.\n        Each example in the provided list will be processed through create_example()\n        to ensure proper formatting and validation.\n    \"\"\"\n    self.examples: List[ExampleType] = []\n    if examples:\n        for example in examples:\n            self.create_example(example)\n</code></pre>"},{"location":"reference/fenic/#fenic.MapExampleCollection.from_polars","title":"from_polars  <code>classmethod</code>","text":"<pre><code>from_polars(df: DataFrame) -&gt; MapExampleCollection\n</code></pre> <p>Create collection from a Polars DataFrame. Must have an 'output' column and at least one input column.</p> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: pl.DataFrame) -&gt; MapExampleCollection:\n    \"\"\"Create collection from a Polars DataFrame. Must have an 'output' column and at least one input column.\"\"\"\n    collection = cls()\n\n    if EXAMPLE_OUTPUT_KEY not in df.columns:\n        raise ValueError(\n            f\"Map Examples DataFrame missing required '{EXAMPLE_OUTPUT_KEY}' column\"\n        )\n\n    input_cols = [col for col in df.columns if col != EXAMPLE_OUTPUT_KEY]\n\n    if not input_cols:\n        raise ValueError(\n            \"Map Examples DataFrame must have at least one input column\"\n        )\n\n    for row in df.iter_rows(named=True):\n        if row[EXAMPLE_OUTPUT_KEY] is None:\n            raise InvalidExampleCollectionError(\n                f\"Map Examples DataFrame contains null values in '{EXAMPLE_OUTPUT_KEY}' column\"\n            )\n\n        input_dict = {\n            col: str(row[col]) for col in input_cols if row[col] is not None\n        }\n\n        example = MapExample(input=input_dict, output=row[EXAMPLE_OUTPUT_KEY])\n        collection.create_example(example)\n\n    return collection\n</code></pre>"},{"location":"reference/fenic/#fenic.OpenAIModelConfig","title":"OpenAIModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for OpenAI models.</p> <p>This class defines the configuration settings for OpenAI language and embedding models, including model selection and rate limiting parameters.</p> <p>Attributes:</p> <ul> <li> <code>model_name</code>               (<code>Union[OPENAI_AVAILABLE_LANGUAGE_MODELS, OPENAI_AVAILABLE_EMBEDDING_MODELS]</code>)           \u2013            <p>The name of the OpenAI model to use.</p> </li> <li> <code>rpm</code>               (<code>int</code>)           \u2013            <p>Requests per minute limit; must be greater than 0.</p> </li> <li> <code>tpm</code>               (<code>int</code>)           \u2013            <p>Tokens per minute limit; must be greater than 0.</p> </li> </ul> <p>Examples:</p> <p>Configuring an OpenAI Language model with rate limits:</p> <pre><code>config = OpenAIModelConfig(model_name=\"gpt-4.1-nano\", rpm=100, tpm=100)\n</code></pre> <p>Configuring an OpenAI Embedding model with rate limits:</p> <pre><code>config = OpenAIModelConfig(model_name=\"text-embedding-3-small\", rpm=100, tpm=100)\n</code></pre>"},{"location":"reference/fenic/#fenic.OperatorMetrics","title":"OperatorMetrics  <code>dataclass</code>","text":"<pre><code>OperatorMetrics(operator_id: str, num_output_rows: int = 0, execution_time_ms: float = 0.0, lm_metrics: LMMetrics = LMMetrics(), rm_metrics: RMMetrics = RMMetrics(), is_cache_hit: bool = False)\n</code></pre> <p>Metrics for a single operator in the query execution plan.</p> <p>Attributes:</p> <ul> <li> <code>operator_id</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the operator</p> </li> <li> <code>num_output_rows</code>               (<code>int</code>)           \u2013            <p>Number of rows output by this operator</p> </li> <li> <code>execution_time_ms</code>               (<code>float</code>)           \u2013            <p>Execution time in milliseconds</p> </li> <li> <code>lm_metrics</code>               (<code>LMMetrics</code>)           \u2013            <p>Language model usage metrics for this operator</p> </li> <li> <code>is_cache_hit</code>               (<code>bool</code>)           \u2013            <p>Whether results were retrieved from cache</p> </li> </ul>"},{"location":"reference/fenic/#fenic.PredicateExample","title":"PredicateExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single semantic example for semantic predicate operations.</p> <p>Predicate examples demonstrate the evaluation of input variables against a specific condition, used in a semantic.predicate operation.</p>"},{"location":"reference/fenic/#fenic.PredicateExampleCollection","title":"PredicateExampleCollection","text":"<pre><code>PredicateExampleCollection(examples: List[ExampleType] = None)\n</code></pre> <p>               Bases: <code>BaseExampleCollection[PredicateExample]</code></p> <p>Collection of examples for semantic predicate operations.</p> <p>Predicate operations evaluate conditions on input variables to produce boolean (True/False) results. This collection manages examples that demonstrate the expected boolean outcomes for different inputs.</p> <p>Examples in this collection can have multiple input variables, each mapped to their respective values, with a single boolean output representing the evaluation result of the predicate.</p> <p>Methods:</p> <ul> <li> <code>from_polars</code>             \u2013              <p>Create collection from a Polars DataFrame.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def __init__(self, examples: List[ExampleType] = None):\n    \"\"\"Initialize a collection of semantic examples.\n\n    Args:\n        examples: Optional list of examples to add to the collection. Each example\n            will be processed through create_example() to ensure proper formatting\n            and validation.\n\n    Note:\n        The examples list is initialized as empty if no examples are provided.\n        Each example in the provided list will be processed through create_example()\n        to ensure proper formatting and validation.\n    \"\"\"\n    self.examples: List[ExampleType] = []\n    if examples:\n        for example in examples:\n            self.create_example(example)\n</code></pre>"},{"location":"reference/fenic/#fenic.PredicateExampleCollection.from_polars","title":"from_polars  <code>classmethod</code>","text":"<pre><code>from_polars(df: DataFrame) -&gt; PredicateExampleCollection\n</code></pre> <p>Create collection from a Polars DataFrame.</p> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: pl.DataFrame) -&gt; PredicateExampleCollection:\n    \"\"\"Create collection from a Polars DataFrame.\"\"\"\n    collection = cls()\n\n    # Validate output column exists\n    if EXAMPLE_OUTPUT_KEY not in df.columns:\n        raise InvalidExampleCollectionError(\n            f\"Predicate Examples DataFrame missing required '{EXAMPLE_OUTPUT_KEY}' column\"\n        )\n\n    input_cols = [col for col in df.columns if col != EXAMPLE_OUTPUT_KEY]\n\n    if not input_cols:\n        raise InvalidExampleCollectionError(\n            \"Predicate Examples DataFrame must have at least one input column\"\n        )\n\n    for row in df.iter_rows(named=True):\n        if row[EXAMPLE_OUTPUT_KEY] is None:\n            raise InvalidExampleCollectionError(\n                f\"Predicate Examples DataFrame contains null values in '{EXAMPLE_OUTPUT_KEY}' column\"\n            )\n\n        input_dict = {col: row[col] for col in input_cols if row[col] is not None}\n\n        example = PredicateExample(input=input_dict, output=row[EXAMPLE_OUTPUT_KEY])\n        collection.create_example(example)\n\n    return collection\n</code></pre>"},{"location":"reference/fenic/#fenic.QueryMetrics","title":"QueryMetrics  <code>dataclass</code>","text":"<pre><code>QueryMetrics(execution_time_ms: float = 0.0, num_output_rows: int = 0, total_lm_metrics: LMMetrics = LMMetrics(), total_rm_metrics: RMMetrics = RMMetrics(), _operator_metrics: Dict[str, OperatorMetrics] = dict(), _plan_repr: PhysicalPlanRepr = lambda: PhysicalPlanRepr(operator_id='empty')())\n</code></pre> <p>Comprehensive metrics for an executed query.</p> <p>Includes overall statistics and detailed metrics for each operator in the execution plan.</p> <p>Attributes:</p> <ul> <li> <code>execution_time_ms</code>               (<code>float</code>)           \u2013            <p>Total query execution time in milliseconds</p> </li> <li> <code>num_output_rows</code>               (<code>int</code>)           \u2013            <p>Total number of rows returned by the query</p> </li> <li> <code>total_lm_metrics</code>               (<code>LMMetrics</code>)           \u2013            <p>Aggregated language model metrics across all operators</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_execution_plan_details</code>             \u2013              <p>Generate a formatted execution plan with detailed metrics.</p> </li> <li> <code>get_summary</code>             \u2013              <p>Summarize the query metrics in a single line.</p> </li> </ul>"},{"location":"reference/fenic/#fenic.QueryMetrics.get_execution_plan_details","title":"get_execution_plan_details","text":"<pre><code>get_execution_plan_details() -&gt; str\n</code></pre> <p>Generate a formatted execution plan with detailed metrics.</p> <p>Produces a hierarchical representation of the query execution plan, including performance metrics and language model usage for each operator.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A formatted string showing the execution plan with metrics.</p> </li> </ul> Source code in <code>src/fenic/core/metrics.py</code> <pre><code>def get_execution_plan_details(self) -&gt; str:\n    \"\"\"Generate a formatted execution plan with detailed metrics.\n\n    Produces a hierarchical representation of the query execution plan,\n    including performance metrics and language model usage for each operator.\n\n    Returns:\n        str: A formatted string showing the execution plan with metrics.\n    \"\"\"\n\n    def _format_node(node: PhysicalPlanRepr, indent: int = 1) -&gt; str:\n        op = self._operator_metrics[node.operator_id]\n        indent_str = \"  \" * indent\n\n        details = [\n            f\"{indent_str}{op.operator_id}\",\n            f\"{indent_str}  Output Rows: {op.num_output_rows:,}\",\n            f\"{indent_str}  Execution Time: {op.execution_time_ms:.2f}ms\",\n            f\"{indent_str}  Cached: {op.is_cache_hit}\",\n        ]\n\n        if op.lm_metrics.cost &gt; 0:\n            details.extend(\n                [\n                    f\"{indent_str}  Language Model Usage: {op.lm_metrics.num_uncached_input_tokens:,} input tokens, {op.lm_metrics.num_cached_input_tokens:,} cached input tokens, {op.lm_metrics.num_output_tokens:,} output tokens\",\n                    f\"{indent_str}  Language Model Cost: ${op.lm_metrics.cost:.6f}\",\n                ]\n            )\n\n        if op.rm_metrics.cost &gt; 0:\n            details.extend(\n                [\n                    f\"{indent_str}  Embedding Model Usage: {op.rm_metrics.num_input_tokens:,} input tokens\",\n                    f\"{indent_str}  Embedding Model Cost: ${op.rm_metrics.cost:.6f}\",\n                ]\n            )\n        return (\n            \"\\n\".join(details)\n            + \"\\n\"\n            + \"\".join(_format_node(child, indent + 1) for child in node.children)\n        )\n\n    return f\"Execution Plan\\n{_format_node(self._plan_repr)}\"\n</code></pre>"},{"location":"reference/fenic/#fenic.QueryMetrics.get_summary","title":"get_summary","text":"<pre><code>get_summary() -&gt; str\n</code></pre> <p>Summarize the query metrics in a single line.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A concise summary of execution time, row count, and LM cost.</p> </li> </ul> Source code in <code>src/fenic/core/metrics.py</code> <pre><code>def get_summary(self) -&gt; str:\n    \"\"\"Summarize the query metrics in a single line.\n\n    Returns:\n        str: A concise summary of execution time, row count, and LM cost.\n    \"\"\"\n    return (\n        f\"Query executed in {self.execution_time_ms:.2f}ms, \"\n        f\"returned {self.num_output_rows:,} rows, \"\n        f\"language model cost: ${self.total_lm_metrics.cost:.6f}, \"\n        f\"embedding model cost: ${self.total_rm_metrics.cost:.6f}\"\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.QueryResult","title":"QueryResult  <code>dataclass</code>","text":"<pre><code>QueryResult(data: DataLike, metrics: QueryMetrics)\n</code></pre> <p>Container for query execution results and associated metadata.</p> <p>This dataclass bundles together the materialized data from a query execution along with metrics about the execution process. It provides a unified interface for accessing both the computed results and performance information.</p> <p>Attributes:</p> <ul> <li> <code>data</code>               (<code>DataLike</code>)           \u2013            <p>The materialized query results in the requested format. Can be any of the supported data types (Polars/Pandas DataFrame, Arrow Table, or Python dict/list structures).</p> </li> <li> <code>metrics</code>               (<code>QueryMetrics</code>)           \u2013            <p>Execution metadata including timing information, memory usage, rows processed, and other performance metrics collected during query execution.</p> </li> </ul> Access query results and metrics <pre><code># Execute query and get results with metrics\nresult = df.filter(col(\"age\") &gt; 25).collect(\"pandas\")\npandas_df = result.data  # Access the Pandas DataFrame\nprint(result.metrics.execution_time)  # Access execution metrics\nprint(result.metrics.rows_processed)  # Access row count\n</code></pre> Work with different data formats <pre><code># Get results in different formats\npolars_result = df.collect(\"polars\")\narrow_result = df.collect(\"arrow\")\ndict_result = df.collect(\"pydict\")\n\n# All contain the same data, different formats\nprint(type(polars_result.data))  # &lt;class 'polars.DataFrame'&gt;\nprint(type(arrow_result.data))   # &lt;class 'pyarrow.lib.Table'&gt;\nprint(type(dict_result.data))    # &lt;class 'dict'&gt;\n</code></pre> Note <p>The actual type of the <code>data</code> attribute depends on the format requested during collection. Use type checking or isinstance() if you need to handle the data differently based on its format.</p>"},{"location":"reference/fenic/#fenic.RMMetrics","title":"RMMetrics  <code>dataclass</code>","text":"<pre><code>RMMetrics(num_input_tokens: int = 0, num_requests: int = 0, cost: float = 0.0)\n</code></pre> <p>Tracks embedding model usage metrics including token counts and costs.</p> <p>Attributes:</p> <ul> <li> <code>num_input_tokens</code>               (<code>int</code>)           \u2013            <p>Number of tokens to embed</p> </li> <li> <code>cost</code>               (<code>float</code>)           \u2013            <p>Total cost in USD to embed the tokens</p> </li> </ul>"},{"location":"reference/fenic/#fenic.Schema","title":"Schema","text":"<p>Represents the schema of a DataFrame.</p> <p>A Schema defines the structure of a DataFrame by specifying an ordered collection of column fields. Each column field defines the name and data type of a column in the DataFrame.</p> <p>Attributes:</p> <ul> <li> <code>column_fields</code>               (<code>List[ColumnField]</code>)           \u2013            <p>An ordered list of ColumnField objects that define the structure of the DataFrame.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>column_names</code>             \u2013              <p>Get a list of all column names in the schema.</p> </li> </ul>"},{"location":"reference/fenic/#fenic.Schema.column_names","title":"column_names","text":"<pre><code>column_names() -&gt; List[str]\n</code></pre> <p>Get a list of all column names in the schema.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>A list of strings containing the names of all columns in the schema.</p> </li> </ul> Source code in <code>src/fenic/core/types/schema.py</code> <pre><code>def column_names(self) -&gt; List[str]:\n    \"\"\"Get a list of all column names in the schema.\n\n    Returns:\n        A list of strings containing the names of all columns in the schema.\n    \"\"\"\n    return [field.name for field in self.column_fields]\n</code></pre>"},{"location":"reference/fenic/#fenic.SemanticConfig","title":"SemanticConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for semantic language and embedding models.</p> <p>This class defines the configuration for both language models and optional embedding models used in semantic operations. It ensures that all configured models are valid and supported by their respective providers.</p> <p>Attributes:</p> <ul> <li> <code>language_models</code>               (<code>dict[str, ModelConfig]</code>)           \u2013            <p>Mapping of model aliases to language model configurations.</p> </li> <li> <code>default_language_model</code>               (<code>Optional[str]</code>)           \u2013            <p>The alias of the default language model to use for semantic operations. Not required if only one language model is configured.</p> </li> <li> <code>embedding_models</code>               (<code>Optional[dict[str, ModelConfig]]</code>)           \u2013            <p>Optional mapping of model aliases to embedding model configurations.</p> </li> <li> <code>default_embedding_model</code>               (<code>Optional[str]</code>)           \u2013            <p>The alias of the default embedding model to use for semantic operations.</p> </li> </ul> Note <p>The embedding model is optional and only required for operations that need semantic search or embedding capabilities.</p> <p>Methods:</p> <ul> <li> <code>model_post_init</code>             \u2013              <p>Post initialization hook to set defaults.</p> </li> <li> <code>validate_models</code>             \u2013              <p>Validates that the selected models are supported by the system.</p> </li> </ul>"},{"location":"reference/fenic/#fenic.SemanticConfig.model_post_init","title":"model_post_init","text":"<pre><code>model_post_init(__context) -&gt; None\n</code></pre> <p>Post initialization hook to set defaults.</p> <p>This hook runs after the model is initialized and validated. It sets the default language and embedding models if they are not set and there is only one model available.</p> Source code in <code>src/fenic/api/session/config.py</code> <pre><code>def model_post_init(self, __context) -&gt; None:\n    \"\"\"Post initialization hook to set defaults.\n\n    This hook runs after the model is initialized and validated.\n    It sets the default language and embedding models if they are not set\n    and there is only one model available.\n    \"\"\"\n    # Set default language model if not set and only one model exists\n    if self.default_language_model is None and len(self.language_models) == 1:\n        self.default_language_model = list(self.language_models.keys())[0]\n    # Set default embedding model if not set and only one model exists\n    if self.embedding_models is not None and self.default_embedding_model is None and len(self.embedding_models) == 1:\n        self.default_embedding_model = list(self.embedding_models.keys())[0]\n</code></pre>"},{"location":"reference/fenic/#fenic.SemanticConfig.validate_models","title":"validate_models","text":"<pre><code>validate_models() -&gt; SemanticConfig\n</code></pre> <p>Validates that the selected models are supported by the system.</p> <p>This validator checks that both the language model and embedding model (if provided) are valid and supported by their respective providers.</p> <p>Returns:</p> <ul> <li> <code>SemanticConfig</code>           \u2013            <p>The validated SemanticConfig instance.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ConfigurationError</code>             \u2013            <p>If any of the models are not supported.</p> </li> </ul> Source code in <code>src/fenic/api/session/config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_models(self) -&gt; SemanticConfig:\n    \"\"\"Validates that the selected models are supported by the system.\n\n    This validator checks that both the language model and embedding model (if provided)\n    are valid and supported by their respective providers.\n\n    Returns:\n        The validated SemanticConfig instance.\n\n    Raises:\n        ConfigurationError: If any of the models are not supported.\n    \"\"\"\n    if len(self.language_models) == 0:\n        raise ConfigurationError(\"You must specify at least one language model configuration.\")\n    available_language_model_aliases = list(self.language_models.keys())\n    if self.default_language_model is None and len(self.language_models) &gt; 1:\n        raise ConfigurationError(f\"default_language_model is not set, and multiple language models are configured. Please specify one of: {available_language_model_aliases} as a default_language_model.\")\n\n    if self.default_language_model is not None and self.default_language_model not in self.language_models:\n        raise ConfigurationError(f\"default_language_model {self.default_language_model} is not in configured map of language models. Available models: {available_language_model_aliases} .\")\n\n    for model_alias, language_model in self.language_models.items():\n        if isinstance(language_model, OpenAIModelConfig):\n            language_model_provider = ModelProvider.OPENAI\n            language_model_name = language_model.model_name\n        elif isinstance(language_model, AnthropicModelConfig):\n            language_model_provider = ModelProvider.ANTHROPIC\n            language_model_name = language_model.model_name\n        elif isinstance(language_model, GoogleGLAModelConfig):\n            language_model_provider = ModelProvider.GOOGLE_GLA\n            language_model_name = language_model.model_name\n        else:\n            raise ConfigurationError(\n                f\"Invalid language model: {model_alias}: {language_model} unsupported model type.\")\n\n        completion_model = model_catalog.get_completion_model_parameters(language_model_provider,\n                                                                         language_model_name)\n        if completion_model is None:\n            raise ConfigurationError(\n                model_catalog.generate_unsupported_completion_model_error_message(\n                    language_model_provider,\n                    language_model_name\n                )\n            )\n        if isinstance(language_model, GoogleGLAModelConfig) and completion_model.requires_reasoning_effort:\n            if language_model.reasoning_effort is None:\n                raise ConfigurationError(f\"Reasoning effort level is required for {language_model_provider.value}:{language_model_name} Please specify reasoning_effort for model {model_alias}.\")\n\n    if self.embedding_models is not None:\n        if self.default_embedding_model is None and len(self.embedding_models) &gt; 1:\n            raise ConfigurationError(\"embedding_models is set but default_embedding_model is missing (ambiguous).\")\n\n        if self.default_embedding_model is not None and self.default_embedding_model not in self.embedding_models:\n            raise ConfigurationError(\n                f\"default_embedding_model {self.default_embedding_model} is not in embedding_models\")\n        for model_alias, embedding_model in self.embedding_models.items():\n            if isinstance(embedding_model, OpenAIModelConfig):\n                embedding_model_provider = ModelProvider.OPENAI\n                embedding_model_name = embedding_model.model_name\n            else:\n                raise ConfigurationError(\n                    f\"Invalid embedding model: {model_alias}: {embedding_model} unsupported model type\")\n            embedding_model_parameters = model_catalog.get_embedding_model_parameters(embedding_model_provider,\n                                                                                 embedding_model_name)\n            if embedding_model_parameters is None:\n                raise ConfigurationError(model_catalog.generate_unsupported_embedding_model_error_message(\n                    embedding_model_provider,\n                    embedding_model_name\n                ))\n\n    return self\n</code></pre>"},{"location":"reference/fenic/#fenic.SemanticExtensions","title":"SemanticExtensions","text":"<pre><code>SemanticExtensions(df: DataFrame)\n</code></pre> <p>A namespace for semantic dataframe operators.</p> <p>Initialize semantic extensions.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to extend with semantic operations.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>group_by</code>             \u2013              <p>Semantically group rows by clustering an embedding column into the specified number of centroids.</p> </li> <li> <code>join</code>             \u2013              <p>Performs a semantic join between two DataFrames using a natural language predicate.</p> </li> <li> <code>sim_join</code>             \u2013              <p>Performs a semantic similarity join between two DataFrames using precomputed text embeddings.</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def __init__(self, df: DataFrame):\n    \"\"\"Initialize semantic extensions.\n\n    Args:\n        df: The DataFrame to extend with semantic operations.\n    \"\"\"\n    self._df = df\n</code></pre>"},{"location":"reference/fenic/#fenic.SemanticExtensions.group_by","title":"group_by","text":"<pre><code>group_by(by: ColumnOrName, num_clusters: int) -&gt; SemGroupedData\n</code></pre> <p>Semantically group rows by clustering an embedding column into the specified number of centroids.</p> <p>This method is useful when you want to uncover natural themes, topics, or intent in embedded free-form text, without needing predefined categories.</p> <p>Parameters:</p> <ul> <li> <code>by</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column containing embeddings to cluster</p> </li> <li> <code>num_clusters</code>               (<code>int</code>)           \u2013            <p>Number of semantic clusters to create</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SemGroupedData</code> (              <code>SemGroupedData</code> )          \u2013            <p>Object for performing aggregations on the clustered data.</p> </li> </ul> Basic semantic grouping <pre><code># Group customer feedback into 5 clusters\ndf.semantic.group_by(\"feedback_embeddings\", 5).agg(count(\"*\"))\n</code></pre> Analyze sentiment by semantic group <pre><code># Analyze sentiment by semantic group\ndf.semantic.group_by(\"feedback_embeddings\", 5).agg(\n    count(\"*\").alias(\"count\"),\n    avg(\"sentiment_score\").alias(\"avg_sentiment\")\n)\n</code></pre> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def group_by(self, by: ColumnOrName, num_clusters: int) -&gt; SemGroupedData:\n    \"\"\"Semantically group rows by clustering an embedding column into the specified number of centroids.\n\n    This method is useful when you want to uncover natural themes, topics, or intent in embedded free-form text,\n    without needing predefined categories.\n\n    Args:\n        by: Column containing embeddings to cluster\n        num_clusters: Number of semantic clusters to create\n\n    Returns:\n        SemGroupedData: Object for performing aggregations on the clustered data.\n\n    Example: Basic semantic grouping\n        ```python\n        # Group customer feedback into 5 clusters\n        df.semantic.group_by(\"feedback_embeddings\", 5).agg(count(\"*\"))\n        ```\n\n    Example: Analyze sentiment by semantic group\n        ```python\n        # Analyze sentiment by semantic group\n        df.semantic.group_by(\"feedback_embeddings\", 5).agg(\n            count(\"*\").alias(\"count\"),\n            avg(\"sentiment_score\").alias(\"avg_sentiment\")\n        )\n        ```\n    \"\"\"\n    return SemGroupedData(self._df, by, num_clusters)\n</code></pre>"},{"location":"reference/fenic/#fenic.SemanticExtensions.join","title":"join","text":"<pre><code>join(other: DataFrame, join_instruction: str, examples: Optional[JoinExampleCollection] = None, model_alias: Optional[str] = None) -&gt; DataFrame\n</code></pre> <p>Performs a semantic join between two DataFrames using a natural language predicate.</p> <p>That evaluates to either true or false for each potential row pair.</p> <p>The join works by: 1. Evaluating the provided join_instruction as a boolean predicate for each possible pair of rows 2. Including ONLY the row pairs where the predicate evaluates to True in the result set 3. Excluding all row pairs where the predicate evaluates to False</p> <p>The instruction must reference exactly two columns, one from each DataFrame, using the <code>:left</code> and <code>:right</code> suffixes to indicate column origin.</p> <p>This is useful when row pairing decisions require complex reasoning based on a custom predicate rather than simple equality or similarity matching.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to join with.</p> </li> <li> <code>join_instruction</code>               (<code>str</code>)           \u2013            <p>A natural language description of how to match values.</p> <ul> <li>Must include one placeholder from the left DataFrame (e.g. <code>{resume_summary:left}</code>) and one from the right (e.g. <code>{job_description:right}</code>).</li> <li>This instruction is evaluated as a boolean predicate - pairs where it's <code>True</code> are included, pairs where it's <code>False</code> are excluded.</li> </ul> </li> <li> <code>examples</code>               (<code>Optional[JoinExampleCollection]</code>, default:                   <code>None</code> )           \u2013            <p>Optional JoinExampleCollection containing labeled pairs (<code>left</code>, <code>right</code>, <code>output</code>) to guide the semantic join behavior.</p> </li> <li> <code>model_alias</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame containing only the row pairs where the join_instruction       predicate evaluates to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If <code>other</code> is not a DataFrame or <code>join_instruction</code> is not a string.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the instruction format is invalid or references invalid columns.</p> </li> </ul> Basic semantic join <pre><code># Match job listings with candidate resumes based on title/skills\n# Only includes pairs where the predicate evaluates to True\ndf_jobs.semantic.join(df_resumes,\n    join_instruction=\"Given a candidate's resume_summary: {resume_summary:left} and a job description: {job_description:right}, does the candidate have the appropriate skills for the job?\"\n)\n</code></pre> Semantic join with examples <pre><code># Improve join quality with examples\nexamples = JoinExampleCollection()\nexamples.create_example(JoinExample(\n    left=\"5 years experience building backend services in Python using asyncio, FastAPI, and PostgreSQL\",\n    right=\"Senior Software Engineer - Backend\",\n    output=True))  # This pair WILL be included in similar cases\nexamples.create_example(JoinExample(\n    left=\"5 years experience with growth strategy, private equity due diligence, and M&amp;A\",\n    right=\"Product Manager - Hardware\",\n    output=False))  # This pair will NOT be included in similar cases\ndf_jobs.semantic.join(df_resumes,\n    join_instruction=\"Given a candidate's resume_summary: {resume_summary:left} and a job description: {job_description:right}, does the candidate have the appropriate skills for the job?\",\n    examples=examples)\n</code></pre> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def join(\n    self,\n    other: DataFrame,\n    join_instruction: str,\n    examples: Optional[JoinExampleCollection] = None,\n    model_alias: Optional[str] = None,\n) -&gt; DataFrame:\n    \"\"\"Performs a semantic join between two DataFrames using a natural language predicate.\n\n    That evaluates to either true or false for each potential row pair.\n\n    The join works by:\n    1. Evaluating the provided join_instruction as a boolean predicate for each possible pair of rows\n    2. Including ONLY the row pairs where the predicate evaluates to True in the result set\n    3. Excluding all row pairs where the predicate evaluates to False\n\n    The instruction must reference **exactly two columns**, one from each DataFrame,\n    using the `:left` and `:right` suffixes to indicate column origin.\n\n    This is useful when row pairing decisions require complex reasoning based on a custom predicate rather than simple equality or similarity matching.\n\n    Args:\n        other: The DataFrame to join with.\n        join_instruction: A natural language description of how to match values.\n\n            - Must include one placeholder from the left DataFrame (e.g. `{resume_summary:left}`)\n            and one from the right (e.g. `{job_description:right}`).\n            - This instruction is evaluated as a boolean predicate - pairs where it's `True` are included,\n            pairs where it's `False` are excluded.\n        examples: Optional JoinExampleCollection containing labeled pairs (`left`, `right`, `output`)\n            to guide the semantic join behavior.\n        model_alias: Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.\n\n    Returns:\n        DataFrame: A new DataFrame containing only the row pairs where the join_instruction\n                  predicate evaluates to True.\n\n    Raises:\n        TypeError: If `other` is not a DataFrame or `join_instruction` is not a string.\n        ValueError: If the instruction format is invalid or references invalid columns.\n\n    Example: Basic semantic join\n        ```python\n        # Match job listings with candidate resumes based on title/skills\n        # Only includes pairs where the predicate evaluates to True\n        df_jobs.semantic.join(df_resumes,\n            join_instruction=\"Given a candidate's resume_summary: {resume_summary:left} and a job description: {job_description:right}, does the candidate have the appropriate skills for the job?\"\n        )\n        ```\n\n    Example: Semantic join with examples\n        ```python\n        # Improve join quality with examples\n        examples = JoinExampleCollection()\n        examples.create_example(JoinExample(\n            left=\"5 years experience building backend services in Python using asyncio, FastAPI, and PostgreSQL\",\n            right=\"Senior Software Engineer - Backend\",\n            output=True))  # This pair WILL be included in similar cases\n        examples.create_example(JoinExample(\n            left=\"5 years experience with growth strategy, private equity due diligence, and M&amp;A\",\n            right=\"Product Manager - Hardware\",\n            output=False))  # This pair will NOT be included in similar cases\n        df_jobs.semantic.join(df_resumes,\n            join_instruction=\"Given a candidate's resume_summary: {resume_summary:left} and a job description: {job_description:right}, does the candidate have the appropriate skills for the job?\",\n            examples=examples)\n        ```\n    \"\"\"\n    from fenic.api.dataframe.dataframe import DataFrame\n\n    if not isinstance(other, DataFrame):\n        raise TypeError(f\"other argument must be a DataFrame, got {type(other)}\")\n\n    if not isinstance(join_instruction, str):\n        raise TypeError(\n            f\"join_instruction argument must be a string, got {type(join_instruction)}\"\n        )\n    join_columns = utils.parse_instruction(join_instruction)\n    if len(join_columns) != 2:\n        raise ValueError(\n            f\"join_instruction must contain exactly two columns, got {len(join_columns)}\"\n        )\n    left_on = None\n    right_on = None\n    for join_col in join_columns:\n        if join_col.endswith(\":left\"):\n            if left_on is not None:\n                raise ValueError(\n                    \"join_instruction cannot contain multiple :left columns\"\n                )\n            left_on = col(join_col.split(\":\")[0])\n        elif join_col.endswith(\":right\"):\n            if right_on is not None:\n                raise ValueError(\n                    \"join_instruction cannot contain multiple :right columns\"\n                )\n            right_on = col(join_col.split(\":\")[0])\n        else:\n            raise ValueError(\n                f\"Column '{join_col}' must end with either :left or :right\"\n            )\n\n    if left_on is None or right_on is None:\n        raise ValueError(\n            \"join_instruction must contain exactly one :left and one :right column\"\n        )\n\n    return self._df._from_logical_plan(\n        SemanticJoin(\n            left=self._df._logical_plan,\n            right=other._logical_plan,\n            left_on=left_on._logical_expr,\n            right_on=right_on._logical_expr,\n            join_instruction=join_instruction,\n            examples=examples,\n            model_alias=model_alias,\n        ),\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.SemanticExtensions.sim_join","title":"sim_join","text":"<pre><code>sim_join(other: DataFrame, left_on: ColumnOrName, right_on: ColumnOrName, k: int = 1, similarity_metric: SemanticSimilarityMetric = 'cosine', return_similarity_scores: bool = False) -&gt; DataFrame\n</code></pre> <p>Performs a semantic similarity join between two DataFrames using precomputed text embeddings.</p> <p>For each row in the left DataFrame, finds the top <code>k</code> most semantically similar rows in the right DataFrame based on the cosine similarity between their text embeddings. This is useful for fuzzy matching tasks when exact matches aren't possible.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>DataFrame</code>)           \u2013            <p>The right-hand DataFrame to join with.</p> </li> <li> <code>left_on</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column in this DataFrame containing text embeddings to compare.</p> </li> <li> <code>right_on</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column in the other DataFrame containing text embeddings to compare.</p> </li> <li> <code>k</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of most similar matches to return per row from the left DataFrame.</p> </li> <li> <code>similarity_metric</code>               (<code>SemanticSimilarityMetric</code>, default:                   <code>'cosine'</code> )           \u2013            <p>The metric to use for calculating distances between vectors. Supported distance metrics: \"l2\", \"cosine\", \"dot\"</p> </li> <li> <code>return_similarity_scores</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, include a <code>_similarity_score</code> column in the output DataFrame                     representing the match confidence (cosine similarity).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame containing matched rows from both sides and optionally similarity scores.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If argument types are incorrect.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If <code>k</code> is not positive or if the columns are invalid.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If <code>similarity_metric</code> is not one of \"l2\", \"cosine\", \"dot\"</p> </li> </ul> Match queries to FAQ entries <pre><code># Match customer queries to FAQ entries\ndf_queries.semantic.sim_join(\n    df_faqs,\n    left_on=embeddings(col(\"query_text\")),\n    right_on=embeddings(col(\"faq_question\")),\n    k=1\n)\n</code></pre> Link headlines to articles <pre><code># Link news headlines to full articles\ndf_headlines.semantic.sim_join(\n    df_articles,\n    left_on=embeddings(col(\"headline\")),\n    right_on=embeddings(col(\"content\")),\n    k=3,\n    return_similarity_scores=True\n)\n</code></pre> Find similar job postings <pre><code># Find similar job postings across two sources\ndf_linkedin.semantic.sim_join(\n    df_indeed,\n    left_on=embeddings(col(\"job_title\")),\n    right_on=embeddings(col(\"job_description\")),\n    k=2\n)\n</code></pre> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def sim_join(\n    self,\n    other: DataFrame,\n    left_on: ColumnOrName,\n    right_on: ColumnOrName,\n    k: int = 1,\n    similarity_metric: SemanticSimilarityMetric = \"cosine\",\n    return_similarity_scores: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Performs a semantic similarity join between two DataFrames using precomputed text embeddings.\n\n    For each row in the left DataFrame, finds the top `k` most semantically similar rows in the right DataFrame\n    based on the cosine similarity between their text embeddings. This is useful for fuzzy matching tasks when exact matches aren't possible.\n\n    Args:\n        other: The right-hand DataFrame to join with.\n        left_on: Column in this DataFrame containing text embeddings to compare.\n        right_on: Column in the other DataFrame containing text embeddings to compare.\n        k: Number of most similar matches to return per row from the left DataFrame.\n        similarity_metric: The metric to use for calculating distances between vectors.\n            Supported distance metrics: \"l2\", \"cosine\", \"dot\"\n        return_similarity_scores: If True, include a `_similarity_score` column in the output DataFrame\n                                representing the match confidence (cosine similarity).\n\n    Returns:\n        DataFrame: A new DataFrame containing matched rows from both sides and optionally similarity scores.\n\n    Raises:\n        TypeError: If argument types are incorrect.\n        ValueError: If `k` is not positive or if the columns are invalid.\n        ValueError: If `similarity_metric` is not one of \"l2\", \"cosine\", \"dot\"\n\n    Example: Match queries to FAQ entries\n        ```python\n        # Match customer queries to FAQ entries\n        df_queries.semantic.sim_join(\n            df_faqs,\n            left_on=embeddings(col(\"query_text\")),\n            right_on=embeddings(col(\"faq_question\")),\n            k=1\n        )\n        ```\n\n    Example: Link headlines to articles\n        ```python\n        # Link news headlines to full articles\n        df_headlines.semantic.sim_join(\n            df_articles,\n            left_on=embeddings(col(\"headline\")),\n            right_on=embeddings(col(\"content\")),\n            k=3,\n            return_similarity_scores=True\n        )\n        ```\n\n    Example: Find similar job postings\n        ```python\n        # Find similar job postings across two sources\n        df_linkedin.semantic.sim_join(\n            df_indeed,\n            left_on=embeddings(col(\"job_title\")),\n            right_on=embeddings(col(\"job_description\")),\n            k=2\n        )\n        ```\n    \"\"\"\n    from fenic.api.dataframe.dataframe import DataFrame\n\n    if not isinstance(right_on, ColumnOrName):\n        raise ValidationError(\n            f\"The `right_on` argument must be a `Column` or a string representing a column name, \"\n            f\"but got `{type(right_on).__name__}` instead.\"\n        )\n    if not isinstance(other, DataFrame):\n        raise ValidationError(\n                        f\"The `other` argument to `sim_join()` must be a DataFrame`, but got `{type(other).__name__}`.\"\n                    )\n    if not (isinstance(k, int) and k &gt; 0):\n        raise ValidationError(\n            f\"The parameter `k` must be a positive integer, but received `{k}`.\"\n        )\n    args = get_args(SemanticSimilarityMetric)\n    if similarity_metric not in args:\n        raise ValidationError(\n            f\"The `similarity_metric` argument must be one of {args}, but got `{similarity_metric}`.\"\n        )\n\n    def _validate_column(column: ColumnOrName, name: str):\n        if column is None:\n            raise ValidationError(f\"The `{name}` argument must not be None.\")\n        if not isinstance(column, ColumnOrName):\n            raise ValidationError(\n                f\"The `{name}` argument must be a `Column` or a string representing a column name, \"\n                f\"but got `{type(column).__name__}` instead.\"\n            )\n\n    _validate_column(left_on, \"left_on\")\n    _validate_column(right_on, \"right_on\")\n\n    return self._df._from_logical_plan(\n        SemanticSimilarityJoin(\n            self._df._logical_plan,\n            other._logical_plan,\n            Column._from_col_or_name(left_on)._logical_expr,\n            Column._from_col_or_name(right_on)._logical_expr,\n            k,\n            similarity_metric,\n            return_similarity_scores,\n        ),\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.Session","title":"Session","text":"<p>The entry point to programming with the DataFrame API. Similar to PySpark's SparkSession.</p> Create a session with default configuration <pre><code>session = Session.get_or_create(SessionConfig(app_name=\"my_app\"))\n</code></pre> Create a session with cloud configuration <pre><code>config = SessionConfig(\n    app_name=\"my_app\",\n    cloud=True,\n    api_key=\"your_api_key\"\n)\nsession = Session.get_or_create(config)\n</code></pre> <p>Methods:</p> <ul> <li> <code>create_dataframe</code>             \u2013              <p>Create a DataFrame from a variety of Python-native data formats.</p> </li> <li> <code>get_or_create</code>             \u2013              <p>Gets an existing Session or creates a new one with the configured settings.</p> </li> <li> <code>sql</code>             \u2013              <p>Execute a read-only SQL query against one or more DataFrames using named placeholders.</p> </li> <li> <code>stop</code>             \u2013              <p>Stops the session and closes all connections.</p> </li> <li> <code>table</code>             \u2013              <p>Returns the specified table as a DataFrame.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>catalog</code>               (<code>Catalog</code>)           \u2013            <p>Interface for catalog operations on the Session.</p> </li> <li> <code>read</code>               (<code>DataFrameReader</code>)           \u2013            <p>Returns a DataFrameReader that can be used to read data in as a DataFrame.</p> </li> </ul>"},{"location":"reference/fenic/#fenic.Session.catalog","title":"catalog  <code>property</code>","text":"<pre><code>catalog: Catalog\n</code></pre> <p>Interface for catalog operations on the Session.</p>"},{"location":"reference/fenic/#fenic.Session.read","title":"read  <code>property</code>","text":"<pre><code>read: DataFrameReader\n</code></pre> <p>Returns a DataFrameReader that can be used to read data in as a DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrameReader</code> (              <code>DataFrameReader</code> )          \u2013            <p>A reader interface to read data into DataFrame</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the session has been stopped</p> </li> </ul>"},{"location":"reference/fenic/#fenic.Session.create_dataframe","title":"create_dataframe","text":"<pre><code>create_dataframe(data: DataLike) -&gt; DataFrame\n</code></pre> <p>Create a DataFrame from a variety of Python-native data formats.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DataLike</code>)           \u2013            <p>Input data. Must be one of: - Polars DataFrame - Pandas DataFrame - dict of column_name -&gt; list of values - list of dicts (each dict representing a row) - pyarrow Table</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>A new DataFrame instance</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input format is unsupported or inconsistent with provided column names.</p> </li> </ul> Create from Polars DataFrame <pre><code>import polars as pl\ndf = pl.DataFrame({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\nsession.create_dataframe(df)\n</code></pre> Create from Pandas DataFrame <pre><code>import pandas as pd\ndf = pd.DataFrame({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\nsession.create_dataframe(df)\n</code></pre> Create from dictionary <pre><code>session.create_dataframe({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n</code></pre> Create from list of dictionaries <pre><code>session.create_dataframe([\n    {\"col1\": 1, \"col2\": \"a\"},\n    {\"col1\": 2, \"col2\": \"b\"}\n])\n</code></pre> Create from pyarrow Table <pre><code>import pyarrow as pa\ntable = pa.Table.from_pydict({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\nsession.create_dataframe(table)\n</code></pre> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>def create_dataframe(\n    self,\n    data: DataLike,\n) -&gt; DataFrame:\n    \"\"\"Create a DataFrame from a variety of Python-native data formats.\n\n    Args:\n        data: Input data. Must be one of:\n            - Polars DataFrame\n            - Pandas DataFrame\n            - dict of column_name -&gt; list of values\n            - list of dicts (each dict representing a row)\n            - pyarrow Table\n\n    Returns:\n        A new DataFrame instance\n\n    Raises:\n        ValueError: If the input format is unsupported or inconsistent with provided column names.\n\n    Example: Create from Polars DataFrame\n        ```python\n        import polars as pl\n        df = pl.DataFrame({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n        session.create_dataframe(df)\n        ```\n\n    Example: Create from Pandas DataFrame\n        ```python\n        import pandas as pd\n        df = pd.DataFrame({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n        session.create_dataframe(df)\n        ```\n\n    Example: Create from dictionary\n        ```python\n        session.create_dataframe({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n        ```\n\n    Example: Create from list of dictionaries\n        ```python\n        session.create_dataframe([\n            {\"col1\": 1, \"col2\": \"a\"},\n            {\"col1\": 2, \"col2\": \"b\"}\n        ])\n        ```\n\n    Example: Create from pyarrow Table\n        ```python\n        import pyarrow as pa\n        table = pa.Table.from_pydict({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n        session.create_dataframe(table)\n        ```\n    \"\"\"\n    try:\n        if isinstance(data, pl.DataFrame):\n            pl_df = data\n        elif isinstance(data, pd.DataFrame):\n            pl_df = pl.from_pandas(data)\n        elif isinstance(data, dict):\n            pl_df = pl.DataFrame(data)\n        elif isinstance(data, list):\n            if not data:\n                raise ValidationError(\n                    \"Cannot create DataFrame from empty list. Provide a non-empty list of dictionaries, lists, or other supported data types.\"\n                )\n\n            if not isinstance(data[0], dict):\n                raise ValidationError(\n                    \"Cannot create DataFrame from list of non-dict values. Provide a list of dictionaries.\"\n                )\n            pl_df = pl.DataFrame(data)\n        elif isinstance(data, pa.Table):\n            pl_df = pl.from_arrow(data)\n\n        else:\n            raise ValidationError(\n                f\"Unsupported data type: {type(data)}. Supported types are: Polars DataFrame, Pandas DataFrame, dict, or list.\"\n            )\n\n    except ValidationError:\n        raise\n    except Exception as e:\n        raise PlanError(f\"Failed to create DataFrame from {data}\") from e\n\n    return DataFrame._from_logical_plan(\n        InMemorySource(pl_df, self._session_state)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.Session.get_or_create","title":"get_or_create  <code>classmethod</code>","text":"<pre><code>get_or_create(config: SessionConfig) -&gt; Session\n</code></pre> <p>Gets an existing Session or creates a new one with the configured settings.</p> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>A Session instance configured with the provided settings</p> </li> </ul> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>@classmethod\ndef get_or_create(\n    cls,\n    config: SessionConfig,\n) -&gt; Session:\n    \"\"\"Gets an existing Session or creates a new one with the configured settings.\n\n    Returns:\n        A Session instance configured with the provided settings\n    \"\"\"\n    resolved_config = config._to_resolved_config()\n    if config.cloud:\n        from fenic._backends.cloud.manager import CloudSessionManager\n\n        cloud_session_manager = CloudSessionManager()\n        if not cloud_session_manager.initialized:\n            session_manager_dependencies = (\n                CloudSessionManager.create_global_session_dependencies()\n            )\n            cloud_session_manager.configure(session_manager_dependencies)\n        future = asyncio.run_coroutine_threadsafe(\n            cloud_session_manager.get_or_create_session_state(resolved_config),\n            cloud_session_manager._asyncio_loop,\n        )\n        cloud_session_state = future.result()\n        return Session._create_cloud_session(cloud_session_state)\n\n    local_session_state: LocalSessionState = LocalSessionManager().get_or_create_session_state(resolved_config)\n    return Session._create_local_session(local_session_state)\n</code></pre>"},{"location":"reference/fenic/#fenic.Session.sql","title":"sql","text":"<pre><code>sql(query: str, /, **tables: DataFrame) -&gt; DataFrame\n</code></pre> <p>Execute a read-only SQL query against one or more DataFrames using named placeholders.</p> <p>This allows you to execute ad hoc SQL queries using familiar syntax when it's more convenient than the DataFrame API. Placeholders in the SQL string (e.g. <code>{df}</code>) should correspond to keyword arguments (e.g. <code>df=my_dataframe</code>).</p> <p>For supported SQL syntax and functions, refer to the DuckDB SQL documentation: https://duckdb.org/docs/sql/introduction.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>A SQL query string with placeholders like <code>{df}</code></p> </li> <li> <code>**tables</code>               (<code>DataFrame</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments mapping placeholder names to DataFrames</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>A lazy DataFrame representing the result of the SQL query</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If a placeholder is used in the query but not passed as a keyword argument</p> </li> </ul> Simple join between two DataFrames <pre><code>df1 = session.create_dataframe({\"id\": [1, 2]})\ndf2 = session.create_dataframe({\"id\": [2, 3]})\nresult = session.sql(\n    \"SELECT * FROM {df1} JOIN {df2} USING (id)\",\n    df1=df1,\n    df2=df2\n)\n</code></pre> Complex query with multiple DataFrames <pre><code>users = session.create_dataframe({\"user_id\": [1, 2], \"name\": [\"Alice\", \"Bob\"]})\norders = session.create_dataframe({\"order_id\": [1, 2], \"user_id\": [1, 2]})\nproducts = session.create_dataframe({\"product_id\": [1, 2], \"name\": [\"Widget\", \"Gadget\"]})\n\nresult = session.sql(\"\"\"\n    SELECT u.name, p.name as product\n    FROM {users} u\n    JOIN {orders} o ON u.user_id = o.user_id\n    JOIN {products} p ON o.product_id = p.product_id\n\"\"\", users=users, orders=orders, products=products)\n</code></pre> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>def sql(self, query: str, /, **tables: DataFrame) -&gt; DataFrame:\n    \"\"\"Execute a read-only SQL query against one or more DataFrames using named placeholders.\n\n    This allows you to execute ad hoc SQL queries using familiar syntax when it's more convenient than the DataFrame API.\n    Placeholders in the SQL string (e.g. `{df}`) should correspond to keyword arguments (e.g. `df=my_dataframe`).\n\n    For supported SQL syntax and functions, refer to the DuckDB SQL documentation:\n    https://duckdb.org/docs/sql/introduction.\n\n    Args:\n        query: A SQL query string with placeholders like `{df}`\n        **tables: Keyword arguments mapping placeholder names to DataFrames\n\n    Returns:\n        A lazy DataFrame representing the result of the SQL query\n\n    Raises:\n        ValidationError: If a placeholder is used in the query but not passed\n            as a keyword argument\n\n    Example: Simple join between two DataFrames\n        ```python\n        df1 = session.create_dataframe({\"id\": [1, 2]})\n        df2 = session.create_dataframe({\"id\": [2, 3]})\n        result = session.sql(\n            \"SELECT * FROM {df1} JOIN {df2} USING (id)\",\n            df1=df1,\n            df2=df2\n        )\n        ```\n\n    Example: Complex query with multiple DataFrames\n        ```python\n        users = session.create_dataframe({\"user_id\": [1, 2], \"name\": [\"Alice\", \"Bob\"]})\n        orders = session.create_dataframe({\"order_id\": [1, 2], \"user_id\": [1, 2]})\n        products = session.create_dataframe({\"product_id\": [1, 2], \"name\": [\"Widget\", \"Gadget\"]})\n\n        result = session.sql(\\\"\\\"\\\"\n            SELECT u.name, p.name as product\n            FROM {users} u\n            JOIN {orders} o ON u.user_id = o.user_id\n            JOIN {products} p ON o.product_id = p.product_id\n        \\\"\\\"\\\", users=users, orders=orders, products=products)\n        ```\n    \"\"\"\n    query = query.strip()\n    if not query:\n        raise ValidationError(\"SQL query must not be empty.\")\n\n    placeholders = set(SQL_PLACEHOLDER_RE.findall(query))\n    missing = placeholders - tables.keys()\n    if missing:\n        raise ValidationError(\n            f\"Missing DataFrames for placeholders in SQL query: {', '.join(sorted(missing))}. \"\n            f\"Make sure to pass them as keyword arguments, e.g., sql(..., {next(iter(missing))}=df).\"\n        )\n\n    logical_plans = []\n    template_names = []\n    for name, table in tables.items():\n        if name in placeholders:\n            template_names.append(name)\n            logical_plans.append(table._logical_plan)\n\n    return DataFrame._from_logical_plan(\n        SQL(logical_plans, template_names, query, self._session_state),\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.Session.stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stops the session and closes all connections.</p> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>def stop(self):\n    \"\"\"Stops the session and closes all connections.\"\"\"\n    self._session_state.stop()\n</code></pre>"},{"location":"reference/fenic/#fenic.Session.table","title":"table","text":"<pre><code>table(table_name: str) -&gt; DataFrame\n</code></pre> <p>Returns the specified table as a DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Name of the table</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Table as a DataFrame</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the table does not exist</p> </li> </ul> Load an existing table <pre><code>df = session.table(\"my_table\")\n</code></pre> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>def table(self, table_name: str) -&gt; DataFrame:\n    \"\"\"Returns the specified table as a DataFrame.\n\n    Args:\n        table_name: Name of the table\n\n    Returns:\n        Table as a DataFrame\n\n    Raises:\n        ValueError: If the table does not exist\n\n    Example: Load an existing table\n        ```python\n        df = session.table(\"my_table\")\n        ```\n    \"\"\"\n    if not self._session_state.catalog.does_table_exist(table_name):\n        raise ValueError(f\"Table {table_name} does not exist\")\n    return DataFrame._from_logical_plan(\n        TableSource(table_name, self._session_state),\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.SessionConfig","title":"SessionConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a user session.</p> <p>This class defines the complete configuration for a user session, including application settings, model configurations, and optional cloud settings. It serves as the central configuration object for all language model operations.</p> <p>Attributes:</p> <ul> <li> <code>app_name</code>               (<code>str</code>)           \u2013            <p>Name of the application using this session. Defaults to \"default_app\".</p> </li> <li> <code>db_path</code>               (<code>Optional[Path]</code>)           \u2013            <p>Optional path to a local database file for persistent storage.</p> </li> <li> <code>semantic</code>               (<code>SemanticConfig</code>)           \u2013            <p>Configuration for semantic models (required).</p> </li> <li> <code>cloud</code>               (<code>Optional[CloudConfig]</code>)           \u2013            <p>Optional configuration for cloud execution.</p> </li> </ul> Note <p>The semantic configuration is required as it defines the language models that will be used for processing. The cloud configuration is optional and only needed for distributed processing.</p>"},{"location":"reference/fenic/#fenic.StructField","title":"StructField","text":"<p>A field in a StructType. Fields are nullable.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the field.</p> </li> <li> <code>data_type</code>               (<code>DataType</code>)           \u2013            <p>The data type of the field.</p> </li> </ul>"},{"location":"reference/fenic/#fenic.StructType","title":"StructType","text":"<p>               Bases: <code>DataType</code></p> <p>A type representing a struct (record) with named fields.</p> <p>Attributes:</p> <ul> <li> <code>fields</code>           \u2013            <p>List of field definitions.</p> </li> </ul> Create a struct with name and age fields <pre><code>StructType([\n    StructField(\"name\", StringType),\n    StructField(\"age\", IntegerType),\n])\n</code></pre>"},{"location":"reference/fenic/#fenic.TranscriptType","title":"TranscriptType","text":"<p>               Bases: <code>_StringBackedType</code></p> <p>Represents a string containing a transcript in a specific format.</p>"},{"location":"reference/fenic/#fenic.array","title":"array","text":"<pre><code>array(*args: Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]) -&gt; Column\n</code></pre> <p>Creates a new array column from multiple input columns.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]</code>, default:                   <code>()</code> )           \u2013            <p>Columns or column names to combine into an array. Can be:</p> <ul> <li>Individual arguments</li> <li>Lists of columns/column names</li> <li>Tuples of columns/column names</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing an array containing values from the input columns</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If any argument is not a Column, string, or collection of Columns/strings</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef array(\n    *args: Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]\n) -&gt; Column:\n    \"\"\"Creates a new array column from multiple input columns.\n\n    Args:\n        *args: Columns or column names to combine into an array. Can be:\n\n            - Individual arguments\n            - Lists of columns/column names\n            - Tuples of columns/column names\n\n    Returns:\n        A Column expression representing an array containing values from the input columns\n\n    Raises:\n        TypeError: If any argument is not a Column, string, or collection of\n            Columns/strings\n    \"\"\"\n    flattened_args = []\n    for arg in args:\n        if isinstance(arg, (list, tuple)):\n            flattened_args.extend(arg)\n        else:\n            flattened_args.append(arg)\n\n    expr_columns = [Column._from_col_or_name(c)._logical_expr for c in flattened_args]\n\n    return Column._from_logical_expr(ArrayExpr(expr_columns))\n</code></pre>"},{"location":"reference/fenic/#fenic.array_agg","title":"array_agg","text":"<pre><code>array_agg(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Alias for collect_list().</p> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef array_agg(column: ColumnOrName) -&gt; Column:\n    \"\"\"Alias for collect_list().\"\"\"\n    return collect_list(column)\n</code></pre>"},{"location":"reference/fenic/#fenic.array_contains","title":"array_contains","text":"<pre><code>array_contains(column: ColumnOrName, value: Union[str, int, float, bool, Column]) -&gt; Column\n</code></pre> <p>Checks if array column contains a specific value.</p> <p>This function returns True if the array in the specified column contains the given value, and False otherwise. Returns False if the array is None.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name containing the arrays to check.</p> </li> <li> <code>value</code>               (<code>Union[str, int, float, bool, Column]</code>)           \u2013            <p>Value to search for in the arrays. Can be: - A literal value (string, number, boolean) - A Column expression</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A boolean Column expression (True if value is found, False otherwise).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If value type is incompatible with the array element type.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If the column does not contain array data.</p> </li> </ul> Check for values in arrays <pre><code># Check if 'python' exists in arrays in the 'tags' column\ndf.select(array_contains(\"tags\", \"python\"))\n\n# Check using a value from another column\ndf.select(array_contains(\"tags\", col(\"search_term\")))\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef array_contains(\n    column: ColumnOrName, value: Union[str, int, float, bool, Column]\n) -&gt; Column:\n    \"\"\"Checks if array column contains a specific value.\n\n    This function returns True if the array in the specified column contains the given value,\n    and False otherwise. Returns False if the array is None.\n\n    Args:\n        column: Column or column name containing the arrays to check.\n\n        value: Value to search for in the arrays. Can be:\n            - A literal value (string, number, boolean)\n            - A Column expression\n\n    Returns:\n        A boolean Column expression (True if value is found, False otherwise).\n\n    Raises:\n        TypeError: If value type is incompatible with the array element type.\n        TypeError: If the column does not contain array data.\n\n    Example: Check for values in arrays\n        ```python\n        # Check if 'python' exists in arrays in the 'tags' column\n        df.select(array_contains(\"tags\", \"python\"))\n\n        # Check using a value from another column\n        df.select(array_contains(\"tags\", col(\"search_term\")))\n        ```\n    \"\"\"\n    value_column = None\n    if isinstance(value, Column):\n        value_column = value\n    else:\n        value_column = lit(value)\n    return Column._from_logical_expr(\n        ArrayContainsExpr(\n            Column._from_col_or_name(column)._logical_expr, value_column._logical_expr\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.array_size","title":"array_size","text":"<pre><code>array_size(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Returns the number of elements in an array column.</p> <p>This function computes the length of arrays stored in the specified column. Returns None for None arrays.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name containing arrays whose length to compute.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the array length.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the column does not contain array data.</p> </li> </ul> Get array sizes <pre><code># Get the size of arrays in 'tags' column\ndf.select(array_size(\"tags\"))\n\n# Use with column reference\ndf.select(array_size(col(\"tags\")))\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef array_size(column: ColumnOrName) -&gt; Column:\n    \"\"\"Returns the number of elements in an array column.\n\n    This function computes the length of arrays stored in the specified column.\n    Returns None for None arrays.\n\n    Args:\n        column: Column or column name containing arrays whose length to compute.\n\n    Returns:\n        A Column expression representing the array length.\n\n    Raises:\n        TypeError: If the column does not contain array data.\n\n    Example: Get array sizes\n        ```python\n        # Get the size of arrays in 'tags' column\n        df.select(array_size(\"tags\"))\n\n        # Use with column reference\n        df.select(array_size(col(\"tags\")))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        ArrayLengthExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.asc","title":"asc","text":"<pre><code>asc(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing an ascending sort order.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the ascending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the ascending sort order.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef asc(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing an ascending sort order.\n\n    Args:\n        column: The column to apply the ascending ordering to.\n\n    Returns:\n        A Column expression representing the column and the ascending sort order.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).asc()\n</code></pre>"},{"location":"reference/fenic/#fenic.asc_nulls_first","title":"asc_nulls_first","text":"<pre><code>asc_nulls_first(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing an ascending sort order with nulls first.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the ascending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the ascending sort order with nulls first.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef asc_nulls_first(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing an ascending sort order with nulls first.\n\n    Args:\n        column: The column to apply the ascending ordering to.\n\n    Returns:\n        A Column expression representing the column and the ascending sort order with nulls first.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).asc_nulls_first()\n</code></pre>"},{"location":"reference/fenic/#fenic.asc_nulls_last","title":"asc_nulls_last","text":"<pre><code>asc_nulls_last(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing an ascending sort order with nulls last.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the ascending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the ascending sort order with nulls last.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef asc_nulls_last(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing an ascending sort order with nulls last.\n\n    Args:\n        column: The column to apply the ascending ordering to.\n\n    Returns:\n        A Column expression representing the column and the ascending sort order with nulls last.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).asc_nulls_last()\n</code></pre>"},{"location":"reference/fenic/#fenic.avg","title":"avg","text":"<pre><code>avg(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the average (mean) of all values in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the average of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the average aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef avg(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the average (mean) of all values in the specified column.\n\n    Args:\n        column: Column or column name to compute the average of\n\n    Returns:\n        A Column expression representing the average aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        AvgExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.coalesce","title":"coalesce","text":"<pre><code>coalesce(*cols: ColumnOrName) -&gt; Column\n</code></pre> <p>Returns the first non-null value from the given columns for each row.</p> <p>This function mimics the behavior of SQL's COALESCE function. It evaluates the input columns in order and returns the first non-null value encountered. If all values are null, returns null.</p> <p>Parameters:</p> <ul> <li> <code>*cols</code>               (<code>ColumnOrName</code>, default:                   <code>()</code> )           \u2013            <p>Column expressions or column names to evaluate. Can be:</p> <ul> <li>Individual arguments</li> <li>Lists of columns/column names</li> <li>Tuples of columns/column names</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression containing the first non-null value from the input columns.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no columns are provided.</p> </li> </ul> Basic coalesce usage <pre><code># Basic usage\ndf.select(coalesce(\"col1\", \"col2\", \"col3\"))\n\n# With nested collections\ndf.select(coalesce([\"col1\", \"col2\"], \"col3\"))\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef coalesce(*cols: ColumnOrName) -&gt; Column:\n    \"\"\"Returns the first non-null value from the given columns for each row.\n\n    This function mimics the behavior of SQL's COALESCE function. It evaluates the input columns\n    in order and returns the first non-null value encountered. If all values are null, returns null.\n\n    Args:\n        *cols: Column expressions or column names to evaluate. Can be:\n\n            - Individual arguments\n            - Lists of columns/column names\n            - Tuples of columns/column names\n\n    Returns:\n        A Column expression containing the first non-null value from the input columns.\n\n    Raises:\n        ValueError: If no columns are provided.\n\n    Example: Basic coalesce usage\n        ```python\n        # Basic usage\n        df.select(coalesce(\"col1\", \"col2\", \"col3\"))\n\n        # With nested collections\n        df.select(coalesce([\"col1\", \"col2\"], \"col3\"))\n        ```\n    \"\"\"\n    if not cols:\n        raise ValueError(\"At least one column must be provided to coalesce method\")\n\n    flattened_args = []\n    for arg in cols:\n        if isinstance(arg, (list, tuple)):\n            flattened_args.extend(arg)\n        else:\n            flattened_args.append(arg)\n\n    flattened_exprs = [\n        Column._from_col_or_name(c)._logical_expr for c in flattened_args\n    ]\n    return Column._from_logical_expr(CoalesceExpr(flattened_exprs))\n</code></pre>"},{"location":"reference/fenic/#fenic.col","title":"col","text":"<pre><code>col(col_name: str) -&gt; Column\n</code></pre> <p>Creates a Column expression referencing a column in the DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>col_name</code>               (<code>str</code>)           \u2013            <p>Name of the column to reference</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression for the specified column</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If colName is not a string</p> </li> </ul> Source code in <code>src/fenic/api/functions/core.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef col(col_name: str) -&gt; Column:\n    \"\"\"Creates a Column expression referencing a column in the DataFrame.\n\n    Args:\n        col_name: Name of the column to reference\n\n    Returns:\n        A Column expression for the specified column\n\n    Raises:\n        TypeError: If colName is not a string\n    \"\"\"\n    return Column._from_column_name(col_name)\n</code></pre>"},{"location":"reference/fenic/#fenic.collect_list","title":"collect_list","text":"<pre><code>collect_list(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: collects all values from the specified column into a list.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to collect values from</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the list aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef collect_list(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: collects all values from the specified column into a list.\n\n    Args:\n        column: Column or column name to collect values from\n\n    Returns:\n        A Column expression representing the list aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        ListExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.configure_logging","title":"configure_logging","text":"<pre><code>configure_logging(log_level: int = logging.INFO, log_format: str = '%(asctime)s [%(name)s] %(levelname)s: %(message)s', log_stream: Optional[TextIO] = None) -&gt; None\n</code></pre> <p>Configure logging for the library and root logger in interactive environments.</p> <p>This function ensures that logs from the library's modules appear in output by setting up a default handler on the root logger only if one does not already exist. This is especially useful in notebooks, scripts, or REPLs where logging is often unset. It configures the root logger and sets the library's top-level logger to propagate logs to the root.</p> <p>If the root logger has no handlers, this function sets up a default configuration and silences noisy dependencies like 'openai' and 'httpx'.</p> <p>In more complex applications or when integrating with existing logging configurations, you might prefer to manage logging setup externally. In such cases, you may not need to call this function.</p> Source code in <code>src/fenic/logging.py</code> <pre><code>def configure_logging(\n    log_level: int = logging.INFO,\n    log_format: str = \"%(asctime)s [%(name)s] %(levelname)s: %(message)s\",\n    log_stream: Optional[TextIO] = None,\n) -&gt; None:\n    \"\"\"Configure logging for the library and root logger in interactive environments.\n\n    This function ensures that logs from the library's modules appear in output by\n    setting up a default handler on the root logger *only if* one does not already\n    exist. This is especially useful in notebooks, scripts, or REPLs where logging\n    is often unset. It configures the root logger and sets the library's top-level\n    logger to propagate logs to the root.\n\n    If the root logger has no handlers, this function sets up a default configuration\n    and silences noisy dependencies like 'openai' and 'httpx'.\n\n    In more complex applications or when integrating with existing logging\n    configurations, you might prefer to manage logging setup externally. In such\n    cases, you may not need to call this function.\n    \"\"\"\n    stream = log_stream or sys.stderr\n    formatter = logging.Formatter(log_format)\n    handler = logging.StreamHandler(stream)\n    handler.setFormatter(formatter)\n\n    root_logger = logging.getLogger()\n    if not root_logger.hasHandlers():\n        # Set up root logger only if not already configured\n        root_logger.setLevel(log_level)\n        root_logger.addHandler(handler)\n\n        # Silence noisy dependencies\n        for noisy_logger_name in (\"openai\", \"httpx\"):\n            noisy_logger = logging.getLogger(noisy_logger_name)\n            noisy_logger.setLevel(logging.ERROR)\n\n    # Set the library logger level and enable propagation\n    library_root_name = __name__.split(\".\")[0]\n    library_logger = logging.getLogger(library_root_name)\n    library_logger.setLevel(log_level)\n    library_logger.propagate = True\n</code></pre>"},{"location":"reference/fenic/#fenic.count","title":"count","text":"<pre><code>count(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the count of non-null values in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to count values in</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the count aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef count(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the count of non-null values in the specified column.\n\n    Args:\n        column: Column or column name to count values in\n\n    Returns:\n        A Column expression representing the count aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    if isinstance(column, str) and column == \"*\":\n        return Column._from_logical_expr(CountExpr(lit(\"*\")._logical_expr))\n    return Column._from_logical_expr(\n        CountExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.desc","title":"desc","text":"<pre><code>desc(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing a descending sort order.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the descending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the descending sort order.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef desc(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing a descending sort order.\n\n    Args:\n        column: The column to apply the descending ordering to.\n\n    Returns:\n        A Column expression representing the column and the descending sort order.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).desc()\n</code></pre>"},{"location":"reference/fenic/#fenic.desc_nulls_first","title":"desc_nulls_first","text":"<pre><code>desc_nulls_first(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing a descending sort order with nulls first.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the descending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the descending sort order with nulls first.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef desc_nulls_first(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing a descending sort order with nulls first.\n\n    Args:\n        column: The column to apply the descending ordering to.\n\n    Returns:\n        A Column expression representing the column and the descending sort order with nulls first.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).desc_nulls_first()\n</code></pre>"},{"location":"reference/fenic/#fenic.desc_nulls_last","title":"desc_nulls_last","text":"<pre><code>desc_nulls_last(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing a descending sort order with nulls last.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the descending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the descending sort order with nulls last.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef desc_nulls_last(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing a descending sort order with nulls last.\n\n    Args:\n        column: The column to apply the descending ordering to.\n\n    Returns:\n        A Column expression representing the column and the descending sort order with nulls last.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).desc_nulls_last()\n</code></pre>"},{"location":"reference/fenic/#fenic.lit","title":"lit","text":"<pre><code>lit(value: Any) -&gt; Column\n</code></pre> <p>Creates a Column expression representing a literal value.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>The literal value to create a column for</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the literal value</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the value cannot be inferred</p> </li> </ul> Source code in <code>src/fenic/api/functions/core.py</code> <pre><code>def lit(value: Any) -&gt; Column:\n    \"\"\"Creates a Column expression representing a literal value.\n\n    Args:\n        value: The literal value to create a column for\n\n    Returns:\n        A Column expression representing the literal value\n\n    Raises:\n        ValueError: If the type of the value cannot be inferred\n    \"\"\"\n    try:\n        inferred_type = infer_dtype_from_pyobj(value)\n    except TypeInferenceError as e:\n        raise ValidationError(f\"`lit` failed to infer type for value `{value}`\") from e\n    literal_expr = LiteralExpr(value, inferred_type)\n    return Column._from_logical_expr(literal_expr)\n</code></pre>"},{"location":"reference/fenic/#fenic.max","title":"max","text":"<pre><code>max(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the maximum value in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the maximum of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the maximum aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef max(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the maximum value in the specified column.\n\n    Args:\n        column: Column or column name to compute the maximum of\n\n    Returns:\n        A Column expression representing the maximum aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        MaxExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.mean","title":"mean","text":"<pre><code>mean(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the mean (average) of all values in the specified column.</p> <p>Alias for avg().</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the mean of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the mean aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef mean(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the mean (average) of all values in the specified column.\n\n    Alias for avg().\n\n    Args:\n        column: Column or column name to compute the mean of\n\n    Returns:\n        A Column expression representing the mean aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        AvgExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.min","title":"min","text":"<pre><code>min(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the minimum value in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the minimum of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the minimum aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef min(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the minimum value in the specified column.\n\n    Args:\n        column: Column or column name to compute the minimum of\n\n    Returns:\n        A Column expression representing the minimum aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        MinExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.struct","title":"struct","text":"<pre><code>struct(*args: Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]) -&gt; Column\n</code></pre> <p>Creates a new struct column from multiple input columns.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]</code>, default:                   <code>()</code> )           \u2013            <p>Columns or column names to combine into a struct. Can be:</p> <ul> <li>Individual arguments</li> <li>Lists of columns/column names</li> <li>Tuples of columns/column names</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing a struct containing the input columns</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If any argument is not a Column, string, or collection of Columns/strings</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef struct(\n    *args: Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]\n) -&gt; Column:\n    \"\"\"Creates a new struct column from multiple input columns.\n\n    Args:\n        *args: Columns or column names to combine into a struct. Can be:\n\n            - Individual arguments\n            - Lists of columns/column names\n            - Tuples of columns/column names\n\n    Returns:\n        A Column expression representing a struct containing the input columns\n\n    Raises:\n        TypeError: If any argument is not a Column, string, or collection of\n            Columns/strings\n    \"\"\"\n    flattened_args = []\n    for arg in args:\n        if isinstance(arg, (list, tuple)):\n            flattened_args.extend(arg)\n        else:\n            flattened_args.append(arg)\n\n    expr_columns = [Column._from_col_or_name(c)._logical_expr for c in flattened_args]\n\n    return Column._from_logical_expr(StructExpr(expr_columns))\n</code></pre>"},{"location":"reference/fenic/#fenic.sum","title":"sum","text":"<pre><code>sum(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the sum of all values in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the sum of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the sum aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef sum(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the sum of all values in the specified column.\n\n    Args:\n        column: Column or column name to compute the sum of\n\n    Returns:\n        A Column expression representing the sum aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        SumExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/#fenic.udf","title":"udf","text":"<pre><code>udf(f: Optional[Callable] = None, *, return_type: DataType)\n</code></pre> <p>A decorator or function for creating user-defined functions (UDFs) that can be applied to DataFrame rows.</p> <p>When applied, UDFs will: - Access <code>StructType</code> columns as Python dictionaries (<code>dict[str, Any]</code>). - Access <code>ArrayType</code> columns as Python lists (<code>list[Any]</code>). - Access primitive types (e.g., <code>int</code>, <code>float</code>, <code>str</code>) as their respective Python types.</p> <p>Parameters:</p> <ul> <li> <code>f</code>               (<code>Optional[Callable]</code>, default:                   <code>None</code> )           \u2013            <p>Python function to convert to UDF</p> </li> <li> <code>return_type</code>               (<code>DataType</code>)           \u2013            <p>Expected return type of the UDF. Required parameter.</p> </li> </ul> UDF with primitive types <pre><code># UDF with primitive types\n@udf(return_type=IntegerType)\ndef add_one(x: int):\n    return x + 1\n\n# Or\nadd_one = udf(lambda x: x + 1, return_type=IntegerType)\n</code></pre> UDF with nested types <pre><code># UDF with nested types\n@udf(return_type=StructType([StructField(\"value1\", IntegerType), StructField(\"value2\", IntegerType)]))\ndef example_udf(x: dict[str, int], y: list[int]):\n    return {\n        \"value1\": x[\"value1\"] + x[\"value2\"] + y[0],\n        \"value2\": x[\"value1\"] + x[\"value2\"] + y[1],\n    }\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef udf(f: Optional[Callable] = None, *, return_type: DataType):\n    \"\"\"A decorator or function for creating user-defined functions (UDFs) that can be applied to DataFrame rows.\n\n    When applied, UDFs will:\n    - Access `StructType` columns as Python dictionaries (`dict[str, Any]`).\n    - Access `ArrayType` columns as Python lists (`list[Any]`).\n    - Access primitive types (e.g., `int`, `float`, `str`) as their respective Python types.\n\n    Args:\n        f: Python function to convert to UDF\n\n        return_type: Expected return type of the UDF. Required parameter.\n\n    Example: UDF with primitive types\n        ```python\n        # UDF with primitive types\n        @udf(return_type=IntegerType)\n        def add_one(x: int):\n            return x + 1\n\n        # Or\n        add_one = udf(lambda x: x + 1, return_type=IntegerType)\n        ```\n\n    Example: UDF with nested types\n        ```python\n        # UDF with nested types\n        @udf(return_type=StructType([StructField(\"value1\", IntegerType), StructField(\"value2\", IntegerType)]))\n        def example_udf(x: dict[str, int], y: list[int]):\n            return {\n                \"value1\": x[\"value1\"] + x[\"value2\"] + y[0],\n                \"value2\": x[\"value1\"] + x[\"value2\"] + y[1],\n            }\n        ```\n    \"\"\"\n\n    def _create_udf(func: Callable) -&gt; Callable:\n        @wraps(func)\n        def _udf_wrapper(*cols: ColumnOrName) -&gt; Column:\n            col_exprs = [Column._from_col_or_name(c)._logical_expr for c in cols]\n            return Column._from_logical_expr(UDFExpr(func, col_exprs, return_type))\n\n        return _udf_wrapper\n\n    if f is not None:\n        return _create_udf(f)\n    return _create_udf\n</code></pre>"},{"location":"reference/fenic/#fenic.when","title":"when","text":"<pre><code>when(condition: Column, value: Column) -&gt; Column\n</code></pre> <p>Evaluates a condition and returns a value if true.</p> <p>This function is used to create conditional expressions. If Column.otherwise() is not invoked, None is returned for unmatched conditions.</p> <p>Parameters:</p> <ul> <li> <code>condition</code>               (<code>Column</code>)           \u2013            <p>A boolean Column expression to evaluate.</p> </li> <li> <code>value</code>               (<code>Column</code>)           \u2013            <p>A Column expression to return if the condition is true.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression that evaluates the condition and returns the specified value when true,</p> </li> <li> <code>Column</code>           \u2013            <p>and None otherwise.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the condition is not a boolean Column expression.</p> </li> </ul> Basic conditional expression <pre><code># Basic usage\ndf.select(when(col(\"age\") &gt; 18, lit(\"adult\")))\n\n# With otherwise\ndf.select(when(col(\"age\") &gt; 18, lit(\"adult\")).otherwise(lit(\"minor\")))\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef when(condition: Column, value: Column) -&gt; Column:\n    \"\"\"Evaluates a condition and returns a value if true.\n\n    This function is used to create conditional expressions. If Column.otherwise() is not invoked,\n    None is returned for unmatched conditions.\n\n    Args:\n        condition: A boolean Column expression to evaluate.\n\n        value: A Column expression to return if the condition is true.\n\n    Returns:\n        A Column expression that evaluates the condition and returns the specified value when true,\n        and None otherwise.\n\n    Raises:\n        TypeError: If the condition is not a boolean Column expression.\n\n    Example: Basic conditional expression\n        ```python\n        # Basic usage\n        df.select(when(col(\"age\") &gt; 18, lit(\"adult\")))\n\n        # With otherwise\n        df.select(when(col(\"age\") &gt; 18, lit(\"adult\")).otherwise(lit(\"minor\")))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        WhenExpr(None, condition._logical_expr, value._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/","title":"fenic.api","text":""},{"location":"reference/fenic/api/#fenic.api","title":"fenic.api","text":"<p>Query module for semantic operations on DataFrames.</p> <p>Classes:</p> <ul> <li> <code>AnthropicModelConfig</code>           \u2013            <p>Configuration for Anthropic models.</p> </li> <li> <code>Catalog</code>           \u2013            <p>Entry point for catalog operations.</p> </li> <li> <code>Column</code>           \u2013            <p>A column expression in a DataFrame.</p> </li> <li> <code>DataFrame</code>           \u2013            <p>A data collection organized into named columns.</p> </li> <li> <code>DataFrameReader</code>           \u2013            <p>Interface used to load a DataFrame from external storage systems.</p> </li> <li> <code>DataFrameWriter</code>           \u2013            <p>Interface used to write a DataFrame to external storage systems.</p> </li> <li> <code>GoogleGLAModelConfig</code>           \u2013            <p>Configuration for Google GenerativeLAnguage (GLA) models.</p> </li> <li> <code>GroupedData</code>           \u2013            <p>Methods for aggregations on a grouped DataFrame.</p> </li> <li> <code>Lineage</code>           \u2013            <p>Query interface for tracing data lineage through a query plan.</p> </li> <li> <code>OpenAIModelConfig</code>           \u2013            <p>Configuration for OpenAI models.</p> </li> <li> <code>SemanticConfig</code>           \u2013            <p>Configuration for semantic language and embedding models.</p> </li> <li> <code>SemanticExtensions</code>           \u2013            <p>A namespace for semantic dataframe operators.</p> </li> <li> <code>Session</code>           \u2013            <p>The entry point to programming with the DataFrame API. Similar to PySpark's SparkSession.</p> </li> <li> <code>SessionConfig</code>           \u2013            <p>Configuration for a user session.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>array</code>             \u2013              <p>Creates a new array column from multiple input columns.</p> </li> <li> <code>array_agg</code>             \u2013              <p>Alias for collect_list().</p> </li> <li> <code>array_contains</code>             \u2013              <p>Checks if array column contains a specific value.</p> </li> <li> <code>array_size</code>             \u2013              <p>Returns the number of elements in an array column.</p> </li> <li> <code>asc</code>             \u2013              <p>Creates a Column expression representing an ascending sort order.</p> </li> <li> <code>asc_nulls_first</code>             \u2013              <p>Creates a Column expression representing an ascending sort order with nulls first.</p> </li> <li> <code>asc_nulls_last</code>             \u2013              <p>Creates a Column expression representing an ascending sort order with nulls last.</p> </li> <li> <code>avg</code>             \u2013              <p>Aggregate function: returns the average (mean) of all values in the specified column.</p> </li> <li> <code>coalesce</code>             \u2013              <p>Returns the first non-null value from the given columns for each row.</p> </li> <li> <code>col</code>             \u2013              <p>Creates a Column expression referencing a column in the DataFrame.</p> </li> <li> <code>collect_list</code>             \u2013              <p>Aggregate function: collects all values from the specified column into a list.</p> </li> <li> <code>count</code>             \u2013              <p>Aggregate function: returns the count of non-null values in the specified column.</p> </li> <li> <code>desc</code>             \u2013              <p>Creates a Column expression representing a descending sort order.</p> </li> <li> <code>desc_nulls_first</code>             \u2013              <p>Creates a Column expression representing a descending sort order with nulls first.</p> </li> <li> <code>desc_nulls_last</code>             \u2013              <p>Creates a Column expression representing a descending sort order with nulls last.</p> </li> <li> <code>lit</code>             \u2013              <p>Creates a Column expression representing a literal value.</p> </li> <li> <code>max</code>             \u2013              <p>Aggregate function: returns the maximum value in the specified column.</p> </li> <li> <code>mean</code>             \u2013              <p>Aggregate function: returns the mean (average) of all values in the specified column.</p> </li> <li> <code>min</code>             \u2013              <p>Aggregate function: returns the minimum value in the specified column.</p> </li> <li> <code>struct</code>             \u2013              <p>Creates a new struct column from multiple input columns.</p> </li> <li> <code>sum</code>             \u2013              <p>Aggregate function: returns the sum of all values in the specified column.</p> </li> <li> <code>udf</code>             \u2013              <p>A decorator or function for creating user-defined functions (UDFs) that can be applied to DataFrame rows.</p> </li> <li> <code>when</code>             \u2013              <p>Evaluates a condition and returns a value if true.</p> </li> </ul>"},{"location":"reference/fenic/api/#fenic.api.AnthropicModelConfig","title":"AnthropicModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Anthropic models.</p> <p>This class defines the configuration settings for Anthropic language models, including model selection and separate rate limiting parameters for input and output tokens.</p> <p>Attributes:</p> <ul> <li> <code>model_name</code>               (<code>ANTHROPIC_AVAILABLE_LANGUAGE_MODELS</code>)           \u2013            <p>The name of the Anthropic model to use.</p> </li> <li> <code>rpm</code>               (<code>int</code>)           \u2013            <p>Requests per minute limit; must be greater than 0.</p> </li> <li> <code>input_tpm</code>               (<code>int</code>)           \u2013            <p>Input tokens per minute limit; must be greater than 0.</p> </li> <li> <code>output_tpm</code>               (<code>int</code>)           \u2013            <p>Output tokens per minute limit; must be greater than 0.</p> </li> </ul> <p>Examples:</p> <p>Configuring an Anthropic model with separate input/output rate limits:</p> <pre><code>config = AnthropicModelConfig(\n    model_name=\"claude-3-5-haiku-latest\",\n    rpm=100,\n    input_tpm=100,\n    output_tpm=100\n)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog","title":"Catalog","text":"<pre><code>Catalog(catalog: BaseCatalog)\n</code></pre> <p>Entry point for catalog operations.</p> <p>The Catalog provides methods to interact with and manage database tables, including listing available tables, describing table schemas, and dropping tables.</p> Basic usage <pre><code># Create a new catalog\nsession.catalog.create_catalog('my_catalog')\n# Returns: True\n\n# Set the current catalog\nsession.catalog.set_current_catalog('my_catalog')\n# Returns: None\n\n# Create a new database\nsession.catalog.create_database('my_database')\n# Returns: True\n\n# Use the new database\nsession.catalog.set_current_database('my_database')\n# Returns: None\n\n# Create a new table\nsession.catalog.create_table('my_table', Schema([\n    ColumnField('id', IntegerType),\n]))\n# Returns: True\n</code></pre> <p>Initialize a Catalog instance.</p> <p>Parameters:</p> <ul> <li> <code>catalog</code>               (<code>BaseCatalog</code>)           \u2013            <p>The underlying catalog implementation.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>create_catalog</code>             \u2013              <p>Creates a new catalog.</p> </li> <li> <code>create_database</code>             \u2013              <p>Creates a new database.</p> </li> <li> <code>create_table</code>             \u2013              <p>Creates a new table.</p> </li> <li> <code>describe_table</code>             \u2013              <p>Returns the schema of the specified table.</p> </li> <li> <code>does_catalog_exist</code>             \u2013              <p>Checks if a catalog with the specified name exists.</p> </li> <li> <code>does_database_exist</code>             \u2013              <p>Checks if a database with the specified name exists.</p> </li> <li> <code>does_table_exist</code>             \u2013              <p>Checks if a table with the specified name exists.</p> </li> <li> <code>drop_catalog</code>             \u2013              <p>Drops a catalog.</p> </li> <li> <code>drop_database</code>             \u2013              <p>Drops a database.</p> </li> <li> <code>drop_table</code>             \u2013              <p>Drops the specified table.</p> </li> <li> <code>get_current_catalog</code>             \u2013              <p>Returns the name of the current catalog.</p> </li> <li> <code>get_current_database</code>             \u2013              <p>Returns the name of the current database in the current catalog.</p> </li> <li> <code>list_catalogs</code>             \u2013              <p>Returns a list of available catalogs.</p> </li> <li> <code>list_databases</code>             \u2013              <p>Returns a list of databases in the current catalog.</p> </li> <li> <code>list_tables</code>             \u2013              <p>Returns a list of tables stored in the current database.</p> </li> <li> <code>set_current_catalog</code>             \u2013              <p>Sets the current catalog.</p> </li> <li> <code>set_current_database</code>             \u2013              <p>Sets the current database.</p> </li> </ul> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def __init__(self, catalog: BaseCatalog):\n    \"\"\"Initialize a Catalog instance.\n\n    Args:\n        catalog: The underlying catalog implementation.\n    \"\"\"\n    self.catalog = catalog\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog.create_catalog","title":"create_catalog","text":"<pre><code>create_catalog(catalog_name: str, ignore_if_exists: bool = True) -&gt; bool\n</code></pre> <p>Creates a new catalog.</p> <p>Parameters:</p> <ul> <li> <code>catalog_name</code>               (<code>str</code>)           \u2013            <p>Name of the catalog to create.</p> </li> <li> <code>ignore_if_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, return False when the catalog already exists. If False, raise an error when the catalog already exists. Defaults to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CatalogAlreadyExistsError</code>             \u2013            <p>If the catalog already exists and ignore_if_exists is False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the catalog was created successfully, False if the catalog</p> </li> <li> <code>bool</code>           \u2013            <p>already exists and ignore_if_exists is True.</p> </li> </ul> Create a new catalog <pre><code># Create a new catalog named 'my_catalog'\nsession.catalog.create_catalog('my_catalog')\n# Returns: True\n</code></pre> Create an existing catalog with ignore_if_exists <pre><code># Try to create an existing catalog with ignore_if_exists=True\nsession.catalog.create_catalog('my_catalog', ignore_if_exists=True)\n# Returns: False\n</code></pre> Create an existing catalog without ignore_if_exists <pre><code># Try to create an existing catalog with ignore_if_exists=False\nsession.catalog.create_catalog('my_catalog', ignore_if_exists=False)\n# Raises: CatalogAlreadyExistsError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef create_catalog(self, catalog_name: str, ignore_if_exists: bool = True) -&gt; bool:\n    \"\"\"Creates a new catalog.\n\n    Args:\n        catalog_name (str): Name of the catalog to create.\n        ignore_if_exists (bool): If True, return False when the catalog already exists.\n            If False, raise an error when the catalog already exists.\n            Defaults to True.\n\n    Raises:\n        CatalogAlreadyExistsError: If the catalog already exists and ignore_if_exists is False.\n\n    Returns:\n        bool: True if the catalog was created successfully, False if the catalog\n        already exists and ignore_if_exists is True.\n\n    Example: Create a new catalog\n        ```python\n        # Create a new catalog named 'my_catalog'\n        session.catalog.create_catalog('my_catalog')\n        # Returns: True\n        ```\n\n    Example: Create an existing catalog with ignore_if_exists\n        ```python\n        # Try to create an existing catalog with ignore_if_exists=True\n        session.catalog.create_catalog('my_catalog', ignore_if_exists=True)\n        # Returns: False\n        ```\n\n    Example: Create an existing catalog without ignore_if_exists\n        ```python\n        # Try to create an existing catalog with ignore_if_exists=False\n        session.catalog.create_catalog('my_catalog', ignore_if_exists=False)\n        # Raises: CatalogAlreadyExistsError\n        ```\n    \"\"\"\n    return self.catalog.create_catalog(catalog_name, ignore_if_exists)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog.create_database","title":"create_database","text":"<pre><code>create_database(database_name: str, ignore_if_exists: bool = True) -&gt; bool\n</code></pre> <p>Creates a new database.</p> <p>Parameters:</p> <ul> <li> <code>database_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative database name to create.</p> </li> <li> <code>ignore_if_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, return False when the database already exists. If False, raise an error when the database already exists. Defaults to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatabaseAlreadyExistsError</code>             \u2013            <p>If the database already exists and ignore_if_exists is False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the database was created successfully, False if the database</p> </li> <li> <code>bool</code>           \u2013            <p>already exists and ignore_if_exists is True.</p> </li> </ul> Create a new database <pre><code># Create a new database named 'my_database'\nsession.catalog.create_database('my_database')\n# Returns: True\n</code></pre> Create an existing database with ignore_if_exists <pre><code># Try to create an existing database with ignore_if_exists=True\nsession.catalog.create_database('my_database', ignore_if_exists=True)\n# Returns: False\n</code></pre> Create an existing database without ignore_if_exists <pre><code># Try to create an existing database with ignore_if_exists=False\nsession.catalog.create_database('my_database', ignore_if_exists=False)\n# Raises: DatabaseAlreadyExistsError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef create_database(\n    self, database_name: str, ignore_if_exists: bool = True\n) -&gt; bool:\n    \"\"\"Creates a new database.\n\n    Args:\n        database_name (str): Fully qualified or relative database name to create.\n        ignore_if_exists (bool): If True, return False when the database already exists.\n            If False, raise an error when the database already exists.\n            Defaults to True.\n\n    Raises:\n        DatabaseAlreadyExistsError: If the database already exists and ignore_if_exists is False.\n\n    Returns:\n        bool: True if the database was created successfully, False if the database\n        already exists and ignore_if_exists is True.\n\n    Example: Create a new database\n        ```python\n        # Create a new database named 'my_database'\n        session.catalog.create_database('my_database')\n        # Returns: True\n        ```\n\n    Example: Create an existing database with ignore_if_exists\n        ```python\n        # Try to create an existing database with ignore_if_exists=True\n        session.catalog.create_database('my_database', ignore_if_exists=True)\n        # Returns: False\n        ```\n\n    Example: Create an existing database without ignore_if_exists\n        ```python\n        # Try to create an existing database with ignore_if_exists=False\n        session.catalog.create_database('my_database', ignore_if_exists=False)\n        # Raises: DatabaseAlreadyExistsError\n        ```\n    \"\"\"\n    return self.catalog.create_database(database_name, ignore_if_exists)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog.create_table","title":"create_table","text":"<pre><code>create_table(table_name: str, schema: Schema, ignore_if_exists: bool = True) -&gt; bool\n</code></pre> <p>Creates a new table.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative table name to create.</p> </li> <li> <code>schema</code>               (<code>Schema</code>)           \u2013            <p>Schema of the table to create.</p> </li> <li> <code>ignore_if_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, return False when the table already exists. If False, raise an error when the table already exists. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the table was created successfully, False if the table</p> </li> <li> <code>bool</code>           \u2013            <p>already exists and ignore_if_exists is True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TableAlreadyExistsError</code>             \u2013            <p>If the table already exists and ignore_if_exists is False</p> </li> </ul> Create a new table <pre><code># Create a new table with an integer column\nsession.catalog.create_table('my_table', Schema([\n    ColumnField('id', IntegerType),\n]))\n# Returns: True\n</code></pre> Create an existing table with ignore_if_exists <pre><code># Try to create an existing table with ignore_if_exists=True\nsession.catalog.create_table('my_table', Schema([\n    ColumnField('id', IntegerType),\n]), ignore_if_exists=True)\n# Returns: False\n</code></pre> Create an existing table without ignore_if_exists <pre><code># Try to create an existing table with ignore_if_exists=False\nsession.catalog.create_table('my_table', Schema([\n    ColumnField('id', IntegerType),\n]), ignore_if_exists=False)\n# Raises: TableAlreadyExistsError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef create_table(\n    self, table_name: str, schema: Schema, ignore_if_exists: bool = True\n) -&gt; bool:\n    \"\"\"Creates a new table.\n\n    Args:\n        table_name (str): Fully qualified or relative table name to create.\n        schema (Schema): Schema of the table to create.\n        ignore_if_exists (bool): If True, return False when the table already exists.\n            If False, raise an error when the table already exists.\n            Defaults to True.\n\n    Returns:\n        bool: True if the table was created successfully, False if the table\n        already exists and ignore_if_exists is True.\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists and ignore_if_exists is False\n\n    Example: Create a new table\n        ```python\n        # Create a new table with an integer column\n        session.catalog.create_table('my_table', Schema([\n            ColumnField('id', IntegerType),\n        ]))\n        # Returns: True\n        ```\n\n    Example: Create an existing table with ignore_if_exists\n        ```python\n        # Try to create an existing table with ignore_if_exists=True\n        session.catalog.create_table('my_table', Schema([\n            ColumnField('id', IntegerType),\n        ]), ignore_if_exists=True)\n        # Returns: False\n        ```\n\n    Example: Create an existing table without ignore_if_exists\n        ```python\n        # Try to create an existing table with ignore_if_exists=False\n        session.catalog.create_table('my_table', Schema([\n            ColumnField('id', IntegerType),\n        ]), ignore_if_exists=False)\n        # Raises: TableAlreadyExistsError\n        ```\n    \"\"\"\n    return self.catalog.create_table(table_name, schema, ignore_if_exists)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog.describe_table","title":"describe_table","text":"<pre><code>describe_table(table_name: str) -&gt; Schema\n</code></pre> <p>Returns the schema of the specified table.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative table name to describe.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Schema</code> (              <code>Schema</code> )          \u2013            <p>A schema object describing the table's structure with field names and types.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TableNotFoundError</code>             \u2013            <p>If the table doesn't exist.</p> </li> </ul> Describe a table's schema <pre><code># For a table created with: CREATE TABLE t1 (id int)\nsession.catalog.describe_table('t1')\n# Returns: Schema([\n#     ColumnField('id', IntegerType),\n# ])\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef describe_table(self, table_name: str) -&gt; Schema:\n    \"\"\"Returns the schema of the specified table.\n\n    Args:\n        table_name (str): Fully qualified or relative table name to describe.\n\n    Returns:\n        Schema: A schema object describing the table's structure with field names and types.\n\n    Raises:\n        TableNotFoundError: If the table doesn't exist.\n\n    Example: Describe a table's schema\n        ```python\n        # For a table created with: CREATE TABLE t1 (id int)\n        session.catalog.describe_table('t1')\n        # Returns: Schema([\n        #     ColumnField('id', IntegerType),\n        # ])\n        ```\n    \"\"\"\n    return self.catalog.describe_table(table_name)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog.does_catalog_exist","title":"does_catalog_exist","text":"<pre><code>does_catalog_exist(catalog_name: str) -&gt; bool\n</code></pre> <p>Checks if a catalog with the specified name exists.</p> <p>Parameters:</p> <ul> <li> <code>catalog_name</code>               (<code>str</code>)           \u2013            <p>Name of the catalog to check.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the catalog exists, False otherwise.</p> </li> </ul> Check if a catalog exists <pre><code># Check if 'my_catalog' exists\nsession.catalog.does_catalog_exist('my_catalog')\n# Returns: True\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef does_catalog_exist(self, catalog_name: str) -&gt; bool:\n    \"\"\"Checks if a catalog with the specified name exists.\n\n    Args:\n        catalog_name (str): Name of the catalog to check.\n\n    Returns:\n        bool: True if the catalog exists, False otherwise.\n\n    Example: Check if a catalog exists\n        ```python\n        # Check if 'my_catalog' exists\n        session.catalog.does_catalog_exist('my_catalog')\n        # Returns: True\n        ```\n    \"\"\"\n    return self.catalog.does_catalog_exist(catalog_name)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog.does_database_exist","title":"does_database_exist","text":"<pre><code>does_database_exist(database_name: str) -&gt; bool\n</code></pre> <p>Checks if a database with the specified name exists.</p> <p>Parameters:</p> <ul> <li> <code>database_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative database name to check.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the database exists, False otherwise.</p> </li> </ul> Check if a database exists <pre><code># Check if 'my_database' exists\nsession.catalog.does_database_exist('my_database')\n# Returns: True\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef does_database_exist(self, database_name: str) -&gt; bool:\n    \"\"\"Checks if a database with the specified name exists.\n\n    Args:\n        database_name (str): Fully qualified or relative database name to check.\n\n    Returns:\n        bool: True if the database exists, False otherwise.\n\n    Example: Check if a database exists\n        ```python\n        # Check if 'my_database' exists\n        session.catalog.does_database_exist('my_database')\n        # Returns: True\n        ```\n    \"\"\"\n    return self.catalog.does_database_exist(database_name)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog.does_table_exist","title":"does_table_exist","text":"<pre><code>does_table_exist(table_name: str) -&gt; bool\n</code></pre> <p>Checks if a table with the specified name exists.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative table name to check.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the table exists, False otherwise.</p> </li> </ul> Check if a table exists <pre><code># Check if 'my_table' exists\nsession.catalog.does_table_exist('my_table')\n# Returns: True\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef does_table_exist(self, table_name: str) -&gt; bool:\n    \"\"\"Checks if a table with the specified name exists.\n\n    Args:\n        table_name (str): Fully qualified or relative table name to check.\n\n    Returns:\n        bool: True if the table exists, False otherwise.\n\n    Example: Check if a table exists\n        ```python\n        # Check if 'my_table' exists\n        session.catalog.does_table_exist('my_table')\n        # Returns: True\n        ```\n    \"\"\"\n    return self.catalog.does_table_exist(table_name)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog.drop_catalog","title":"drop_catalog","text":"<pre><code>drop_catalog(catalog_name: str, ignore_if_not_exists: bool = True) -&gt; bool\n</code></pre> <p>Drops a catalog.</p> <p>Parameters:</p> <ul> <li> <code>catalog_name</code>               (<code>str</code>)           \u2013            <p>Name of the catalog to drop.</p> </li> <li> <code>ignore_if_not_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, silently return if the catalog doesn't exist. If False, raise an error if the catalog doesn't exist. Defaults to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CatalogNotFoundError</code>             \u2013            <p>If the catalog does not exist and ignore_if_not_exists is False</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the catalog was dropped successfully, False if the catalog</p> </li> <li> <code>bool</code>           \u2013            <p>didn't exist and ignore_if_not_exists is True.</p> </li> </ul> Drop a non-existent catalog <pre><code># Try to drop a non-existent catalog\nsession.catalog.drop_catalog('my_catalog')\n# Returns: False\n</code></pre> Drop a non-existent catalog without ignore_if_not_exists <pre><code># Try to drop a non-existent catalog with ignore_if_not_exists=False\nsession.catalog.drop_catalog('my_catalog', ignore_if_not_exists=False)\n# Raises: CatalogNotFoundError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef drop_catalog(\n    self, catalog_name: str, ignore_if_not_exists: bool = True\n) -&gt; bool:\n    \"\"\"Drops a catalog.\n\n    Args:\n        catalog_name (str): Name of the catalog to drop.\n        ignore_if_not_exists (bool): If True, silently return if the catalog doesn't exist.\n            If False, raise an error if the catalog doesn't exist.\n            Defaults to True.\n\n    Raises:\n        CatalogNotFoundError: If the catalog does not exist and ignore_if_not_exists is False\n\n    Returns:\n        bool: True if the catalog was dropped successfully, False if the catalog\n        didn't exist and ignore_if_not_exists is True.\n\n    Example: Drop a non-existent catalog\n        ```python\n        # Try to drop a non-existent catalog\n        session.catalog.drop_catalog('my_catalog')\n        # Returns: False\n        ```\n\n    Example: Drop a non-existent catalog without ignore_if_not_exists\n        ```python\n        # Try to drop a non-existent catalog with ignore_if_not_exists=False\n        session.catalog.drop_catalog('my_catalog', ignore_if_not_exists=False)\n        # Raises: CatalogNotFoundError\n        ```\n    \"\"\"\n    return self.catalog.drop_catalog(catalog_name, ignore_if_not_exists)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog.drop_database","title":"drop_database","text":"<pre><code>drop_database(database_name: str, cascade: bool = False, ignore_if_not_exists: bool = True) -&gt; bool\n</code></pre> <p>Drops a database.</p> <p>Parameters:</p> <ul> <li> <code>database_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative database name to drop.</p> </li> <li> <code>cascade</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, drop all tables in the database. Defaults to False.</p> </li> <li> <code>ignore_if_not_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, silently return if the database doesn't exist. If False, raise an error if the database doesn't exist. Defaults to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatabaseNotFoundError</code>             \u2013            <p>If the database does not exist and ignore_if_not_exists is False</p> </li> <li> <code>CatalogError</code>             \u2013            <p>If the current database is being dropped, if the database is not empty and cascade is False</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the database was dropped successfully, False if the database</p> </li> <li> <code>bool</code>           \u2013            <p>didn't exist and ignore_if_not_exists is True.</p> </li> </ul> Drop a non-existent database <pre><code># Try to drop a non-existent database\nsession.catalog.drop_database('my_database')\n# Returns: False\n</code></pre> Drop a non-existent database without ignore_if_not_exists <pre><code># Try to drop a non-existent database with ignore_if_not_exists=False\nsession.catalog.drop_database('my_database', ignore_if_not_exists=False)\n# Raises: DatabaseNotFoundError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef drop_database(\n    self,\n    database_name: str,\n    cascade: bool = False,\n    ignore_if_not_exists: bool = True,\n) -&gt; bool:\n    \"\"\"Drops a database.\n\n    Args:\n        database_name (str): Fully qualified or relative database name to drop.\n        cascade (bool): If True, drop all tables in the database.\n            Defaults to False.\n        ignore_if_not_exists (bool): If True, silently return if the database doesn't exist.\n            If False, raise an error if the database doesn't exist.\n            Defaults to True.\n\n    Raises:\n        DatabaseNotFoundError: If the database does not exist and ignore_if_not_exists is False\n        CatalogError: If the current database is being dropped, if the database is not empty and cascade is False\n\n    Returns:\n        bool: True if the database was dropped successfully, False if the database\n        didn't exist and ignore_if_not_exists is True.\n\n    Example: Drop a non-existent database\n        ```python\n        # Try to drop a non-existent database\n        session.catalog.drop_database('my_database')\n        # Returns: False\n        ```\n\n    Example: Drop a non-existent database without ignore_if_not_exists\n        ```python\n        # Try to drop a non-existent database with ignore_if_not_exists=False\n        session.catalog.drop_database('my_database', ignore_if_not_exists=False)\n        # Raises: DatabaseNotFoundError\n        ```\n    \"\"\"\n    return self.catalog.drop_database(database_name, cascade, ignore_if_not_exists)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog.drop_table","title":"drop_table","text":"<pre><code>drop_table(table_name: str, ignore_if_not_exists: bool = True) -&gt; bool\n</code></pre> <p>Drops the specified table.</p> <p>By default this method will return False if the table doesn't exist.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative table name to drop.</p> </li> <li> <code>ignore_if_not_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, return False when the table doesn't exist. If False, raise an error when the table doesn't exist. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the table was dropped successfully, False if the table</p> </li> <li> <code>bool</code>           \u2013            <p>didn't exist and ignore_if_not_exist is True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TableNotFoundError</code>             \u2013            <p>If the table doesn't exist and ignore_if_not_exists is False</p> </li> </ul> Drop an existing table <pre><code># Drop an existing table 't1'\nsession.catalog.drop_table('t1')\n# Returns: True\n</code></pre> Drop a non-existent table with ignore_if_not_exists <pre><code># Try to drop a non-existent table with ignore_if_not_exists=True\nsession.catalog.drop_table('t2', ignore_if_not_exists=True)\n# Returns: False\n</code></pre> Drop a non-existent table without ignore_if_not_exists <pre><code># Try to drop a non-existent table with ignore_if_not_exists=False\nsession.catalog.drop_table('t2', ignore_if_not_exists=False)\n# Raises: TableNotFoundError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef drop_table(self, table_name: str, ignore_if_not_exists: bool = True) -&gt; bool:\n    \"\"\"Drops the specified table.\n\n    By default this method will return False if the table doesn't exist.\n\n    Args:\n        table_name (str): Fully qualified or relative table name to drop.\n        ignore_if_not_exists (bool): If True, return False when the table doesn't exist.\n            If False, raise an error when the table doesn't exist.\n            Defaults to True.\n\n    Returns:\n        bool: True if the table was dropped successfully, False if the table\n        didn't exist and ignore_if_not_exist is True.\n\n    Raises:\n        TableNotFoundError: If the table doesn't exist and ignore_if_not_exists is False\n\n    Example: Drop an existing table\n        ```python\n        # Drop an existing table 't1'\n        session.catalog.drop_table('t1')\n        # Returns: True\n        ```\n\n    Example: Drop a non-existent table with ignore_if_not_exists\n        ```python\n        # Try to drop a non-existent table with ignore_if_not_exists=True\n        session.catalog.drop_table('t2', ignore_if_not_exists=True)\n        # Returns: False\n        ```\n\n    Example: Drop a non-existent table without ignore_if_not_exists\n        ```python\n        # Try to drop a non-existent table with ignore_if_not_exists=False\n        session.catalog.drop_table('t2', ignore_if_not_exists=False)\n        # Raises: TableNotFoundError\n        ```\n    \"\"\"\n    return self.catalog.drop_table(table_name, ignore_if_not_exists)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog.get_current_catalog","title":"get_current_catalog","text":"<pre><code>get_current_catalog() -&gt; str\n</code></pre> <p>Returns the name of the current catalog.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The name of the current catalog.</p> </li> </ul> Get current catalog name <pre><code># Get the name of the current catalog\nsession.catalog.get_current_catalog()\n# Returns: 'default'\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def get_current_catalog(self) -&gt; str:\n    \"\"\"Returns the name of the current catalog.\n\n    Returns:\n        str: The name of the current catalog.\n\n    Example: Get current catalog name\n        ```python\n        # Get the name of the current catalog\n        session.catalog.get_current_catalog()\n        # Returns: 'default'\n        ```\n    \"\"\"\n    return self.catalog.get_current_catalog()\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog.get_current_database","title":"get_current_database","text":"<pre><code>get_current_database() -&gt; str\n</code></pre> <p>Returns the name of the current database in the current catalog.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The name of the current database.</p> </li> </ul> Get current database name <pre><code># Get the name of the current database\nsession.catalog.get_current_database()\n# Returns: 'default'\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def get_current_database(self) -&gt; str:\n    \"\"\"Returns the name of the current database in the current catalog.\n\n    Returns:\n        str: The name of the current database.\n\n    Example: Get current database name\n        ```python\n        # Get the name of the current database\n        session.catalog.get_current_database()\n        # Returns: 'default'\n        ```\n    \"\"\"\n    return self.catalog.get_current_database()\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog.list_catalogs","title":"list_catalogs","text":"<pre><code>list_catalogs() -&gt; List[str]\n</code></pre> <p>Returns a list of available catalogs.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of catalog names available in the system.</p> </li> <li> <code>List[str]</code>           \u2013            <p>Returns an empty list if no catalogs are found.</p> </li> </ul> List all catalogs <pre><code># Get all available catalogs\nsession.catalog.list_catalogs()\n# Returns: ['default', 'my_catalog', 'other_catalog']\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def list_catalogs(self) -&gt; List[str]:\n    \"\"\"Returns a list of available catalogs.\n\n    Returns:\n        List[str]: A list of catalog names available in the system.\n        Returns an empty list if no catalogs are found.\n\n    Example: List all catalogs\n        ```python\n        # Get all available catalogs\n        session.catalog.list_catalogs()\n        # Returns: ['default', 'my_catalog', 'other_catalog']\n        ```\n    \"\"\"\n    return self.catalog.list_catalogs()\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog.list_databases","title":"list_databases","text":"<pre><code>list_databases() -&gt; List[str]\n</code></pre> <p>Returns a list of databases in the current catalog.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of database names in the current catalog.</p> </li> <li> <code>List[str]</code>           \u2013            <p>Returns an empty list if no databases are found.</p> </li> </ul> List all databases <pre><code># Get all databases in the current catalog\nsession.catalog.list_databases()\n# Returns: ['default', 'my_database', 'other_database']\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def list_databases(self) -&gt; List[str]:\n    \"\"\"Returns a list of databases in the current catalog.\n\n    Returns:\n        List[str]: A list of database names in the current catalog.\n        Returns an empty list if no databases are found.\n\n    Example: List all databases\n        ```python\n        # Get all databases in the current catalog\n        session.catalog.list_databases()\n        # Returns: ['default', 'my_database', 'other_database']\n        ```\n    \"\"\"\n    return self.catalog.list_databases()\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog.list_tables","title":"list_tables","text":"<pre><code>list_tables() -&gt; List[str]\n</code></pre> <p>Returns a list of tables stored in the current database.</p> <p>This method queries the current database to retrieve all available table names.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of table names stored in the database.</p> </li> <li> <code>List[str]</code>           \u2013            <p>Returns an empty list if no tables are found.</p> </li> </ul> List all tables <pre><code># Get all tables in the current database\nsession.catalog.list_tables()\n# Returns: ['table1', 'table2', 'table3']\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def list_tables(self) -&gt; List[str]:\n    \"\"\"Returns a list of tables stored in the current database.\n\n    This method queries the current database to retrieve all available table names.\n\n    Returns:\n        List[str]: A list of table names stored in the database.\n        Returns an empty list if no tables are found.\n\n    Example: List all tables\n        ```python\n        # Get all tables in the current database\n        session.catalog.list_tables()\n        # Returns: ['table1', 'table2', 'table3']\n        ```\n    \"\"\"\n    return self.catalog.list_tables()\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog.set_current_catalog","title":"set_current_catalog","text":"<pre><code>set_current_catalog(catalog_name: str) -&gt; None\n</code></pre> <p>Sets the current catalog.</p> <p>Parameters:</p> <ul> <li> <code>catalog_name</code>               (<code>str</code>)           \u2013            <p>Name of the catalog to set as current.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the specified catalog doesn't exist.</p> </li> </ul> Set current catalog <pre><code># Set 'my_catalog' as the current catalog\nsession.catalog.set_current_catalog('my_catalog')\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef set_current_catalog(self, catalog_name: str) -&gt; None:\n    \"\"\"Sets the current catalog.\n\n    Args:\n        catalog_name (str): Name of the catalog to set as current.\n\n    Raises:\n        ValueError: If the specified catalog doesn't exist.\n\n    Example: Set current catalog\n        ```python\n        # Set 'my_catalog' as the current catalog\n        session.catalog.set_current_catalog('my_catalog')\n        ```\n    \"\"\"\n    self.catalog.set_current_catalog(catalog_name)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Catalog.set_current_database","title":"set_current_database","text":"<pre><code>set_current_database(database_name: str) -&gt; None\n</code></pre> <p>Sets the current database.</p> <p>Parameters:</p> <ul> <li> <code>database_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative database name to set as current.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatabaseNotFoundError</code>             \u2013            <p>If the specified database doesn't exist.</p> </li> </ul> Set current database <pre><code># Set 'my_database' as the current database\nsession.catalog.set_current_database('my_database')\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef set_current_database(self, database_name: str) -&gt; None:\n    \"\"\"Sets the current database.\n\n    Args:\n        database_name (str): Fully qualified or relative database name to set as current.\n\n    Raises:\n        DatabaseNotFoundError: If the specified database doesn't exist.\n\n    Example: Set current database\n        ```python\n        # Set 'my_database' as the current database\n        session.catalog.set_current_database('my_database')\n        ```\n    \"\"\"\n    self.catalog.set_current_database(database_name)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column","title":"Column","text":"<p>A column expression in a DataFrame.</p> <p>This class represents a column expression that can be used in DataFrame operations. It provides methods for accessing, transforming, and combining column data.</p> Create a column reference <pre><code># Reference a column by name using col() function\ncol(\"column_name\")\n</code></pre> Use column in operations <pre><code># Perform arithmetic operations\ndf.select(col(\"price\") * col(\"quantity\"))\n</code></pre> Chain column operations <pre><code># Chain multiple operations\ndf.select(col(\"name\").upper().contains(\"John\"))\n</code></pre> <p>Methods:</p> <ul> <li> <code>alias</code>             \u2013              <p>Create an alias for this column.</p> </li> <li> <code>asc</code>             \u2013              <p>Apply ascending order to this column during a dataframe sort or order_by.</p> </li> <li> <code>asc_nulls_first</code>             \u2013              <p>Apply ascending order putting nulls first to this column during a dataframe sort or order_by.</p> </li> <li> <code>asc_nulls_last</code>             \u2013              <p>Apply ascending order putting nulls last to this column during a dataframe sort or order_by.</p> </li> <li> <code>cast</code>             \u2013              <p>Cast the column to a new data type.</p> </li> <li> <code>contains</code>             \u2013              <p>Check if the column contains a substring.</p> </li> <li> <code>contains_any</code>             \u2013              <p>Check if the column contains any of the specified substrings.</p> </li> <li> <code>desc</code>             \u2013              <p>Apply descending order to this column during a dataframe sort or order_by.</p> </li> <li> <code>desc_nulls_first</code>             \u2013              <p>Apply descending order putting nulls first to this column during a dataframe sort or order_by.</p> </li> <li> <code>desc_nulls_last</code>             \u2013              <p>Apply descending order putting nulls last to this column during a dataframe sort or order_by.</p> </li> <li> <code>ends_with</code>             \u2013              <p>Check if the column ends with a substring.</p> </li> <li> <code>get_item</code>             \u2013              <p>Access an item in a struct or array column.</p> </li> <li> <code>ilike</code>             \u2013              <p>Check if the column matches a SQL LIKE pattern (case-insensitive).</p> </li> <li> <code>is_in</code>             \u2013              <p>Check if the column is in a list of values or a column expression.</p> </li> <li> <code>is_not_null</code>             \u2013              <p>Check if the column contains non-NULL values.</p> </li> <li> <code>is_null</code>             \u2013              <p>Check if the column contains NULL values.</p> </li> <li> <code>like</code>             \u2013              <p>Check if the column matches a SQL LIKE pattern.</p> </li> <li> <code>otherwise</code>             \u2013              <p>Evaluates a list of conditions and returns one of multiple possible result expressions.</p> </li> <li> <code>rlike</code>             \u2013              <p>Check if the column matches a regular expression pattern.</p> </li> <li> <code>starts_with</code>             \u2013              <p>Check if the column starts with a substring.</p> </li> <li> <code>when</code>             \u2013              <p>Evaluates a list of conditions and returns one of multiple possible result expressions.</p> </li> </ul>"},{"location":"reference/fenic/api/#fenic.api.Column.alias","title":"alias","text":"<pre><code>alias(name: str) -&gt; Column\n</code></pre> <p>Create an alias for this column.</p> <p>This method assigns a new name to the column expression, which is useful for renaming columns or providing names for complex expressions.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The alias name to assign</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>Column with the specified alias</p> </li> </ul> Rename a column <pre><code># Rename a column to a new name\ndf.select(col(\"original_name\").alias(\"new_name\"))\n</code></pre> Name a complex expression <pre><code># Give a name to a calculated column\ndf.select((col(\"price\") * col(\"quantity\")).alias(\"total_value\"))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def alias(self, name: str) -&gt; Column:\n    \"\"\"Create an alias for this column.\n\n    This method assigns a new name to the column expression, which is useful\n    for renaming columns or providing names for complex expressions.\n\n    Args:\n        name (str): The alias name to assign\n\n    Returns:\n        Column: Column with the specified alias\n\n    Example: Rename a column\n        ```python\n        # Rename a column to a new name\n        df.select(col(\"original_name\").alias(\"new_name\"))\n        ```\n\n    Example: Name a complex expression\n        ```python\n        # Give a name to a calculated column\n        df.select((col(\"price\") * col(\"quantity\")).alias(\"total_value\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(AliasExpr(self._logical_expr, name))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.asc","title":"asc","text":"<pre><code>asc() -&gt; Column\n</code></pre> <p>Apply ascending order to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in ascending order <pre><code># Sort a dataframe by age in ascending order\ndf.sort(col(\"age\").asc()).show()\n</code></pre> Sort using column reference <pre><code># Sort using column reference with ascending order\ndf.sort(col(\"age\").asc()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def asc(self) -&gt; Column:\n    \"\"\"Apply ascending order to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function.\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in ascending order\n        ```python\n        # Sort a dataframe by age in ascending order\n        df.sort(col(\"age\").asc()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        # Sort using column reference with ascending order\n        df.sort(col(\"age\").asc()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(SortExpr(self._logical_expr, ascending=True))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.asc_nulls_first","title":"asc_nulls_first","text":"<pre><code>asc_nulls_first() -&gt; Column\n</code></pre> <p>Apply ascending order putting nulls first to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in ascending order with nulls first <pre><code># Sort a dataframe by age in ascending order, with nulls appearing first\ndf.sort(col(\"age\").asc_nulls_first()).show()\n</code></pre> Sort using column reference <pre><code># Sort using column reference with ascending order and nulls first\ndf.sort(col(\"age\").asc_nulls_first()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def asc_nulls_first(self) -&gt; Column:\n    \"\"\"Apply ascending order putting nulls first to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function.\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in ascending order with nulls first\n        ```python\n        # Sort a dataframe by age in ascending order, with nulls appearing first\n        df.sort(col(\"age\").asc_nulls_first()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        # Sort using column reference with ascending order and nulls first\n        df.sort(col(\"age\").asc_nulls_first()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        SortExpr(self._logical_expr, ascending=True, nulls_last=False)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.asc_nulls_last","title":"asc_nulls_last","text":"<pre><code>asc_nulls_last() -&gt; Column\n</code></pre> <p>Apply ascending order putting nulls last to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in ascending order with nulls last <pre><code># Sort a dataframe by age in ascending order, with nulls appearing last\ndf.sort(col(\"age\").asc_nulls_last()).show()\n</code></pre> Sort using column reference <pre><code># Sort using column reference with ascending order and nulls last\ndf.sort(col(\"age\").asc_nulls_last()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def asc_nulls_last(self) -&gt; Column:\n    \"\"\"Apply ascending order putting nulls last to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function.\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in ascending order with nulls last\n        ```python\n        # Sort a dataframe by age in ascending order, with nulls appearing last\n        df.sort(col(\"age\").asc_nulls_last()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        # Sort using column reference with ascending order and nulls last\n        df.sort(col(\"age\").asc_nulls_last()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        SortExpr(self._logical_expr, ascending=True, nulls_last=True)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.cast","title":"cast","text":"<pre><code>cast(data_type: DataType) -&gt; Column\n</code></pre> <p>Cast the column to a new data type.</p> <p>This method creates an expression that casts the column to a specified data type. The casting behavior depends on the source and target types:</p> <p>Primitive type casting:</p> <ul> <li>Numeric types (IntegerType, FloatType, DoubleType) can be cast between each other</li> <li>Numeric types can be cast to/from StringType</li> <li>BooleanType can be cast to/from numeric types and StringType</li> <li>StringType cannot be directly cast to BooleanType (will raise TypeError)</li> </ul> <p>Complex type casting:</p> <ul> <li>ArrayType can only be cast to another ArrayType (with castable element types)</li> <li>StructType can only be cast to another StructType (with matching/castable fields)</li> <li>Primitive types cannot be cast to/from complex types</li> </ul> <p>Parameters:</p> <ul> <li> <code>data_type</code>               (<code>DataType</code>)           \u2013            <p>The target DataType to cast the column to</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column representing the casted expression</p> </li> </ul> Cast integer to string <pre><code># Convert an integer column to string type\ndf.select(col(\"int_col\").cast(StringType))\n</code></pre> Cast array of integers to array of strings <pre><code># Convert an array of integers to an array of strings\ndf.select(col(\"int_array\").cast(ArrayType(element_type=StringType)))\n</code></pre> Cast struct fields to different types <pre><code># Convert struct fields to different types\nnew_type = StructType([\n    StructField(\"id\", StringType),\n    StructField(\"value\", FloatType)\n])\ndf.select(col(\"data_struct\").cast(new_type))\n</code></pre> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the requested cast operation is not supported</p> </li> </ul> Source code in <code>src/fenic/api/column.py</code> <pre><code>def cast(self, data_type: DataType) -&gt; Column:\n    \"\"\"Cast the column to a new data type.\n\n    This method creates an expression that casts the column to a specified data type.\n    The casting behavior depends on the source and target types:\n\n    Primitive type casting:\n\n    - Numeric types (IntegerType, FloatType, DoubleType) can be cast between each other\n    - Numeric types can be cast to/from StringType\n    - BooleanType can be cast to/from numeric types and StringType\n    - StringType cannot be directly cast to BooleanType (will raise TypeError)\n\n    Complex type casting:\n\n    - ArrayType can only be cast to another ArrayType (with castable element types)\n    - StructType can only be cast to another StructType (with matching/castable fields)\n    - Primitive types cannot be cast to/from complex types\n\n    Args:\n        data_type (DataType): The target DataType to cast the column to\n\n    Returns:\n        Column: A Column representing the casted expression\n\n    Example: Cast integer to string\n        ```python\n        # Convert an integer column to string type\n        df.select(col(\"int_col\").cast(StringType))\n        ```\n\n    Example: Cast array of integers to array of strings\n        ```python\n        # Convert an array of integers to an array of strings\n        df.select(col(\"int_array\").cast(ArrayType(element_type=StringType)))\n        ```\n\n    Example: Cast struct fields to different types\n        ```python\n        # Convert struct fields to different types\n        new_type = StructType([\n            StructField(\"id\", StringType),\n            StructField(\"value\", FloatType)\n        ])\n        df.select(col(\"data_struct\").cast(new_type))\n        ```\n\n    Raises:\n        TypeError: If the requested cast operation is not supported\n    \"\"\"\n    return Column._from_logical_expr(CastExpr(self._logical_expr, data_type))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.contains","title":"contains","text":"<pre><code>contains(other: Union[str, Column]) -&gt; Column\n</code></pre> <p>Check if the column contains a substring.</p> <p>This method creates a boolean expression that checks if each value in the column contains the specified substring. The substring can be either a literal string or a column expression.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[str, Column]</code>)           \u2013            <p>The substring to search for (can be a string or column expression)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value contains the substring</p> </li> </ul> Find rows where name contains \"john\" <pre><code># Filter rows where the name column contains \"john\"\ndf.filter(col(\"name\").contains(\"john\"))\n</code></pre> Find rows where text contains a dynamic pattern <pre><code># Filter rows where text contains a value from another column\ndf.filter(col(\"text\").contains(col(\"pattern\")))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def contains(self, other: Union[str, Column]) -&gt; Column:\n    \"\"\"Check if the column contains a substring.\n\n    This method creates a boolean expression that checks if each value in the column\n    contains the specified substring. The substring can be either a literal string\n    or a column expression.\n\n    Args:\n        other (Union[str, Column]): The substring to search for (can be a string or column expression)\n\n    Returns:\n        Column: A boolean column indicating whether each value contains the substring\n\n    Example: Find rows where name contains \"john\"\n        ```python\n        # Filter rows where the name column contains \"john\"\n        df.filter(col(\"name\").contains(\"john\"))\n        ```\n\n    Example: Find rows where text contains a dynamic pattern\n        ```python\n        # Filter rows where text contains a value from another column\n        df.filter(col(\"text\").contains(col(\"pattern\")))\n        ```\n    \"\"\"\n    if isinstance(other, str):\n        return Column._from_logical_expr(ContainsExpr(self._logical_expr, other))\n    else:\n        return Column._from_logical_expr(\n            ContainsExpr(self._logical_expr, other._logical_expr)\n        )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.contains_any","title":"contains_any","text":"<pre><code>contains_any(others: List[str], case_insensitive: bool = True) -&gt; Column\n</code></pre> <p>Check if the column contains any of the specified substrings.</p> <p>This method creates a boolean expression that checks if each value in the column contains any of the specified substrings. The matching can be case-sensitive or case-insensitive.</p> <p>Parameters:</p> <ul> <li> <code>others</code>               (<code>List[str]</code>)           \u2013            <p>List of substrings to search for</p> </li> <li> <code>case_insensitive</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to perform case-insensitive matching (default: True)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value contains any substring</p> </li> </ul> Find rows where name contains \"john\" or \"jane\" (case-insensitive) <pre><code># Filter rows where name contains either \"john\" or \"jane\"\ndf.filter(col(\"name\").contains_any([\"john\", \"jane\"]))\n</code></pre> Case-sensitive matching <pre><code># Filter rows with case-sensitive matching\ndf.filter(col(\"name\").contains_any([\"John\", \"Jane\"], case_insensitive=False))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def contains_any(self, others: List[str], case_insensitive: bool = True) -&gt; Column:\n    \"\"\"Check if the column contains any of the specified substrings.\n\n    This method creates a boolean expression that checks if each value in the column\n    contains any of the specified substrings. The matching can be case-sensitive or\n    case-insensitive.\n\n    Args:\n        others (List[str]): List of substrings to search for\n        case_insensitive (bool): Whether to perform case-insensitive matching (default: True)\n\n    Returns:\n        Column: A boolean column indicating whether each value contains any substring\n\n    Example: Find rows where name contains \"john\" or \"jane\" (case-insensitive)\n        ```python\n        # Filter rows where name contains either \"john\" or \"jane\"\n        df.filter(col(\"name\").contains_any([\"john\", \"jane\"]))\n        ```\n\n    Example: Case-sensitive matching\n        ```python\n        # Filter rows with case-sensitive matching\n        df.filter(col(\"name\").contains_any([\"John\", \"Jane\"], case_insensitive=False))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        ContainsAnyExpr(self._logical_expr, others, case_insensitive)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.desc","title":"desc","text":"<pre><code>desc() -&gt; Column\n</code></pre> <p>Apply descending order to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in descending order <pre><code># Sort a dataframe by age in descending order\ndf.sort(col(\"age\").desc()).show()\n</code></pre> Sort using column reference <pre><code># Sort using column reference with descending order\ndf.sort(col(\"age\").desc()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def desc(self) -&gt; Column:\n    \"\"\"Apply descending order to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function.\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in descending order\n        ```python\n        # Sort a dataframe by age in descending order\n        df.sort(col(\"age\").desc()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        # Sort using column reference with descending order\n        df.sort(col(\"age\").desc()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(SortExpr(self._logical_expr, ascending=False))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.desc_nulls_first","title":"desc_nulls_first","text":"<pre><code>desc_nulls_first() -&gt; Column\n</code></pre> <p>Apply descending order putting nulls first to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in descending order with nulls first <pre><code>df.sort(col(\"age\").desc_nulls_first()).show()\n</code></pre> Sort using column reference <pre><code>df.sort(col(\"age\").desc_nulls_first()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def desc_nulls_first(self) -&gt; Column:\n    \"\"\"Apply descending order putting nulls first to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in descending order with nulls first\n        ```python\n        df.sort(col(\"age\").desc_nulls_first()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        df.sort(col(\"age\").desc_nulls_first()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        SortExpr(self._logical_expr, ascending=False, nulls_last=False)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.desc_nulls_last","title":"desc_nulls_last","text":"<pre><code>desc_nulls_last() -&gt; Column\n</code></pre> <p>Apply descending order putting nulls last to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in descending order with nulls last <pre><code># Sort a dataframe by age in descending order, with nulls appearing last\ndf.sort(col(\"age\").desc_nulls_last()).show()\n</code></pre> Sort using column reference <pre><code># Sort using column reference with descending order and nulls last\ndf.sort(col(\"age\").desc_nulls_last()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def desc_nulls_last(self) -&gt; Column:\n    \"\"\"Apply descending order putting nulls last to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function.\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in descending order with nulls last\n        ```python\n        # Sort a dataframe by age in descending order, with nulls appearing last\n        df.sort(col(\"age\").desc_nulls_last()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        # Sort using column reference with descending order and nulls last\n        df.sort(col(\"age\").desc_nulls_last()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        SortExpr(self._logical_expr, ascending=False, nulls_last=True)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.ends_with","title":"ends_with","text":"<pre><code>ends_with(other: Union[str, Column]) -&gt; Column\n</code></pre> <p>Check if the column ends with a substring.</p> <p>This method creates a boolean expression that checks if each value in the column ends with the specified substring. The substring can be either a literal string or a column expression.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[str, Column]</code>)           \u2013            <p>The substring to check for at the end (can be a string or column expression)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value ends with the substring</p> </li> </ul> Find rows where email ends with \"@gmail.com\" <pre><code>df.filter(col(\"email\").ends_with(\"@gmail.com\"))\n</code></pre> Find rows where text ends with a dynamic pattern <pre><code>df.filter(col(\"text\").ends_with(col(\"suffix\")))\n</code></pre> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the substring ends with a regular expression anchor ($)</p> </li> </ul> Source code in <code>src/fenic/api/column.py</code> <pre><code>def ends_with(self, other: Union[str, Column]) -&gt; Column:\n    \"\"\"Check if the column ends with a substring.\n\n    This method creates a boolean expression that checks if each value in the column\n    ends with the specified substring. The substring can be either a literal string\n    or a column expression.\n\n    Args:\n        other (Union[str, Column]): The substring to check for at the end (can be a string or column expression)\n\n    Returns:\n        Column: A boolean column indicating whether each value ends with the substring\n\n    Example: Find rows where email ends with \"@gmail.com\"\n        ```python\n        df.filter(col(\"email\").ends_with(\"@gmail.com\"))\n        ```\n\n    Example: Find rows where text ends with a dynamic pattern\n        ```python\n        df.filter(col(\"text\").ends_with(col(\"suffix\")))\n        ```\n\n    Raises:\n        ValueError: If the substring ends with a regular expression anchor ($)\n    \"\"\"\n    if isinstance(other, str):\n        return Column._from_logical_expr(EndsWithExpr(self._logical_expr, other))\n    else:\n        return Column._from_logical_expr(\n            EndsWithExpr(self._logical_expr, other._logical_expr)\n        )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.get_item","title":"get_item","text":"<pre><code>get_item(key: Union[str, int]) -&gt; Column\n</code></pre> <p>Access an item in a struct or array column.</p> <p>This method allows accessing elements in complex data types:</p> <ul> <li>For array columns, the key should be an integer index</li> <li>For struct columns, the key should be a field name</li> </ul> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>Union[str, int]</code>)           \u2013            <p>The index (for arrays) or field name (for structs) to access</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column representing the accessed item</p> </li> </ul> Access an array element <pre><code># Get the first element from an array column\ndf.select(col(\"array_column\").get_item(0))\n</code></pre> Access a struct field <pre><code># Get a field from a struct column\ndf.select(col(\"struct_column\").get_item(\"field_name\"))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def get_item(self, key: Union[str, int]) -&gt; Column:\n    \"\"\"Access an item in a struct or array column.\n\n    This method allows accessing elements in complex data types:\n\n    - For array columns, the key should be an integer index\n    - For struct columns, the key should be a field name\n\n    Args:\n        key (Union[str, int]): The index (for arrays) or field name (for structs) to access\n\n    Returns:\n        Column: A Column representing the accessed item\n\n    Example: Access an array element\n        ```python\n        # Get the first element from an array column\n        df.select(col(\"array_column\").get_item(0))\n        ```\n\n    Example: Access a struct field\n        ```python\n        # Get a field from a struct column\n        df.select(col(\"struct_column\").get_item(\"field_name\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(IndexExpr(self._logical_expr, key))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.ilike","title":"ilike","text":"<pre><code>ilike(other: str) -&gt; Column\n</code></pre> <p>Check if the column matches a SQL LIKE pattern (case-insensitive).</p> <p>This method creates a boolean expression that checks if each value in the column matches the specified SQL LIKE pattern, ignoring case. The pattern must be a literal string and cannot be a column expression.</p> <p>SQL LIKE pattern syntax:</p> <ul> <li>% matches any sequence of characters</li> <li>_ matches any single character</li> </ul> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>str</code>)           \u2013            <p>The SQL LIKE pattern to match against</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value matches the pattern</p> </li> </ul> Find rows where name starts with \"j\" and ends with \"n\" (case-insensitive) <pre><code># Filter rows where name matches the pattern \"j%n\" (case-insensitive)\ndf.filter(col(\"name\").ilike(\"j%n\"))\n</code></pre> Find rows where code matches pattern (case-insensitive) <pre><code># Filter rows where code matches the pattern \"a_b%\" (case-insensitive)\ndf.filter(col(\"code\").ilike(\"a_b%\"))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def ilike(self, other: str) -&gt; Column:\n    r\"\"\"Check if the column matches a SQL LIKE pattern (case-insensitive).\n\n    This method creates a boolean expression that checks if each value in the column\n    matches the specified SQL LIKE pattern, ignoring case. The pattern must be a literal string\n    and cannot be a column expression.\n\n    SQL LIKE pattern syntax:\n\n    - % matches any sequence of characters\n    - _ matches any single character\n\n    Args:\n        other (str): The SQL LIKE pattern to match against\n\n    Returns:\n        Column: A boolean column indicating whether each value matches the pattern\n\n    Example: Find rows where name starts with \"j\" and ends with \"n\" (case-insensitive)\n        ```python\n        # Filter rows where name matches the pattern \"j%n\" (case-insensitive)\n        df.filter(col(\"name\").ilike(\"j%n\"))\n        ```\n\n    Example: Find rows where code matches pattern (case-insensitive)\n        ```python\n        # Filter rows where code matches the pattern \"a_b%\" (case-insensitive)\n        df.filter(col(\"code\").ilike(\"a_b%\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(ILikeExpr(self._logical_expr, other))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.is_in","title":"is_in","text":"<pre><code>is_in(other: Union[List[Any], ColumnOrName]) -&gt; Column\n</code></pre> <p>Check if the column is in a list of values or a column expression.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[List[Any], ColumnOrName]</code>)           \u2013            <p>A list of values or a Column expression</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression representing whether each element of Column is in the list</p> </li> </ul> Check if name is in a list of values <pre><code># Filter rows where name is in a list of values\ndf.filter(col(\"name\").is_in([\"Alice\", \"Bob\"]))\n</code></pre> Check if value is in another column <pre><code># Filter rows where name is in another column\ndf.filter(col(\"name\").is_in(col(\"other_column\")))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def is_in(self, other: Union[List[Any], ColumnOrName]) -&gt; Column:\n    \"\"\"Check if the column is in a list of values or a column expression.\n\n    Args:\n        other (Union[List[Any], ColumnOrName]): A list of values or a Column expression\n\n    Returns:\n        Column: A Column expression representing whether each element of Column is in the list\n\n    Example: Check if name is in a list of values\n        ```python\n        # Filter rows where name is in a list of values\n        df.filter(col(\"name\").is_in([\"Alice\", \"Bob\"]))\n        ```\n\n    Example: Check if value is in another column\n        ```python\n        # Filter rows where name is in another column\n        df.filter(col(\"name\").is_in(col(\"other_column\")))\n        ```\n    \"\"\"\n    if isinstance(other, list):\n        try:\n            type_ = infer_dtype_from_pyobj(other)\n            return Column._from_logical_expr(InExpr(self._logical_expr, LiteralExpr(other, type_)))\n        except TypeInferenceError as e:\n            raise ValidationError(f\"Cannot apply IN on {other}. List argument to IN must be be a valid Python List literal.\") from e\n    else:\n        return Column._from_logical_expr(InExpr(self._logical_expr, other._logical_expr))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.is_not_null","title":"is_not_null","text":"<pre><code>is_not_null() -&gt; Column\n</code></pre> <p>Check if the column contains non-NULL values.</p> <p>This method creates an expression that evaluates to TRUE when the column value is not NULL.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column representing a boolean expression that is TRUE when this column is not NULL</p> </li> </ul> Filter rows where a column is not NULL <pre><code>df.filter(col(\"some_column\").is_not_null())\n</code></pre> Use in a complex condition <pre><code>df.filter(col(\"col1\").is_not_null() &amp; (col(\"col2\") &lt;= 50))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def is_not_null(self) -&gt; Column:\n    \"\"\"Check if the column contains non-NULL values.\n\n    This method creates an expression that evaluates to TRUE when the column value is not NULL.\n\n    Returns:\n        Column: A Column representing a boolean expression that is TRUE when this column is not NULL\n\n    Example: Filter rows where a column is not NULL\n        ```python\n        df.filter(col(\"some_column\").is_not_null())\n        ```\n\n    Example: Use in a complex condition\n        ```python\n        df.filter(col(\"col1\").is_not_null() &amp; (col(\"col2\") &lt;= 50))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(IsNullExpr(self._logical_expr, False))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.is_null","title":"is_null","text":"<pre><code>is_null() -&gt; Column\n</code></pre> <p>Check if the column contains NULL values.</p> <p>This method creates an expression that evaluates to TRUE when the column value is NULL.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column representing a boolean expression that is TRUE when this column is NULL</p> </li> </ul> Filter rows where a column is NULL <pre><code># Filter rows where some_column is NULL\ndf.filter(col(\"some_column\").is_null())\n</code></pre> Use in a complex condition <pre><code># Filter rows where col1 is NULL or col2 is greater than 100\ndf.filter(col(\"col1\").is_null() | (col(\"col2\") &gt; 100))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def is_null(self) -&gt; Column:\n    \"\"\"Check if the column contains NULL values.\n\n    This method creates an expression that evaluates to TRUE when the column value is NULL.\n\n    Returns:\n        Column: A Column representing a boolean expression that is TRUE when this column is NULL\n\n    Example: Filter rows where a column is NULL\n        ```python\n        # Filter rows where some_column is NULL\n        df.filter(col(\"some_column\").is_null())\n        ```\n\n    Example: Use in a complex condition\n        ```python\n        # Filter rows where col1 is NULL or col2 is greater than 100\n        df.filter(col(\"col1\").is_null() | (col(\"col2\") &gt; 100))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(IsNullExpr(self._logical_expr, True))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.like","title":"like","text":"<pre><code>like(other: str) -&gt; Column\n</code></pre> <p>Check if the column matches a SQL LIKE pattern.</p> <p>This method creates a boolean expression that checks if each value in the column matches the specified SQL LIKE pattern. The pattern must be a literal string and cannot be a column expression.</p> <p>SQL LIKE pattern syntax:</p> <ul> <li>% matches any sequence of characters</li> <li>_ matches any single character</li> </ul> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>str</code>)           \u2013            <p>The SQL LIKE pattern to match against</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value matches the pattern</p> </li> </ul> Find rows where name starts with \"J\" and ends with \"n\" <pre><code># Filter rows where name matches the pattern \"J%n\"\ndf.filter(col(\"name\").like(\"J%n\"))\n</code></pre> Find rows where code matches specific pattern <pre><code># Filter rows where code matches the pattern \"A_B%\"\ndf.filter(col(\"code\").like(\"A_B%\"))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def like(self, other: str) -&gt; Column:\n    r\"\"\"Check if the column matches a SQL LIKE pattern.\n\n    This method creates a boolean expression that checks if each value in the column\n    matches the specified SQL LIKE pattern. The pattern must be a literal string\n    and cannot be a column expression.\n\n    SQL LIKE pattern syntax:\n\n    - % matches any sequence of characters\n    - _ matches any single character\n\n    Args:\n        other (str): The SQL LIKE pattern to match against\n\n    Returns:\n        Column: A boolean column indicating whether each value matches the pattern\n\n    Example: Find rows where name starts with \"J\" and ends with \"n\"\n        ```python\n        # Filter rows where name matches the pattern \"J%n\"\n        df.filter(col(\"name\").like(\"J%n\"))\n        ```\n\n    Example: Find rows where code matches specific pattern\n        ```python\n        # Filter rows where code matches the pattern \"A_B%\"\n        df.filter(col(\"code\").like(\"A_B%\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(LikeExpr(self._logical_expr, other))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.otherwise","title":"otherwise","text":"<pre><code>otherwise(value: Column) -&gt; Column\n</code></pre> <p>Evaluates a list of conditions and returns one of multiple possible result expressions.</p> <p>If Column.otherwise() is not invoked, None is returned for unmatched conditions. Otherwise() will return for rows with None inputs.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Column</code>)           \u2013            <p>A literal value or Column expression to return</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression representing whether each element of Column is not matched by any previous conditions</p> </li> </ul> Use when/otherwise for conditional logic <pre><code># Create a DataFrame with age and name columns\ndf = session.createDataFrame(\n    {\"age\": [2, 5]}, {\"name\": [\"Alice\", \"Bob\"]}\n)\n\n# Use when/otherwise to create a case result column\ndf.select(\n    col(\"name\"),\n    when(col(\"age\") &gt; 3, 1).otherwise(0).alias(\"case_result\")\n).show()\n# Output:\n# +-----+-----------+\n# | name|case_result|\n# +-----+-----------+\n# |Alice|          0|\n# |  Bob|          1|\n# +-----+-----------+\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def otherwise(self, value: Column) -&gt; Column:\n    \"\"\"Evaluates a list of conditions and returns one of multiple possible result expressions.\n\n    If Column.otherwise() is not invoked, None is returned for unmatched conditions.\n    Otherwise() will return for rows with None inputs.\n\n    Args:\n        value (Column): A literal value or Column expression to return\n\n    Returns:\n        Column: A Column expression representing whether each element of Column is not matched by any previous conditions\n\n    Example: Use when/otherwise for conditional logic\n        ```python\n        # Create a DataFrame with age and name columns\n        df = session.createDataFrame(\n            {\"age\": [2, 5]}, {\"name\": [\"Alice\", \"Bob\"]}\n        )\n\n        # Use when/otherwise to create a case result column\n        df.select(\n            col(\"name\"),\n            when(col(\"age\") &gt; 3, 1).otherwise(0).alias(\"case_result\")\n        ).show()\n        # Output:\n        # +-----+-----------+\n        # | name|case_result|\n        # +-----+-----------+\n        # |Alice|          0|\n        # |  Bob|          1|\n        # +-----+-----------+\n        ```\n    \"\"\"\n    return Column._from_logical_expr(OtherwiseExpr(self._logical_expr, value._logical_expr))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.rlike","title":"rlike","text":"<pre><code>rlike(other: str) -&gt; Column\n</code></pre> <p>Check if the column matches a regular expression pattern.</p> <p>This method creates a boolean expression that checks if each value in the column matches the specified regular expression pattern. The pattern must be a literal string and cannot be a column expression.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>str</code>)           \u2013            <p>The regular expression pattern to match against</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value matches the pattern</p> </li> </ul> Find rows where phone number matches pattern <pre><code># Filter rows where phone number matches a specific pattern\ndf.filter(col(\"phone\").rlike(r\"^\\d{3}-\\d{3}-\\d{4}$\"))\n</code></pre> Find rows where text contains word boundaries <pre><code># Filter rows where text contains a word with boundaries\ndf.filter(col(\"text\").rlike(r\"\\bhello\\b\"))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def rlike(self, other: str) -&gt; Column:\n    r\"\"\"Check if the column matches a regular expression pattern.\n\n    This method creates a boolean expression that checks if each value in the column\n    matches the specified regular expression pattern. The pattern must be a literal string\n    and cannot be a column expression.\n\n    Args:\n        other (str): The regular expression pattern to match against\n\n    Returns:\n        Column: A boolean column indicating whether each value matches the pattern\n\n    Example: Find rows where phone number matches pattern\n        ```python\n        # Filter rows where phone number matches a specific pattern\n        df.filter(col(\"phone\").rlike(r\"^\\d{3}-\\d{3}-\\d{4}$\"))\n        ```\n\n    Example: Find rows where text contains word boundaries\n        ```python\n        # Filter rows where text contains a word with boundaries\n        df.filter(col(\"text\").rlike(r\"\\bhello\\b\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(RLikeExpr(self._logical_expr, other))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.starts_with","title":"starts_with","text":"<pre><code>starts_with(other: Union[str, Column]) -&gt; Column\n</code></pre> <p>Check if the column starts with a substring.</p> <p>This method creates a boolean expression that checks if each value in the column starts with the specified substring. The substring can be either a literal string or a column expression.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[str, Column]</code>)           \u2013            <p>The substring to check for at the start (can be a string or column expression)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value starts with the substring</p> </li> </ul> Find rows where name starts with \"Mr\" <pre><code># Filter rows where name starts with \"Mr\"\ndf.filter(col(\"name\").starts_with(\"Mr\"))\n</code></pre> Find rows where text starts with a dynamic pattern <pre><code># Filter rows where text starts with a value from another column\ndf.filter(col(\"text\").starts_with(col(\"prefix\")))\n</code></pre> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the substring starts with a regular expression anchor (^)</p> </li> </ul> Source code in <code>src/fenic/api/column.py</code> <pre><code>def starts_with(self, other: Union[str, Column]) -&gt; Column:\n    \"\"\"Check if the column starts with a substring.\n\n    This method creates a boolean expression that checks if each value in the column\n    starts with the specified substring. The substring can be either a literal string\n    or a column expression.\n\n    Args:\n        other (Union[str, Column]): The substring to check for at the start (can be a string or column expression)\n\n    Returns:\n        Column: A boolean column indicating whether each value starts with the substring\n\n    Example: Find rows where name starts with \"Mr\"\n        ```python\n        # Filter rows where name starts with \"Mr\"\n        df.filter(col(\"name\").starts_with(\"Mr\"))\n        ```\n\n    Example: Find rows where text starts with a dynamic pattern\n        ```python\n        # Filter rows where text starts with a value from another column\n        df.filter(col(\"text\").starts_with(col(\"prefix\")))\n        ```\n\n    Raises:\n        ValueError: If the substring starts with a regular expression anchor (^)\n    \"\"\"\n    if isinstance(other, str):\n        return Column._from_logical_expr(StartsWithExpr(self._logical_expr, other))\n    else:\n        return Column._from_logical_expr(\n            StartsWithExpr(self._logical_expr, other._logical_expr)\n        )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Column.when","title":"when","text":"<pre><code>when(condition: Column, value: Column) -&gt; Column\n</code></pre> <p>Evaluates a list of conditions and returns one of multiple possible result expressions.</p> <p>If Column.otherwise() is not invoked, None is returned for unmatched conditions. Otherwise() will return for rows with None inputs.</p> <p>Parameters:</p> <ul> <li> <code>condition</code>               (<code>Column</code>)           \u2013            <p>A boolean Column expression</p> </li> <li> <code>value</code>               (<code>Column</code>)           \u2013            <p>A literal value or Column expression to return if the condition is true</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression representing whether each element of Column matches the condition</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the condition is not a boolean Column expression</p> </li> </ul> Use when/otherwise for conditional logic <pre><code># Create a DataFrame with age and name columns\ndf = session.createDataFrame(\n    {\"age\": [2, 5]}, {\"name\": [\"Alice\", \"Bob\"]}\n)\n\n# Use when/otherwise to create a case result column\ndf.select(\n    col(\"name\"),\n    when(col(\"age\") &gt; 3, 1).otherwise(0).alias(\"case_result\")\n).show()\n# Output:\n# +-----+-----------+\n# | name|case_result|\n# +-----+-----------+\n# |Alice|          0|\n# |  Bob|          1|\n# +-----+-----------+\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def when(self, condition: Column, value: Column) -&gt; Column:\n    \"\"\"Evaluates a list of conditions and returns one of multiple possible result expressions.\n\n    If Column.otherwise() is not invoked, None is returned for unmatched conditions.\n    Otherwise() will return for rows with None inputs.\n\n    Args:\n        condition (Column): A boolean Column expression\n        value (Column): A literal value or Column expression to return if the condition is true\n\n    Returns:\n        Column: A Column expression representing whether each element of Column matches the condition\n\n    Raises:\n        TypeError: If the condition is not a boolean Column expression\n\n    Example: Use when/otherwise for conditional logic\n        ```python\n        # Create a DataFrame with age and name columns\n        df = session.createDataFrame(\n            {\"age\": [2, 5]}, {\"name\": [\"Alice\", \"Bob\"]}\n        )\n\n        # Use when/otherwise to create a case result column\n        df.select(\n            col(\"name\"),\n            when(col(\"age\") &gt; 3, 1).otherwise(0).alias(\"case_result\")\n        ).show()\n        # Output:\n        # +-----+-----------+\n        # | name|case_result|\n        # +-----+-----------+\n        # |Alice|          0|\n        # |  Bob|          1|\n        # +-----+-----------+\n        ```\n    \"\"\"\n    return Column._from_logical_expr(WhenExpr(self._logical_expr, condition._logical_expr, value._logical_expr))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame","title":"DataFrame","text":"<p>A data collection organized into named columns.</p> <p>The DataFrame class represents a lazily evaluated computation on data. Operations on DataFrame build up a logical query plan that is only executed when an action like show(), to_polars(), to_pandas(), to_arrow(), to_pydict(), to_pylist(), or count() is called.</p> <p>The DataFrame supports method chaining for building complex transformations.</p> Create and transform a DataFrame <pre><code># Create a DataFrame from a dictionary\ndf = session.create_dataframe({\"id\": [1, 2, 3], \"value\": [\"a\", \"b\", \"c\"]})\n\n# Chain transformations\nresult = df.filter(col(\"id\") &gt; 1).select(\"id\", \"value\")\n\n# Show results\nresult.show()\n# Output:\n# +---+-----+\n# | id|value|\n# +---+-----+\n# |  2|    b|\n# |  3|    c|\n# +---+-----+\n</code></pre> <p>Methods:</p> <ul> <li> <code>agg</code>             \u2013              <p>Aggregate on the entire DataFrame without groups.</p> </li> <li> <code>cache</code>             \u2013              <p>Alias for persist(). Mark DataFrame for caching after first computation.</p> </li> <li> <code>collect</code>             \u2013              <p>Execute the DataFrame computation and return the result as a QueryResult.</p> </li> <li> <code>count</code>             \u2013              <p>Count the number of rows in the DataFrame.</p> </li> <li> <code>drop</code>             \u2013              <p>Remove one or more columns from this DataFrame.</p> </li> <li> <code>drop_duplicates</code>             \u2013              <p>Return a DataFrame with duplicate rows removed.</p> </li> <li> <code>explain</code>             \u2013              <p>Display the logical plan of the DataFrame.</p> </li> <li> <code>explode</code>             \u2013              <p>Create a new row for each element in an array column.</p> </li> <li> <code>filter</code>             \u2013              <p>Filters rows using the given condition.</p> </li> <li> <code>group_by</code>             \u2013              <p>Groups the DataFrame using the specified columns.</p> </li> <li> <code>join</code>             \u2013              <p>Joins this DataFrame with another DataFrame.</p> </li> <li> <code>limit</code>             \u2013              <p>Limits the number of rows to the specified number.</p> </li> <li> <code>lineage</code>             \u2013              <p>Create a Lineage object to trace data through transformations.</p> </li> <li> <code>order_by</code>             \u2013              <p>Sort the DataFrame by the specified columns. Alias for sort().</p> </li> <li> <code>persist</code>             \u2013              <p>Mark this DataFrame to be persisted after first computation.</p> </li> <li> <code>select</code>             \u2013              <p>Projects a set of Column expressions or column names.</p> </li> <li> <code>show</code>             \u2013              <p>Display the DataFrame content in a tabular form.</p> </li> <li> <code>sort</code>             \u2013              <p>Sort the DataFrame by the specified columns.</p> </li> <li> <code>to_arrow</code>             \u2013              <p>Execute the DataFrame computation and return an Apache Arrow Table.</p> </li> <li> <code>to_pandas</code>             \u2013              <p>Execute the DataFrame computation and return a Pandas DataFrame.</p> </li> <li> <code>to_polars</code>             \u2013              <p>Execute the DataFrame computation and return the result as a Polars DataFrame.</p> </li> <li> <code>to_pydict</code>             \u2013              <p>Execute the DataFrame computation and return a dictionary of column arrays.</p> </li> <li> <code>to_pylist</code>             \u2013              <p>Execute the DataFrame computation and return a list of row dictionaries.</p> </li> <li> <code>union</code>             \u2013              <p>Return a new DataFrame containing the union of rows in this and another DataFrame.</p> </li> <li> <code>unnest</code>             \u2013              <p>Unnest the specified struct columns into separate columns.</p> </li> <li> <code>where</code>             \u2013              <p>Filters rows using the given condition (alias for filter()).</p> </li> <li> <code>with_column</code>             \u2013              <p>Add a new column or replace an existing column.</p> </li> <li> <code>with_column_renamed</code>             \u2013              <p>Rename a column. No-op if the column does not exist.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>columns</code>               (<code>List[str]</code>)           \u2013            <p>Get list of column names.</p> </li> <li> <code>schema</code>               (<code>Schema</code>)           \u2013            <p>Get the schema of this DataFrame.</p> </li> <li> <code>semantic</code>               (<code>SemanticExtensions</code>)           \u2013            <p>Interface for semantic operations on the DataFrame.</p> </li> <li> <code>write</code>               (<code>DataFrameWriter</code>)           \u2013            <p>Interface for saving the content of the DataFrame.</p> </li> </ul>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: List[str]\n</code></pre> <p>Get list of column names.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: List of all column names in the DataFrame</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df.columns\n['name', 'age', 'city']\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.schema","title":"schema  <code>property</code>","text":"<pre><code>schema: Schema\n</code></pre> <p>Get the schema of this DataFrame.</p> <p>Returns:</p> <ul> <li> <code>Schema</code> (              <code>Schema</code> )          \u2013            <p>Schema containing field names and data types</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df.schema\nSchema([\n    ColumnField('name', StringType),\n    ColumnField('age', IntegerType)\n])\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.semantic","title":"semantic  <code>property</code>","text":"<pre><code>semantic: SemanticExtensions\n</code></pre> <p>Interface for semantic operations on the DataFrame.</p>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.write","title":"write  <code>property</code>","text":"<pre><code>write: DataFrameWriter\n</code></pre> <p>Interface for saving the content of the DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrameWriter</code> (              <code>DataFrameWriter</code> )          \u2013            <p>Writer interface to write DataFrame.</p> </li> </ul>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.agg","title":"agg","text":"<pre><code>agg(*exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame\n</code></pre> <p>Aggregate on the entire DataFrame without groups.</p> <p>This is equivalent to group_by() without any grouping columns.</p> <p>Parameters:</p> <ul> <li> <code>*exprs</code>               (<code>Union[Column, Dict[str, str]]</code>, default:                   <code>()</code> )           \u2013            <p>Aggregation expressions or dictionary of aggregations.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Aggregation results.</p> </li> </ul> Multiple aggregations <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"salary\": [80000, 70000, 90000, 75000, 85000],\n    \"age\": [25, 30, 35, 28, 32]\n})\n\n# Multiple aggregations\ndf.agg(\n    count().alias(\"total_rows\"),\n    avg(col(\"salary\")).alias(\"avg_salary\")\n).show()\n# Output:\n# +----------+-----------+\n# |total_rows|avg_salary|\n# +----------+-----------+\n# |         5|   80000.0|\n# +----------+-----------+\n</code></pre> Dictionary style <pre><code># Dictionary style\ndf.agg({col(\"salary\"): \"avg\", col(\"age\"): \"max\"}).show()\n# Output:\n# +-----------+--------+\n# |avg(salary)|max(age)|\n# +-----------+--------+\n# |    80000.0|      35|\n# +-----------+--------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def agg(self, *exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame:\n    \"\"\"Aggregate on the entire DataFrame without groups.\n\n    This is equivalent to group_by() without any grouping columns.\n\n    Args:\n        *exprs: Aggregation expressions or dictionary of aggregations.\n\n    Returns:\n        DataFrame: Aggregation results.\n\n    Example: Multiple aggregations\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"salary\": [80000, 70000, 90000, 75000, 85000],\n            \"age\": [25, 30, 35, 28, 32]\n        })\n\n        # Multiple aggregations\n        df.agg(\n            count().alias(\"total_rows\"),\n            avg(col(\"salary\")).alias(\"avg_salary\")\n        ).show()\n        # Output:\n        # +----------+-----------+\n        # |total_rows|avg_salary|\n        # +----------+-----------+\n        # |         5|   80000.0|\n        # +----------+-----------+\n        ```\n\n    Example: Dictionary style\n        ```python\n        # Dictionary style\n        df.agg({col(\"salary\"): \"avg\", col(\"age\"): \"max\"}).show()\n        # Output:\n        # +-----------+--------+\n        # |avg(salary)|max(age)|\n        # +-----------+--------+\n        # |    80000.0|      35|\n        # +-----------+--------+\n        ```\n    \"\"\"\n    return self.group_by().agg(*exprs)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.cache","title":"cache","text":"<pre><code>cache() -&gt; DataFrame\n</code></pre> <p>Alias for persist(). Mark DataFrame for caching after first computation.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Same DataFrame, but marked for caching</p> </li> </ul> See Also <p>persist(): Full documentation of caching behavior</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def cache(self) -&gt; DataFrame:\n    \"\"\"Alias for persist(). Mark DataFrame for caching after first computation.\n\n    Returns:\n        DataFrame: Same DataFrame, but marked for caching\n\n    See Also:\n        persist(): Full documentation of caching behavior\n    \"\"\"\n    return self.persist()\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.collect","title":"collect","text":"<pre><code>collect(data_type: DataLikeType = 'polars') -&gt; QueryResult\n</code></pre> <p>Execute the DataFrame computation and return the result as a QueryResult.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a QueryResult, which contains both the result data and the query metrics.</p> <p>Parameters:</p> <ul> <li> <code>data_type</code>               (<code>DataLikeType</code>, default:                   <code>'polars'</code> )           \u2013            <p>The type of data to return</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>QueryResult</code> (              <code>QueryResult</code> )          \u2013            <p>A QueryResult with materialized data and query metrics</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def collect(self, data_type: DataLikeType = \"polars\") -&gt; QueryResult:\n    \"\"\"Execute the DataFrame computation and return the result as a QueryResult.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a QueryResult, which contains both the result data and the query metrics.\n\n    Args:\n        data_type: The type of data to return\n\n    Returns:\n        QueryResult: A QueryResult with materialized data and query metrics\n    \"\"\"\n    result: Tuple[pl.DataFrame, QueryMetrics] = self._logical_plan.session_state.execution.collect(self._logical_plan)\n    df, metrics = result\n    logger.info(metrics.get_summary())\n\n    if data_type == \"polars\":\n        return QueryResult(df, metrics)\n    elif data_type == \"pandas\":\n        return QueryResult(df.to_pandas(use_pyarrow_extension_array=True), metrics)\n    elif data_type == \"arrow\":\n        return QueryResult(df.to_arrow(), metrics)\n    elif data_type == \"pydict\":\n        return QueryResult(df.to_dict(as_series=False), metrics)\n    elif data_type == \"pylist\":\n        return QueryResult(df.to_dicts(), metrics)\n    else:\n        raise ValidationError(f\"Invalid data type: {data_type} in collect(). Valid data types are: polars, pandas, arrow, pydict, pylist\")\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.count","title":"count","text":"<pre><code>count() -&gt; int\n</code></pre> <p>Count the number of rows in the DataFrame.</p> <p>This is an action that triggers computation of the DataFrame. The output is an integer representing the number of rows.</p> <p>Returns:</p> <ul> <li> <code>int</code> (              <code>int</code> )          \u2013            <p>The number of rows in the DataFrame</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def count(self) -&gt; int:\n    \"\"\"Count the number of rows in the DataFrame.\n\n    This is an action that triggers computation of the DataFrame.\n    The output is an integer representing the number of rows.\n\n    Returns:\n        int: The number of rows in the DataFrame\n    \"\"\"\n    return self._logical_plan.session_state.execution.count(self._logical_plan)[0]\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.drop","title":"drop","text":"<pre><code>drop(*col_names: str) -&gt; DataFrame\n</code></pre> <p>Remove one or more columns from this DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>*col_names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>Names of columns to drop.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>New DataFrame without specified columns.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If any specified column doesn't exist in the DataFrame.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If dropping the columns would result in an empty DataFrame.</p> </li> </ul> Drop single column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35]\n})\n\n# Drop single column\ndf.drop(\"age\").show()\n# Output:\n# +---+-------+\n# | id|   name|\n# +---+-------+\n# |  1|  Alice|\n# |  2|    Bob|\n# |  3|Charlie|\n# +---+-------+\n</code></pre> Drop multiple columns <pre><code># Drop multiple columns\ndf.drop(col(\"id\"), \"age\").show()\n# Output:\n# +-------+\n# |   name|\n# +-------+\n# |  Alice|\n# |    Bob|\n# |Charlie|\n# +-------+\n</code></pre> Error when dropping non-existent column <pre><code># This will raise a ValueError\ndf.drop(\"non_existent_column\")\n# ValueError: Column 'non_existent_column' not found in DataFrame\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def drop(self, *col_names: str) -&gt; DataFrame:\n    \"\"\"Remove one or more columns from this DataFrame.\n\n    Args:\n        *col_names: Names of columns to drop.\n\n    Returns:\n        DataFrame: New DataFrame without specified columns.\n\n    Raises:\n        ValueError: If any specified column doesn't exist in the DataFrame.\n        ValueError: If dropping the columns would result in an empty DataFrame.\n\n    Example: Drop single column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"id\": [1, 2, 3],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n            \"age\": [25, 30, 35]\n        })\n\n        # Drop single column\n        df.drop(\"age\").show()\n        # Output:\n        # +---+-------+\n        # | id|   name|\n        # +---+-------+\n        # |  1|  Alice|\n        # |  2|    Bob|\n        # |  3|Charlie|\n        # +---+-------+\n        ```\n\n    Example: Drop multiple columns\n        ```python\n        # Drop multiple columns\n        df.drop(col(\"id\"), \"age\").show()\n        # Output:\n        # +-------+\n        # |   name|\n        # +-------+\n        # |  Alice|\n        # |    Bob|\n        # |Charlie|\n        # +-------+\n        ```\n\n    Example: Error when dropping non-existent column\n        ```python\n        # This will raise a ValueError\n        df.drop(\"non_existent_column\")\n        # ValueError: Column 'non_existent_column' not found in DataFrame\n        ```\n    \"\"\"\n    if not col_names:\n        return self\n\n    current_cols = set(self.columns)\n    to_drop = set(col_names)\n    missing = to_drop - current_cols\n\n    if missing:\n        missing_str = (\n            f\"Column '{next(iter(missing))}'\"\n            if len(missing) == 1\n            else f\"Columns {sorted(missing)}\"\n        )\n        raise ValueError(f\"{missing_str} not found in DataFrame\")\n\n    remaining_cols = [\n        col(c)._logical_expr for c in self.columns if c not in to_drop\n    ]\n\n    if not remaining_cols:\n        raise ValueError(\"Cannot drop all columns from DataFrame\")\n\n    return self._from_logical_plan(\n        Projection(self._logical_plan, remaining_cols)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.drop_duplicates","title":"drop_duplicates","text":"<pre><code>drop_duplicates(subset: Optional[List[str]] = None) -&gt; DataFrame\n</code></pre> <p>Return a DataFrame with duplicate rows removed.</p> <p>Parameters:</p> <ul> <li> <code>subset</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Column names to consider when identifying duplicates. If not provided, all columns are considered.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with duplicate rows removed.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a specified column is not present in the current DataFrame schema.</p> </li> </ul> Remove duplicates considering specific columns <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"c1\": [1, 2, 3, 1],\n    \"c2\": [\"a\", \"a\", \"a\", \"a\"],\n    \"c3\": [\"b\", \"b\", \"b\", \"b\"]\n})\n\n# Remove duplicates considering all columns\ndf.drop_duplicates([col(\"c1\"), col(\"c2\"), col(\"c3\")]).show()\n# Output:\n# +---+---+---+\n# | c1| c2| c3|\n# +---+---+---+\n# |  1|  a|  b|\n# |  2|  a|  b|\n# |  3|  a|  b|\n# +---+---+---+\n\n# Remove duplicates considering only c1\ndf.drop_duplicates([col(\"c1\")]).show()\n# Output:\n# +---+---+---+\n# | c1| c2| c3|\n# +---+---+---+\n# |  1|  a|  b|\n# |  2|  a|  b|\n# |  3|  a|  b|\n# +---+---+---+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def drop_duplicates(\n    self,\n    subset: Optional[List[str]] = None,\n) -&gt; DataFrame:\n    \"\"\"Return a DataFrame with duplicate rows removed.\n\n    Args:\n        subset: Column names to consider when identifying duplicates. If not provided, all columns are considered.\n\n    Returns:\n        DataFrame: A new DataFrame with duplicate rows removed.\n\n    Raises:\n        ValueError: If a specified column is not present in the current DataFrame schema.\n\n    Example: Remove duplicates considering specific columns\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"c1\": [1, 2, 3, 1],\n            \"c2\": [\"a\", \"a\", \"a\", \"a\"],\n            \"c3\": [\"b\", \"b\", \"b\", \"b\"]\n        })\n\n        # Remove duplicates considering all columns\n        df.drop_duplicates([col(\"c1\"), col(\"c2\"), col(\"c3\")]).show()\n        # Output:\n        # +---+---+---+\n        # | c1| c2| c3|\n        # +---+---+---+\n        # |  1|  a|  b|\n        # |  2|  a|  b|\n        # |  3|  a|  b|\n        # +---+---+---+\n\n        # Remove duplicates considering only c1\n        df.drop_duplicates([col(\"c1\")]).show()\n        # Output:\n        # +---+---+---+\n        # | c1| c2| c3|\n        # +---+---+---+\n        # |  1|  a|  b|\n        # |  2|  a|  b|\n        # |  3|  a|  b|\n        # +---+---+---+\n        ```\n    \"\"\"\n    exprs = []\n    if subset:\n        for c in subset:\n            if c not in self.columns:\n                raise TypeError(f\"Column {c} not found in DataFrame.\")\n            exprs.append(col(c)._logical_expr)\n\n    return self._from_logical_plan(\n        DropDuplicates(self._logical_plan, exprs),\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.explain","title":"explain","text":"<pre><code>explain() -&gt; None\n</code></pre> <p>Display the logical plan of the DataFrame.</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def explain(self) -&gt; None:\n    \"\"\"Display the logical plan of the DataFrame.\"\"\"\n    print(str(self._logical_plan))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.explode","title":"explode","text":"<pre><code>explode(column: ColumnOrName) -&gt; DataFrame\n</code></pre> <p>Create a new row for each element in an array column.</p> <p>This operation is useful for flattening nested data structures. For each row in the input DataFrame that contains an array/list in the specified column, this method will: 1. Create N new rows, where N is the length of the array 2. Each new row will be identical to the original row, except the array column will    contain just a single element from the original array 3. Rows with NULL values or empty arrays in the specified column are filtered out</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Name of array column to explode (as string) or Column expression.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>New DataFrame with the array column exploded into multiple rows.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column argument is not a string or Column.</p> </li> </ul> Explode array column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"id\": [1, 2, 3, 4],\n    \"tags\": [[\"red\", \"blue\"], [\"green\"], [], None],\n    \"name\": [\"Alice\", \"Bob\", \"Carol\", \"Dave\"]\n})\n\n# Explode the tags column\ndf.explode(\"tags\").show()\n# Output:\n# +---+-----+-----+\n# | id| tags| name|\n# +---+-----+-----+\n# |  1|  red|Alice|\n# |  1| blue|Alice|\n# |  2|green|  Bob|\n# +---+-----+-----+\n</code></pre> Using column expression <pre><code># Explode using column expression\ndf.explode(col(\"tags\")).show()\n# Output:\n# +---+-----+-----+\n# | id| tags| name|\n# +---+-----+-----+\n# |  1|  red|Alice|\n# |  1| blue|Alice|\n# |  2|green|  Bob|\n# +---+-----+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def explode(self, column: ColumnOrName) -&gt; DataFrame:\n    \"\"\"Create a new row for each element in an array column.\n\n    This operation is useful for flattening nested data structures. For each row in the\n    input DataFrame that contains an array/list in the specified column, this method will:\n    1. Create N new rows, where N is the length of the array\n    2. Each new row will be identical to the original row, except the array column will\n       contain just a single element from the original array\n    3. Rows with NULL values or empty arrays in the specified column are filtered out\n\n    Args:\n        column: Name of array column to explode (as string) or Column expression.\n\n    Returns:\n        DataFrame: New DataFrame with the array column exploded into multiple rows.\n\n    Raises:\n        TypeError: If column argument is not a string or Column.\n\n    Example: Explode array column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"id\": [1, 2, 3, 4],\n            \"tags\": [[\"red\", \"blue\"], [\"green\"], [], None],\n            \"name\": [\"Alice\", \"Bob\", \"Carol\", \"Dave\"]\n        })\n\n        # Explode the tags column\n        df.explode(\"tags\").show()\n        # Output:\n        # +---+-----+-----+\n        # | id| tags| name|\n        # +---+-----+-----+\n        # |  1|  red|Alice|\n        # |  1| blue|Alice|\n        # |  2|green|  Bob|\n        # +---+-----+-----+\n        ```\n\n    Example: Using column expression\n        ```python\n        # Explode using column expression\n        df.explode(col(\"tags\")).show()\n        # Output:\n        # +---+-----+-----+\n        # | id| tags| name|\n        # +---+-----+-----+\n        # |  1|  red|Alice|\n        # |  1| blue|Alice|\n        # |  2|green|  Bob|\n        # +---+-----+-----+\n        ```\n    \"\"\"\n    return self._from_logical_plan(\n        Explode(self._logical_plan, Column._from_col_or_name(column)._logical_expr),\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.filter","title":"filter","text":"<pre><code>filter(condition: Column) -&gt; DataFrame\n</code></pre> <p>Filters rows using the given condition.</p> <p>Parameters:</p> <ul> <li> <code>condition</code>               (<code>Column</code>)           \u2013            <p>A Column expression that evaluates to a boolean</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Filtered DataFrame</p> </li> </ul> Filter with numeric comparison <pre><code># Create a DataFrame\ndf = session.create_dataframe({\"age\": [25, 30, 35], \"name\": [\"Alice\", \"Bob\", \"Charlie\"]})\n\n# Filter with numeric comparison\ndf.filter(col(\"age\") &gt; 25).show()\n# Output:\n# +---+-------+\n# |age|   name|\n# +---+-------+\n# | 30|    Bob|\n# | 35|Charlie|\n# +---+-------+\n</code></pre> Filter with semantic predicate <pre><code># Filter with semantic predicate\ndf.filter((col(\"age\") &gt; 25) &amp; semantic.predicate(\"This {feedback} mentions problems with the user interface or navigation\")).show()\n# Output:\n# +---+-------+\n# |age|   name|\n# +---+-------+\n# | 30|    Bob|\n# | 35|Charlie|\n# +---+-------+\n</code></pre> Filter with multiple conditions <pre><code># Filter with multiple conditions\ndf.filter((col(\"age\") &gt; 25) &amp; (col(\"age\") &lt;= 35)).show()\n# Output:\n# +---+-------+\n# |age|   name|\n# +---+-------+\n# | 30|    Bob|\n# | 35|Charlie|\n# +---+-------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def filter(self, condition: Column) -&gt; DataFrame:\n    \"\"\"Filters rows using the given condition.\n\n    Args:\n        condition: A Column expression that evaluates to a boolean\n\n    Returns:\n        DataFrame: Filtered DataFrame\n\n    Example: Filter with numeric comparison\n        ```python\n        # Create a DataFrame\n        df = session.create_dataframe({\"age\": [25, 30, 35], \"name\": [\"Alice\", \"Bob\", \"Charlie\"]})\n\n        # Filter with numeric comparison\n        df.filter(col(\"age\") &gt; 25).show()\n        # Output:\n        # +---+-------+\n        # |age|   name|\n        # +---+-------+\n        # | 30|    Bob|\n        # | 35|Charlie|\n        # +---+-------+\n        ```\n\n    Example: Filter with semantic predicate\n        ```python\n        # Filter with semantic predicate\n        df.filter((col(\"age\") &gt; 25) &amp; semantic.predicate(\"This {feedback} mentions problems with the user interface or navigation\")).show()\n        # Output:\n        # +---+-------+\n        # |age|   name|\n        # +---+-------+\n        # | 30|    Bob|\n        # | 35|Charlie|\n        # +---+-------+\n        ```\n\n    Example: Filter with multiple conditions\n        ```python\n        # Filter with multiple conditions\n        df.filter((col(\"age\") &gt; 25) &amp; (col(\"age\") &lt;= 35)).show()\n        # Output:\n        # +---+-------+\n        # |age|   name|\n        # +---+-------+\n        # | 30|    Bob|\n        # | 35|Charlie|\n        # +---+-------+\n        ```\n    \"\"\"\n    return self._from_logical_plan(\n        Filter(self._logical_plan, condition._logical_expr),\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.group_by","title":"group_by","text":"<pre><code>group_by(*cols: ColumnOrName) -&gt; GroupedData\n</code></pre> <p>Groups the DataFrame using the specified columns.</p> <p>Parameters:</p> <ul> <li> <code>*cols</code>               (<code>ColumnOrName</code>, default:                   <code>()</code> )           \u2013            <p>Columns to group by. Can be column names as strings or Column expressions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GroupedData</code> (              <code>GroupedData</code> )          \u2013            <p>Object for performing aggregations on the grouped data.</p> </li> </ul> Group by single column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"department\": [\"IT\", \"HR\", \"IT\", \"HR\", \"IT\"],\n    \"salary\": [80000, 70000, 90000, 75000, 85000]\n})\n\n# Group by single column\ndf.group_by(col(\"department\")).count().show()\n# Output:\n# +----------+-----+\n# |department|count|\n# +----------+-----+\n# |        IT|    3|\n# |        HR|    2|\n# +----------+-----+\n</code></pre> Group by multiple columns <pre><code># Group by multiple columns\ndf.group_by(col(\"department\"), col(\"location\")).agg({\"salary\": \"avg\"}).show()\n# Output:\n# +----------+--------+-----------+\n# |department|location|avg(salary)|\n# +----------+--------+-----------+\n# |        IT|    NYC|    85000.0|\n# |        HR|    NYC|    72500.0|\n# +----------+--------+-----------+\n</code></pre> Group by expression <pre><code># Group by expression\ndf.group_by(col(\"age\").cast(\"int\").alias(\"age_group\")).count().show()\n# Output:\n# +---------+-----+\n# |age_group|count|\n# +---------+-----+\n# |       20|    2|\n# |       30|    3|\n# |       40|    1|\n# +---------+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def group_by(self, *cols: ColumnOrName) -&gt; GroupedData:\n    \"\"\"Groups the DataFrame using the specified columns.\n\n    Args:\n        *cols: Columns to group by. Can be column names as strings or Column expressions.\n\n    Returns:\n        GroupedData: Object for performing aggregations on the grouped data.\n\n    Example: Group by single column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"department\": [\"IT\", \"HR\", \"IT\", \"HR\", \"IT\"],\n            \"salary\": [80000, 70000, 90000, 75000, 85000]\n        })\n\n        # Group by single column\n        df.group_by(col(\"department\")).count().show()\n        # Output:\n        # +----------+-----+\n        # |department|count|\n        # +----------+-----+\n        # |        IT|    3|\n        # |        HR|    2|\n        # +----------+-----+\n        ```\n\n    Example: Group by multiple columns\n        ```python\n        # Group by multiple columns\n        df.group_by(col(\"department\"), col(\"location\")).agg({\"salary\": \"avg\"}).show()\n        # Output:\n        # +----------+--------+-----------+\n        # |department|location|avg(salary)|\n        # +----------+--------+-----------+\n        # |        IT|    NYC|    85000.0|\n        # |        HR|    NYC|    72500.0|\n        # +----------+--------+-----------+\n        ```\n\n    Example: Group by expression\n        ```python\n        # Group by expression\n        df.group_by(col(\"age\").cast(\"int\").alias(\"age_group\")).count().show()\n        # Output:\n        # +---------+-----+\n        # |age_group|count|\n        # +---------+-----+\n        # |       20|    2|\n        # |       30|    3|\n        # |       40|    1|\n        # +---------+-----+\n        ```\n    \"\"\"\n    return GroupedData(self, list(cols) if cols else None)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.join","title":"join","text":"<pre><code>join(other: DataFrame, on: Union[str, List[str]], *, how: JoinType = 'inner') -&gt; DataFrame\n</code></pre><pre><code>join(other: DataFrame, *, left_on: Union[ColumnOrName, List[ColumnOrName]], right_on: Union[ColumnOrName, List[ColumnOrName]], how: JoinType = 'inner') -&gt; DataFrame\n</code></pre> <pre><code>join(other: DataFrame, on: Optional[Union[str, List[str]]] = None, *, left_on: Optional[Union[ColumnOrName, List[ColumnOrName]]] = None, right_on: Optional[Union[ColumnOrName, List[ColumnOrName]]] = None, how: JoinType = 'inner') -&gt; DataFrame\n</code></pre> <p>Joins this DataFrame with another DataFrame.</p> <p>The Dataframes must have no duplicate column names between them. This API only supports equi-joins. For non-equi-joins, use session.sql().</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>DataFrame</code>)           \u2013            <p>DataFrame to join with.</p> </li> <li> <code>on</code>               (<code>Optional[Union[str, List[str]]]</code>, default:                   <code>None</code> )           \u2013            <p>Join condition(s). Can be: - A column name (str) - A list of column names (List[str]) - A Column expression (e.g., col('a')) - A list of Column expressions - <code>None</code> for cross joins</p> </li> <li> <code>left_on</code>               (<code>Optional[Union[ColumnOrName, List[ColumnOrName]]]</code>, default:                   <code>None</code> )           \u2013            <p>Column(s) from the left DataFrame to join on. Can be: - A column name (str) - A Column expression (e.g., col('a'), col('a') + 1) - A list of column names or expressions</p> </li> <li> <code>right_on</code>               (<code>Optional[Union[ColumnOrName, List[ColumnOrName]]]</code>, default:                   <code>None</code> )           \u2013            <p>Column(s) from the right DataFrame to join on. Can be: - A column name (str) - A Column expression (e.g., col('b'), upper(col('b'))) - A list of column names or expressions</p> </li> <li> <code>how</code>               (<code>JoinType</code>, default:                   <code>'inner'</code> )           \u2013            <p>Type of join to perform.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Joined DataFrame.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If cross join is used with an ON clause.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If join condition is invalid.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If both 'on' and 'left_on'/'right_on' parameters are provided.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If only one of 'left_on' or 'right_on' is provided.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If 'left_on' and 'right_on' have different lengths</p> </li> </ul> Inner join on column name <pre><code># Create sample DataFrames\ndf1 = session.create_dataframe({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n})\ndf2 = session.create_dataframe({\n    \"id\": [1, 2, 4],\n    \"age\": [25, 30, 35]\n})\n\n# Join on single column\ndf1.join(df2, on=col(\"id\")).show()\n# Output:\n# +---+-----+---+\n# | id| name|age|\n# +---+-----+---+\n# |  1|Alice| 25|\n# |  2|  Bob| 30|\n# +---+-----+---+\n</code></pre> Join with expression <pre><code># Join with Column expressions\ndf1.join(\n    df2,\n    left_on=col(\"id\"),\n    right_on=col(\"id\"),\n).show()\n# Output:\n# +---+-----+---+\n# | id| name|age|\n# +---+-----+---+\n# |  1|Alice| 25|\n# |  2|  Bob| 30|\n# +---+-----+---+\n</code></pre> Cross join <pre><code># Cross join (cartesian product)\ndf1.join(df2, how=\"cross\").show()\n# Output:\n# +---+-----+---+---+\n# | id| name| id|age|\n# +---+-----+---+---+\n# |  1|Alice|  1| 25|\n# |  1|Alice|  2| 30|\n# |  1|Alice|  4| 35|\n# |  2|  Bob|  1| 25|\n# |  2|  Bob|  2| 30|\n# |  2|  Bob|  4| 35|\n# |  3|Charlie| 1| 25|\n# |  3|Charlie| 2| 30|\n# |  3|Charlie| 4| 35|\n# +---+-----+---+---+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def join(\n    self,\n    other: DataFrame,\n    on: Optional[Union[str, List[str]]] = None,\n    *,\n    left_on: Optional[Union[ColumnOrName, List[ColumnOrName]]] = None,\n    right_on: Optional[Union[ColumnOrName, List[ColumnOrName]]] = None,\n    how: JoinType = \"inner\",\n) -&gt; DataFrame:\n    \"\"\"Joins this DataFrame with another DataFrame.\n\n    The Dataframes must have no duplicate column names between them. This API only supports equi-joins.\n    For non-equi-joins, use session.sql().\n\n    Args:\n        other: DataFrame to join with.\n        on: Join condition(s). Can be:\n            - A column name (str)\n            - A list of column names (List[str])\n            - A Column expression (e.g., col('a'))\n            - A list of Column expressions\n            - `None` for cross joins\n        left_on: Column(s) from the left DataFrame to join on. Can be:\n            - A column name (str)\n            - A Column expression (e.g., col('a'), col('a') + 1)\n            - A list of column names or expressions\n        right_on: Column(s) from the right DataFrame to join on. Can be:\n            - A column name (str)\n            - A Column expression (e.g., col('b'), upper(col('b')))\n            - A list of column names or expressions\n        how: Type of join to perform.\n\n    Returns:\n        Joined DataFrame.\n\n    Raises:\n        ValidationError: If cross join is used with an ON clause.\n        ValidationError: If join condition is invalid.\n        ValidationError: If both 'on' and 'left_on'/'right_on' parameters are provided.\n        ValidationError: If only one of 'left_on' or 'right_on' is provided.\n        ValidationError: If 'left_on' and 'right_on' have different lengths\n\n    Example: Inner join on column name\n        ```python\n        # Create sample DataFrames\n        df1 = session.create_dataframe({\n            \"id\": [1, 2, 3],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n        })\n        df2 = session.create_dataframe({\n            \"id\": [1, 2, 4],\n            \"age\": [25, 30, 35]\n        })\n\n        # Join on single column\n        df1.join(df2, on=col(\"id\")).show()\n        # Output:\n        # +---+-----+---+\n        # | id| name|age|\n        # +---+-----+---+\n        # |  1|Alice| 25|\n        # |  2|  Bob| 30|\n        # +---+-----+---+\n        ```\n\n    Example: Join with expression\n        ```python\n        # Join with Column expressions\n        df1.join(\n            df2,\n            left_on=col(\"id\"),\n            right_on=col(\"id\"),\n        ).show()\n        # Output:\n        # +---+-----+---+\n        # | id| name|age|\n        # +---+-----+---+\n        # |  1|Alice| 25|\n        # |  2|  Bob| 30|\n        # +---+-----+---+\n        ```\n\n    Example: Cross join\n        ```python\n        # Cross join (cartesian product)\n        df1.join(df2, how=\"cross\").show()\n        # Output:\n        # +---+-----+---+---+\n        # | id| name| id|age|\n        # +---+-----+---+---+\n        # |  1|Alice|  1| 25|\n        # |  1|Alice|  2| 30|\n        # |  1|Alice|  4| 35|\n        # |  2|  Bob|  1| 25|\n        # |  2|  Bob|  2| 30|\n        # |  2|  Bob|  4| 35|\n        # |  3|Charlie| 1| 25|\n        # |  3|Charlie| 2| 30|\n        # |  3|Charlie| 4| 35|\n        # +---+-----+---+---+\n        ```\n    \"\"\"\n    validate_join_parameters(self, on, left_on, right_on, how)\n\n    # Build join conditions\n    left_conditions, right_conditions = build_join_conditions(on, left_on, right_on)\n\n    return self._from_logical_plan(\n        Join(self._logical_plan, other._logical_plan, left_conditions, right_conditions, how),\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.limit","title":"limit","text":"<pre><code>limit(n: int) -&gt; DataFrame\n</code></pre> <p>Limits the number of rows to the specified number.</p> <p>Parameters:</p> <ul> <li> <code>n</code>               (<code>int</code>)           \u2013            <p>Maximum number of rows to return.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>DataFrame with at most n rows.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If n is not an integer.</p> </li> </ul> Limit rows <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"id\": [1, 2, 3, 4, 5],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Eve\"]\n})\n\n# Get first 3 rows\ndf.limit(3).show()\n# Output:\n# +---+-------+\n# | id|   name|\n# +---+-------+\n# |  1|  Alice|\n# |  2|    Bob|\n# |  3|Charlie|\n# +---+-------+\n</code></pre> Limit with other operations <pre><code># Limit after filtering\ndf.filter(col(\"id\") &gt; 2).limit(2).show()\n# Output:\n# +---+-------+\n# | id|   name|\n# +---+-------+\n# |  3|Charlie|\n# |  4|   Dave|\n# +---+-------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def limit(self, n: int) -&gt; DataFrame:\n    \"\"\"Limits the number of rows to the specified number.\n\n    Args:\n        n: Maximum number of rows to return.\n\n    Returns:\n        DataFrame: DataFrame with at most n rows.\n\n    Raises:\n        TypeError: If n is not an integer.\n\n    Example: Limit rows\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"id\": [1, 2, 3, 4, 5],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Eve\"]\n        })\n\n        # Get first 3 rows\n        df.limit(3).show()\n        # Output:\n        # +---+-------+\n        # | id|   name|\n        # +---+-------+\n        # |  1|  Alice|\n        # |  2|    Bob|\n        # |  3|Charlie|\n        # +---+-------+\n        ```\n\n    Example: Limit with other operations\n        ```python\n        # Limit after filtering\n        df.filter(col(\"id\") &gt; 2).limit(2).show()\n        # Output:\n        # +---+-------+\n        # | id|   name|\n        # +---+-------+\n        # |  3|Charlie|\n        # |  4|   Dave|\n        # +---+-------+\n        ```\n    \"\"\"\n    return self._from_logical_plan(Limit(self._logical_plan, n))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.lineage","title":"lineage","text":"<pre><code>lineage() -&gt; Lineage\n</code></pre> <p>Create a Lineage object to trace data through transformations.</p> <p>The Lineage interface allows you to trace how specific rows are transformed through your DataFrame operations, both forwards and backwards through the computation graph.</p> <p>Returns:</p> <ul> <li> <code>Lineage</code> (              <code>Lineage</code> )          \u2013            <p>Interface for querying data lineage</p> </li> </ul> Example <pre><code># Create lineage query\nlineage = df.lineage()\n\n# Trace specific rows backwards through transformations\nsource_rows = lineage.backward([\"result_uuid1\", \"result_uuid2\"])\n\n# Or trace forwards to see outputs\nresult_rows = lineage.forward([\"source_uuid1\"])\n</code></pre> See Also <p>LineageQuery: Full documentation of lineage querying capabilities</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def lineage(self) -&gt; Lineage:\n    \"\"\"Create a Lineage object to trace data through transformations.\n\n    The Lineage interface allows you to trace how specific rows are transformed\n    through your DataFrame operations, both forwards and backwards through the\n    computation graph.\n\n    Returns:\n        Lineage: Interface for querying data lineage\n\n    Example:\n        ```python\n        # Create lineage query\n        lineage = df.lineage()\n\n        # Trace specific rows backwards through transformations\n        source_rows = lineage.backward([\"result_uuid1\", \"result_uuid2\"])\n\n        # Or trace forwards to see outputs\n        result_rows = lineage.forward([\"source_uuid1\"])\n        ```\n\n    See Also:\n        LineageQuery: Full documentation of lineage querying capabilities\n    \"\"\"\n    return Lineage(self._logical_plan.session_state.execution.build_lineage(self._logical_plan))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.order_by","title":"order_by","text":"<pre><code>order_by(cols: Union[ColumnOrName, List[ColumnOrName], None] = None, ascending: Optional[Union[bool, List[bool]]] = None) -&gt; 'DataFrame'\n</code></pre> <p>Sort the DataFrame by the specified columns. Alias for sort().</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>'DataFrame'</code> )          \u2013            <p>sorted Dataframe.</p> </li> </ul> See Also <p>sort(): Full documentation of sorting behavior and parameters.</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def order_by(\n    self,\n    cols: Union[ColumnOrName, List[ColumnOrName], None] = None,\n    ascending: Optional[Union[bool, List[bool]]] = None,\n) -&gt; \"DataFrame\":\n    \"\"\"Sort the DataFrame by the specified columns. Alias for sort().\n\n    Returns:\n        DataFrame: sorted Dataframe.\n\n    See Also:\n        sort(): Full documentation of sorting behavior and parameters.\n    \"\"\"\n    return self.sort(cols, ascending)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.persist","title":"persist","text":"<pre><code>persist() -&gt; DataFrame\n</code></pre> <p>Mark this DataFrame to be persisted after first computation.</p> <p>The persisted DataFrame will be cached after its first computation, avoiding recomputation in subsequent operations. This is useful for DataFrames that are reused multiple times in your workflow.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Same DataFrame, but marked for persistence</p> </li> </ul> Example <pre><code># Cache intermediate results for reuse\nfiltered_df = (df\n    .filter(col(\"age\") &gt; 25)\n    .persist()  # Cache these results\n)\n\n# Both operations will use cached results\nresult1 = filtered_df.group_by(\"department\").count()\nresult2 = filtered_df.select(\"name\", \"salary\")\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def persist(self) -&gt; DataFrame:\n    \"\"\"Mark this DataFrame to be persisted after first computation.\n\n    The persisted DataFrame will be cached after its first computation,\n    avoiding recomputation in subsequent operations. This is useful for DataFrames\n    that are reused multiple times in your workflow.\n\n    Returns:\n        DataFrame: Same DataFrame, but marked for persistence\n\n    Example:\n        ```python\n        # Cache intermediate results for reuse\n        filtered_df = (df\n            .filter(col(\"age\") &gt; 25)\n            .persist()  # Cache these results\n        )\n\n        # Both operations will use cached results\n        result1 = filtered_df.group_by(\"department\").count()\n        result2 = filtered_df.select(\"name\", \"salary\")\n        ```\n    \"\"\"\n    table_name = f\"cache_{uuid.uuid4().hex}\"\n    cache_info = CacheInfo(duckdb_table_name=table_name)\n    self._logical_plan.set_cache_info(cache_info)\n    return self._from_logical_plan(self._logical_plan)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.select","title":"select","text":"<pre><code>select(*cols: ColumnOrName) -&gt; DataFrame\n</code></pre> <p>Projects a set of Column expressions or column names.</p> <p>Parameters:</p> <ul> <li> <code>*cols</code>               (<code>ColumnOrName</code>, default:                   <code>()</code> )           \u2013            <p>Column expressions to select. Can be: - String column names (e.g., \"id\", \"name\") - Column objects (e.g., col(\"id\"), col(\"age\") + 1)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with selected columns</p> </li> </ul> Select by column names <pre><code># Create a DataFrame\ndf = session.create_dataframe({\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]})\n\n# Select by column names\ndf.select(col(\"name\"), col(\"age\")).show()\n# Output:\n# +-----+---+\n# | name|age|\n# +-----+---+\n# |Alice| 25|\n# |  Bob| 30|\n# +-----+---+\n</code></pre> Select with expressions <pre><code># Select with expressions\ndf.select(col(\"name\"), col(\"age\") + 1).show()\n# Output:\n# +-----+-------+\n# | name|age + 1|\n# +-----+-------+\n# |Alice|     26|\n# |  Bob|     31|\n# +-----+-------+\n</code></pre> Mix strings and expressions <pre><code># Mix strings and expressions\ndf.select(col(\"name\"), col(\"age\") * 2).show()\n# Output:\n# +-----+-------+\n# | name|age * 2|\n# +-----+-------+\n# |Alice|     50|\n# |  Bob|     60|\n# +-----+-------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def select(self, *cols: ColumnOrName) -&gt; DataFrame:\n    \"\"\"Projects a set of Column expressions or column names.\n\n    Args:\n        *cols: Column expressions to select. Can be:\n            - String column names (e.g., \"id\", \"name\")\n            - Column objects (e.g., col(\"id\"), col(\"age\") + 1)\n\n    Returns:\n        DataFrame: A new DataFrame with selected columns\n\n    Example: Select by column names\n        ```python\n        # Create a DataFrame\n        df = session.create_dataframe({\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]})\n\n        # Select by column names\n        df.select(col(\"name\"), col(\"age\")).show()\n        # Output:\n        # +-----+---+\n        # | name|age|\n        # +-----+---+\n        # |Alice| 25|\n        # |  Bob| 30|\n        # +-----+---+\n        ```\n\n    Example: Select with expressions\n        ```python\n        # Select with expressions\n        df.select(col(\"name\"), col(\"age\") + 1).show()\n        # Output:\n        # +-----+-------+\n        # | name|age + 1|\n        # +-----+-------+\n        # |Alice|     26|\n        # |  Bob|     31|\n        # +-----+-------+\n        ```\n\n    Example: Mix strings and expressions\n        ```python\n        # Mix strings and expressions\n        df.select(col(\"name\"), col(\"age\") * 2).show()\n        # Output:\n        # +-----+-------+\n        # | name|age * 2|\n        # +-----+-------+\n        # |Alice|     50|\n        # |  Bob|     60|\n        # +-----+-------+\n        ```\n    \"\"\"\n    exprs = []\n    if not cols:\n        return self\n    for c in cols:\n        if isinstance(c, str):\n            if c == \"*\":\n                exprs.extend(col(field)._logical_expr for field in self.columns)\n            else:\n                exprs.append(col(c)._logical_expr)\n        else:\n            exprs.append(c._logical_expr)\n\n    return self._from_logical_plan(\n        Projection(self._logical_plan, exprs)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.show","title":"show","text":"<pre><code>show(n: int = 10, explain_analyze: bool = False) -&gt; None\n</code></pre> <p>Display the DataFrame content in a tabular form.</p> <p>This is an action that triggers computation of the DataFrame. The output is printed to stdout in a formatted table.</p> <p>Parameters:</p> <ul> <li> <code>n</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of rows to display</p> </li> <li> <code>explain_analyze</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to print the explain analyze plan</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def show(self, n: int = 10, explain_analyze: bool = False) -&gt; None:\n    \"\"\"Display the DataFrame content in a tabular form.\n\n    This is an action that triggers computation of the DataFrame.\n    The output is printed to stdout in a formatted table.\n\n    Args:\n        n: Number of rows to display\n        explain_analyze: Whether to print the explain analyze plan\n    \"\"\"\n    output, metrics = self._logical_plan.session_state.execution.show(self._logical_plan, n)\n    logger.info(metrics.get_summary())\n    print(output)\n    if explain_analyze:\n        print(metrics.get_execution_plan_details())\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.sort","title":"sort","text":"<pre><code>sort(cols: Union[ColumnOrName, List[ColumnOrName], None] = None, ascending: Optional[Union[bool, List[bool]]] = None) -&gt; DataFrame\n</code></pre> <p>Sort the DataFrame by the specified columns.</p> <p>Parameters:</p> <ul> <li> <code>cols</code>               (<code>Union[ColumnOrName, List[ColumnOrName], None]</code>, default:                   <code>None</code> )           \u2013            <p>Columns to sort by. This can be: - A single column name (str) - A Column expression (e.g., <code>col(\"name\")</code>) - A list of column names or Column expressions - Column expressions may include sorting directives such as <code>asc(\"col\")</code>, <code>desc(\"col\")</code>, <code>asc_nulls_last(\"col\")</code>, etc. - If no columns are provided, the operation is a no-op.</p> </li> <li> <code>ascending</code>               (<code>Optional[Union[bool, List[bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>A boolean or list of booleans indicating sort order. - If <code>True</code>, sorts in ascending order; if <code>False</code>, descending. - If a list is provided, its length must match the number of columns. - Cannot be used if any of the columns use <code>asc()</code>/<code>desc()</code> expressions. - If not specified and no sort expressions are used, columns will be sorted in ascending order by default.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame sorted by the specified columns.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <ul> <li>If <code>ascending</code> is provided and its length does not match <code>cols</code></li> <li>If both <code>ascending</code> and column expressions like <code>asc()</code>/<code>desc()</code> are used</li> </ul> </li> <li> <code>TypeError</code>             \u2013            <ul> <li>If <code>cols</code> is not a column name, Column, or list of column names/Columns</li> <li>If <code>ascending</code> is not a boolean or list of booleans</li> </ul> </li> </ul> Sort in ascending order <pre><code># Create sample DataFrame\ndf = session.create_dataframe([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n\n# Sort by age in ascending order\ndf.sort(asc(col(\"age\"))).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  2|Alice|\n# |  5|  Bob|\n# +---+-----+\n</code></pre> Sort in descending order <pre><code># Sort by age in descending order\ndf.sort(col(\"age\").desc()).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  5|  Bob|\n# |  2|Alice|\n# +---+-----+\n</code></pre> Sort with boolean ascending parameter <pre><code># Sort by age in descending order using boolean\ndf.sort(col(\"age\"), ascending=False).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  5|  Bob|\n# |  2|Alice|\n# +---+-----+\n</code></pre> Multiple columns with different sort orders <pre><code># Create sample DataFrame\ndf = session.create_dataframe([(2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n\n# Sort by age descending, then name ascending\ndf.sort(desc(col(\"age\")), col(\"name\")).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  5|  Bob|\n# |  2|Alice|\n# |  2|  Bob|\n# +---+-----+\n</code></pre> Multiple columns with list of ascending strategies <pre><code># Sort both columns in descending order\ndf.sort([col(\"age\"), col(\"name\")], ascending=[False, False]).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  5|  Bob|\n# |  2|  Bob|\n# |  2|Alice|\n# +---+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def sort(\n    self,\n    cols: Union[ColumnOrName, List[ColumnOrName], None] = None,\n    ascending: Optional[Union[bool, List[bool]]] = None,\n) -&gt; DataFrame:\n    \"\"\"Sort the DataFrame by the specified columns.\n\n    Args:\n        cols: Columns to sort by. This can be:\n            - A single column name (str)\n            - A Column expression (e.g., `col(\"name\")`)\n            - A list of column names or Column expressions\n            - Column expressions may include sorting directives such as `asc(\"col\")`, `desc(\"col\")`,\n            `asc_nulls_last(\"col\")`, etc.\n            - If no columns are provided, the operation is a no-op.\n\n        ascending: A boolean or list of booleans indicating sort order.\n            - If `True`, sorts in ascending order; if `False`, descending.\n            - If a list is provided, its length must match the number of columns.\n            - Cannot be used if any of the columns use `asc()`/`desc()` expressions.\n            - If not specified and no sort expressions are used, columns will be sorted in ascending order by default.\n\n    Returns:\n        DataFrame: A new DataFrame sorted by the specified columns.\n\n    Raises:\n        ValueError:\n            - If `ascending` is provided and its length does not match `cols`\n            - If both `ascending` and column expressions like `asc()`/`desc()` are used\n        TypeError:\n            - If `cols` is not a column name, Column, or list of column names/Columns\n            - If `ascending` is not a boolean or list of booleans\n\n    Example: Sort in ascending order\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n\n        # Sort by age in ascending order\n        df.sort(asc(col(\"age\"))).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  2|Alice|\n        # |  5|  Bob|\n        # +---+-----+\n        ```\n\n    Example: Sort in descending order\n        ```python\n        # Sort by age in descending order\n        df.sort(col(\"age\").desc()).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  5|  Bob|\n        # |  2|Alice|\n        # +---+-----+\n        ```\n\n    Example: Sort with boolean ascending parameter\n        ```python\n        # Sort by age in descending order using boolean\n        df.sort(col(\"age\"), ascending=False).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  5|  Bob|\n        # |  2|Alice|\n        # +---+-----+\n        ```\n\n    Example: Multiple columns with different sort orders\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe([(2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n\n        # Sort by age descending, then name ascending\n        df.sort(desc(col(\"age\")), col(\"name\")).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  5|  Bob|\n        # |  2|Alice|\n        # |  2|  Bob|\n        # +---+-----+\n        ```\n\n    Example: Multiple columns with list of ascending strategies\n        ```python\n        # Sort both columns in descending order\n        df.sort([col(\"age\"), col(\"name\")], ascending=[False, False]).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  5|  Bob|\n        # |  2|  Bob|\n        # |  2|Alice|\n        # +---+-----+\n        ```\n    \"\"\"\n    col_args = cols\n    if cols is None:\n        return self._from_logical_plan(\n            Sort(self._logical_plan, [])\n        )\n    elif not isinstance(cols, List):\n        col_args = [cols]\n\n    # parse the ascending arguments\n    bool_ascending = []\n    using_default_ascending = False\n    if ascending is None:\n        using_default_ascending = True\n        bool_ascending = [True] * len(col_args)\n    elif isinstance(ascending, bool):\n        bool_ascending = [ascending] * len(col_args)\n    elif isinstance(ascending, List):\n        bool_ascending = ascending\n        if len(bool_ascending) != len(cols):\n            raise ValueError(\n                f\"the list length of ascending sort strategies must match the specified sort columns\"\n                f\"Got {len(cols)} column expressions and {len(bool_ascending)} ascending strategies. \"\n            )\n    else:\n        raise TypeError(\n            f\"Invalid ascending strategy type: {type(ascending)}.  Must be a boolean or list of booleans.\"\n        )\n\n    # create our list of sort expressions, for each column expression\n    # that isn't already provided as a asc()/desc() SortExpr\n    sort_exprs = []\n    for c, asc_bool in zip(col_args, bool_ascending, strict=True):\n        if isinstance(c, ColumnOrName):\n            c_expr = Column._from_col_or_name(c)._logical_expr\n        else:\n            raise TypeError(\n                f\"Invalid column type: {type(c).__name__}.  Must be a string or Column Expression.\"\n            )\n        if not isinstance(asc_bool, bool):\n            raise TypeError(\n                f\"Invalid ascending strategy type: {type(asc_bool).__name__}.  Must be a boolean.\"\n            )\n        if isinstance(c_expr, SortExpr):\n            if not using_default_ascending:\n                raise TypeError(\n                    \"Cannot specify both asc()/desc() expressions and boolean ascending strategies.\"\n                    f\"Got expression: {c_expr} and ascending argument: {bool_ascending}\"\n                )\n            sort_exprs.append(c_expr)\n        else:\n            sort_exprs.append(SortExpr(c_expr, ascending=asc_bool))\n\n    return self._from_logical_plan(\n        Sort(self._logical_plan, sort_exprs),\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.to_arrow","title":"to_arrow","text":"<pre><code>to_arrow() -&gt; pa.Table\n</code></pre> <p>Execute the DataFrame computation and return an Apache Arrow Table.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into an Apache Arrow Table with columnar memory layout optimized for analytics and zero-copy data exchange.</p> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>pa.Table: An Apache Arrow Table containing the computed results</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_arrow(self) -&gt; pa.Table:\n    \"\"\"Execute the DataFrame computation and return an Apache Arrow Table.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into an Apache Arrow Table with columnar memory layout\n    optimized for analytics and zero-copy data exchange.\n\n    Returns:\n        pa.Table: An Apache Arrow Table containing the computed results\n    \"\"\"\n    return self.collect(\"arrow\").data\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; pd.DataFrame\n</code></pre> <p>Execute the DataFrame computation and return a Pandas DataFrame.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a Pandas DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A Pandas DataFrame containing the computed results with</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"Execute the DataFrame computation and return a Pandas DataFrame.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a Pandas DataFrame.\n\n    Returns:\n        pd.DataFrame: A Pandas DataFrame containing the computed results with\n    \"\"\"\n    return self.collect(\"pandas\").data\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.to_polars","title":"to_polars","text":"<pre><code>to_polars() -&gt; pl.DataFrame\n</code></pre> <p>Execute the DataFrame computation and return the result as a Polars DataFrame.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a Polars DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pl.DataFrame: A Polars DataFrame with materialized results</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_polars(self) -&gt; pl.DataFrame:\n    \"\"\"Execute the DataFrame computation and return the result as a Polars DataFrame.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a Polars DataFrame.\n\n    Returns:\n        pl.DataFrame: A Polars DataFrame with materialized results\n    \"\"\"\n    return self.collect(\"polars\").data\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.to_pydict","title":"to_pydict","text":"<pre><code>to_pydict() -&gt; Dict[str, List[Any]]\n</code></pre> <p>Execute the DataFrame computation and return a dictionary of column arrays.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a Python dictionary where each column becomes a list of values.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, List[Any]]</code>           \u2013            <p>Dict[str, List[Any]]: A dictionary containing the computed results with: - Keys: Column names as strings - Values: Lists containing all values for each column</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_pydict(self) -&gt; Dict[str, List[Any]]:\n    \"\"\"Execute the DataFrame computation and return a dictionary of column arrays.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a Python dictionary where each column becomes a list of values.\n\n    Returns:\n        Dict[str, List[Any]]: A dictionary containing the computed results with:\n            - Keys: Column names as strings\n            - Values: Lists containing all values for each column\n    \"\"\"\n    return self.collect(\"pydict\").data\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.to_pylist","title":"to_pylist","text":"<pre><code>to_pylist() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Execute the DataFrame computation and return a list of row dictionaries.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a Python list where each element is a dictionary representing one row with column names as keys.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>List[Dict[str, Any]]: A list containing the computed results with: - Each element: A dictionary representing one row - Dictionary keys: Column names as strings - Dictionary values: Cell values in Python native types - List length equals number of rows in the result</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_pylist(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Execute the DataFrame computation and return a list of row dictionaries.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a Python list where each element is a dictionary\n    representing one row with column names as keys.\n\n    Returns:\n        List[Dict[str, Any]]: A list containing the computed results with:\n            - Each element: A dictionary representing one row\n            - Dictionary keys: Column names as strings\n            - Dictionary values: Cell values in Python native types\n            - List length equals number of rows in the result\n    \"\"\"\n    return self.collect(\"pylist\").data\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.union","title":"union","text":"<pre><code>union(other: DataFrame) -&gt; DataFrame\n</code></pre> <p>Return a new DataFrame containing the union of rows in this and another DataFrame.</p> <p>This is equivalent to UNION ALL in SQL. To remove duplicates, use drop_duplicates() after union().</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>DataFrame</code>)           \u2013            <p>Another DataFrame with the same schema.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame containing rows from both DataFrames.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the DataFrames have different schemas.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If other is not a DataFrame.</p> </li> </ul> Union two DataFrames <pre><code># Create two DataFrames\ndf1 = session.create_dataframe({\n    \"id\": [1, 2],\n    \"value\": [\"a\", \"b\"]\n})\ndf2 = session.create_dataframe({\n    \"id\": [3, 4],\n    \"value\": [\"c\", \"d\"]\n})\n\n# Union the DataFrames\ndf1.union(df2).show()\n# Output:\n# +---+-----+\n# | id|value|\n# +---+-----+\n# |  1|    a|\n# |  2|    b|\n# |  3|    c|\n# |  4|    d|\n# +---+-----+\n</code></pre> Union with duplicates <pre><code># Create DataFrames with overlapping data\ndf1 = session.create_dataframe({\n    \"id\": [1, 2],\n    \"value\": [\"a\", \"b\"]\n})\ndf2 = session.create_dataframe({\n    \"id\": [2, 3],\n    \"value\": [\"b\", \"c\"]\n})\n\n# Union with duplicates\ndf1.union(df2).show()\n# Output:\n# +---+-----+\n# | id|value|\n# +---+-----+\n# |  1|    a|\n# |  2|    b|\n# |  2|    b|\n# |  3|    c|\n# +---+-----+\n\n# Remove duplicates after union\ndf1.union(df2).drop_duplicates().show()\n# Output:\n# +---+-----+\n# | id|value|\n# +---+-----+\n# |  1|    a|\n# |  2|    b|\n# |  3|    c|\n# +---+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def union(self, other: DataFrame) -&gt; DataFrame:\n    \"\"\"Return a new DataFrame containing the union of rows in this and another DataFrame.\n\n    This is equivalent to UNION ALL in SQL. To remove duplicates, use drop_duplicates() after union().\n\n    Args:\n        other: Another DataFrame with the same schema.\n\n    Returns:\n        DataFrame: A new DataFrame containing rows from both DataFrames.\n\n    Raises:\n        ValueError: If the DataFrames have different schemas.\n        TypeError: If other is not a DataFrame.\n\n    Example: Union two DataFrames\n        ```python\n        # Create two DataFrames\n        df1 = session.create_dataframe({\n            \"id\": [1, 2],\n            \"value\": [\"a\", \"b\"]\n        })\n        df2 = session.create_dataframe({\n            \"id\": [3, 4],\n            \"value\": [\"c\", \"d\"]\n        })\n\n        # Union the DataFrames\n        df1.union(df2).show()\n        # Output:\n        # +---+-----+\n        # | id|value|\n        # +---+-----+\n        # |  1|    a|\n        # |  2|    b|\n        # |  3|    c|\n        # |  4|    d|\n        # +---+-----+\n        ```\n\n    Example: Union with duplicates\n        ```python\n        # Create DataFrames with overlapping data\n        df1 = session.create_dataframe({\n            \"id\": [1, 2],\n            \"value\": [\"a\", \"b\"]\n        })\n        df2 = session.create_dataframe({\n            \"id\": [2, 3],\n            \"value\": [\"b\", \"c\"]\n        })\n\n        # Union with duplicates\n        df1.union(df2).show()\n        # Output:\n        # +---+-----+\n        # | id|value|\n        # +---+-----+\n        # |  1|    a|\n        # |  2|    b|\n        # |  2|    b|\n        # |  3|    c|\n        # +---+-----+\n\n        # Remove duplicates after union\n        df1.union(df2).drop_duplicates().show()\n        # Output:\n        # +---+-----+\n        # | id|value|\n        # +---+-----+\n        # |  1|    a|\n        # |  2|    b|\n        # |  3|    c|\n        # +---+-----+\n        ```\n    \"\"\"\n    return self._from_logical_plan(\n        UnionLogicalPlan([self._logical_plan, other._logical_plan]),\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.unnest","title":"unnest","text":"<pre><code>unnest(*col_names: str) -&gt; DataFrame\n</code></pre> <p>Unnest the specified struct columns into separate columns.</p> <p>This operation flattens nested struct data by expanding each field of a struct into its own top-level column.</p> <p>For each specified column containing a struct: 1. Each field in the struct becomes a separate column. 2. New columns are named after the corresponding struct fields. 3. The new columns are inserted into the DataFrame in place of the original struct column. 4. The overall column order is preserved.</p> <p>Parameters:</p> <ul> <li> <code>*col_names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>One or more struct columns to unnest. Each can be a string (column name) or a Column expression.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with the specified struct columns expanded.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If any argument is not a string or Column.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If a specified column does not contain struct data.</p> </li> </ul> Unnest struct column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"id\": [1, 2],\n    \"tags\": [{\"red\": 1, \"blue\": 2}, {\"red\": 3}],\n    \"name\": [\"Alice\", \"Bob\"]\n})\n\n# Unnest the tags column\ndf.unnest(col(\"tags\")).show()\n# Output:\n# +---+---+----+-----+\n# | id| red|blue| name|\n# +---+---+----+-----+\n# |  1|  1|   2|Alice|\n# |  2|  3|null|  Bob|\n# +---+---+----+-----+\n</code></pre> Unnest multiple struct columns <pre><code># Create sample DataFrame with multiple struct columns\ndf = session.create_dataframe({\n    \"id\": [1, 2],\n    \"tags\": [{\"red\": 1, \"blue\": 2}, {\"red\": 3}],\n    \"info\": [{\"age\": 25, \"city\": \"NY\"}, {\"age\": 30, \"city\": \"LA\"}],\n    \"name\": [\"Alice\", \"Bob\"]\n})\n\n# Unnest multiple struct columns\ndf.unnest(col(\"tags\"), col(\"info\")).show()\n# Output:\n# +---+---+----+---+----+-----+\n# | id| red|blue|age|city| name|\n# +---+---+----+---+----+-----+\n# |  1|  1|   2| 25|  NY|Alice|\n# |  2|  3|null| 30|  LA|  Bob|\n# +---+---+----+---+----+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def unnest(self, *col_names: str) -&gt; DataFrame:\n    \"\"\"Unnest the specified struct columns into separate columns.\n\n    This operation flattens nested struct data by expanding each field of a struct\n    into its own top-level column.\n\n    For each specified column containing a struct:\n    1. Each field in the struct becomes a separate column.\n    2. New columns are named after the corresponding struct fields.\n    3. The new columns are inserted into the DataFrame in place of the original struct column.\n    4. The overall column order is preserved.\n\n    Args:\n        *col_names: One or more struct columns to unnest. Each can be a string (column name)\n            or a Column expression.\n\n    Returns:\n        DataFrame: A new DataFrame with the specified struct columns expanded.\n\n    Raises:\n        TypeError: If any argument is not a string or Column.\n        ValueError: If a specified column does not contain struct data.\n\n    Example: Unnest struct column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"id\": [1, 2],\n            \"tags\": [{\"red\": 1, \"blue\": 2}, {\"red\": 3}],\n            \"name\": [\"Alice\", \"Bob\"]\n        })\n\n        # Unnest the tags column\n        df.unnest(col(\"tags\")).show()\n        # Output:\n        # +---+---+----+-----+\n        # | id| red|blue| name|\n        # +---+---+----+-----+\n        # |  1|  1|   2|Alice|\n        # |  2|  3|null|  Bob|\n        # +---+---+----+-----+\n        ```\n\n    Example: Unnest multiple struct columns\n        ```python\n        # Create sample DataFrame with multiple struct columns\n        df = session.create_dataframe({\n            \"id\": [1, 2],\n            \"tags\": [{\"red\": 1, \"blue\": 2}, {\"red\": 3}],\n            \"info\": [{\"age\": 25, \"city\": \"NY\"}, {\"age\": 30, \"city\": \"LA\"}],\n            \"name\": [\"Alice\", \"Bob\"]\n        })\n\n        # Unnest multiple struct columns\n        df.unnest(col(\"tags\"), col(\"info\")).show()\n        # Output:\n        # +---+---+----+---+----+-----+\n        # | id| red|blue|age|city| name|\n        # +---+---+----+---+----+-----+\n        # |  1|  1|   2| 25|  NY|Alice|\n        # |  2|  3|null| 30|  LA|  Bob|\n        # +---+---+----+---+----+-----+\n        ```\n    \"\"\"\n    if not col_names:\n        return self\n    exprs = []\n    for c in col_names:\n        if c not in self.columns:\n            raise TypeError(f\"Column {c} not found in DataFrame.\")\n        exprs.append(col(c)._logical_expr)\n    return self._from_logical_plan(\n        Unnest(self._logical_plan, exprs),\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.where","title":"where","text":"<pre><code>where(condition: Column) -&gt; DataFrame\n</code></pre> <p>Filters rows using the given condition (alias for filter()).</p> <p>Parameters:</p> <ul> <li> <code>condition</code>               (<code>Column</code>)           \u2013            <p>A Column expression that evaluates to a boolean</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Filtered DataFrame</p> </li> </ul> See Also <p>filter(): Full documentation of filtering behavior</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def where(self, condition: Column) -&gt; DataFrame:\n    \"\"\"Filters rows using the given condition (alias for filter()).\n\n    Args:\n        condition: A Column expression that evaluates to a boolean\n\n    Returns:\n        DataFrame: Filtered DataFrame\n\n    See Also:\n        filter(): Full documentation of filtering behavior\n    \"\"\"\n    return self.filter(condition)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.with_column","title":"with_column","text":"<pre><code>with_column(col_name: str, col: Union[Any, Column]) -&gt; DataFrame\n</code></pre> <p>Add a new column or replace an existing column.</p> <p>Parameters:</p> <ul> <li> <code>col_name</code>               (<code>str</code>)           \u2013            <p>Name of the new column</p> </li> <li> <code>col</code>               (<code>Union[Any, Column]</code>)           \u2013            <p>Column expression or value to assign to the column. If not a Column, it will be treated as a literal value.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>New DataFrame with added/replaced column</p> </li> </ul> Add literal column <pre><code># Create a DataFrame\ndf = session.create_dataframe({\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]})\n\n# Add literal column\ndf.with_column(\"constant\", lit(1)).show()\n# Output:\n# +-----+---+--------+\n# | name|age|constant|\n# +-----+---+--------+\n# |Alice| 25|       1|\n# |  Bob| 30|       1|\n# +-----+---+--------+\n</code></pre> Add computed column <pre><code># Add computed column\ndf.with_column(\"double_age\", col(\"age\") * 2).show()\n# Output:\n# +-----+---+----------+\n# | name|age|double_age|\n# +-----+---+----------+\n# |Alice| 25|        50|\n# |  Bob| 30|        60|\n# +-----+---+----------+\n</code></pre> Replace existing column <pre><code># Replace existing column\ndf.with_column(\"age\", col(\"age\") + 1).show()\n# Output:\n# +-----+---+\n# | name|age|\n# +-----+---+\n# |Alice| 26|\n# |  Bob| 31|\n# +-----+---+\n</code></pre> Add column with complex expression <pre><code># Add column with complex expression\ndf.with_column(\n    \"age_category\",\n    when(col(\"age\") &lt; 30, \"young\")\n    .when(col(\"age\") &lt; 50, \"middle\")\n    .otherwise(\"senior\")\n).show()\n# Output:\n# +-----+---+------------+\n# | name|age|age_category|\n# +-----+---+------------+\n# |Alice| 25|       young|\n# |  Bob| 30|     middle|\n# +-----+---+------------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def with_column(self, col_name: str, col: Union[Any, Column]) -&gt; DataFrame:\n    \"\"\"Add a new column or replace an existing column.\n\n    Args:\n        col_name: Name of the new column\n        col: Column expression or value to assign to the column. If not a Column,\n            it will be treated as a literal value.\n\n    Returns:\n        DataFrame: New DataFrame with added/replaced column\n\n    Example: Add literal column\n        ```python\n        # Create a DataFrame\n        df = session.create_dataframe({\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]})\n\n        # Add literal column\n        df.with_column(\"constant\", lit(1)).show()\n        # Output:\n        # +-----+---+--------+\n        # | name|age|constant|\n        # +-----+---+--------+\n        # |Alice| 25|       1|\n        # |  Bob| 30|       1|\n        # +-----+---+--------+\n        ```\n\n    Example: Add computed column\n        ```python\n        # Add computed column\n        df.with_column(\"double_age\", col(\"age\") * 2).show()\n        # Output:\n        # +-----+---+----------+\n        # | name|age|double_age|\n        # +-----+---+----------+\n        # |Alice| 25|        50|\n        # |  Bob| 30|        60|\n        # +-----+---+----------+\n        ```\n\n    Example: Replace existing column\n        ```python\n        # Replace existing column\n        df.with_column(\"age\", col(\"age\") + 1).show()\n        # Output:\n        # +-----+---+\n        # | name|age|\n        # +-----+---+\n        # |Alice| 26|\n        # |  Bob| 31|\n        # +-----+---+\n        ```\n\n    Example: Add column with complex expression\n        ```python\n        # Add column with complex expression\n        df.with_column(\n            \"age_category\",\n            when(col(\"age\") &lt; 30, \"young\")\n            .when(col(\"age\") &lt; 50, \"middle\")\n            .otherwise(\"senior\")\n        ).show()\n        # Output:\n        # +-----+---+------------+\n        # | name|age|age_category|\n        # +-----+---+------------+\n        # |Alice| 25|       young|\n        # |  Bob| 30|     middle|\n        # +-----+---+------------+\n        ```\n    \"\"\"\n    exprs = []\n    if not isinstance(col, Column):\n        col = lit(col)\n\n    for field in self.columns:\n        if field != col_name:\n            exprs.append(Column._from_column_name(field)._logical_expr)\n\n    # Add the new column with alias\n    exprs.append(col.alias(col_name)._logical_expr)\n\n    return self._from_logical_plan(\n        Projection(self._logical_plan, exprs)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrame.with_column_renamed","title":"with_column_renamed","text":"<pre><code>with_column_renamed(col_name: str, new_col_name: str) -&gt; DataFrame\n</code></pre> <p>Rename a column. No-op if the column does not exist.</p> <p>Parameters:</p> <ul> <li> <code>col_name</code>               (<code>str</code>)           \u2013            <p>Name of the column to rename.</p> </li> <li> <code>new_col_name</code>               (<code>str</code>)           \u2013            <p>New name for the column.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>New DataFrame with the column renamed.</p> </li> </ul> Rename a column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"age\": [25, 30, 35],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n})\n\n# Rename a column\ndf.with_column_renamed(\"age\", \"age_in_years\").show()\n# Output:\n# +------------+-------+\n# |age_in_years|   name|\n# +------------+-------+\n# |         25|  Alice|\n# |         30|    Bob|\n# |         35|Charlie|\n# +------------+-------+\n</code></pre> Rename multiple columns <pre><code># Rename multiple columns\ndf = (df\n    .with_column_renamed(\"age\", \"age_in_years\")\n    .with_column_renamed(\"name\", \"full_name\")\n).show()\n# Output:\n# +------------+----------+\n# |age_in_years|full_name |\n# +------------+----------+\n# |         25|     Alice|\n# |         30|       Bob|\n# |         35|   Charlie|\n# +------------+----------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def with_column_renamed(self, col_name: str, new_col_name: str) -&gt; DataFrame:\n    \"\"\"Rename a column. No-op if the column does not exist.\n\n    Args:\n        col_name: Name of the column to rename.\n        new_col_name: New name for the column.\n\n    Returns:\n        DataFrame: New DataFrame with the column renamed.\n\n    Example: Rename a column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"age\": [25, 30, 35],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n        })\n\n        # Rename a column\n        df.with_column_renamed(\"age\", \"age_in_years\").show()\n        # Output:\n        # +------------+-------+\n        # |age_in_years|   name|\n        # +------------+-------+\n        # |         25|  Alice|\n        # |         30|    Bob|\n        # |         35|Charlie|\n        # +------------+-------+\n        ```\n\n    Example: Rename multiple columns\n        ```python\n        # Rename multiple columns\n        df = (df\n            .with_column_renamed(\"age\", \"age_in_years\")\n            .with_column_renamed(\"name\", \"full_name\")\n        ).show()\n        # Output:\n        # +------------+----------+\n        # |age_in_years|full_name |\n        # +------------+----------+\n        # |         25|     Alice|\n        # |         30|       Bob|\n        # |         35|   Charlie|\n        # +------------+----------+\n        ```\n    \"\"\"\n    exprs = []\n    renamed = False\n\n    for field in self.schema.column_fields:\n        name = field.name\n        if name == col_name:\n            exprs.append(col(name).alias(new_col_name)._logical_expr)\n            renamed = True\n        else:\n            exprs.append(col(name)._logical_expr)\n\n    if not renamed:\n        return self\n\n    return self._from_logical_plan(\n        Projection(self._logical_plan, exprs)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrameReader","title":"DataFrameReader","text":"<pre><code>DataFrameReader(session_state: BaseSessionState)\n</code></pre> <p>Interface used to load a DataFrame from external storage systems.</p> <p>Similar to PySpark's DataFrameReader.</p> <p>Creates a DataFrameReader.</p> <p>Parameters:</p> <ul> <li> <code>session_state</code>               (<code>BaseSessionState</code>)           \u2013            <p>The session state to use for reading</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>csv</code>             \u2013              <p>Load a DataFrame from one or more CSV files.</p> </li> <li> <code>parquet</code>             \u2013              <p>Load a DataFrame from one or more Parquet files.</p> </li> </ul> Source code in <code>src/fenic/api/io/reader.py</code> <pre><code>def __init__(self, session_state: BaseSessionState):\n    \"\"\"Creates a DataFrameReader.\n\n    Args:\n        session_state: The session state to use for reading\n    \"\"\"\n    self._options: Dict[str, Any] = {}\n    self._session_state = session_state\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrameReader.csv","title":"csv","text":"<pre><code>csv(paths: Union[str, Path, list[Union[str, Path]]], schema: Optional[Schema] = None, merge_schemas: bool = False) -&gt; DataFrame\n</code></pre> <p>Load a DataFrame from one or more CSV files.</p> <p>Parameters:</p> <ul> <li> <code>paths</code>               (<code>Union[str, Path, list[Union[str, Path]]]</code>)           \u2013            <p>A single file path, a glob pattern (e.g., \"data/*.csv\"), or a list of paths.</p> </li> <li> <code>schema</code>               (<code>Optional[Schema]</code>, default:                   <code>None</code> )           \u2013            <p>(optional) A complete schema definition of column names and their types. Only primitive types are supported. - For e.g.:     - Schema([ColumnField(name=\"id\", data_type=IntegerType), ColumnField(name=\"name\", data_type=StringType)]) - If provided, all files must match this schema exactly\u2014all column names must be present, and values must be convertible to the specified types. Partial schemas are not allowed.</p> </li> <li> <code>merge_schemas</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to merge schemas across all files. - If True: Column names are unified across files. Missing columns are filled with nulls. Column types are inferred and widened as needed. - If False (default): Only accepts columns from the first file. Column types from the first file are inferred and applied across all files. If subsequent files do not have the same column name and order as the first file, an error is raised. - The \"first file\" is defined as:     - The first file in lexicographic order (for glob patterns), or     - The first file in the provided list (for lists of paths).</p> </li> </ul> Notes <ul> <li>The first row in each file is assumed to be a header row.</li> <li>Delimiters (e.g., comma, tab) are automatically inferred.</li> <li>You may specify either <code>schema</code> or <code>merge_schemas=True</code>, but not both.</li> <li>Any date/datetime columns are cast to strings during ingestion.</li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If both <code>schema</code> and <code>merge_schemas=True</code> are provided.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If any path does not end with <code>.csv</code>.</p> </li> <li> <code>PlanError</code>             \u2013            <p>If schemas cannot be merged or if there's a schema mismatch when merge_schemas=False.</p> </li> </ul> Read a single CSV file <pre><code>df = session.read.csv(\"file.csv\")\n</code></pre> Read multiple CSV files with schema merging <pre><code>df = session.read.csv(\"data/*.csv\", merge_schemas=True)\n</code></pre> Read CSV files with explicit schema <p><code>python df = session.read.csv(     [\"a.csv\", \"b.csv\"],     schema=Schema([         ColumnField(name=\"id\", data_type=IntegerType),         ColumnField(name=\"value\", data_type=FloatType)     ]) )</code></p> Source code in <code>src/fenic/api/io/reader.py</code> <pre><code>def csv(\n    self,\n    paths: Union[str, Path, list[Union[str, Path]]],\n    schema: Optional[Schema] = None,\n    merge_schemas: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Load a DataFrame from one or more CSV files.\n\n    Args:\n        paths: A single file path, a glob pattern (e.g., \"data/*.csv\"), or a list of paths.\n        schema: (optional) A complete schema definition of column names and their types. Only primitive types are supported.\n            - For e.g.:\n                - Schema([ColumnField(name=\"id\", data_type=IntegerType), ColumnField(name=\"name\", data_type=StringType)])\n            - If provided, all files must match this schema exactly\u2014all column names must be present, and values must be\n            convertible to the specified types. Partial schemas are not allowed.\n        merge_schemas: Whether to merge schemas across all files.\n            - If True: Column names are unified across files. Missing columns are filled with nulls. Column types are\n            inferred and widened as needed.\n            - If False (default): Only accepts columns from the first file. Column types from the first file are\n            inferred and applied across all files. If subsequent files do not have the same column name and order as the first file, an error is raised.\n            - The \"first file\" is defined as:\n                - The first file in lexicographic order (for glob patterns), or\n                - The first file in the provided list (for lists of paths).\n\n    Notes:\n        - The first row in each file is assumed to be a header row.\n        - Delimiters (e.g., comma, tab) are automatically inferred.\n        - You may specify either `schema` or `merge_schemas=True`, but not both.\n        - Any date/datetime columns are cast to strings during ingestion.\n\n    Raises:\n        ValidationError: If both `schema` and `merge_schemas=True` are provided.\n        ValidationError: If any path does not end with `.csv`.\n        PlanError: If schemas cannot be merged or if there's a schema mismatch when merge_schemas=False.\n\n    Example: Read a single CSV file\n        ```python\n        df = session.read.csv(\"file.csv\")\n        ```\n\n    Example: Read multiple CSV files with schema merging\n        ```python\n        df = session.read.csv(\"data/*.csv\", merge_schemas=True)\n        ```\n\n    Example: Read CSV files with explicit schema\n        ```python\n        df = session.read.csv(\n            [\"a.csv\", \"b.csv\"],\n            schema=Schema([\n                ColumnField(name=\"id\", data_type=IntegerType),\n                ColumnField(name=\"value\", data_type=FloatType)\n            ])\n        )            ```\n    \"\"\"\n    if schema is not None and merge_schemas:\n        raise ValidationError(\n            \"Cannot specify both 'schema' and 'merge_schemas=True' - these options conflict. \"\n            \"Choose one approach: \"\n            \"1) Use 'schema' to enforce a specific schema: csv(paths, schema=your_schema), \"\n            \"2) Use 'merge_schemas=True' to automatically merge schemas: csv(paths, merge_schemas=True), \"\n            \"3) Use neither to inherit schema from the first file: csv(paths)\"\n        )\n    if schema is not None:\n        for col_field in schema.column_fields:\n            if not isinstance(\n                col_field.data_type,\n                _PrimitiveType,\n            ):\n                raise ValidationError(\n                    f\"CSV files only support primitive data types in schema definitions. \"\n                    f\"Column '{col_field.name}' has type {type(col_field.data_type).__name__}, but CSV schemas must use: \"\n                    f\"IntegerType, FloatType, DoubleType, BooleanType, or StringType. \"\n                    f\"Example: Schema([ColumnField(name='id', data_type=IntegerType), ColumnField(name='name', data_type=StringType)])\"\n                )\n    options = {\n        \"schema\": schema,\n        \"merge_schemas\": merge_schemas,\n    }\n    return self._read_file(\n        paths, file_format=\"csv\", file_extension=\".csv\", **options\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrameReader.parquet","title":"parquet","text":"<pre><code>parquet(paths: Union[str, Path, list[Union[str, Path]]], merge_schemas: bool = False) -&gt; DataFrame\n</code></pre> <p>Load a DataFrame from one or more Parquet files.</p> <p>Parameters:</p> <ul> <li> <code>paths</code>               (<code>Union[str, Path, list[Union[str, Path]]]</code>)           \u2013            <p>A single file path, a glob pattern (e.g., \"data/*.parquet\"), or a list of paths.</p> </li> <li> <code>merge_schemas</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, infers and merges schemas across all files. Missing columns are filled with nulls, and differing types are widened to a common supertype.</p> </li> </ul> Behavior <ul> <li>If <code>merge_schemas=False</code> (default), all files must match the schema of the first file exactly. Subsequent files must contain all columns from the first file with compatible data types. If any column is missing or has incompatible types, an error is raised.</li> <li>If <code>merge_schemas=True</code>, column names are unified across all files, and data types are automatically widened to accommodate all values.</li> <li>The \"first file\" is defined as:<ul> <li>The first file in lexicographic order (for glob patterns), or</li> <li>The first file in the provided list (for lists of paths).</li> </ul> </li> </ul> Notes <ul> <li>Date and datetime columns are cast to strings during ingestion.</li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If any file does not have a <code>.parquet</code> extension.</p> </li> <li> <code>PlanError</code>             \u2013            <p>If schemas cannot be merged or if there's a schema mismatch when merge_schemas=False.</p> </li> </ul> Read a single Parquet file <pre><code>df = session.read.parquet(\"file.parquet\")\n</code></pre> Read multiple Parquet files <pre><code>df = session.read.parquet(\"data/*.parquet\")\n</code></pre> Read Parquet files with schema merging <pre><code>df = session.read.parquet([\"a.parquet\", \"b.parquet\"], merge_schemas=True)\n</code></pre> Source code in <code>src/fenic/api/io/reader.py</code> <pre><code>def parquet(\n    self,\n    paths: Union[str, Path, list[Union[str, Path]]],\n    merge_schemas: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Load a DataFrame from one or more Parquet files.\n\n    Args:\n        paths: A single file path, a glob pattern (e.g., \"data/*.parquet\"), or a list of paths.\n        merge_schemas: If True, infers and merges schemas across all files.\n            Missing columns are filled with nulls, and differing types are widened to a common supertype.\n\n    Behavior:\n        - If `merge_schemas=False` (default), all files must match the schema of the first file exactly.\n        Subsequent files must contain all columns from the first file with compatible data types.\n        If any column is missing or has incompatible types, an error is raised.\n        - If `merge_schemas=True`, column names are unified across all files, and data types are automatically\n        widened to accommodate all values.\n        - The \"first file\" is defined as:\n            - The first file in lexicographic order (for glob patterns), or\n            - The first file in the provided list (for lists of paths).\n\n    Notes:\n        - Date and datetime columns are cast to strings during ingestion.\n\n    Raises:\n        ValidationError: If any file does not have a `.parquet` extension.\n        PlanError: If schemas cannot be merged or if there's a schema mismatch when merge_schemas=False.\n\n    Example: Read a single Parquet file\n        ```python\n        df = session.read.parquet(\"file.parquet\")\n        ```\n\n    Example: Read multiple Parquet files\n        ```python\n        df = session.read.parquet(\"data/*.parquet\")\n        ```\n\n    Example: Read Parquet files with schema merging\n        ```python\n        df = session.read.parquet([\"a.parquet\", \"b.parquet\"], merge_schemas=True)\n        ```\n    \"\"\"\n    options = {\n        \"merge_schemas\": merge_schemas,\n    }\n    return self._read_file(\n        paths, file_format=\"parquet\", file_extension=\".parquet\", **options\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrameWriter","title":"DataFrameWriter","text":"<pre><code>DataFrameWriter(dataframe: DataFrame)\n</code></pre> <p>Interface used to write a DataFrame to external storage systems.</p> <p>Similar to PySpark's DataFrameWriter.</p> <p>Initialize a DataFrameWriter.</p> <p>Parameters:</p> <ul> <li> <code>dataframe</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to write.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>csv</code>             \u2013              <p>Saves the content of the DataFrame as a single CSV file with comma as the delimiter and headers in the first row.</p> </li> <li> <code>parquet</code>             \u2013              <p>Saves the content of the DataFrame as a single Parquet file.</p> </li> <li> <code>save_as_table</code>             \u2013              <p>Saves the content of the DataFrame as the specified table.</p> </li> </ul> Source code in <code>src/fenic/api/io/writer.py</code> <pre><code>def __init__(self, dataframe: DataFrame):\n    \"\"\"Initialize a DataFrameWriter.\n\n    Args:\n        dataframe: The DataFrame to write.\n    \"\"\"\n    self._dataframe = dataframe\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrameWriter.csv","title":"csv","text":"<pre><code>csv(file_path: Union[str, Path], mode: Literal['error', 'overwrite', 'ignore'] = 'overwrite') -&gt; QueryMetrics\n</code></pre> <p>Saves the content of the DataFrame as a single CSV file with comma as the delimiter and headers in the first row.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path to save the CSV file to</p> </li> <li> <code>mode</code>               (<code>Literal['error', 'overwrite', 'ignore']</code>, default:                   <code>'overwrite'</code> )           \u2013            <p>Write mode. Default is \"overwrite\".  - error: Raises an error if file exists  - overwrite: Overwrites the file if it exists  - ignore: Silently ignores operation if file exists</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>QueryMetrics</code> (              <code>QueryMetrics</code> )          \u2013            <p>The query metrics</p> </li> </ul> Save with overwrite mode (default) <pre><code>df.write.csv(\"output.csv\")  # Overwrites if exists\n</code></pre> Save with error mode <pre><code>df.write.csv(\"output.csv\", mode=\"error\")  # Raises error if exists\n</code></pre> Save with ignore mode <pre><code>df.write.csv(\"output.csv\", mode=\"ignore\")  # Skips if exists\n</code></pre> Source code in <code>src/fenic/api/io/writer.py</code> <pre><code>def csv(\n    self,\n    file_path: Union[str, Path],\n    mode: Literal[\"error\", \"overwrite\", \"ignore\"] = \"overwrite\",\n) -&gt; QueryMetrics:\n    \"\"\"Saves the content of the DataFrame as a single CSV file with comma as the delimiter and headers in the first row.\n\n    Args:\n        file_path: Path to save the CSV file to\n        mode: Write mode. Default is \"overwrite\".\n             - error: Raises an error if file exists\n             - overwrite: Overwrites the file if it exists\n             - ignore: Silently ignores operation if file exists\n\n    Returns:\n        QueryMetrics: The query metrics\n\n    Example: Save with overwrite mode (default)\n        ```python\n        df.write.csv(\"output.csv\")  # Overwrites if exists\n        ```\n\n    Example: Save with error mode\n        ```python\n        df.write.csv(\"output.csv\", mode=\"error\")  # Raises error if exists\n        ```\n\n    Example: Save with ignore mode\n        ```python\n        df.write.csv(\"output.csv\", mode=\"ignore\")  # Skips if exists\n        ```\n    \"\"\"\n    file_path = str(file_path)\n    if not file_path.endswith(\".csv\"):\n        raise ValidationError(\n            f\"CSV writer requires a '.csv' file extension. \"\n            f\"Your path '{file_path}' is missing the extension.\"\n        )\n\n    sink_plan = FileSink(\n        child=self._dataframe._logical_plan,\n        sink_type=\"csv\",\n        path=file_path,\n        mode=mode,\n    )\n\n    metrics = self._dataframe._logical_plan.session_state.execution.save_to_file(\n        sink_plan, file_path=file_path, mode=mode\n    )\n    logger.info(metrics.get_summary())\n    return metrics\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrameWriter.parquet","title":"parquet","text":"<pre><code>parquet(file_path: Union[str, Path], mode: Literal['error', 'overwrite', 'ignore'] = 'overwrite') -&gt; QueryMetrics\n</code></pre> <p>Saves the content of the DataFrame as a single Parquet file.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path to save the Parquet file to</p> </li> <li> <code>mode</code>               (<code>Literal['error', 'overwrite', 'ignore']</code>, default:                   <code>'overwrite'</code> )           \u2013            <p>Write mode. Default is \"overwrite\".  - error: Raises an error if file exists  - overwrite: Overwrites the file if it exists  - ignore: Silently ignores operation if file exists</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>QueryMetrics</code> (              <code>QueryMetrics</code> )          \u2013            <p>The query metrics</p> </li> </ul> Save with overwrite mode (default) <pre><code>df.write.parquet(\"output.parquet\")  # Overwrites if exists\n</code></pre> Save with error mode <pre><code>df.write.parquet(\"output.parquet\", mode=\"error\")  # Raises error if exists\n</code></pre> Save with ignore mode <pre><code>df.write.parquet(\"output.parquet\", mode=\"ignore\")  # Skips if exists\n</code></pre> Source code in <code>src/fenic/api/io/writer.py</code> <pre><code>def parquet(\n    self,\n    file_path: Union[str, Path],\n    mode: Literal[\"error\", \"overwrite\", \"ignore\"] = \"overwrite\",\n) -&gt; QueryMetrics:\n    \"\"\"Saves the content of the DataFrame as a single Parquet file.\n\n    Args:\n        file_path: Path to save the Parquet file to\n        mode: Write mode. Default is \"overwrite\".\n             - error: Raises an error if file exists\n             - overwrite: Overwrites the file if it exists\n             - ignore: Silently ignores operation if file exists\n\n    Returns:\n        QueryMetrics: The query metrics\n\n    Example: Save with overwrite mode (default)\n        ```python\n        df.write.parquet(\"output.parquet\")  # Overwrites if exists\n        ```\n\n    Example: Save with error mode\n        ```python\n        df.write.parquet(\"output.parquet\", mode=\"error\")  # Raises error if exists\n        ```\n\n    Example: Save with ignore mode\n        ```python\n        df.write.parquet(\"output.parquet\", mode=\"ignore\")  # Skips if exists\n        ```\n    \"\"\"\n    file_path = str(file_path)\n    if not file_path.endswith(\".parquet\"):\n        raise ValidationError(\n            f\"Parquet writer requires a '.parquet' file extension. \"\n            f\"Your path '{file_path}' is missing the extension.\"\n        )\n\n    sink_plan = FileSink(\n        child=self._dataframe._logical_plan,\n        sink_type=\"parquet\",\n        path=file_path,\n        mode=mode,\n    )\n\n    metrics = self._dataframe._logical_plan.session_state.execution.save_to_file(\n        sink_plan, file_path=file_path, mode=mode\n    )\n    logger.info(metrics.get_summary())\n    return metrics\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.DataFrameWriter.save_as_table","title":"save_as_table","text":"<pre><code>save_as_table(table_name: str, mode: Literal['error', 'append', 'overwrite', 'ignore'] = 'error') -&gt; QueryMetrics\n</code></pre> <p>Saves the content of the DataFrame as the specified table.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Name of the table to save to</p> </li> <li> <code>mode</code>               (<code>Literal['error', 'append', 'overwrite', 'ignore']</code>, default:                   <code>'error'</code> )           \u2013            <p>Write mode. Default is \"error\".  - error: Raises an error if table exists  - append: Appends data to table if it exists  - overwrite: Overwrites existing table  - ignore: Silently ignores operation if table exists</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>QueryMetrics</code> (              <code>QueryMetrics</code> )          \u2013            <p>The query metrics</p> </li> </ul> Save with error mode (default) <pre><code>df.write.save_as_table(\"my_table\")  # Raises error if table exists\n</code></pre> Save with append mode <pre><code>df.write.save_as_table(\"my_table\", mode=\"append\")  # Adds to existing table\n</code></pre> Save with overwrite mode <pre><code>df.write.save_as_table(\"my_table\", mode=\"overwrite\")  # Replaces existing table\n</code></pre> Source code in <code>src/fenic/api/io/writer.py</code> <pre><code>def save_as_table(\n    self,\n    table_name: str,\n    mode: Literal[\"error\", \"append\", \"overwrite\", \"ignore\"] = \"error\",\n) -&gt; QueryMetrics:\n    \"\"\"Saves the content of the DataFrame as the specified table.\n\n    Args:\n        table_name: Name of the table to save to\n        mode: Write mode. Default is \"error\".\n             - error: Raises an error if table exists\n             - append: Appends data to table if it exists\n             - overwrite: Overwrites existing table\n             - ignore: Silently ignores operation if table exists\n\n    Returns:\n        QueryMetrics: The query metrics\n\n    Example: Save with error mode (default)\n        ```python\n        df.write.save_as_table(\"my_table\")  # Raises error if table exists\n        ```\n\n    Example: Save with append mode\n        ```python\n        df.write.save_as_table(\"my_table\", mode=\"append\")  # Adds to existing table\n        ```\n\n    Example: Save with overwrite mode\n        ```python\n        df.write.save_as_table(\"my_table\", mode=\"overwrite\")  # Replaces existing table\n        ```\n    \"\"\"\n    sink_plan = TableSink(\n        child=self._dataframe._logical_plan, table_name=table_name, mode=mode\n    )\n\n    metrics = self._dataframe._logical_plan.session_state.execution.save_as_table(\n        sink_plan, table_name=table_name, mode=mode\n    )\n    logger.info(metrics.get_summary())\n    return metrics\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.GoogleGLAModelConfig","title":"GoogleGLAModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Google GenerativeLAnguage (GLA) models.</p> <p>This class defines the configuration settings for models available in Google Developer AI Studio, including model selection and rate limiting parameters. These models are accessible using a GEMINI_API_KEY environment variable.</p>"},{"location":"reference/fenic/api/#fenic.api.GroupedData","title":"GroupedData","text":"<pre><code>GroupedData(df: DataFrame, by: Optional[List[ColumnOrName]] = None)\n</code></pre> <p>               Bases: <code>BaseGroupedData</code></p> <p>Methods for aggregations on a grouped DataFrame.</p> <p>Initialize grouped data.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to group.</p> </li> <li> <code>by</code>               (<code>Optional[List[ColumnOrName]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of columns to group by.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>agg</code>             \u2013              <p>Compute aggregations on grouped data and return the result as a DataFrame.</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/grouped_data.py</code> <pre><code>def __init__(self, df: DataFrame, by: Optional[List[ColumnOrName]] = None):\n    \"\"\"Initialize grouped data.\n\n    Args:\n        df: The DataFrame to group.\n        by: Optional list of columns to group by.\n    \"\"\"\n    super().__init__(df)\n    self._by: List[Column] = []\n    for c in by or []:\n        if isinstance(c, str):\n            self._by.append(col(c))\n        elif isinstance(c, Column):\n            # Allow any expression except literals\n            if isinstance(c._logical_expr, LiteralExpr):\n                raise ValueError(f\"Cannot group by literal value: {c}\")\n            self._by.append(c)\n        else:\n            raise TypeError(\n                f\"Group by expressions must be string or Column, got {type(c)}\"\n            )\n    self._by_exprs = [c._logical_expr for c in self._by]\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.GroupedData.agg","title":"agg","text":"<pre><code>agg(*exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame\n</code></pre> <p>Compute aggregations on grouped data and return the result as a DataFrame.</p> <p>This method applies aggregate functions to the grouped data.</p> <p>Parameters:</p> <ul> <li> <code>*exprs</code>               (<code>Union[Column, Dict[str, str]]</code>, default:                   <code>()</code> )           \u2013            <p>Aggregation expressions. Can be:</p> <ul> <li>Column expressions with aggregate functions (e.g., <code>count(\"*\")</code>, <code>sum(\"amount\")</code>)</li> <li>A dictionary mapping column names to aggregate function names (e.g., <code>{\"amount\": \"sum\", \"age\": \"avg\"}</code>)</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with one row per group and columns for group keys and aggregated values</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If arguments are not Column expressions or a dictionary</p> </li> <li> <code>ValueError</code>             \u2013            <p>If dictionary values are not valid aggregate function names</p> </li> </ul> Count employees by department <pre><code># Group by department and count employees\ndf.group_by(\"department\").agg(count(\"*\").alias(\"employee_count\"))\n</code></pre> Multiple aggregations <pre><code># Multiple aggregations\ndf.group_by(\"department\").agg(\n    count(\"*\").alias(\"employee_count\"),\n    avg(\"salary\").alias(\"avg_salary\"),\n    max(\"age\").alias(\"max_age\")\n)\n</code></pre> Dictionary style aggregations <pre><code># Dictionary style for simple aggregations\ndf.group_by(\"department\", \"location\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n</code></pre> Source code in <code>src/fenic/api/dataframe/grouped_data.py</code> <pre><code>def agg(self, *exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame:\n    \"\"\"Compute aggregations on grouped data and return the result as a DataFrame.\n\n    This method applies aggregate functions to the grouped data.\n\n    Args:\n        *exprs: Aggregation expressions. Can be:\n\n            - Column expressions with aggregate functions (e.g., `count(\"*\")`, `sum(\"amount\")`)\n            - A dictionary mapping column names to aggregate function names (e.g., `{\"amount\": \"sum\", \"age\": \"avg\"}`)\n\n    Returns:\n        DataFrame: A new DataFrame with one row per group and columns for group keys and aggregated values\n\n    Raises:\n        ValueError: If arguments are not Column expressions or a dictionary\n        ValueError: If dictionary values are not valid aggregate function names\n\n    Example: Count employees by department\n        ```python\n        # Group by department and count employees\n        df.group_by(\"department\").agg(count(\"*\").alias(\"employee_count\"))\n        ```\n\n    Example: Multiple aggregations\n        ```python\n        # Multiple aggregations\n        df.group_by(\"department\").agg(\n            count(\"*\").alias(\"employee_count\"),\n            avg(\"salary\").alias(\"avg_salary\"),\n            max(\"age\").alias(\"max_age\")\n        )\n        ```\n\n    Example: Dictionary style aggregations\n        ```python\n        # Dictionary style for simple aggregations\n        df.group_by(\"department\", \"location\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n        ```\n    \"\"\"\n    self._validate_agg_exprs(*exprs)\n    if len(exprs) == 1 and isinstance(exprs[0], dict):\n        agg_dict = exprs[0]\n        return self.agg(*self._process_agg_dict(agg_dict))\n\n    agg_exprs = self._process_agg_exprs(exprs)\n    return self._df._from_logical_plan(\n        Aggregate(self._df._logical_plan, self._by_exprs, agg_exprs),\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Lineage","title":"Lineage","text":"<pre><code>Lineage(lineage: BaseLineage)\n</code></pre> <p>Query interface for tracing data lineage through a query plan.</p> <p>This class allows you to navigate through the query plan both forwards and backwards, tracing how specific rows are transformed through each operation.</p> Example <pre><code># Create a lineage query starting from the root\nquery = LineageQuery(lineage, session.execution)\n\n# Or start from a specific source\nquery.start_from_source(\"my_table\")\n\n# Trace rows backwards through a transformation\nresult = query.backward([\"uuid1\", \"uuid2\"])\n\n# Trace rows forward to see their outputs\nresult = query.forward([\"uuid3\", \"uuid4\"])\n</code></pre> <p>Initialize a Lineage instance.</p> <p>Parameters:</p> <ul> <li> <code>lineage</code>               (<code>BaseLineage</code>)           \u2013            <p>The underlying lineage implementation.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>backwards</code>             \u2013              <p>Trace rows backwards to see which input rows produced them.</p> </li> <li> <code>forwards</code>             \u2013              <p>Trace rows forward to see how they are transformed by the next operation.</p> </li> <li> <code>get_result_df</code>             \u2013              <p>Get the result of the query as a Polars DataFrame.</p> </li> <li> <code>get_source_df</code>             \u2013              <p>Get a query source by name as a Polars DataFrame.</p> </li> <li> <code>get_source_names</code>             \u2013              <p>Get the names of all sources in the query plan. Used to determine where to start the lineage traversal.</p> </li> <li> <code>show</code>             \u2013              <p>Print the operator tree of the query.</p> </li> <li> <code>skip_backwards</code>             \u2013              <p>[Not Implemented] Trace rows backwards through multiple operations at once.</p> </li> <li> <code>skip_forwards</code>             \u2013              <p>[Not Implemented] Trace rows forward through multiple operations at once.</p> </li> <li> <code>start_from_source</code>             \u2013              <p>Set the current position to a specific source in the query plan.</p> </li> </ul> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>def __init__(self, lineage: BaseLineage):\n    \"\"\"Initialize a Lineage instance.\n\n    Args:\n        lineage: The underlying lineage implementation.\n    \"\"\"\n    self.lineage = lineage\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Lineage.backwards","title":"backwards","text":"<pre><code>backwards(ids: List[str], branch_side: Optional[BranchSide] = None) -&gt; pl.DataFrame\n</code></pre> <p>Trace rows backwards to see which input rows produced them.</p> <p>Parameters:</p> <ul> <li> <code>ids</code>               (<code>List[str]</code>)           \u2013            <p>List of UUIDs identifying the rows to trace back</p> </li> <li> <code>branch_side</code>               (<code>Optional[BranchSide]</code>, default:                   <code>None</code> )           \u2013            <p>For operators with multiple inputs (like joins), specify which input to trace (\"left\" or \"right\"). Not needed for single-input operations.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame containing the source rows that produced the specified outputs</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If invalid ids format or incorrect branch_side specification</p> </li> </ul> Example <pre><code># Simple backward trace\nsource_rows = query.backward([\"result_uuid1\"])\n\n# Trace back through a join\nleft_rows = query.backward([\"join_uuid1\"], branch_side=\"left\")\nright_rows = query.backward([\"join_uuid1\"], branch_side=\"right\")\n</code></pre> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef backwards(\n    self, ids: List[str], branch_side: Optional[BranchSide] = None\n) -&gt; pl.DataFrame:\n    \"\"\"Trace rows backwards to see which input rows produced them.\n\n    Args:\n        ids: List of UUIDs identifying the rows to trace back\n        branch_side: For operators with multiple inputs (like joins), specify which\n            input to trace (\"left\" or \"right\"). Not needed for single-input operations.\n\n    Returns:\n        DataFrame containing the source rows that produced the specified outputs\n\n    Raises:\n        ValueError: If invalid ids format or incorrect branch_side specification\n\n    Example:\n        ```python\n        # Simple backward trace\n        source_rows = query.backward([\"result_uuid1\"])\n\n        # Trace back through a join\n        left_rows = query.backward([\"join_uuid1\"], branch_side=\"left\")\n        right_rows = query.backward([\"join_uuid1\"], branch_side=\"right\")\n        ```\n    \"\"\"\n    return self.lineage.backwards(ids, branch_side)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Lineage.forwards","title":"forwards","text":"<pre><code>forwards(row_ids: List[str]) -&gt; pl.DataFrame\n</code></pre> <p>Trace rows forward to see how they are transformed by the next operation.</p> <p>Parameters:</p> <ul> <li> <code>row_ids</code>               (<code>List[str]</code>)           \u2013            <p>List of UUIDs identifying the rows to trace</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame containing the transformed rows in the next operation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If at root node or if row_ids format is invalid</p> </li> </ul> Example <pre><code># Trace how specific customer rows are transformed\ntransformed = query.forward([\"customer_uuid1\", \"customer_uuid2\"])\n</code></pre> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef forwards(self, row_ids: List[str]) -&gt; pl.DataFrame:\n    \"\"\"Trace rows forward to see how they are transformed by the next operation.\n\n    Args:\n        row_ids: List of UUIDs identifying the rows to trace\n\n    Returns:\n        DataFrame containing the transformed rows in the next operation\n\n    Raises:\n        ValueError: If at root node or if row_ids format is invalid\n\n    Example:\n        ```python\n        # Trace how specific customer rows are transformed\n        transformed = query.forward([\"customer_uuid1\", \"customer_uuid2\"])\n        ```\n    \"\"\"\n    return self.lineage.forwards(row_ids)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Lineage.get_result_df","title":"get_result_df","text":"<pre><code>get_result_df() -&gt; pl.DataFrame\n</code></pre> <p>Get the result of the query as a Polars DataFrame.</p> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>def get_result_df(self) -&gt; pl.DataFrame:\n    \"\"\"Get the result of the query as a Polars DataFrame.\"\"\"\n    return self.lineage.get_result_df()\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Lineage.get_source_df","title":"get_source_df","text":"<pre><code>get_source_df(source_name: str) -&gt; pl.DataFrame\n</code></pre> <p>Get a query source by name as a Polars DataFrame.</p> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef get_source_df(self, source_name: str) -&gt; pl.DataFrame:\n    \"\"\"Get a query source by name as a Polars DataFrame.\"\"\"\n    return self.lineage.get_source_df(source_name)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Lineage.get_source_names","title":"get_source_names","text":"<pre><code>get_source_names() -&gt; List[str]\n</code></pre> <p>Get the names of all sources in the query plan. Used to determine where to start the lineage traversal.</p> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef get_source_names(self) -&gt; List[str]:\n    \"\"\"Get the names of all sources in the query plan. Used to determine where to start the lineage traversal.\"\"\"\n    return self.lineage.get_source_names()\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Lineage.show","title":"show","text":"<pre><code>show() -&gt; None\n</code></pre> <p>Print the operator tree of the query.</p> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>def show(self) -&gt; None:\n    \"\"\"Print the operator tree of the query.\"\"\"\n    print(self.lineage.stringify_graph())\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Lineage.skip_backwards","title":"skip_backwards","text":"<pre><code>skip_backwards(ids: List[str]) -&gt; Dict[str, pl.DataFrame]\n</code></pre> <p>[Not Implemented] Trace rows backwards through multiple operations at once.</p> <p>This method will allow efficient tracing through multiple operations without intermediate results.</p> <p>Parameters:</p> <ul> <li> <code>ids</code>               (<code>List[str]</code>)           \u2013            <p>List of UUIDs identifying the rows to trace back</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, DataFrame]</code>           \u2013            <p>Dictionary mapping operation names to their source DataFrames</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>This method is not yet implemented</p> </li> </ul> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>def skip_backwards(self, ids: List[str]) -&gt; Dict[str, pl.DataFrame]:\n    \"\"\"[Not Implemented] Trace rows backwards through multiple operations at once.\n\n    This method will allow efficient tracing through multiple operations without\n    intermediate results.\n\n    Args:\n        ids: List of UUIDs identifying the rows to trace back\n\n    Returns:\n        Dictionary mapping operation names to their source DataFrames\n\n    Raises:\n        NotImplementedError: This method is not yet implemented\n    \"\"\"\n    raise NotImplementedError(\"Skip backwards not yet implemented\")\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Lineage.skip_forwards","title":"skip_forwards","text":"<pre><code>skip_forwards(row_ids: List[str]) -&gt; pl.DataFrame\n</code></pre> <p>[Not Implemented] Trace rows forward through multiple operations at once.</p> <p>This method will allow efficient tracing through multiple operations without intermediate results.</p> <p>Parameters:</p> <ul> <li> <code>row_ids</code>               (<code>List[str]</code>)           \u2013            <p>List of UUIDs identifying the rows to trace</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame containing the final transformed rows</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>This method is not yet implemented</p> </li> </ul> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>def skip_forwards(self, row_ids: List[str]) -&gt; pl.DataFrame:\n    \"\"\"[Not Implemented] Trace rows forward through multiple operations at once.\n\n    This method will allow efficient tracing through multiple operations without\n    intermediate results.\n\n    Args:\n        row_ids: List of UUIDs identifying the rows to trace\n\n    Returns:\n        DataFrame containing the final transformed rows\n\n    Raises:\n        NotImplementedError: This method is not yet implemented\n    \"\"\"\n    raise NotImplementedError(\"Skip forwards not yet implemented\")\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Lineage.start_from_source","title":"start_from_source","text":"<pre><code>start_from_source(source_name: str) -&gt; None\n</code></pre> <p>Set the current position to a specific source in the query plan.</p> <p>Parameters:</p> <ul> <li> <code>source_name</code>               (<code>str</code>)           \u2013            <p>Name of the source table to start from</p> </li> </ul> Example <pre><code>query.start_from_source(\"customers\")\n# Now you can trace forward from the customers table\n</code></pre> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef start_from_source(self, source_name: str) -&gt; None:\n    \"\"\"Set the current position to a specific source in the query plan.\n\n    Args:\n        source_name: Name of the source table to start from\n\n    Example:\n        ```python\n        query.start_from_source(\"customers\")\n        # Now you can trace forward from the customers table\n        ```\n    \"\"\"\n    self.lineage.start_from_source(source_name)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.OpenAIModelConfig","title":"OpenAIModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for OpenAI models.</p> <p>This class defines the configuration settings for OpenAI language and embedding models, including model selection and rate limiting parameters.</p> <p>Attributes:</p> <ul> <li> <code>model_name</code>               (<code>Union[OPENAI_AVAILABLE_LANGUAGE_MODELS, OPENAI_AVAILABLE_EMBEDDING_MODELS]</code>)           \u2013            <p>The name of the OpenAI model to use.</p> </li> <li> <code>rpm</code>               (<code>int</code>)           \u2013            <p>Requests per minute limit; must be greater than 0.</p> </li> <li> <code>tpm</code>               (<code>int</code>)           \u2013            <p>Tokens per minute limit; must be greater than 0.</p> </li> </ul> <p>Examples:</p> <p>Configuring an OpenAI Language model with rate limits:</p> <pre><code>config = OpenAIModelConfig(model_name=\"gpt-4.1-nano\", rpm=100, tpm=100)\n</code></pre> <p>Configuring an OpenAI Embedding model with rate limits:</p> <pre><code>config = OpenAIModelConfig(model_name=\"text-embedding-3-small\", rpm=100, tpm=100)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.SemanticConfig","title":"SemanticConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for semantic language and embedding models.</p> <p>This class defines the configuration for both language models and optional embedding models used in semantic operations. It ensures that all configured models are valid and supported by their respective providers.</p> <p>Attributes:</p> <ul> <li> <code>language_models</code>               (<code>dict[str, ModelConfig]</code>)           \u2013            <p>Mapping of model aliases to language model configurations.</p> </li> <li> <code>default_language_model</code>               (<code>Optional[str]</code>)           \u2013            <p>The alias of the default language model to use for semantic operations. Not required if only one language model is configured.</p> </li> <li> <code>embedding_models</code>               (<code>Optional[dict[str, ModelConfig]]</code>)           \u2013            <p>Optional mapping of model aliases to embedding model configurations.</p> </li> <li> <code>default_embedding_model</code>               (<code>Optional[str]</code>)           \u2013            <p>The alias of the default embedding model to use for semantic operations.</p> </li> </ul> Note <p>The embedding model is optional and only required for operations that need semantic search or embedding capabilities.</p> <p>Methods:</p> <ul> <li> <code>model_post_init</code>             \u2013              <p>Post initialization hook to set defaults.</p> </li> <li> <code>validate_models</code>             \u2013              <p>Validates that the selected models are supported by the system.</p> </li> </ul>"},{"location":"reference/fenic/api/#fenic.api.SemanticConfig.model_post_init","title":"model_post_init","text":"<pre><code>model_post_init(__context) -&gt; None\n</code></pre> <p>Post initialization hook to set defaults.</p> <p>This hook runs after the model is initialized and validated. It sets the default language and embedding models if they are not set and there is only one model available.</p> Source code in <code>src/fenic/api/session/config.py</code> <pre><code>def model_post_init(self, __context) -&gt; None:\n    \"\"\"Post initialization hook to set defaults.\n\n    This hook runs after the model is initialized and validated.\n    It sets the default language and embedding models if they are not set\n    and there is only one model available.\n    \"\"\"\n    # Set default language model if not set and only one model exists\n    if self.default_language_model is None and len(self.language_models) == 1:\n        self.default_language_model = list(self.language_models.keys())[0]\n    # Set default embedding model if not set and only one model exists\n    if self.embedding_models is not None and self.default_embedding_model is None and len(self.embedding_models) == 1:\n        self.default_embedding_model = list(self.embedding_models.keys())[0]\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.SemanticConfig.validate_models","title":"validate_models","text":"<pre><code>validate_models() -&gt; SemanticConfig\n</code></pre> <p>Validates that the selected models are supported by the system.</p> <p>This validator checks that both the language model and embedding model (if provided) are valid and supported by their respective providers.</p> <p>Returns:</p> <ul> <li> <code>SemanticConfig</code>           \u2013            <p>The validated SemanticConfig instance.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ConfigurationError</code>             \u2013            <p>If any of the models are not supported.</p> </li> </ul> Source code in <code>src/fenic/api/session/config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_models(self) -&gt; SemanticConfig:\n    \"\"\"Validates that the selected models are supported by the system.\n\n    This validator checks that both the language model and embedding model (if provided)\n    are valid and supported by their respective providers.\n\n    Returns:\n        The validated SemanticConfig instance.\n\n    Raises:\n        ConfigurationError: If any of the models are not supported.\n    \"\"\"\n    if len(self.language_models) == 0:\n        raise ConfigurationError(\"You must specify at least one language model configuration.\")\n    available_language_model_aliases = list(self.language_models.keys())\n    if self.default_language_model is None and len(self.language_models) &gt; 1:\n        raise ConfigurationError(f\"default_language_model is not set, and multiple language models are configured. Please specify one of: {available_language_model_aliases} as a default_language_model.\")\n\n    if self.default_language_model is not None and self.default_language_model not in self.language_models:\n        raise ConfigurationError(f\"default_language_model {self.default_language_model} is not in configured map of language models. Available models: {available_language_model_aliases} .\")\n\n    for model_alias, language_model in self.language_models.items():\n        if isinstance(language_model, OpenAIModelConfig):\n            language_model_provider = ModelProvider.OPENAI\n            language_model_name = language_model.model_name\n        elif isinstance(language_model, AnthropicModelConfig):\n            language_model_provider = ModelProvider.ANTHROPIC\n            language_model_name = language_model.model_name\n        elif isinstance(language_model, GoogleGLAModelConfig):\n            language_model_provider = ModelProvider.GOOGLE_GLA\n            language_model_name = language_model.model_name\n        else:\n            raise ConfigurationError(\n                f\"Invalid language model: {model_alias}: {language_model} unsupported model type.\")\n\n        completion_model = model_catalog.get_completion_model_parameters(language_model_provider,\n                                                                         language_model_name)\n        if completion_model is None:\n            raise ConfigurationError(\n                model_catalog.generate_unsupported_completion_model_error_message(\n                    language_model_provider,\n                    language_model_name\n                )\n            )\n        if isinstance(language_model, GoogleGLAModelConfig) and completion_model.requires_reasoning_effort:\n            if language_model.reasoning_effort is None:\n                raise ConfigurationError(f\"Reasoning effort level is required for {language_model_provider.value}:{language_model_name} Please specify reasoning_effort for model {model_alias}.\")\n\n    if self.embedding_models is not None:\n        if self.default_embedding_model is None and len(self.embedding_models) &gt; 1:\n            raise ConfigurationError(\"embedding_models is set but default_embedding_model is missing (ambiguous).\")\n\n        if self.default_embedding_model is not None and self.default_embedding_model not in self.embedding_models:\n            raise ConfigurationError(\n                f\"default_embedding_model {self.default_embedding_model} is not in embedding_models\")\n        for model_alias, embedding_model in self.embedding_models.items():\n            if isinstance(embedding_model, OpenAIModelConfig):\n                embedding_model_provider = ModelProvider.OPENAI\n                embedding_model_name = embedding_model.model_name\n            else:\n                raise ConfigurationError(\n                    f\"Invalid embedding model: {model_alias}: {embedding_model} unsupported model type\")\n            embedding_model_parameters = model_catalog.get_embedding_model_parameters(embedding_model_provider,\n                                                                                 embedding_model_name)\n            if embedding_model_parameters is None:\n                raise ConfigurationError(model_catalog.generate_unsupported_embedding_model_error_message(\n                    embedding_model_provider,\n                    embedding_model_name\n                ))\n\n    return self\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.SemanticExtensions","title":"SemanticExtensions","text":"<pre><code>SemanticExtensions(df: DataFrame)\n</code></pre> <p>A namespace for semantic dataframe operators.</p> <p>Initialize semantic extensions.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to extend with semantic operations.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>group_by</code>             \u2013              <p>Semantically group rows by clustering an embedding column into the specified number of centroids.</p> </li> <li> <code>join</code>             \u2013              <p>Performs a semantic join between two DataFrames using a natural language predicate.</p> </li> <li> <code>sim_join</code>             \u2013              <p>Performs a semantic similarity join between two DataFrames using precomputed text embeddings.</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def __init__(self, df: DataFrame):\n    \"\"\"Initialize semantic extensions.\n\n    Args:\n        df: The DataFrame to extend with semantic operations.\n    \"\"\"\n    self._df = df\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.SemanticExtensions.group_by","title":"group_by","text":"<pre><code>group_by(by: ColumnOrName, num_clusters: int) -&gt; SemGroupedData\n</code></pre> <p>Semantically group rows by clustering an embedding column into the specified number of centroids.</p> <p>This method is useful when you want to uncover natural themes, topics, or intent in embedded free-form text, without needing predefined categories.</p> <p>Parameters:</p> <ul> <li> <code>by</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column containing embeddings to cluster</p> </li> <li> <code>num_clusters</code>               (<code>int</code>)           \u2013            <p>Number of semantic clusters to create</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SemGroupedData</code> (              <code>SemGroupedData</code> )          \u2013            <p>Object for performing aggregations on the clustered data.</p> </li> </ul> Basic semantic grouping <pre><code># Group customer feedback into 5 clusters\ndf.semantic.group_by(\"feedback_embeddings\", 5).agg(count(\"*\"))\n</code></pre> Analyze sentiment by semantic group <pre><code># Analyze sentiment by semantic group\ndf.semantic.group_by(\"feedback_embeddings\", 5).agg(\n    count(\"*\").alias(\"count\"),\n    avg(\"sentiment_score\").alias(\"avg_sentiment\")\n)\n</code></pre> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def group_by(self, by: ColumnOrName, num_clusters: int) -&gt; SemGroupedData:\n    \"\"\"Semantically group rows by clustering an embedding column into the specified number of centroids.\n\n    This method is useful when you want to uncover natural themes, topics, or intent in embedded free-form text,\n    without needing predefined categories.\n\n    Args:\n        by: Column containing embeddings to cluster\n        num_clusters: Number of semantic clusters to create\n\n    Returns:\n        SemGroupedData: Object for performing aggregations on the clustered data.\n\n    Example: Basic semantic grouping\n        ```python\n        # Group customer feedback into 5 clusters\n        df.semantic.group_by(\"feedback_embeddings\", 5).agg(count(\"*\"))\n        ```\n\n    Example: Analyze sentiment by semantic group\n        ```python\n        # Analyze sentiment by semantic group\n        df.semantic.group_by(\"feedback_embeddings\", 5).agg(\n            count(\"*\").alias(\"count\"),\n            avg(\"sentiment_score\").alias(\"avg_sentiment\")\n        )\n        ```\n    \"\"\"\n    return SemGroupedData(self._df, by, num_clusters)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.SemanticExtensions.join","title":"join","text":"<pre><code>join(other: DataFrame, join_instruction: str, examples: Optional[JoinExampleCollection] = None, model_alias: Optional[str] = None) -&gt; DataFrame\n</code></pre> <p>Performs a semantic join between two DataFrames using a natural language predicate.</p> <p>That evaluates to either true or false for each potential row pair.</p> <p>The join works by: 1. Evaluating the provided join_instruction as a boolean predicate for each possible pair of rows 2. Including ONLY the row pairs where the predicate evaluates to True in the result set 3. Excluding all row pairs where the predicate evaluates to False</p> <p>The instruction must reference exactly two columns, one from each DataFrame, using the <code>:left</code> and <code>:right</code> suffixes to indicate column origin.</p> <p>This is useful when row pairing decisions require complex reasoning based on a custom predicate rather than simple equality or similarity matching.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to join with.</p> </li> <li> <code>join_instruction</code>               (<code>str</code>)           \u2013            <p>A natural language description of how to match values.</p> <ul> <li>Must include one placeholder from the left DataFrame (e.g. <code>{resume_summary:left}</code>) and one from the right (e.g. <code>{job_description:right}</code>).</li> <li>This instruction is evaluated as a boolean predicate - pairs where it's <code>True</code> are included, pairs where it's <code>False</code> are excluded.</li> </ul> </li> <li> <code>examples</code>               (<code>Optional[JoinExampleCollection]</code>, default:                   <code>None</code> )           \u2013            <p>Optional JoinExampleCollection containing labeled pairs (<code>left</code>, <code>right</code>, <code>output</code>) to guide the semantic join behavior.</p> </li> <li> <code>model_alias</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame containing only the row pairs where the join_instruction       predicate evaluates to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If <code>other</code> is not a DataFrame or <code>join_instruction</code> is not a string.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the instruction format is invalid or references invalid columns.</p> </li> </ul> Basic semantic join <pre><code># Match job listings with candidate resumes based on title/skills\n# Only includes pairs where the predicate evaluates to True\ndf_jobs.semantic.join(df_resumes,\n    join_instruction=\"Given a candidate's resume_summary: {resume_summary:left} and a job description: {job_description:right}, does the candidate have the appropriate skills for the job?\"\n)\n</code></pre> Semantic join with examples <pre><code># Improve join quality with examples\nexamples = JoinExampleCollection()\nexamples.create_example(JoinExample(\n    left=\"5 years experience building backend services in Python using asyncio, FastAPI, and PostgreSQL\",\n    right=\"Senior Software Engineer - Backend\",\n    output=True))  # This pair WILL be included in similar cases\nexamples.create_example(JoinExample(\n    left=\"5 years experience with growth strategy, private equity due diligence, and M&amp;A\",\n    right=\"Product Manager - Hardware\",\n    output=False))  # This pair will NOT be included in similar cases\ndf_jobs.semantic.join(df_resumes,\n    join_instruction=\"Given a candidate's resume_summary: {resume_summary:left} and a job description: {job_description:right}, does the candidate have the appropriate skills for the job?\",\n    examples=examples)\n</code></pre> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def join(\n    self,\n    other: DataFrame,\n    join_instruction: str,\n    examples: Optional[JoinExampleCollection] = None,\n    model_alias: Optional[str] = None,\n) -&gt; DataFrame:\n    \"\"\"Performs a semantic join between two DataFrames using a natural language predicate.\n\n    That evaluates to either true or false for each potential row pair.\n\n    The join works by:\n    1. Evaluating the provided join_instruction as a boolean predicate for each possible pair of rows\n    2. Including ONLY the row pairs where the predicate evaluates to True in the result set\n    3. Excluding all row pairs where the predicate evaluates to False\n\n    The instruction must reference **exactly two columns**, one from each DataFrame,\n    using the `:left` and `:right` suffixes to indicate column origin.\n\n    This is useful when row pairing decisions require complex reasoning based on a custom predicate rather than simple equality or similarity matching.\n\n    Args:\n        other: The DataFrame to join with.\n        join_instruction: A natural language description of how to match values.\n\n            - Must include one placeholder from the left DataFrame (e.g. `{resume_summary:left}`)\n            and one from the right (e.g. `{job_description:right}`).\n            - This instruction is evaluated as a boolean predicate - pairs where it's `True` are included,\n            pairs where it's `False` are excluded.\n        examples: Optional JoinExampleCollection containing labeled pairs (`left`, `right`, `output`)\n            to guide the semantic join behavior.\n        model_alias: Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.\n\n    Returns:\n        DataFrame: A new DataFrame containing only the row pairs where the join_instruction\n                  predicate evaluates to True.\n\n    Raises:\n        TypeError: If `other` is not a DataFrame or `join_instruction` is not a string.\n        ValueError: If the instruction format is invalid or references invalid columns.\n\n    Example: Basic semantic join\n        ```python\n        # Match job listings with candidate resumes based on title/skills\n        # Only includes pairs where the predicate evaluates to True\n        df_jobs.semantic.join(df_resumes,\n            join_instruction=\"Given a candidate's resume_summary: {resume_summary:left} and a job description: {job_description:right}, does the candidate have the appropriate skills for the job?\"\n        )\n        ```\n\n    Example: Semantic join with examples\n        ```python\n        # Improve join quality with examples\n        examples = JoinExampleCollection()\n        examples.create_example(JoinExample(\n            left=\"5 years experience building backend services in Python using asyncio, FastAPI, and PostgreSQL\",\n            right=\"Senior Software Engineer - Backend\",\n            output=True))  # This pair WILL be included in similar cases\n        examples.create_example(JoinExample(\n            left=\"5 years experience with growth strategy, private equity due diligence, and M&amp;A\",\n            right=\"Product Manager - Hardware\",\n            output=False))  # This pair will NOT be included in similar cases\n        df_jobs.semantic.join(df_resumes,\n            join_instruction=\"Given a candidate's resume_summary: {resume_summary:left} and a job description: {job_description:right}, does the candidate have the appropriate skills for the job?\",\n            examples=examples)\n        ```\n    \"\"\"\n    from fenic.api.dataframe.dataframe import DataFrame\n\n    if not isinstance(other, DataFrame):\n        raise TypeError(f\"other argument must be a DataFrame, got {type(other)}\")\n\n    if not isinstance(join_instruction, str):\n        raise TypeError(\n            f\"join_instruction argument must be a string, got {type(join_instruction)}\"\n        )\n    join_columns = utils.parse_instruction(join_instruction)\n    if len(join_columns) != 2:\n        raise ValueError(\n            f\"join_instruction must contain exactly two columns, got {len(join_columns)}\"\n        )\n    left_on = None\n    right_on = None\n    for join_col in join_columns:\n        if join_col.endswith(\":left\"):\n            if left_on is not None:\n                raise ValueError(\n                    \"join_instruction cannot contain multiple :left columns\"\n                )\n            left_on = col(join_col.split(\":\")[0])\n        elif join_col.endswith(\":right\"):\n            if right_on is not None:\n                raise ValueError(\n                    \"join_instruction cannot contain multiple :right columns\"\n                )\n            right_on = col(join_col.split(\":\")[0])\n        else:\n            raise ValueError(\n                f\"Column '{join_col}' must end with either :left or :right\"\n            )\n\n    if left_on is None or right_on is None:\n        raise ValueError(\n            \"join_instruction must contain exactly one :left and one :right column\"\n        )\n\n    return self._df._from_logical_plan(\n        SemanticJoin(\n            left=self._df._logical_plan,\n            right=other._logical_plan,\n            left_on=left_on._logical_expr,\n            right_on=right_on._logical_expr,\n            join_instruction=join_instruction,\n            examples=examples,\n            model_alias=model_alias,\n        ),\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.SemanticExtensions.sim_join","title":"sim_join","text":"<pre><code>sim_join(other: DataFrame, left_on: ColumnOrName, right_on: ColumnOrName, k: int = 1, similarity_metric: SemanticSimilarityMetric = 'cosine', return_similarity_scores: bool = False) -&gt; DataFrame\n</code></pre> <p>Performs a semantic similarity join between two DataFrames using precomputed text embeddings.</p> <p>For each row in the left DataFrame, finds the top <code>k</code> most semantically similar rows in the right DataFrame based on the cosine similarity between their text embeddings. This is useful for fuzzy matching tasks when exact matches aren't possible.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>DataFrame</code>)           \u2013            <p>The right-hand DataFrame to join with.</p> </li> <li> <code>left_on</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column in this DataFrame containing text embeddings to compare.</p> </li> <li> <code>right_on</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column in the other DataFrame containing text embeddings to compare.</p> </li> <li> <code>k</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of most similar matches to return per row from the left DataFrame.</p> </li> <li> <code>similarity_metric</code>               (<code>SemanticSimilarityMetric</code>, default:                   <code>'cosine'</code> )           \u2013            <p>The metric to use for calculating distances between vectors. Supported distance metrics: \"l2\", \"cosine\", \"dot\"</p> </li> <li> <code>return_similarity_scores</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, include a <code>_similarity_score</code> column in the output DataFrame                     representing the match confidence (cosine similarity).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame containing matched rows from both sides and optionally similarity scores.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If argument types are incorrect.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If <code>k</code> is not positive or if the columns are invalid.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If <code>similarity_metric</code> is not one of \"l2\", \"cosine\", \"dot\"</p> </li> </ul> Match queries to FAQ entries <pre><code># Match customer queries to FAQ entries\ndf_queries.semantic.sim_join(\n    df_faqs,\n    left_on=embeddings(col(\"query_text\")),\n    right_on=embeddings(col(\"faq_question\")),\n    k=1\n)\n</code></pre> Link headlines to articles <pre><code># Link news headlines to full articles\ndf_headlines.semantic.sim_join(\n    df_articles,\n    left_on=embeddings(col(\"headline\")),\n    right_on=embeddings(col(\"content\")),\n    k=3,\n    return_similarity_scores=True\n)\n</code></pre> Find similar job postings <pre><code># Find similar job postings across two sources\ndf_linkedin.semantic.sim_join(\n    df_indeed,\n    left_on=embeddings(col(\"job_title\")),\n    right_on=embeddings(col(\"job_description\")),\n    k=2\n)\n</code></pre> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def sim_join(\n    self,\n    other: DataFrame,\n    left_on: ColumnOrName,\n    right_on: ColumnOrName,\n    k: int = 1,\n    similarity_metric: SemanticSimilarityMetric = \"cosine\",\n    return_similarity_scores: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Performs a semantic similarity join between two DataFrames using precomputed text embeddings.\n\n    For each row in the left DataFrame, finds the top `k` most semantically similar rows in the right DataFrame\n    based on the cosine similarity between their text embeddings. This is useful for fuzzy matching tasks when exact matches aren't possible.\n\n    Args:\n        other: The right-hand DataFrame to join with.\n        left_on: Column in this DataFrame containing text embeddings to compare.\n        right_on: Column in the other DataFrame containing text embeddings to compare.\n        k: Number of most similar matches to return per row from the left DataFrame.\n        similarity_metric: The metric to use for calculating distances between vectors.\n            Supported distance metrics: \"l2\", \"cosine\", \"dot\"\n        return_similarity_scores: If True, include a `_similarity_score` column in the output DataFrame\n                                representing the match confidence (cosine similarity).\n\n    Returns:\n        DataFrame: A new DataFrame containing matched rows from both sides and optionally similarity scores.\n\n    Raises:\n        TypeError: If argument types are incorrect.\n        ValueError: If `k` is not positive or if the columns are invalid.\n        ValueError: If `similarity_metric` is not one of \"l2\", \"cosine\", \"dot\"\n\n    Example: Match queries to FAQ entries\n        ```python\n        # Match customer queries to FAQ entries\n        df_queries.semantic.sim_join(\n            df_faqs,\n            left_on=embeddings(col(\"query_text\")),\n            right_on=embeddings(col(\"faq_question\")),\n            k=1\n        )\n        ```\n\n    Example: Link headlines to articles\n        ```python\n        # Link news headlines to full articles\n        df_headlines.semantic.sim_join(\n            df_articles,\n            left_on=embeddings(col(\"headline\")),\n            right_on=embeddings(col(\"content\")),\n            k=3,\n            return_similarity_scores=True\n        )\n        ```\n\n    Example: Find similar job postings\n        ```python\n        # Find similar job postings across two sources\n        df_linkedin.semantic.sim_join(\n            df_indeed,\n            left_on=embeddings(col(\"job_title\")),\n            right_on=embeddings(col(\"job_description\")),\n            k=2\n        )\n        ```\n    \"\"\"\n    from fenic.api.dataframe.dataframe import DataFrame\n\n    if not isinstance(right_on, ColumnOrName):\n        raise ValidationError(\n            f\"The `right_on` argument must be a `Column` or a string representing a column name, \"\n            f\"but got `{type(right_on).__name__}` instead.\"\n        )\n    if not isinstance(other, DataFrame):\n        raise ValidationError(\n                        f\"The `other` argument to `sim_join()` must be a DataFrame`, but got `{type(other).__name__}`.\"\n                    )\n    if not (isinstance(k, int) and k &gt; 0):\n        raise ValidationError(\n            f\"The parameter `k` must be a positive integer, but received `{k}`.\"\n        )\n    args = get_args(SemanticSimilarityMetric)\n    if similarity_metric not in args:\n        raise ValidationError(\n            f\"The `similarity_metric` argument must be one of {args}, but got `{similarity_metric}`.\"\n        )\n\n    def _validate_column(column: ColumnOrName, name: str):\n        if column is None:\n            raise ValidationError(f\"The `{name}` argument must not be None.\")\n        if not isinstance(column, ColumnOrName):\n            raise ValidationError(\n                f\"The `{name}` argument must be a `Column` or a string representing a column name, \"\n                f\"but got `{type(column).__name__}` instead.\"\n            )\n\n    _validate_column(left_on, \"left_on\")\n    _validate_column(right_on, \"right_on\")\n\n    return self._df._from_logical_plan(\n        SemanticSimilarityJoin(\n            self._df._logical_plan,\n            other._logical_plan,\n            Column._from_col_or_name(left_on)._logical_expr,\n            Column._from_col_or_name(right_on)._logical_expr,\n            k,\n            similarity_metric,\n            return_similarity_scores,\n        ),\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Session","title":"Session","text":"<p>The entry point to programming with the DataFrame API. Similar to PySpark's SparkSession.</p> Create a session with default configuration <pre><code>session = Session.get_or_create(SessionConfig(app_name=\"my_app\"))\n</code></pre> Create a session with cloud configuration <pre><code>config = SessionConfig(\n    app_name=\"my_app\",\n    cloud=True,\n    api_key=\"your_api_key\"\n)\nsession = Session.get_or_create(config)\n</code></pre> <p>Methods:</p> <ul> <li> <code>create_dataframe</code>             \u2013              <p>Create a DataFrame from a variety of Python-native data formats.</p> </li> <li> <code>get_or_create</code>             \u2013              <p>Gets an existing Session or creates a new one with the configured settings.</p> </li> <li> <code>sql</code>             \u2013              <p>Execute a read-only SQL query against one or more DataFrames using named placeholders.</p> </li> <li> <code>stop</code>             \u2013              <p>Stops the session and closes all connections.</p> </li> <li> <code>table</code>             \u2013              <p>Returns the specified table as a DataFrame.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>catalog</code>               (<code>Catalog</code>)           \u2013            <p>Interface for catalog operations on the Session.</p> </li> <li> <code>read</code>               (<code>DataFrameReader</code>)           \u2013            <p>Returns a DataFrameReader that can be used to read data in as a DataFrame.</p> </li> </ul>"},{"location":"reference/fenic/api/#fenic.api.Session.catalog","title":"catalog  <code>property</code>","text":"<pre><code>catalog: Catalog\n</code></pre> <p>Interface for catalog operations on the Session.</p>"},{"location":"reference/fenic/api/#fenic.api.Session.read","title":"read  <code>property</code>","text":"<pre><code>read: DataFrameReader\n</code></pre> <p>Returns a DataFrameReader that can be used to read data in as a DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrameReader</code> (              <code>DataFrameReader</code> )          \u2013            <p>A reader interface to read data into DataFrame</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the session has been stopped</p> </li> </ul>"},{"location":"reference/fenic/api/#fenic.api.Session.create_dataframe","title":"create_dataframe","text":"<pre><code>create_dataframe(data: DataLike) -&gt; DataFrame\n</code></pre> <p>Create a DataFrame from a variety of Python-native data formats.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DataLike</code>)           \u2013            <p>Input data. Must be one of: - Polars DataFrame - Pandas DataFrame - dict of column_name -&gt; list of values - list of dicts (each dict representing a row) - pyarrow Table</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>A new DataFrame instance</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input format is unsupported or inconsistent with provided column names.</p> </li> </ul> Create from Polars DataFrame <pre><code>import polars as pl\ndf = pl.DataFrame({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\nsession.create_dataframe(df)\n</code></pre> Create from Pandas DataFrame <pre><code>import pandas as pd\ndf = pd.DataFrame({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\nsession.create_dataframe(df)\n</code></pre> Create from dictionary <pre><code>session.create_dataframe({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n</code></pre> Create from list of dictionaries <pre><code>session.create_dataframe([\n    {\"col1\": 1, \"col2\": \"a\"},\n    {\"col1\": 2, \"col2\": \"b\"}\n])\n</code></pre> Create from pyarrow Table <pre><code>import pyarrow as pa\ntable = pa.Table.from_pydict({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\nsession.create_dataframe(table)\n</code></pre> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>def create_dataframe(\n    self,\n    data: DataLike,\n) -&gt; DataFrame:\n    \"\"\"Create a DataFrame from a variety of Python-native data formats.\n\n    Args:\n        data: Input data. Must be one of:\n            - Polars DataFrame\n            - Pandas DataFrame\n            - dict of column_name -&gt; list of values\n            - list of dicts (each dict representing a row)\n            - pyarrow Table\n\n    Returns:\n        A new DataFrame instance\n\n    Raises:\n        ValueError: If the input format is unsupported or inconsistent with provided column names.\n\n    Example: Create from Polars DataFrame\n        ```python\n        import polars as pl\n        df = pl.DataFrame({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n        session.create_dataframe(df)\n        ```\n\n    Example: Create from Pandas DataFrame\n        ```python\n        import pandas as pd\n        df = pd.DataFrame({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n        session.create_dataframe(df)\n        ```\n\n    Example: Create from dictionary\n        ```python\n        session.create_dataframe({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n        ```\n\n    Example: Create from list of dictionaries\n        ```python\n        session.create_dataframe([\n            {\"col1\": 1, \"col2\": \"a\"},\n            {\"col1\": 2, \"col2\": \"b\"}\n        ])\n        ```\n\n    Example: Create from pyarrow Table\n        ```python\n        import pyarrow as pa\n        table = pa.Table.from_pydict({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n        session.create_dataframe(table)\n        ```\n    \"\"\"\n    try:\n        if isinstance(data, pl.DataFrame):\n            pl_df = data\n        elif isinstance(data, pd.DataFrame):\n            pl_df = pl.from_pandas(data)\n        elif isinstance(data, dict):\n            pl_df = pl.DataFrame(data)\n        elif isinstance(data, list):\n            if not data:\n                raise ValidationError(\n                    \"Cannot create DataFrame from empty list. Provide a non-empty list of dictionaries, lists, or other supported data types.\"\n                )\n\n            if not isinstance(data[0], dict):\n                raise ValidationError(\n                    \"Cannot create DataFrame from list of non-dict values. Provide a list of dictionaries.\"\n                )\n            pl_df = pl.DataFrame(data)\n        elif isinstance(data, pa.Table):\n            pl_df = pl.from_arrow(data)\n\n        else:\n            raise ValidationError(\n                f\"Unsupported data type: {type(data)}. Supported types are: Polars DataFrame, Pandas DataFrame, dict, or list.\"\n            )\n\n    except ValidationError:\n        raise\n    except Exception as e:\n        raise PlanError(f\"Failed to create DataFrame from {data}\") from e\n\n    return DataFrame._from_logical_plan(\n        InMemorySource(pl_df, self._session_state)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Session.get_or_create","title":"get_or_create  <code>classmethod</code>","text":"<pre><code>get_or_create(config: SessionConfig) -&gt; Session\n</code></pre> <p>Gets an existing Session or creates a new one with the configured settings.</p> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>A Session instance configured with the provided settings</p> </li> </ul> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>@classmethod\ndef get_or_create(\n    cls,\n    config: SessionConfig,\n) -&gt; Session:\n    \"\"\"Gets an existing Session or creates a new one with the configured settings.\n\n    Returns:\n        A Session instance configured with the provided settings\n    \"\"\"\n    resolved_config = config._to_resolved_config()\n    if config.cloud:\n        from fenic._backends.cloud.manager import CloudSessionManager\n\n        cloud_session_manager = CloudSessionManager()\n        if not cloud_session_manager.initialized:\n            session_manager_dependencies = (\n                CloudSessionManager.create_global_session_dependencies()\n            )\n            cloud_session_manager.configure(session_manager_dependencies)\n        future = asyncio.run_coroutine_threadsafe(\n            cloud_session_manager.get_or_create_session_state(resolved_config),\n            cloud_session_manager._asyncio_loop,\n        )\n        cloud_session_state = future.result()\n        return Session._create_cloud_session(cloud_session_state)\n\n    local_session_state: LocalSessionState = LocalSessionManager().get_or_create_session_state(resolved_config)\n    return Session._create_local_session(local_session_state)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Session.sql","title":"sql","text":"<pre><code>sql(query: str, /, **tables: DataFrame) -&gt; DataFrame\n</code></pre> <p>Execute a read-only SQL query against one or more DataFrames using named placeholders.</p> <p>This allows you to execute ad hoc SQL queries using familiar syntax when it's more convenient than the DataFrame API. Placeholders in the SQL string (e.g. <code>{df}</code>) should correspond to keyword arguments (e.g. <code>df=my_dataframe</code>).</p> <p>For supported SQL syntax and functions, refer to the DuckDB SQL documentation: https://duckdb.org/docs/sql/introduction.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>A SQL query string with placeholders like <code>{df}</code></p> </li> <li> <code>**tables</code>               (<code>DataFrame</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments mapping placeholder names to DataFrames</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>A lazy DataFrame representing the result of the SQL query</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If a placeholder is used in the query but not passed as a keyword argument</p> </li> </ul> Simple join between two DataFrames <pre><code>df1 = session.create_dataframe({\"id\": [1, 2]})\ndf2 = session.create_dataframe({\"id\": [2, 3]})\nresult = session.sql(\n    \"SELECT * FROM {df1} JOIN {df2} USING (id)\",\n    df1=df1,\n    df2=df2\n)\n</code></pre> Complex query with multiple DataFrames <pre><code>users = session.create_dataframe({\"user_id\": [1, 2], \"name\": [\"Alice\", \"Bob\"]})\norders = session.create_dataframe({\"order_id\": [1, 2], \"user_id\": [1, 2]})\nproducts = session.create_dataframe({\"product_id\": [1, 2], \"name\": [\"Widget\", \"Gadget\"]})\n\nresult = session.sql(\"\"\"\n    SELECT u.name, p.name as product\n    FROM {users} u\n    JOIN {orders} o ON u.user_id = o.user_id\n    JOIN {products} p ON o.product_id = p.product_id\n\"\"\", users=users, orders=orders, products=products)\n</code></pre> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>def sql(self, query: str, /, **tables: DataFrame) -&gt; DataFrame:\n    \"\"\"Execute a read-only SQL query against one or more DataFrames using named placeholders.\n\n    This allows you to execute ad hoc SQL queries using familiar syntax when it's more convenient than the DataFrame API.\n    Placeholders in the SQL string (e.g. `{df}`) should correspond to keyword arguments (e.g. `df=my_dataframe`).\n\n    For supported SQL syntax and functions, refer to the DuckDB SQL documentation:\n    https://duckdb.org/docs/sql/introduction.\n\n    Args:\n        query: A SQL query string with placeholders like `{df}`\n        **tables: Keyword arguments mapping placeholder names to DataFrames\n\n    Returns:\n        A lazy DataFrame representing the result of the SQL query\n\n    Raises:\n        ValidationError: If a placeholder is used in the query but not passed\n            as a keyword argument\n\n    Example: Simple join between two DataFrames\n        ```python\n        df1 = session.create_dataframe({\"id\": [1, 2]})\n        df2 = session.create_dataframe({\"id\": [2, 3]})\n        result = session.sql(\n            \"SELECT * FROM {df1} JOIN {df2} USING (id)\",\n            df1=df1,\n            df2=df2\n        )\n        ```\n\n    Example: Complex query with multiple DataFrames\n        ```python\n        users = session.create_dataframe({\"user_id\": [1, 2], \"name\": [\"Alice\", \"Bob\"]})\n        orders = session.create_dataframe({\"order_id\": [1, 2], \"user_id\": [1, 2]})\n        products = session.create_dataframe({\"product_id\": [1, 2], \"name\": [\"Widget\", \"Gadget\"]})\n\n        result = session.sql(\\\"\\\"\\\"\n            SELECT u.name, p.name as product\n            FROM {users} u\n            JOIN {orders} o ON u.user_id = o.user_id\n            JOIN {products} p ON o.product_id = p.product_id\n        \\\"\\\"\\\", users=users, orders=orders, products=products)\n        ```\n    \"\"\"\n    query = query.strip()\n    if not query:\n        raise ValidationError(\"SQL query must not be empty.\")\n\n    placeholders = set(SQL_PLACEHOLDER_RE.findall(query))\n    missing = placeholders - tables.keys()\n    if missing:\n        raise ValidationError(\n            f\"Missing DataFrames for placeholders in SQL query: {', '.join(sorted(missing))}. \"\n            f\"Make sure to pass them as keyword arguments, e.g., sql(..., {next(iter(missing))}=df).\"\n        )\n\n    logical_plans = []\n    template_names = []\n    for name, table in tables.items():\n        if name in placeholders:\n            template_names.append(name)\n            logical_plans.append(table._logical_plan)\n\n    return DataFrame._from_logical_plan(\n        SQL(logical_plans, template_names, query, self._session_state),\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Session.stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stops the session and closes all connections.</p> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>def stop(self):\n    \"\"\"Stops the session and closes all connections.\"\"\"\n    self._session_state.stop()\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.Session.table","title":"table","text":"<pre><code>table(table_name: str) -&gt; DataFrame\n</code></pre> <p>Returns the specified table as a DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Name of the table</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Table as a DataFrame</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the table does not exist</p> </li> </ul> Load an existing table <pre><code>df = session.table(\"my_table\")\n</code></pre> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>def table(self, table_name: str) -&gt; DataFrame:\n    \"\"\"Returns the specified table as a DataFrame.\n\n    Args:\n        table_name: Name of the table\n\n    Returns:\n        Table as a DataFrame\n\n    Raises:\n        ValueError: If the table does not exist\n\n    Example: Load an existing table\n        ```python\n        df = session.table(\"my_table\")\n        ```\n    \"\"\"\n    if not self._session_state.catalog.does_table_exist(table_name):\n        raise ValueError(f\"Table {table_name} does not exist\")\n    return DataFrame._from_logical_plan(\n        TableSource(table_name, self._session_state),\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.SessionConfig","title":"SessionConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a user session.</p> <p>This class defines the complete configuration for a user session, including application settings, model configurations, and optional cloud settings. It serves as the central configuration object for all language model operations.</p> <p>Attributes:</p> <ul> <li> <code>app_name</code>               (<code>str</code>)           \u2013            <p>Name of the application using this session. Defaults to \"default_app\".</p> </li> <li> <code>db_path</code>               (<code>Optional[Path]</code>)           \u2013            <p>Optional path to a local database file for persistent storage.</p> </li> <li> <code>semantic</code>               (<code>SemanticConfig</code>)           \u2013            <p>Configuration for semantic models (required).</p> </li> <li> <code>cloud</code>               (<code>Optional[CloudConfig]</code>)           \u2013            <p>Optional configuration for cloud execution.</p> </li> </ul> Note <p>The semantic configuration is required as it defines the language models that will be used for processing. The cloud configuration is optional and only needed for distributed processing.</p>"},{"location":"reference/fenic/api/#fenic.api.array","title":"array","text":"<pre><code>array(*args: Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]) -&gt; Column\n</code></pre> <p>Creates a new array column from multiple input columns.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]</code>, default:                   <code>()</code> )           \u2013            <p>Columns or column names to combine into an array. Can be:</p> <ul> <li>Individual arguments</li> <li>Lists of columns/column names</li> <li>Tuples of columns/column names</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing an array containing values from the input columns</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If any argument is not a Column, string, or collection of Columns/strings</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef array(\n    *args: Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]\n) -&gt; Column:\n    \"\"\"Creates a new array column from multiple input columns.\n\n    Args:\n        *args: Columns or column names to combine into an array. Can be:\n\n            - Individual arguments\n            - Lists of columns/column names\n            - Tuples of columns/column names\n\n    Returns:\n        A Column expression representing an array containing values from the input columns\n\n    Raises:\n        TypeError: If any argument is not a Column, string, or collection of\n            Columns/strings\n    \"\"\"\n    flattened_args = []\n    for arg in args:\n        if isinstance(arg, (list, tuple)):\n            flattened_args.extend(arg)\n        else:\n            flattened_args.append(arg)\n\n    expr_columns = [Column._from_col_or_name(c)._logical_expr for c in flattened_args]\n\n    return Column._from_logical_expr(ArrayExpr(expr_columns))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.array_agg","title":"array_agg","text":"<pre><code>array_agg(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Alias for collect_list().</p> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef array_agg(column: ColumnOrName) -&gt; Column:\n    \"\"\"Alias for collect_list().\"\"\"\n    return collect_list(column)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.array_contains","title":"array_contains","text":"<pre><code>array_contains(column: ColumnOrName, value: Union[str, int, float, bool, Column]) -&gt; Column\n</code></pre> <p>Checks if array column contains a specific value.</p> <p>This function returns True if the array in the specified column contains the given value, and False otherwise. Returns False if the array is None.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name containing the arrays to check.</p> </li> <li> <code>value</code>               (<code>Union[str, int, float, bool, Column]</code>)           \u2013            <p>Value to search for in the arrays. Can be: - A literal value (string, number, boolean) - A Column expression</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A boolean Column expression (True if value is found, False otherwise).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If value type is incompatible with the array element type.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If the column does not contain array data.</p> </li> </ul> Check for values in arrays <pre><code># Check if 'python' exists in arrays in the 'tags' column\ndf.select(array_contains(\"tags\", \"python\"))\n\n# Check using a value from another column\ndf.select(array_contains(\"tags\", col(\"search_term\")))\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef array_contains(\n    column: ColumnOrName, value: Union[str, int, float, bool, Column]\n) -&gt; Column:\n    \"\"\"Checks if array column contains a specific value.\n\n    This function returns True if the array in the specified column contains the given value,\n    and False otherwise. Returns False if the array is None.\n\n    Args:\n        column: Column or column name containing the arrays to check.\n\n        value: Value to search for in the arrays. Can be:\n            - A literal value (string, number, boolean)\n            - A Column expression\n\n    Returns:\n        A boolean Column expression (True if value is found, False otherwise).\n\n    Raises:\n        TypeError: If value type is incompatible with the array element type.\n        TypeError: If the column does not contain array data.\n\n    Example: Check for values in arrays\n        ```python\n        # Check if 'python' exists in arrays in the 'tags' column\n        df.select(array_contains(\"tags\", \"python\"))\n\n        # Check using a value from another column\n        df.select(array_contains(\"tags\", col(\"search_term\")))\n        ```\n    \"\"\"\n    value_column = None\n    if isinstance(value, Column):\n        value_column = value\n    else:\n        value_column = lit(value)\n    return Column._from_logical_expr(\n        ArrayContainsExpr(\n            Column._from_col_or_name(column)._logical_expr, value_column._logical_expr\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.array_size","title":"array_size","text":"<pre><code>array_size(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Returns the number of elements in an array column.</p> <p>This function computes the length of arrays stored in the specified column. Returns None for None arrays.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name containing arrays whose length to compute.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the array length.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the column does not contain array data.</p> </li> </ul> Get array sizes <pre><code># Get the size of arrays in 'tags' column\ndf.select(array_size(\"tags\"))\n\n# Use with column reference\ndf.select(array_size(col(\"tags\")))\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef array_size(column: ColumnOrName) -&gt; Column:\n    \"\"\"Returns the number of elements in an array column.\n\n    This function computes the length of arrays stored in the specified column.\n    Returns None for None arrays.\n\n    Args:\n        column: Column or column name containing arrays whose length to compute.\n\n    Returns:\n        A Column expression representing the array length.\n\n    Raises:\n        TypeError: If the column does not contain array data.\n\n    Example: Get array sizes\n        ```python\n        # Get the size of arrays in 'tags' column\n        df.select(array_size(\"tags\"))\n\n        # Use with column reference\n        df.select(array_size(col(\"tags\")))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        ArrayLengthExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.asc","title":"asc","text":"<pre><code>asc(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing an ascending sort order.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the ascending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the ascending sort order.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef asc(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing an ascending sort order.\n\n    Args:\n        column: The column to apply the ascending ordering to.\n\n    Returns:\n        A Column expression representing the column and the ascending sort order.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).asc()\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.asc_nulls_first","title":"asc_nulls_first","text":"<pre><code>asc_nulls_first(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing an ascending sort order with nulls first.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the ascending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the ascending sort order with nulls first.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef asc_nulls_first(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing an ascending sort order with nulls first.\n\n    Args:\n        column: The column to apply the ascending ordering to.\n\n    Returns:\n        A Column expression representing the column and the ascending sort order with nulls first.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).asc_nulls_first()\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.asc_nulls_last","title":"asc_nulls_last","text":"<pre><code>asc_nulls_last(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing an ascending sort order with nulls last.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the ascending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the ascending sort order with nulls last.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef asc_nulls_last(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing an ascending sort order with nulls last.\n\n    Args:\n        column: The column to apply the ascending ordering to.\n\n    Returns:\n        A Column expression representing the column and the ascending sort order with nulls last.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).asc_nulls_last()\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.avg","title":"avg","text":"<pre><code>avg(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the average (mean) of all values in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the average of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the average aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef avg(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the average (mean) of all values in the specified column.\n\n    Args:\n        column: Column or column name to compute the average of\n\n    Returns:\n        A Column expression representing the average aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        AvgExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.coalesce","title":"coalesce","text":"<pre><code>coalesce(*cols: ColumnOrName) -&gt; Column\n</code></pre> <p>Returns the first non-null value from the given columns for each row.</p> <p>This function mimics the behavior of SQL's COALESCE function. It evaluates the input columns in order and returns the first non-null value encountered. If all values are null, returns null.</p> <p>Parameters:</p> <ul> <li> <code>*cols</code>               (<code>ColumnOrName</code>, default:                   <code>()</code> )           \u2013            <p>Column expressions or column names to evaluate. Can be:</p> <ul> <li>Individual arguments</li> <li>Lists of columns/column names</li> <li>Tuples of columns/column names</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression containing the first non-null value from the input columns.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no columns are provided.</p> </li> </ul> Basic coalesce usage <pre><code># Basic usage\ndf.select(coalesce(\"col1\", \"col2\", \"col3\"))\n\n# With nested collections\ndf.select(coalesce([\"col1\", \"col2\"], \"col3\"))\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef coalesce(*cols: ColumnOrName) -&gt; Column:\n    \"\"\"Returns the first non-null value from the given columns for each row.\n\n    This function mimics the behavior of SQL's COALESCE function. It evaluates the input columns\n    in order and returns the first non-null value encountered. If all values are null, returns null.\n\n    Args:\n        *cols: Column expressions or column names to evaluate. Can be:\n\n            - Individual arguments\n            - Lists of columns/column names\n            - Tuples of columns/column names\n\n    Returns:\n        A Column expression containing the first non-null value from the input columns.\n\n    Raises:\n        ValueError: If no columns are provided.\n\n    Example: Basic coalesce usage\n        ```python\n        # Basic usage\n        df.select(coalesce(\"col1\", \"col2\", \"col3\"))\n\n        # With nested collections\n        df.select(coalesce([\"col1\", \"col2\"], \"col3\"))\n        ```\n    \"\"\"\n    if not cols:\n        raise ValueError(\"At least one column must be provided to coalesce method\")\n\n    flattened_args = []\n    for arg in cols:\n        if isinstance(arg, (list, tuple)):\n            flattened_args.extend(arg)\n        else:\n            flattened_args.append(arg)\n\n    flattened_exprs = [\n        Column._from_col_or_name(c)._logical_expr for c in flattened_args\n    ]\n    return Column._from_logical_expr(CoalesceExpr(flattened_exprs))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.col","title":"col","text":"<pre><code>col(col_name: str) -&gt; Column\n</code></pre> <p>Creates a Column expression referencing a column in the DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>col_name</code>               (<code>str</code>)           \u2013            <p>Name of the column to reference</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression for the specified column</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If colName is not a string</p> </li> </ul> Source code in <code>src/fenic/api/functions/core.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef col(col_name: str) -&gt; Column:\n    \"\"\"Creates a Column expression referencing a column in the DataFrame.\n\n    Args:\n        col_name: Name of the column to reference\n\n    Returns:\n        A Column expression for the specified column\n\n    Raises:\n        TypeError: If colName is not a string\n    \"\"\"\n    return Column._from_column_name(col_name)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.collect_list","title":"collect_list","text":"<pre><code>collect_list(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: collects all values from the specified column into a list.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to collect values from</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the list aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef collect_list(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: collects all values from the specified column into a list.\n\n    Args:\n        column: Column or column name to collect values from\n\n    Returns:\n        A Column expression representing the list aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        ListExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.count","title":"count","text":"<pre><code>count(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the count of non-null values in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to count values in</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the count aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef count(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the count of non-null values in the specified column.\n\n    Args:\n        column: Column or column name to count values in\n\n    Returns:\n        A Column expression representing the count aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    if isinstance(column, str) and column == \"*\":\n        return Column._from_logical_expr(CountExpr(lit(\"*\")._logical_expr))\n    return Column._from_logical_expr(\n        CountExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.desc","title":"desc","text":"<pre><code>desc(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing a descending sort order.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the descending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the descending sort order.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef desc(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing a descending sort order.\n\n    Args:\n        column: The column to apply the descending ordering to.\n\n    Returns:\n        A Column expression representing the column and the descending sort order.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).desc()\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.desc_nulls_first","title":"desc_nulls_first","text":"<pre><code>desc_nulls_first(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing a descending sort order with nulls first.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the descending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the descending sort order with nulls first.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef desc_nulls_first(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing a descending sort order with nulls first.\n\n    Args:\n        column: The column to apply the descending ordering to.\n\n    Returns:\n        A Column expression representing the column and the descending sort order with nulls first.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).desc_nulls_first()\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.desc_nulls_last","title":"desc_nulls_last","text":"<pre><code>desc_nulls_last(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing a descending sort order with nulls last.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the descending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the descending sort order with nulls last.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef desc_nulls_last(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing a descending sort order with nulls last.\n\n    Args:\n        column: The column to apply the descending ordering to.\n\n    Returns:\n        A Column expression representing the column and the descending sort order with nulls last.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).desc_nulls_last()\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.lit","title":"lit","text":"<pre><code>lit(value: Any) -&gt; Column\n</code></pre> <p>Creates a Column expression representing a literal value.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>The literal value to create a column for</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the literal value</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the value cannot be inferred</p> </li> </ul> Source code in <code>src/fenic/api/functions/core.py</code> <pre><code>def lit(value: Any) -&gt; Column:\n    \"\"\"Creates a Column expression representing a literal value.\n\n    Args:\n        value: The literal value to create a column for\n\n    Returns:\n        A Column expression representing the literal value\n\n    Raises:\n        ValueError: If the type of the value cannot be inferred\n    \"\"\"\n    try:\n        inferred_type = infer_dtype_from_pyobj(value)\n    except TypeInferenceError as e:\n        raise ValidationError(f\"`lit` failed to infer type for value `{value}`\") from e\n    literal_expr = LiteralExpr(value, inferred_type)\n    return Column._from_logical_expr(literal_expr)\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.max","title":"max","text":"<pre><code>max(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the maximum value in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the maximum of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the maximum aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef max(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the maximum value in the specified column.\n\n    Args:\n        column: Column or column name to compute the maximum of\n\n    Returns:\n        A Column expression representing the maximum aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        MaxExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.mean","title":"mean","text":"<pre><code>mean(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the mean (average) of all values in the specified column.</p> <p>Alias for avg().</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the mean of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the mean aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef mean(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the mean (average) of all values in the specified column.\n\n    Alias for avg().\n\n    Args:\n        column: Column or column name to compute the mean of\n\n    Returns:\n        A Column expression representing the mean aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        AvgExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.min","title":"min","text":"<pre><code>min(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the minimum value in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the minimum of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the minimum aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef min(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the minimum value in the specified column.\n\n    Args:\n        column: Column or column name to compute the minimum of\n\n    Returns:\n        A Column expression representing the minimum aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        MinExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.struct","title":"struct","text":"<pre><code>struct(*args: Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]) -&gt; Column\n</code></pre> <p>Creates a new struct column from multiple input columns.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]</code>, default:                   <code>()</code> )           \u2013            <p>Columns or column names to combine into a struct. Can be:</p> <ul> <li>Individual arguments</li> <li>Lists of columns/column names</li> <li>Tuples of columns/column names</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing a struct containing the input columns</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If any argument is not a Column, string, or collection of Columns/strings</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef struct(\n    *args: Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]\n) -&gt; Column:\n    \"\"\"Creates a new struct column from multiple input columns.\n\n    Args:\n        *args: Columns or column names to combine into a struct. Can be:\n\n            - Individual arguments\n            - Lists of columns/column names\n            - Tuples of columns/column names\n\n    Returns:\n        A Column expression representing a struct containing the input columns\n\n    Raises:\n        TypeError: If any argument is not a Column, string, or collection of\n            Columns/strings\n    \"\"\"\n    flattened_args = []\n    for arg in args:\n        if isinstance(arg, (list, tuple)):\n            flattened_args.extend(arg)\n        else:\n            flattened_args.append(arg)\n\n    expr_columns = [Column._from_col_or_name(c)._logical_expr for c in flattened_args]\n\n    return Column._from_logical_expr(StructExpr(expr_columns))\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.sum","title":"sum","text":"<pre><code>sum(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the sum of all values in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the sum of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the sum aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef sum(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the sum of all values in the specified column.\n\n    Args:\n        column: Column or column name to compute the sum of\n\n    Returns:\n        A Column expression representing the sum aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        SumExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.udf","title":"udf","text":"<pre><code>udf(f: Optional[Callable] = None, *, return_type: DataType)\n</code></pre> <p>A decorator or function for creating user-defined functions (UDFs) that can be applied to DataFrame rows.</p> <p>When applied, UDFs will: - Access <code>StructType</code> columns as Python dictionaries (<code>dict[str, Any]</code>). - Access <code>ArrayType</code> columns as Python lists (<code>list[Any]</code>). - Access primitive types (e.g., <code>int</code>, <code>float</code>, <code>str</code>) as their respective Python types.</p> <p>Parameters:</p> <ul> <li> <code>f</code>               (<code>Optional[Callable]</code>, default:                   <code>None</code> )           \u2013            <p>Python function to convert to UDF</p> </li> <li> <code>return_type</code>               (<code>DataType</code>)           \u2013            <p>Expected return type of the UDF. Required parameter.</p> </li> </ul> UDF with primitive types <pre><code># UDF with primitive types\n@udf(return_type=IntegerType)\ndef add_one(x: int):\n    return x + 1\n\n# Or\nadd_one = udf(lambda x: x + 1, return_type=IntegerType)\n</code></pre> UDF with nested types <pre><code># UDF with nested types\n@udf(return_type=StructType([StructField(\"value1\", IntegerType), StructField(\"value2\", IntegerType)]))\ndef example_udf(x: dict[str, int], y: list[int]):\n    return {\n        \"value1\": x[\"value1\"] + x[\"value2\"] + y[0],\n        \"value2\": x[\"value1\"] + x[\"value2\"] + y[1],\n    }\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef udf(f: Optional[Callable] = None, *, return_type: DataType):\n    \"\"\"A decorator or function for creating user-defined functions (UDFs) that can be applied to DataFrame rows.\n\n    When applied, UDFs will:\n    - Access `StructType` columns as Python dictionaries (`dict[str, Any]`).\n    - Access `ArrayType` columns as Python lists (`list[Any]`).\n    - Access primitive types (e.g., `int`, `float`, `str`) as their respective Python types.\n\n    Args:\n        f: Python function to convert to UDF\n\n        return_type: Expected return type of the UDF. Required parameter.\n\n    Example: UDF with primitive types\n        ```python\n        # UDF with primitive types\n        @udf(return_type=IntegerType)\n        def add_one(x: int):\n            return x + 1\n\n        # Or\n        add_one = udf(lambda x: x + 1, return_type=IntegerType)\n        ```\n\n    Example: UDF with nested types\n        ```python\n        # UDF with nested types\n        @udf(return_type=StructType([StructField(\"value1\", IntegerType), StructField(\"value2\", IntegerType)]))\n        def example_udf(x: dict[str, int], y: list[int]):\n            return {\n                \"value1\": x[\"value1\"] + x[\"value2\"] + y[0],\n                \"value2\": x[\"value1\"] + x[\"value2\"] + y[1],\n            }\n        ```\n    \"\"\"\n\n    def _create_udf(func: Callable) -&gt; Callable:\n        @wraps(func)\n        def _udf_wrapper(*cols: ColumnOrName) -&gt; Column:\n            col_exprs = [Column._from_col_or_name(c)._logical_expr for c in cols]\n            return Column._from_logical_expr(UDFExpr(func, col_exprs, return_type))\n\n        return _udf_wrapper\n\n    if f is not None:\n        return _create_udf(f)\n    return _create_udf\n</code></pre>"},{"location":"reference/fenic/api/#fenic.api.when","title":"when","text":"<pre><code>when(condition: Column, value: Column) -&gt; Column\n</code></pre> <p>Evaluates a condition and returns a value if true.</p> <p>This function is used to create conditional expressions. If Column.otherwise() is not invoked, None is returned for unmatched conditions.</p> <p>Parameters:</p> <ul> <li> <code>condition</code>               (<code>Column</code>)           \u2013            <p>A boolean Column expression to evaluate.</p> </li> <li> <code>value</code>               (<code>Column</code>)           \u2013            <p>A Column expression to return if the condition is true.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression that evaluates the condition and returns the specified value when true,</p> </li> <li> <code>Column</code>           \u2013            <p>and None otherwise.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the condition is not a boolean Column expression.</p> </li> </ul> Basic conditional expression <pre><code># Basic usage\ndf.select(when(col(\"age\") &gt; 18, lit(\"adult\")))\n\n# With otherwise\ndf.select(when(col(\"age\") &gt; 18, lit(\"adult\")).otherwise(lit(\"minor\")))\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef when(condition: Column, value: Column) -&gt; Column:\n    \"\"\"Evaluates a condition and returns a value if true.\n\n    This function is used to create conditional expressions. If Column.otherwise() is not invoked,\n    None is returned for unmatched conditions.\n\n    Args:\n        condition: A boolean Column expression to evaluate.\n\n        value: A Column expression to return if the condition is true.\n\n    Returns:\n        A Column expression that evaluates the condition and returns the specified value when true,\n        and None otherwise.\n\n    Raises:\n        TypeError: If the condition is not a boolean Column expression.\n\n    Example: Basic conditional expression\n        ```python\n        # Basic usage\n        df.select(when(col(\"age\") &gt; 18, lit(\"adult\")))\n\n        # With otherwise\n        df.select(when(col(\"age\") &gt; 18, lit(\"adult\")).otherwise(lit(\"minor\")))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        WhenExpr(None, condition._logical_expr, value._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/catalog/","title":"fenic.api.catalog","text":""},{"location":"reference/fenic/api/catalog/#fenic.api.catalog","title":"fenic.api.catalog","text":"<p>Catalog API for managing database objects in Fenic.</p> <p>Classes:</p> <ul> <li> <code>Catalog</code>           \u2013            <p>Entry point for catalog operations.</p> </li> </ul>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog","title":"Catalog","text":"<pre><code>Catalog(catalog: BaseCatalog)\n</code></pre> <p>Entry point for catalog operations.</p> <p>The Catalog provides methods to interact with and manage database tables, including listing available tables, describing table schemas, and dropping tables.</p> Basic usage <pre><code># Create a new catalog\nsession.catalog.create_catalog('my_catalog')\n# Returns: True\n\n# Set the current catalog\nsession.catalog.set_current_catalog('my_catalog')\n# Returns: None\n\n# Create a new database\nsession.catalog.create_database('my_database')\n# Returns: True\n\n# Use the new database\nsession.catalog.set_current_database('my_database')\n# Returns: None\n\n# Create a new table\nsession.catalog.create_table('my_table', Schema([\n    ColumnField('id', IntegerType),\n]))\n# Returns: True\n</code></pre> <p>Initialize a Catalog instance.</p> <p>Parameters:</p> <ul> <li> <code>catalog</code>               (<code>BaseCatalog</code>)           \u2013            <p>The underlying catalog implementation.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>create_catalog</code>             \u2013              <p>Creates a new catalog.</p> </li> <li> <code>create_database</code>             \u2013              <p>Creates a new database.</p> </li> <li> <code>create_table</code>             \u2013              <p>Creates a new table.</p> </li> <li> <code>describe_table</code>             \u2013              <p>Returns the schema of the specified table.</p> </li> <li> <code>does_catalog_exist</code>             \u2013              <p>Checks if a catalog with the specified name exists.</p> </li> <li> <code>does_database_exist</code>             \u2013              <p>Checks if a database with the specified name exists.</p> </li> <li> <code>does_table_exist</code>             \u2013              <p>Checks if a table with the specified name exists.</p> </li> <li> <code>drop_catalog</code>             \u2013              <p>Drops a catalog.</p> </li> <li> <code>drop_database</code>             \u2013              <p>Drops a database.</p> </li> <li> <code>drop_table</code>             \u2013              <p>Drops the specified table.</p> </li> <li> <code>get_current_catalog</code>             \u2013              <p>Returns the name of the current catalog.</p> </li> <li> <code>get_current_database</code>             \u2013              <p>Returns the name of the current database in the current catalog.</p> </li> <li> <code>list_catalogs</code>             \u2013              <p>Returns a list of available catalogs.</p> </li> <li> <code>list_databases</code>             \u2013              <p>Returns a list of databases in the current catalog.</p> </li> <li> <code>list_tables</code>             \u2013              <p>Returns a list of tables stored in the current database.</p> </li> <li> <code>set_current_catalog</code>             \u2013              <p>Sets the current catalog.</p> </li> <li> <code>set_current_database</code>             \u2013              <p>Sets the current database.</p> </li> </ul> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def __init__(self, catalog: BaseCatalog):\n    \"\"\"Initialize a Catalog instance.\n\n    Args:\n        catalog: The underlying catalog implementation.\n    \"\"\"\n    self.catalog = catalog\n</code></pre>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog.create_catalog","title":"create_catalog","text":"<pre><code>create_catalog(catalog_name: str, ignore_if_exists: bool = True) -&gt; bool\n</code></pre> <p>Creates a new catalog.</p> <p>Parameters:</p> <ul> <li> <code>catalog_name</code>               (<code>str</code>)           \u2013            <p>Name of the catalog to create.</p> </li> <li> <code>ignore_if_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, return False when the catalog already exists. If False, raise an error when the catalog already exists. Defaults to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CatalogAlreadyExistsError</code>             \u2013            <p>If the catalog already exists and ignore_if_exists is False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the catalog was created successfully, False if the catalog</p> </li> <li> <code>bool</code>           \u2013            <p>already exists and ignore_if_exists is True.</p> </li> </ul> Create a new catalog <pre><code># Create a new catalog named 'my_catalog'\nsession.catalog.create_catalog('my_catalog')\n# Returns: True\n</code></pre> Create an existing catalog with ignore_if_exists <pre><code># Try to create an existing catalog with ignore_if_exists=True\nsession.catalog.create_catalog('my_catalog', ignore_if_exists=True)\n# Returns: False\n</code></pre> Create an existing catalog without ignore_if_exists <pre><code># Try to create an existing catalog with ignore_if_exists=False\nsession.catalog.create_catalog('my_catalog', ignore_if_exists=False)\n# Raises: CatalogAlreadyExistsError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef create_catalog(self, catalog_name: str, ignore_if_exists: bool = True) -&gt; bool:\n    \"\"\"Creates a new catalog.\n\n    Args:\n        catalog_name (str): Name of the catalog to create.\n        ignore_if_exists (bool): If True, return False when the catalog already exists.\n            If False, raise an error when the catalog already exists.\n            Defaults to True.\n\n    Raises:\n        CatalogAlreadyExistsError: If the catalog already exists and ignore_if_exists is False.\n\n    Returns:\n        bool: True if the catalog was created successfully, False if the catalog\n        already exists and ignore_if_exists is True.\n\n    Example: Create a new catalog\n        ```python\n        # Create a new catalog named 'my_catalog'\n        session.catalog.create_catalog('my_catalog')\n        # Returns: True\n        ```\n\n    Example: Create an existing catalog with ignore_if_exists\n        ```python\n        # Try to create an existing catalog with ignore_if_exists=True\n        session.catalog.create_catalog('my_catalog', ignore_if_exists=True)\n        # Returns: False\n        ```\n\n    Example: Create an existing catalog without ignore_if_exists\n        ```python\n        # Try to create an existing catalog with ignore_if_exists=False\n        session.catalog.create_catalog('my_catalog', ignore_if_exists=False)\n        # Raises: CatalogAlreadyExistsError\n        ```\n    \"\"\"\n    return self.catalog.create_catalog(catalog_name, ignore_if_exists)\n</code></pre>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog.create_database","title":"create_database","text":"<pre><code>create_database(database_name: str, ignore_if_exists: bool = True) -&gt; bool\n</code></pre> <p>Creates a new database.</p> <p>Parameters:</p> <ul> <li> <code>database_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative database name to create.</p> </li> <li> <code>ignore_if_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, return False when the database already exists. If False, raise an error when the database already exists. Defaults to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatabaseAlreadyExistsError</code>             \u2013            <p>If the database already exists and ignore_if_exists is False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the database was created successfully, False if the database</p> </li> <li> <code>bool</code>           \u2013            <p>already exists and ignore_if_exists is True.</p> </li> </ul> Create a new database <pre><code># Create a new database named 'my_database'\nsession.catalog.create_database('my_database')\n# Returns: True\n</code></pre> Create an existing database with ignore_if_exists <pre><code># Try to create an existing database with ignore_if_exists=True\nsession.catalog.create_database('my_database', ignore_if_exists=True)\n# Returns: False\n</code></pre> Create an existing database without ignore_if_exists <pre><code># Try to create an existing database with ignore_if_exists=False\nsession.catalog.create_database('my_database', ignore_if_exists=False)\n# Raises: DatabaseAlreadyExistsError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef create_database(\n    self, database_name: str, ignore_if_exists: bool = True\n) -&gt; bool:\n    \"\"\"Creates a new database.\n\n    Args:\n        database_name (str): Fully qualified or relative database name to create.\n        ignore_if_exists (bool): If True, return False when the database already exists.\n            If False, raise an error when the database already exists.\n            Defaults to True.\n\n    Raises:\n        DatabaseAlreadyExistsError: If the database already exists and ignore_if_exists is False.\n\n    Returns:\n        bool: True if the database was created successfully, False if the database\n        already exists and ignore_if_exists is True.\n\n    Example: Create a new database\n        ```python\n        # Create a new database named 'my_database'\n        session.catalog.create_database('my_database')\n        # Returns: True\n        ```\n\n    Example: Create an existing database with ignore_if_exists\n        ```python\n        # Try to create an existing database with ignore_if_exists=True\n        session.catalog.create_database('my_database', ignore_if_exists=True)\n        # Returns: False\n        ```\n\n    Example: Create an existing database without ignore_if_exists\n        ```python\n        # Try to create an existing database with ignore_if_exists=False\n        session.catalog.create_database('my_database', ignore_if_exists=False)\n        # Raises: DatabaseAlreadyExistsError\n        ```\n    \"\"\"\n    return self.catalog.create_database(database_name, ignore_if_exists)\n</code></pre>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog.create_table","title":"create_table","text":"<pre><code>create_table(table_name: str, schema: Schema, ignore_if_exists: bool = True) -&gt; bool\n</code></pre> <p>Creates a new table.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative table name to create.</p> </li> <li> <code>schema</code>               (<code>Schema</code>)           \u2013            <p>Schema of the table to create.</p> </li> <li> <code>ignore_if_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, return False when the table already exists. If False, raise an error when the table already exists. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the table was created successfully, False if the table</p> </li> <li> <code>bool</code>           \u2013            <p>already exists and ignore_if_exists is True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TableAlreadyExistsError</code>             \u2013            <p>If the table already exists and ignore_if_exists is False</p> </li> </ul> Create a new table <pre><code># Create a new table with an integer column\nsession.catalog.create_table('my_table', Schema([\n    ColumnField('id', IntegerType),\n]))\n# Returns: True\n</code></pre> Create an existing table with ignore_if_exists <pre><code># Try to create an existing table with ignore_if_exists=True\nsession.catalog.create_table('my_table', Schema([\n    ColumnField('id', IntegerType),\n]), ignore_if_exists=True)\n# Returns: False\n</code></pre> Create an existing table without ignore_if_exists <pre><code># Try to create an existing table with ignore_if_exists=False\nsession.catalog.create_table('my_table', Schema([\n    ColumnField('id', IntegerType),\n]), ignore_if_exists=False)\n# Raises: TableAlreadyExistsError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef create_table(\n    self, table_name: str, schema: Schema, ignore_if_exists: bool = True\n) -&gt; bool:\n    \"\"\"Creates a new table.\n\n    Args:\n        table_name (str): Fully qualified or relative table name to create.\n        schema (Schema): Schema of the table to create.\n        ignore_if_exists (bool): If True, return False when the table already exists.\n            If False, raise an error when the table already exists.\n            Defaults to True.\n\n    Returns:\n        bool: True if the table was created successfully, False if the table\n        already exists and ignore_if_exists is True.\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists and ignore_if_exists is False\n\n    Example: Create a new table\n        ```python\n        # Create a new table with an integer column\n        session.catalog.create_table('my_table', Schema([\n            ColumnField('id', IntegerType),\n        ]))\n        # Returns: True\n        ```\n\n    Example: Create an existing table with ignore_if_exists\n        ```python\n        # Try to create an existing table with ignore_if_exists=True\n        session.catalog.create_table('my_table', Schema([\n            ColumnField('id', IntegerType),\n        ]), ignore_if_exists=True)\n        # Returns: False\n        ```\n\n    Example: Create an existing table without ignore_if_exists\n        ```python\n        # Try to create an existing table with ignore_if_exists=False\n        session.catalog.create_table('my_table', Schema([\n            ColumnField('id', IntegerType),\n        ]), ignore_if_exists=False)\n        # Raises: TableAlreadyExistsError\n        ```\n    \"\"\"\n    return self.catalog.create_table(table_name, schema, ignore_if_exists)\n</code></pre>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog.describe_table","title":"describe_table","text":"<pre><code>describe_table(table_name: str) -&gt; Schema\n</code></pre> <p>Returns the schema of the specified table.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative table name to describe.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Schema</code> (              <code>Schema</code> )          \u2013            <p>A schema object describing the table's structure with field names and types.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TableNotFoundError</code>             \u2013            <p>If the table doesn't exist.</p> </li> </ul> Describe a table's schema <pre><code># For a table created with: CREATE TABLE t1 (id int)\nsession.catalog.describe_table('t1')\n# Returns: Schema([\n#     ColumnField('id', IntegerType),\n# ])\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef describe_table(self, table_name: str) -&gt; Schema:\n    \"\"\"Returns the schema of the specified table.\n\n    Args:\n        table_name (str): Fully qualified or relative table name to describe.\n\n    Returns:\n        Schema: A schema object describing the table's structure with field names and types.\n\n    Raises:\n        TableNotFoundError: If the table doesn't exist.\n\n    Example: Describe a table's schema\n        ```python\n        # For a table created with: CREATE TABLE t1 (id int)\n        session.catalog.describe_table('t1')\n        # Returns: Schema([\n        #     ColumnField('id', IntegerType),\n        # ])\n        ```\n    \"\"\"\n    return self.catalog.describe_table(table_name)\n</code></pre>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog.does_catalog_exist","title":"does_catalog_exist","text":"<pre><code>does_catalog_exist(catalog_name: str) -&gt; bool\n</code></pre> <p>Checks if a catalog with the specified name exists.</p> <p>Parameters:</p> <ul> <li> <code>catalog_name</code>               (<code>str</code>)           \u2013            <p>Name of the catalog to check.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the catalog exists, False otherwise.</p> </li> </ul> Check if a catalog exists <pre><code># Check if 'my_catalog' exists\nsession.catalog.does_catalog_exist('my_catalog')\n# Returns: True\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef does_catalog_exist(self, catalog_name: str) -&gt; bool:\n    \"\"\"Checks if a catalog with the specified name exists.\n\n    Args:\n        catalog_name (str): Name of the catalog to check.\n\n    Returns:\n        bool: True if the catalog exists, False otherwise.\n\n    Example: Check if a catalog exists\n        ```python\n        # Check if 'my_catalog' exists\n        session.catalog.does_catalog_exist('my_catalog')\n        # Returns: True\n        ```\n    \"\"\"\n    return self.catalog.does_catalog_exist(catalog_name)\n</code></pre>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog.does_database_exist","title":"does_database_exist","text":"<pre><code>does_database_exist(database_name: str) -&gt; bool\n</code></pre> <p>Checks if a database with the specified name exists.</p> <p>Parameters:</p> <ul> <li> <code>database_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative database name to check.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the database exists, False otherwise.</p> </li> </ul> Check if a database exists <pre><code># Check if 'my_database' exists\nsession.catalog.does_database_exist('my_database')\n# Returns: True\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef does_database_exist(self, database_name: str) -&gt; bool:\n    \"\"\"Checks if a database with the specified name exists.\n\n    Args:\n        database_name (str): Fully qualified or relative database name to check.\n\n    Returns:\n        bool: True if the database exists, False otherwise.\n\n    Example: Check if a database exists\n        ```python\n        # Check if 'my_database' exists\n        session.catalog.does_database_exist('my_database')\n        # Returns: True\n        ```\n    \"\"\"\n    return self.catalog.does_database_exist(database_name)\n</code></pre>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog.does_table_exist","title":"does_table_exist","text":"<pre><code>does_table_exist(table_name: str) -&gt; bool\n</code></pre> <p>Checks if a table with the specified name exists.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative table name to check.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the table exists, False otherwise.</p> </li> </ul> Check if a table exists <pre><code># Check if 'my_table' exists\nsession.catalog.does_table_exist('my_table')\n# Returns: True\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef does_table_exist(self, table_name: str) -&gt; bool:\n    \"\"\"Checks if a table with the specified name exists.\n\n    Args:\n        table_name (str): Fully qualified or relative table name to check.\n\n    Returns:\n        bool: True if the table exists, False otherwise.\n\n    Example: Check if a table exists\n        ```python\n        # Check if 'my_table' exists\n        session.catalog.does_table_exist('my_table')\n        # Returns: True\n        ```\n    \"\"\"\n    return self.catalog.does_table_exist(table_name)\n</code></pre>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog.drop_catalog","title":"drop_catalog","text":"<pre><code>drop_catalog(catalog_name: str, ignore_if_not_exists: bool = True) -&gt; bool\n</code></pre> <p>Drops a catalog.</p> <p>Parameters:</p> <ul> <li> <code>catalog_name</code>               (<code>str</code>)           \u2013            <p>Name of the catalog to drop.</p> </li> <li> <code>ignore_if_not_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, silently return if the catalog doesn't exist. If False, raise an error if the catalog doesn't exist. Defaults to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>CatalogNotFoundError</code>             \u2013            <p>If the catalog does not exist and ignore_if_not_exists is False</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the catalog was dropped successfully, False if the catalog</p> </li> <li> <code>bool</code>           \u2013            <p>didn't exist and ignore_if_not_exists is True.</p> </li> </ul> Drop a non-existent catalog <pre><code># Try to drop a non-existent catalog\nsession.catalog.drop_catalog('my_catalog')\n# Returns: False\n</code></pre> Drop a non-existent catalog without ignore_if_not_exists <pre><code># Try to drop a non-existent catalog with ignore_if_not_exists=False\nsession.catalog.drop_catalog('my_catalog', ignore_if_not_exists=False)\n# Raises: CatalogNotFoundError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef drop_catalog(\n    self, catalog_name: str, ignore_if_not_exists: bool = True\n) -&gt; bool:\n    \"\"\"Drops a catalog.\n\n    Args:\n        catalog_name (str): Name of the catalog to drop.\n        ignore_if_not_exists (bool): If True, silently return if the catalog doesn't exist.\n            If False, raise an error if the catalog doesn't exist.\n            Defaults to True.\n\n    Raises:\n        CatalogNotFoundError: If the catalog does not exist and ignore_if_not_exists is False\n\n    Returns:\n        bool: True if the catalog was dropped successfully, False if the catalog\n        didn't exist and ignore_if_not_exists is True.\n\n    Example: Drop a non-existent catalog\n        ```python\n        # Try to drop a non-existent catalog\n        session.catalog.drop_catalog('my_catalog')\n        # Returns: False\n        ```\n\n    Example: Drop a non-existent catalog without ignore_if_not_exists\n        ```python\n        # Try to drop a non-existent catalog with ignore_if_not_exists=False\n        session.catalog.drop_catalog('my_catalog', ignore_if_not_exists=False)\n        # Raises: CatalogNotFoundError\n        ```\n    \"\"\"\n    return self.catalog.drop_catalog(catalog_name, ignore_if_not_exists)\n</code></pre>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog.drop_database","title":"drop_database","text":"<pre><code>drop_database(database_name: str, cascade: bool = False, ignore_if_not_exists: bool = True) -&gt; bool\n</code></pre> <p>Drops a database.</p> <p>Parameters:</p> <ul> <li> <code>database_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative database name to drop.</p> </li> <li> <code>cascade</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, drop all tables in the database. Defaults to False.</p> </li> <li> <code>ignore_if_not_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, silently return if the database doesn't exist. If False, raise an error if the database doesn't exist. Defaults to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatabaseNotFoundError</code>             \u2013            <p>If the database does not exist and ignore_if_not_exists is False</p> </li> <li> <code>CatalogError</code>             \u2013            <p>If the current database is being dropped, if the database is not empty and cascade is False</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the database was dropped successfully, False if the database</p> </li> <li> <code>bool</code>           \u2013            <p>didn't exist and ignore_if_not_exists is True.</p> </li> </ul> Drop a non-existent database <pre><code># Try to drop a non-existent database\nsession.catalog.drop_database('my_database')\n# Returns: False\n</code></pre> Drop a non-existent database without ignore_if_not_exists <pre><code># Try to drop a non-existent database with ignore_if_not_exists=False\nsession.catalog.drop_database('my_database', ignore_if_not_exists=False)\n# Raises: DatabaseNotFoundError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef drop_database(\n    self,\n    database_name: str,\n    cascade: bool = False,\n    ignore_if_not_exists: bool = True,\n) -&gt; bool:\n    \"\"\"Drops a database.\n\n    Args:\n        database_name (str): Fully qualified or relative database name to drop.\n        cascade (bool): If True, drop all tables in the database.\n            Defaults to False.\n        ignore_if_not_exists (bool): If True, silently return if the database doesn't exist.\n            If False, raise an error if the database doesn't exist.\n            Defaults to True.\n\n    Raises:\n        DatabaseNotFoundError: If the database does not exist and ignore_if_not_exists is False\n        CatalogError: If the current database is being dropped, if the database is not empty and cascade is False\n\n    Returns:\n        bool: True if the database was dropped successfully, False if the database\n        didn't exist and ignore_if_not_exists is True.\n\n    Example: Drop a non-existent database\n        ```python\n        # Try to drop a non-existent database\n        session.catalog.drop_database('my_database')\n        # Returns: False\n        ```\n\n    Example: Drop a non-existent database without ignore_if_not_exists\n        ```python\n        # Try to drop a non-existent database with ignore_if_not_exists=False\n        session.catalog.drop_database('my_database', ignore_if_not_exists=False)\n        # Raises: DatabaseNotFoundError\n        ```\n    \"\"\"\n    return self.catalog.drop_database(database_name, cascade, ignore_if_not_exists)\n</code></pre>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog.drop_table","title":"drop_table","text":"<pre><code>drop_table(table_name: str, ignore_if_not_exists: bool = True) -&gt; bool\n</code></pre> <p>Drops the specified table.</p> <p>By default this method will return False if the table doesn't exist.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative table name to drop.</p> </li> <li> <code>ignore_if_not_exists</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, return False when the table doesn't exist. If False, raise an error when the table doesn't exist. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the table was dropped successfully, False if the table</p> </li> <li> <code>bool</code>           \u2013            <p>didn't exist and ignore_if_not_exist is True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TableNotFoundError</code>             \u2013            <p>If the table doesn't exist and ignore_if_not_exists is False</p> </li> </ul> Drop an existing table <pre><code># Drop an existing table 't1'\nsession.catalog.drop_table('t1')\n# Returns: True\n</code></pre> Drop a non-existent table with ignore_if_not_exists <pre><code># Try to drop a non-existent table with ignore_if_not_exists=True\nsession.catalog.drop_table('t2', ignore_if_not_exists=True)\n# Returns: False\n</code></pre> Drop a non-existent table without ignore_if_not_exists <pre><code># Try to drop a non-existent table with ignore_if_not_exists=False\nsession.catalog.drop_table('t2', ignore_if_not_exists=False)\n# Raises: TableNotFoundError\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef drop_table(self, table_name: str, ignore_if_not_exists: bool = True) -&gt; bool:\n    \"\"\"Drops the specified table.\n\n    By default this method will return False if the table doesn't exist.\n\n    Args:\n        table_name (str): Fully qualified or relative table name to drop.\n        ignore_if_not_exists (bool): If True, return False when the table doesn't exist.\n            If False, raise an error when the table doesn't exist.\n            Defaults to True.\n\n    Returns:\n        bool: True if the table was dropped successfully, False if the table\n        didn't exist and ignore_if_not_exist is True.\n\n    Raises:\n        TableNotFoundError: If the table doesn't exist and ignore_if_not_exists is False\n\n    Example: Drop an existing table\n        ```python\n        # Drop an existing table 't1'\n        session.catalog.drop_table('t1')\n        # Returns: True\n        ```\n\n    Example: Drop a non-existent table with ignore_if_not_exists\n        ```python\n        # Try to drop a non-existent table with ignore_if_not_exists=True\n        session.catalog.drop_table('t2', ignore_if_not_exists=True)\n        # Returns: False\n        ```\n\n    Example: Drop a non-existent table without ignore_if_not_exists\n        ```python\n        # Try to drop a non-existent table with ignore_if_not_exists=False\n        session.catalog.drop_table('t2', ignore_if_not_exists=False)\n        # Raises: TableNotFoundError\n        ```\n    \"\"\"\n    return self.catalog.drop_table(table_name, ignore_if_not_exists)\n</code></pre>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog.get_current_catalog","title":"get_current_catalog","text":"<pre><code>get_current_catalog() -&gt; str\n</code></pre> <p>Returns the name of the current catalog.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The name of the current catalog.</p> </li> </ul> Get current catalog name <pre><code># Get the name of the current catalog\nsession.catalog.get_current_catalog()\n# Returns: 'default'\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def get_current_catalog(self) -&gt; str:\n    \"\"\"Returns the name of the current catalog.\n\n    Returns:\n        str: The name of the current catalog.\n\n    Example: Get current catalog name\n        ```python\n        # Get the name of the current catalog\n        session.catalog.get_current_catalog()\n        # Returns: 'default'\n        ```\n    \"\"\"\n    return self.catalog.get_current_catalog()\n</code></pre>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog.get_current_database","title":"get_current_database","text":"<pre><code>get_current_database() -&gt; str\n</code></pre> <p>Returns the name of the current database in the current catalog.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The name of the current database.</p> </li> </ul> Get current database name <pre><code># Get the name of the current database\nsession.catalog.get_current_database()\n# Returns: 'default'\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def get_current_database(self) -&gt; str:\n    \"\"\"Returns the name of the current database in the current catalog.\n\n    Returns:\n        str: The name of the current database.\n\n    Example: Get current database name\n        ```python\n        # Get the name of the current database\n        session.catalog.get_current_database()\n        # Returns: 'default'\n        ```\n    \"\"\"\n    return self.catalog.get_current_database()\n</code></pre>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog.list_catalogs","title":"list_catalogs","text":"<pre><code>list_catalogs() -&gt; List[str]\n</code></pre> <p>Returns a list of available catalogs.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of catalog names available in the system.</p> </li> <li> <code>List[str]</code>           \u2013            <p>Returns an empty list if no catalogs are found.</p> </li> </ul> List all catalogs <pre><code># Get all available catalogs\nsession.catalog.list_catalogs()\n# Returns: ['default', 'my_catalog', 'other_catalog']\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def list_catalogs(self) -&gt; List[str]:\n    \"\"\"Returns a list of available catalogs.\n\n    Returns:\n        List[str]: A list of catalog names available in the system.\n        Returns an empty list if no catalogs are found.\n\n    Example: List all catalogs\n        ```python\n        # Get all available catalogs\n        session.catalog.list_catalogs()\n        # Returns: ['default', 'my_catalog', 'other_catalog']\n        ```\n    \"\"\"\n    return self.catalog.list_catalogs()\n</code></pre>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog.list_databases","title":"list_databases","text":"<pre><code>list_databases() -&gt; List[str]\n</code></pre> <p>Returns a list of databases in the current catalog.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of database names in the current catalog.</p> </li> <li> <code>List[str]</code>           \u2013            <p>Returns an empty list if no databases are found.</p> </li> </ul> List all databases <pre><code># Get all databases in the current catalog\nsession.catalog.list_databases()\n# Returns: ['default', 'my_database', 'other_database']\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def list_databases(self) -&gt; List[str]:\n    \"\"\"Returns a list of databases in the current catalog.\n\n    Returns:\n        List[str]: A list of database names in the current catalog.\n        Returns an empty list if no databases are found.\n\n    Example: List all databases\n        ```python\n        # Get all databases in the current catalog\n        session.catalog.list_databases()\n        # Returns: ['default', 'my_database', 'other_database']\n        ```\n    \"\"\"\n    return self.catalog.list_databases()\n</code></pre>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog.list_tables","title":"list_tables","text":"<pre><code>list_tables() -&gt; List[str]\n</code></pre> <p>Returns a list of tables stored in the current database.</p> <p>This method queries the current database to retrieve all available table names.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of table names stored in the database.</p> </li> <li> <code>List[str]</code>           \u2013            <p>Returns an empty list if no tables are found.</p> </li> </ul> List all tables <pre><code># Get all tables in the current database\nsession.catalog.list_tables()\n# Returns: ['table1', 'table2', 'table3']\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>def list_tables(self) -&gt; List[str]:\n    \"\"\"Returns a list of tables stored in the current database.\n\n    This method queries the current database to retrieve all available table names.\n\n    Returns:\n        List[str]: A list of table names stored in the database.\n        Returns an empty list if no tables are found.\n\n    Example: List all tables\n        ```python\n        # Get all tables in the current database\n        session.catalog.list_tables()\n        # Returns: ['table1', 'table2', 'table3']\n        ```\n    \"\"\"\n    return self.catalog.list_tables()\n</code></pre>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog.set_current_catalog","title":"set_current_catalog","text":"<pre><code>set_current_catalog(catalog_name: str) -&gt; None\n</code></pre> <p>Sets the current catalog.</p> <p>Parameters:</p> <ul> <li> <code>catalog_name</code>               (<code>str</code>)           \u2013            <p>Name of the catalog to set as current.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the specified catalog doesn't exist.</p> </li> </ul> Set current catalog <pre><code># Set 'my_catalog' as the current catalog\nsession.catalog.set_current_catalog('my_catalog')\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef set_current_catalog(self, catalog_name: str) -&gt; None:\n    \"\"\"Sets the current catalog.\n\n    Args:\n        catalog_name (str): Name of the catalog to set as current.\n\n    Raises:\n        ValueError: If the specified catalog doesn't exist.\n\n    Example: Set current catalog\n        ```python\n        # Set 'my_catalog' as the current catalog\n        session.catalog.set_current_catalog('my_catalog')\n        ```\n    \"\"\"\n    self.catalog.set_current_catalog(catalog_name)\n</code></pre>"},{"location":"reference/fenic/api/catalog/#fenic.api.catalog.Catalog.set_current_database","title":"set_current_database","text":"<pre><code>set_current_database(database_name: str) -&gt; None\n</code></pre> <p>Sets the current database.</p> <p>Parameters:</p> <ul> <li> <code>database_name</code>               (<code>str</code>)           \u2013            <p>Fully qualified or relative database name to set as current.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DatabaseNotFoundError</code>             \u2013            <p>If the specified database doesn't exist.</p> </li> </ul> Set current database <pre><code># Set 'my_database' as the current database\nsession.catalog.set_current_database('my_database')\n</code></pre> Source code in <code>src/fenic/api/catalog.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef set_current_database(self, database_name: str) -&gt; None:\n    \"\"\"Sets the current database.\n\n    Args:\n        database_name (str): Fully qualified or relative database name to set as current.\n\n    Raises:\n        DatabaseNotFoundError: If the specified database doesn't exist.\n\n    Example: Set current database\n        ```python\n        # Set 'my_database' as the current database\n        session.catalog.set_current_database('my_database')\n        ```\n    \"\"\"\n    self.catalog.set_current_database(database_name)\n</code></pre>"},{"location":"reference/fenic/api/column/","title":"fenic.api.column","text":""},{"location":"reference/fenic/api/column/#fenic.api.column","title":"fenic.api.column","text":"<p>Column API for Fenic DataFrames - represents column expressions and operations.</p> <p>Classes:</p> <ul> <li> <code>Column</code>           \u2013            <p>A column expression in a DataFrame.</p> </li> </ul>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column","title":"Column","text":"<p>A column expression in a DataFrame.</p> <p>This class represents a column expression that can be used in DataFrame operations. It provides methods for accessing, transforming, and combining column data.</p> Create a column reference <pre><code># Reference a column by name using col() function\ncol(\"column_name\")\n</code></pre> Use column in operations <pre><code># Perform arithmetic operations\ndf.select(col(\"price\") * col(\"quantity\"))\n</code></pre> Chain column operations <pre><code># Chain multiple operations\ndf.select(col(\"name\").upper().contains(\"John\"))\n</code></pre> <p>Methods:</p> <ul> <li> <code>alias</code>             \u2013              <p>Create an alias for this column.</p> </li> <li> <code>asc</code>             \u2013              <p>Apply ascending order to this column during a dataframe sort or order_by.</p> </li> <li> <code>asc_nulls_first</code>             \u2013              <p>Apply ascending order putting nulls first to this column during a dataframe sort or order_by.</p> </li> <li> <code>asc_nulls_last</code>             \u2013              <p>Apply ascending order putting nulls last to this column during a dataframe sort or order_by.</p> </li> <li> <code>cast</code>             \u2013              <p>Cast the column to a new data type.</p> </li> <li> <code>contains</code>             \u2013              <p>Check if the column contains a substring.</p> </li> <li> <code>contains_any</code>             \u2013              <p>Check if the column contains any of the specified substrings.</p> </li> <li> <code>desc</code>             \u2013              <p>Apply descending order to this column during a dataframe sort or order_by.</p> </li> <li> <code>desc_nulls_first</code>             \u2013              <p>Apply descending order putting nulls first to this column during a dataframe sort or order_by.</p> </li> <li> <code>desc_nulls_last</code>             \u2013              <p>Apply descending order putting nulls last to this column during a dataframe sort or order_by.</p> </li> <li> <code>ends_with</code>             \u2013              <p>Check if the column ends with a substring.</p> </li> <li> <code>get_item</code>             \u2013              <p>Access an item in a struct or array column.</p> </li> <li> <code>ilike</code>             \u2013              <p>Check if the column matches a SQL LIKE pattern (case-insensitive).</p> </li> <li> <code>is_in</code>             \u2013              <p>Check if the column is in a list of values or a column expression.</p> </li> <li> <code>is_not_null</code>             \u2013              <p>Check if the column contains non-NULL values.</p> </li> <li> <code>is_null</code>             \u2013              <p>Check if the column contains NULL values.</p> </li> <li> <code>like</code>             \u2013              <p>Check if the column matches a SQL LIKE pattern.</p> </li> <li> <code>otherwise</code>             \u2013              <p>Evaluates a list of conditions and returns one of multiple possible result expressions.</p> </li> <li> <code>rlike</code>             \u2013              <p>Check if the column matches a regular expression pattern.</p> </li> <li> <code>starts_with</code>             \u2013              <p>Check if the column starts with a substring.</p> </li> <li> <code>when</code>             \u2013              <p>Evaluates a list of conditions and returns one of multiple possible result expressions.</p> </li> </ul>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.alias","title":"alias","text":"<pre><code>alias(name: str) -&gt; Column\n</code></pre> <p>Create an alias for this column.</p> <p>This method assigns a new name to the column expression, which is useful for renaming columns or providing names for complex expressions.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The alias name to assign</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>Column with the specified alias</p> </li> </ul> Rename a column <pre><code># Rename a column to a new name\ndf.select(col(\"original_name\").alias(\"new_name\"))\n</code></pre> Name a complex expression <pre><code># Give a name to a calculated column\ndf.select((col(\"price\") * col(\"quantity\")).alias(\"total_value\"))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def alias(self, name: str) -&gt; Column:\n    \"\"\"Create an alias for this column.\n\n    This method assigns a new name to the column expression, which is useful\n    for renaming columns or providing names for complex expressions.\n\n    Args:\n        name (str): The alias name to assign\n\n    Returns:\n        Column: Column with the specified alias\n\n    Example: Rename a column\n        ```python\n        # Rename a column to a new name\n        df.select(col(\"original_name\").alias(\"new_name\"))\n        ```\n\n    Example: Name a complex expression\n        ```python\n        # Give a name to a calculated column\n        df.select((col(\"price\") * col(\"quantity\")).alias(\"total_value\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(AliasExpr(self._logical_expr, name))\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.asc","title":"asc","text":"<pre><code>asc() -&gt; Column\n</code></pre> <p>Apply ascending order to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in ascending order <pre><code># Sort a dataframe by age in ascending order\ndf.sort(col(\"age\").asc()).show()\n</code></pre> Sort using column reference <pre><code># Sort using column reference with ascending order\ndf.sort(col(\"age\").asc()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def asc(self) -&gt; Column:\n    \"\"\"Apply ascending order to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function.\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in ascending order\n        ```python\n        # Sort a dataframe by age in ascending order\n        df.sort(col(\"age\").asc()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        # Sort using column reference with ascending order\n        df.sort(col(\"age\").asc()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(SortExpr(self._logical_expr, ascending=True))\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.asc_nulls_first","title":"asc_nulls_first","text":"<pre><code>asc_nulls_first() -&gt; Column\n</code></pre> <p>Apply ascending order putting nulls first to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in ascending order with nulls first <pre><code># Sort a dataframe by age in ascending order, with nulls appearing first\ndf.sort(col(\"age\").asc_nulls_first()).show()\n</code></pre> Sort using column reference <pre><code># Sort using column reference with ascending order and nulls first\ndf.sort(col(\"age\").asc_nulls_first()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def asc_nulls_first(self) -&gt; Column:\n    \"\"\"Apply ascending order putting nulls first to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function.\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in ascending order with nulls first\n        ```python\n        # Sort a dataframe by age in ascending order, with nulls appearing first\n        df.sort(col(\"age\").asc_nulls_first()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        # Sort using column reference with ascending order and nulls first\n        df.sort(col(\"age\").asc_nulls_first()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        SortExpr(self._logical_expr, ascending=True, nulls_last=False)\n    )\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.asc_nulls_last","title":"asc_nulls_last","text":"<pre><code>asc_nulls_last() -&gt; Column\n</code></pre> <p>Apply ascending order putting nulls last to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in ascending order with nulls last <pre><code># Sort a dataframe by age in ascending order, with nulls appearing last\ndf.sort(col(\"age\").asc_nulls_last()).show()\n</code></pre> Sort using column reference <pre><code># Sort using column reference with ascending order and nulls last\ndf.sort(col(\"age\").asc_nulls_last()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def asc_nulls_last(self) -&gt; Column:\n    \"\"\"Apply ascending order putting nulls last to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function.\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in ascending order with nulls last\n        ```python\n        # Sort a dataframe by age in ascending order, with nulls appearing last\n        df.sort(col(\"age\").asc_nulls_last()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        # Sort using column reference with ascending order and nulls last\n        df.sort(col(\"age\").asc_nulls_last()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        SortExpr(self._logical_expr, ascending=True, nulls_last=True)\n    )\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.cast","title":"cast","text":"<pre><code>cast(data_type: DataType) -&gt; Column\n</code></pre> <p>Cast the column to a new data type.</p> <p>This method creates an expression that casts the column to a specified data type. The casting behavior depends on the source and target types:</p> <p>Primitive type casting:</p> <ul> <li>Numeric types (IntegerType, FloatType, DoubleType) can be cast between each other</li> <li>Numeric types can be cast to/from StringType</li> <li>BooleanType can be cast to/from numeric types and StringType</li> <li>StringType cannot be directly cast to BooleanType (will raise TypeError)</li> </ul> <p>Complex type casting:</p> <ul> <li>ArrayType can only be cast to another ArrayType (with castable element types)</li> <li>StructType can only be cast to another StructType (with matching/castable fields)</li> <li>Primitive types cannot be cast to/from complex types</li> </ul> <p>Parameters:</p> <ul> <li> <code>data_type</code>               (<code>DataType</code>)           \u2013            <p>The target DataType to cast the column to</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column representing the casted expression</p> </li> </ul> Cast integer to string <pre><code># Convert an integer column to string type\ndf.select(col(\"int_col\").cast(StringType))\n</code></pre> Cast array of integers to array of strings <pre><code># Convert an array of integers to an array of strings\ndf.select(col(\"int_array\").cast(ArrayType(element_type=StringType)))\n</code></pre> Cast struct fields to different types <pre><code># Convert struct fields to different types\nnew_type = StructType([\n    StructField(\"id\", StringType),\n    StructField(\"value\", FloatType)\n])\ndf.select(col(\"data_struct\").cast(new_type))\n</code></pre> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the requested cast operation is not supported</p> </li> </ul> Source code in <code>src/fenic/api/column.py</code> <pre><code>def cast(self, data_type: DataType) -&gt; Column:\n    \"\"\"Cast the column to a new data type.\n\n    This method creates an expression that casts the column to a specified data type.\n    The casting behavior depends on the source and target types:\n\n    Primitive type casting:\n\n    - Numeric types (IntegerType, FloatType, DoubleType) can be cast between each other\n    - Numeric types can be cast to/from StringType\n    - BooleanType can be cast to/from numeric types and StringType\n    - StringType cannot be directly cast to BooleanType (will raise TypeError)\n\n    Complex type casting:\n\n    - ArrayType can only be cast to another ArrayType (with castable element types)\n    - StructType can only be cast to another StructType (with matching/castable fields)\n    - Primitive types cannot be cast to/from complex types\n\n    Args:\n        data_type (DataType): The target DataType to cast the column to\n\n    Returns:\n        Column: A Column representing the casted expression\n\n    Example: Cast integer to string\n        ```python\n        # Convert an integer column to string type\n        df.select(col(\"int_col\").cast(StringType))\n        ```\n\n    Example: Cast array of integers to array of strings\n        ```python\n        # Convert an array of integers to an array of strings\n        df.select(col(\"int_array\").cast(ArrayType(element_type=StringType)))\n        ```\n\n    Example: Cast struct fields to different types\n        ```python\n        # Convert struct fields to different types\n        new_type = StructType([\n            StructField(\"id\", StringType),\n            StructField(\"value\", FloatType)\n        ])\n        df.select(col(\"data_struct\").cast(new_type))\n        ```\n\n    Raises:\n        TypeError: If the requested cast operation is not supported\n    \"\"\"\n    return Column._from_logical_expr(CastExpr(self._logical_expr, data_type))\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.contains","title":"contains","text":"<pre><code>contains(other: Union[str, Column]) -&gt; Column\n</code></pre> <p>Check if the column contains a substring.</p> <p>This method creates a boolean expression that checks if each value in the column contains the specified substring. The substring can be either a literal string or a column expression.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[str, Column]</code>)           \u2013            <p>The substring to search for (can be a string or column expression)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value contains the substring</p> </li> </ul> Find rows where name contains \"john\" <pre><code># Filter rows where the name column contains \"john\"\ndf.filter(col(\"name\").contains(\"john\"))\n</code></pre> Find rows where text contains a dynamic pattern <pre><code># Filter rows where text contains a value from another column\ndf.filter(col(\"text\").contains(col(\"pattern\")))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def contains(self, other: Union[str, Column]) -&gt; Column:\n    \"\"\"Check if the column contains a substring.\n\n    This method creates a boolean expression that checks if each value in the column\n    contains the specified substring. The substring can be either a literal string\n    or a column expression.\n\n    Args:\n        other (Union[str, Column]): The substring to search for (can be a string or column expression)\n\n    Returns:\n        Column: A boolean column indicating whether each value contains the substring\n\n    Example: Find rows where name contains \"john\"\n        ```python\n        # Filter rows where the name column contains \"john\"\n        df.filter(col(\"name\").contains(\"john\"))\n        ```\n\n    Example: Find rows where text contains a dynamic pattern\n        ```python\n        # Filter rows where text contains a value from another column\n        df.filter(col(\"text\").contains(col(\"pattern\")))\n        ```\n    \"\"\"\n    if isinstance(other, str):\n        return Column._from_logical_expr(ContainsExpr(self._logical_expr, other))\n    else:\n        return Column._from_logical_expr(\n            ContainsExpr(self._logical_expr, other._logical_expr)\n        )\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.contains_any","title":"contains_any","text":"<pre><code>contains_any(others: List[str], case_insensitive: bool = True) -&gt; Column\n</code></pre> <p>Check if the column contains any of the specified substrings.</p> <p>This method creates a boolean expression that checks if each value in the column contains any of the specified substrings. The matching can be case-sensitive or case-insensitive.</p> <p>Parameters:</p> <ul> <li> <code>others</code>               (<code>List[str]</code>)           \u2013            <p>List of substrings to search for</p> </li> <li> <code>case_insensitive</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to perform case-insensitive matching (default: True)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value contains any substring</p> </li> </ul> Find rows where name contains \"john\" or \"jane\" (case-insensitive) <pre><code># Filter rows where name contains either \"john\" or \"jane\"\ndf.filter(col(\"name\").contains_any([\"john\", \"jane\"]))\n</code></pre> Case-sensitive matching <pre><code># Filter rows with case-sensitive matching\ndf.filter(col(\"name\").contains_any([\"John\", \"Jane\"], case_insensitive=False))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def contains_any(self, others: List[str], case_insensitive: bool = True) -&gt; Column:\n    \"\"\"Check if the column contains any of the specified substrings.\n\n    This method creates a boolean expression that checks if each value in the column\n    contains any of the specified substrings. The matching can be case-sensitive or\n    case-insensitive.\n\n    Args:\n        others (List[str]): List of substrings to search for\n        case_insensitive (bool): Whether to perform case-insensitive matching (default: True)\n\n    Returns:\n        Column: A boolean column indicating whether each value contains any substring\n\n    Example: Find rows where name contains \"john\" or \"jane\" (case-insensitive)\n        ```python\n        # Filter rows where name contains either \"john\" or \"jane\"\n        df.filter(col(\"name\").contains_any([\"john\", \"jane\"]))\n        ```\n\n    Example: Case-sensitive matching\n        ```python\n        # Filter rows with case-sensitive matching\n        df.filter(col(\"name\").contains_any([\"John\", \"Jane\"], case_insensitive=False))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        ContainsAnyExpr(self._logical_expr, others, case_insensitive)\n    )\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.desc","title":"desc","text":"<pre><code>desc() -&gt; Column\n</code></pre> <p>Apply descending order to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in descending order <pre><code># Sort a dataframe by age in descending order\ndf.sort(col(\"age\").desc()).show()\n</code></pre> Sort using column reference <pre><code># Sort using column reference with descending order\ndf.sort(col(\"age\").desc()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def desc(self) -&gt; Column:\n    \"\"\"Apply descending order to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function.\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in descending order\n        ```python\n        # Sort a dataframe by age in descending order\n        df.sort(col(\"age\").desc()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        # Sort using column reference with descending order\n        df.sort(col(\"age\").desc()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(SortExpr(self._logical_expr, ascending=False))\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.desc_nulls_first","title":"desc_nulls_first","text":"<pre><code>desc_nulls_first() -&gt; Column\n</code></pre> <p>Apply descending order putting nulls first to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in descending order with nulls first <pre><code>df.sort(col(\"age\").desc_nulls_first()).show()\n</code></pre> Sort using column reference <pre><code>df.sort(col(\"age\").desc_nulls_first()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def desc_nulls_first(self) -&gt; Column:\n    \"\"\"Apply descending order putting nulls first to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in descending order with nulls first\n        ```python\n        df.sort(col(\"age\").desc_nulls_first()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        df.sort(col(\"age\").desc_nulls_first()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        SortExpr(self._logical_expr, ascending=False, nulls_last=False)\n    )\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.desc_nulls_last","title":"desc_nulls_last","text":"<pre><code>desc_nulls_last() -&gt; Column\n</code></pre> <p>Apply descending order putting nulls last to this column during a dataframe sort or order_by.</p> <p>This method creates an expression that provides a column and sort order to the sort function.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression that provides a column and sort order to the sort function</p> </li> </ul> Sort by age in descending order with nulls last <pre><code># Sort a dataframe by age in descending order, with nulls appearing last\ndf.sort(col(\"age\").desc_nulls_last()).show()\n</code></pre> Sort using column reference <pre><code># Sort using column reference with descending order and nulls last\ndf.sort(col(\"age\").desc_nulls_last()).show()\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def desc_nulls_last(self) -&gt; Column:\n    \"\"\"Apply descending order putting nulls last to this column during a dataframe sort or order_by.\n\n    This method creates an expression that provides a column and sort order to the sort function.\n\n    Returns:\n        Column: A Column expression that provides a column and sort order to the sort function\n\n    Example: Sort by age in descending order with nulls last\n        ```python\n        # Sort a dataframe by age in descending order, with nulls appearing last\n        df.sort(col(\"age\").desc_nulls_last()).show()\n        ```\n\n    Example: Sort using column reference\n        ```python\n        # Sort using column reference with descending order and nulls last\n        df.sort(col(\"age\").desc_nulls_last()).show()\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        SortExpr(self._logical_expr, ascending=False, nulls_last=True)\n    )\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.ends_with","title":"ends_with","text":"<pre><code>ends_with(other: Union[str, Column]) -&gt; Column\n</code></pre> <p>Check if the column ends with a substring.</p> <p>This method creates a boolean expression that checks if each value in the column ends with the specified substring. The substring can be either a literal string or a column expression.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[str, Column]</code>)           \u2013            <p>The substring to check for at the end (can be a string or column expression)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value ends with the substring</p> </li> </ul> Find rows where email ends with \"@gmail.com\" <pre><code>df.filter(col(\"email\").ends_with(\"@gmail.com\"))\n</code></pre> Find rows where text ends with a dynamic pattern <pre><code>df.filter(col(\"text\").ends_with(col(\"suffix\")))\n</code></pre> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the substring ends with a regular expression anchor ($)</p> </li> </ul> Source code in <code>src/fenic/api/column.py</code> <pre><code>def ends_with(self, other: Union[str, Column]) -&gt; Column:\n    \"\"\"Check if the column ends with a substring.\n\n    This method creates a boolean expression that checks if each value in the column\n    ends with the specified substring. The substring can be either a literal string\n    or a column expression.\n\n    Args:\n        other (Union[str, Column]): The substring to check for at the end (can be a string or column expression)\n\n    Returns:\n        Column: A boolean column indicating whether each value ends with the substring\n\n    Example: Find rows where email ends with \"@gmail.com\"\n        ```python\n        df.filter(col(\"email\").ends_with(\"@gmail.com\"))\n        ```\n\n    Example: Find rows where text ends with a dynamic pattern\n        ```python\n        df.filter(col(\"text\").ends_with(col(\"suffix\")))\n        ```\n\n    Raises:\n        ValueError: If the substring ends with a regular expression anchor ($)\n    \"\"\"\n    if isinstance(other, str):\n        return Column._from_logical_expr(EndsWithExpr(self._logical_expr, other))\n    else:\n        return Column._from_logical_expr(\n            EndsWithExpr(self._logical_expr, other._logical_expr)\n        )\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.get_item","title":"get_item","text":"<pre><code>get_item(key: Union[str, int]) -&gt; Column\n</code></pre> <p>Access an item in a struct or array column.</p> <p>This method allows accessing elements in complex data types:</p> <ul> <li>For array columns, the key should be an integer index</li> <li>For struct columns, the key should be a field name</li> </ul> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>Union[str, int]</code>)           \u2013            <p>The index (for arrays) or field name (for structs) to access</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column representing the accessed item</p> </li> </ul> Access an array element <pre><code># Get the first element from an array column\ndf.select(col(\"array_column\").get_item(0))\n</code></pre> Access a struct field <pre><code># Get a field from a struct column\ndf.select(col(\"struct_column\").get_item(\"field_name\"))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def get_item(self, key: Union[str, int]) -&gt; Column:\n    \"\"\"Access an item in a struct or array column.\n\n    This method allows accessing elements in complex data types:\n\n    - For array columns, the key should be an integer index\n    - For struct columns, the key should be a field name\n\n    Args:\n        key (Union[str, int]): The index (for arrays) or field name (for structs) to access\n\n    Returns:\n        Column: A Column representing the accessed item\n\n    Example: Access an array element\n        ```python\n        # Get the first element from an array column\n        df.select(col(\"array_column\").get_item(0))\n        ```\n\n    Example: Access a struct field\n        ```python\n        # Get a field from a struct column\n        df.select(col(\"struct_column\").get_item(\"field_name\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(IndexExpr(self._logical_expr, key))\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.ilike","title":"ilike","text":"<pre><code>ilike(other: str) -&gt; Column\n</code></pre> <p>Check if the column matches a SQL LIKE pattern (case-insensitive).</p> <p>This method creates a boolean expression that checks if each value in the column matches the specified SQL LIKE pattern, ignoring case. The pattern must be a literal string and cannot be a column expression.</p> <p>SQL LIKE pattern syntax:</p> <ul> <li>% matches any sequence of characters</li> <li>_ matches any single character</li> </ul> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>str</code>)           \u2013            <p>The SQL LIKE pattern to match against</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value matches the pattern</p> </li> </ul> Find rows where name starts with \"j\" and ends with \"n\" (case-insensitive) <pre><code># Filter rows where name matches the pattern \"j%n\" (case-insensitive)\ndf.filter(col(\"name\").ilike(\"j%n\"))\n</code></pre> Find rows where code matches pattern (case-insensitive) <pre><code># Filter rows where code matches the pattern \"a_b%\" (case-insensitive)\ndf.filter(col(\"code\").ilike(\"a_b%\"))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def ilike(self, other: str) -&gt; Column:\n    r\"\"\"Check if the column matches a SQL LIKE pattern (case-insensitive).\n\n    This method creates a boolean expression that checks if each value in the column\n    matches the specified SQL LIKE pattern, ignoring case. The pattern must be a literal string\n    and cannot be a column expression.\n\n    SQL LIKE pattern syntax:\n\n    - % matches any sequence of characters\n    - _ matches any single character\n\n    Args:\n        other (str): The SQL LIKE pattern to match against\n\n    Returns:\n        Column: A boolean column indicating whether each value matches the pattern\n\n    Example: Find rows where name starts with \"j\" and ends with \"n\" (case-insensitive)\n        ```python\n        # Filter rows where name matches the pattern \"j%n\" (case-insensitive)\n        df.filter(col(\"name\").ilike(\"j%n\"))\n        ```\n\n    Example: Find rows where code matches pattern (case-insensitive)\n        ```python\n        # Filter rows where code matches the pattern \"a_b%\" (case-insensitive)\n        df.filter(col(\"code\").ilike(\"a_b%\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(ILikeExpr(self._logical_expr, other))\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.is_in","title":"is_in","text":"<pre><code>is_in(other: Union[List[Any], ColumnOrName]) -&gt; Column\n</code></pre> <p>Check if the column is in a list of values or a column expression.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[List[Any], ColumnOrName]</code>)           \u2013            <p>A list of values or a Column expression</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression representing whether each element of Column is in the list</p> </li> </ul> Check if name is in a list of values <pre><code># Filter rows where name is in a list of values\ndf.filter(col(\"name\").is_in([\"Alice\", \"Bob\"]))\n</code></pre> Check if value is in another column <pre><code># Filter rows where name is in another column\ndf.filter(col(\"name\").is_in(col(\"other_column\")))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def is_in(self, other: Union[List[Any], ColumnOrName]) -&gt; Column:\n    \"\"\"Check if the column is in a list of values or a column expression.\n\n    Args:\n        other (Union[List[Any], ColumnOrName]): A list of values or a Column expression\n\n    Returns:\n        Column: A Column expression representing whether each element of Column is in the list\n\n    Example: Check if name is in a list of values\n        ```python\n        # Filter rows where name is in a list of values\n        df.filter(col(\"name\").is_in([\"Alice\", \"Bob\"]))\n        ```\n\n    Example: Check if value is in another column\n        ```python\n        # Filter rows where name is in another column\n        df.filter(col(\"name\").is_in(col(\"other_column\")))\n        ```\n    \"\"\"\n    if isinstance(other, list):\n        try:\n            type_ = infer_dtype_from_pyobj(other)\n            return Column._from_logical_expr(InExpr(self._logical_expr, LiteralExpr(other, type_)))\n        except TypeInferenceError as e:\n            raise ValidationError(f\"Cannot apply IN on {other}. List argument to IN must be be a valid Python List literal.\") from e\n    else:\n        return Column._from_logical_expr(InExpr(self._logical_expr, other._logical_expr))\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.is_not_null","title":"is_not_null","text":"<pre><code>is_not_null() -&gt; Column\n</code></pre> <p>Check if the column contains non-NULL values.</p> <p>This method creates an expression that evaluates to TRUE when the column value is not NULL.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column representing a boolean expression that is TRUE when this column is not NULL</p> </li> </ul> Filter rows where a column is not NULL <pre><code>df.filter(col(\"some_column\").is_not_null())\n</code></pre> Use in a complex condition <pre><code>df.filter(col(\"col1\").is_not_null() &amp; (col(\"col2\") &lt;= 50))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def is_not_null(self) -&gt; Column:\n    \"\"\"Check if the column contains non-NULL values.\n\n    This method creates an expression that evaluates to TRUE when the column value is not NULL.\n\n    Returns:\n        Column: A Column representing a boolean expression that is TRUE when this column is not NULL\n\n    Example: Filter rows where a column is not NULL\n        ```python\n        df.filter(col(\"some_column\").is_not_null())\n        ```\n\n    Example: Use in a complex condition\n        ```python\n        df.filter(col(\"col1\").is_not_null() &amp; (col(\"col2\") &lt;= 50))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(IsNullExpr(self._logical_expr, False))\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.is_null","title":"is_null","text":"<pre><code>is_null() -&gt; Column\n</code></pre> <p>Check if the column contains NULL values.</p> <p>This method creates an expression that evaluates to TRUE when the column value is NULL.</p> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column representing a boolean expression that is TRUE when this column is NULL</p> </li> </ul> Filter rows where a column is NULL <pre><code># Filter rows where some_column is NULL\ndf.filter(col(\"some_column\").is_null())\n</code></pre> Use in a complex condition <pre><code># Filter rows where col1 is NULL or col2 is greater than 100\ndf.filter(col(\"col1\").is_null() | (col(\"col2\") &gt; 100))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def is_null(self) -&gt; Column:\n    \"\"\"Check if the column contains NULL values.\n\n    This method creates an expression that evaluates to TRUE when the column value is NULL.\n\n    Returns:\n        Column: A Column representing a boolean expression that is TRUE when this column is NULL\n\n    Example: Filter rows where a column is NULL\n        ```python\n        # Filter rows where some_column is NULL\n        df.filter(col(\"some_column\").is_null())\n        ```\n\n    Example: Use in a complex condition\n        ```python\n        # Filter rows where col1 is NULL or col2 is greater than 100\n        df.filter(col(\"col1\").is_null() | (col(\"col2\") &gt; 100))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(IsNullExpr(self._logical_expr, True))\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.like","title":"like","text":"<pre><code>like(other: str) -&gt; Column\n</code></pre> <p>Check if the column matches a SQL LIKE pattern.</p> <p>This method creates a boolean expression that checks if each value in the column matches the specified SQL LIKE pattern. The pattern must be a literal string and cannot be a column expression.</p> <p>SQL LIKE pattern syntax:</p> <ul> <li>% matches any sequence of characters</li> <li>_ matches any single character</li> </ul> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>str</code>)           \u2013            <p>The SQL LIKE pattern to match against</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value matches the pattern</p> </li> </ul> Find rows where name starts with \"J\" and ends with \"n\" <pre><code># Filter rows where name matches the pattern \"J%n\"\ndf.filter(col(\"name\").like(\"J%n\"))\n</code></pre> Find rows where code matches specific pattern <pre><code># Filter rows where code matches the pattern \"A_B%\"\ndf.filter(col(\"code\").like(\"A_B%\"))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def like(self, other: str) -&gt; Column:\n    r\"\"\"Check if the column matches a SQL LIKE pattern.\n\n    This method creates a boolean expression that checks if each value in the column\n    matches the specified SQL LIKE pattern. The pattern must be a literal string\n    and cannot be a column expression.\n\n    SQL LIKE pattern syntax:\n\n    - % matches any sequence of characters\n    - _ matches any single character\n\n    Args:\n        other (str): The SQL LIKE pattern to match against\n\n    Returns:\n        Column: A boolean column indicating whether each value matches the pattern\n\n    Example: Find rows where name starts with \"J\" and ends with \"n\"\n        ```python\n        # Filter rows where name matches the pattern \"J%n\"\n        df.filter(col(\"name\").like(\"J%n\"))\n        ```\n\n    Example: Find rows where code matches specific pattern\n        ```python\n        # Filter rows where code matches the pattern \"A_B%\"\n        df.filter(col(\"code\").like(\"A_B%\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(LikeExpr(self._logical_expr, other))\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.otherwise","title":"otherwise","text":"<pre><code>otherwise(value: Column) -&gt; Column\n</code></pre> <p>Evaluates a list of conditions and returns one of multiple possible result expressions.</p> <p>If Column.otherwise() is not invoked, None is returned for unmatched conditions. Otherwise() will return for rows with None inputs.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Column</code>)           \u2013            <p>A literal value or Column expression to return</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression representing whether each element of Column is not matched by any previous conditions</p> </li> </ul> Use when/otherwise for conditional logic <pre><code># Create a DataFrame with age and name columns\ndf = session.createDataFrame(\n    {\"age\": [2, 5]}, {\"name\": [\"Alice\", \"Bob\"]}\n)\n\n# Use when/otherwise to create a case result column\ndf.select(\n    col(\"name\"),\n    when(col(\"age\") &gt; 3, 1).otherwise(0).alias(\"case_result\")\n).show()\n# Output:\n# +-----+-----------+\n# | name|case_result|\n# +-----+-----------+\n# |Alice|          0|\n# |  Bob|          1|\n# +-----+-----------+\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def otherwise(self, value: Column) -&gt; Column:\n    \"\"\"Evaluates a list of conditions and returns one of multiple possible result expressions.\n\n    If Column.otherwise() is not invoked, None is returned for unmatched conditions.\n    Otherwise() will return for rows with None inputs.\n\n    Args:\n        value (Column): A literal value or Column expression to return\n\n    Returns:\n        Column: A Column expression representing whether each element of Column is not matched by any previous conditions\n\n    Example: Use when/otherwise for conditional logic\n        ```python\n        # Create a DataFrame with age and name columns\n        df = session.createDataFrame(\n            {\"age\": [2, 5]}, {\"name\": [\"Alice\", \"Bob\"]}\n        )\n\n        # Use when/otherwise to create a case result column\n        df.select(\n            col(\"name\"),\n            when(col(\"age\") &gt; 3, 1).otherwise(0).alias(\"case_result\")\n        ).show()\n        # Output:\n        # +-----+-----------+\n        # | name|case_result|\n        # +-----+-----------+\n        # |Alice|          0|\n        # |  Bob|          1|\n        # +-----+-----------+\n        ```\n    \"\"\"\n    return Column._from_logical_expr(OtherwiseExpr(self._logical_expr, value._logical_expr))\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.rlike","title":"rlike","text":"<pre><code>rlike(other: str) -&gt; Column\n</code></pre> <p>Check if the column matches a regular expression pattern.</p> <p>This method creates a boolean expression that checks if each value in the column matches the specified regular expression pattern. The pattern must be a literal string and cannot be a column expression.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>str</code>)           \u2013            <p>The regular expression pattern to match against</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value matches the pattern</p> </li> </ul> Find rows where phone number matches pattern <pre><code># Filter rows where phone number matches a specific pattern\ndf.filter(col(\"phone\").rlike(r\"^\\d{3}-\\d{3}-\\d{4}$\"))\n</code></pre> Find rows where text contains word boundaries <pre><code># Filter rows where text contains a word with boundaries\ndf.filter(col(\"text\").rlike(r\"\\bhello\\b\"))\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def rlike(self, other: str) -&gt; Column:\n    r\"\"\"Check if the column matches a regular expression pattern.\n\n    This method creates a boolean expression that checks if each value in the column\n    matches the specified regular expression pattern. The pattern must be a literal string\n    and cannot be a column expression.\n\n    Args:\n        other (str): The regular expression pattern to match against\n\n    Returns:\n        Column: A boolean column indicating whether each value matches the pattern\n\n    Example: Find rows where phone number matches pattern\n        ```python\n        # Filter rows where phone number matches a specific pattern\n        df.filter(col(\"phone\").rlike(r\"^\\d{3}-\\d{3}-\\d{4}$\"))\n        ```\n\n    Example: Find rows where text contains word boundaries\n        ```python\n        # Filter rows where text contains a word with boundaries\n        df.filter(col(\"text\").rlike(r\"\\bhello\\b\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(RLikeExpr(self._logical_expr, other))\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.starts_with","title":"starts_with","text":"<pre><code>starts_with(other: Union[str, Column]) -&gt; Column\n</code></pre> <p>Check if the column starts with a substring.</p> <p>This method creates a boolean expression that checks if each value in the column starts with the specified substring. The substring can be either a literal string or a column expression.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[str, Column]</code>)           \u2013            <p>The substring to check for at the start (can be a string or column expression)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A boolean column indicating whether each value starts with the substring</p> </li> </ul> Find rows where name starts with \"Mr\" <pre><code># Filter rows where name starts with \"Mr\"\ndf.filter(col(\"name\").starts_with(\"Mr\"))\n</code></pre> Find rows where text starts with a dynamic pattern <pre><code># Filter rows where text starts with a value from another column\ndf.filter(col(\"text\").starts_with(col(\"prefix\")))\n</code></pre> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the substring starts with a regular expression anchor (^)</p> </li> </ul> Source code in <code>src/fenic/api/column.py</code> <pre><code>def starts_with(self, other: Union[str, Column]) -&gt; Column:\n    \"\"\"Check if the column starts with a substring.\n\n    This method creates a boolean expression that checks if each value in the column\n    starts with the specified substring. The substring can be either a literal string\n    or a column expression.\n\n    Args:\n        other (Union[str, Column]): The substring to check for at the start (can be a string or column expression)\n\n    Returns:\n        Column: A boolean column indicating whether each value starts with the substring\n\n    Example: Find rows where name starts with \"Mr\"\n        ```python\n        # Filter rows where name starts with \"Mr\"\n        df.filter(col(\"name\").starts_with(\"Mr\"))\n        ```\n\n    Example: Find rows where text starts with a dynamic pattern\n        ```python\n        # Filter rows where text starts with a value from another column\n        df.filter(col(\"text\").starts_with(col(\"prefix\")))\n        ```\n\n    Raises:\n        ValueError: If the substring starts with a regular expression anchor (^)\n    \"\"\"\n    if isinstance(other, str):\n        return Column._from_logical_expr(StartsWithExpr(self._logical_expr, other))\n    else:\n        return Column._from_logical_expr(\n            StartsWithExpr(self._logical_expr, other._logical_expr)\n        )\n</code></pre>"},{"location":"reference/fenic/api/column/#fenic.api.column.Column.when","title":"when","text":"<pre><code>when(condition: Column, value: Column) -&gt; Column\n</code></pre> <p>Evaluates a list of conditions and returns one of multiple possible result expressions.</p> <p>If Column.otherwise() is not invoked, None is returned for unmatched conditions. Otherwise() will return for rows with None inputs.</p> <p>Parameters:</p> <ul> <li> <code>condition</code>               (<code>Column</code>)           \u2013            <p>A boolean Column expression</p> </li> <li> <code>value</code>               (<code>Column</code>)           \u2013            <p>A literal value or Column expression to return if the condition is true</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A Column expression representing whether each element of Column matches the condition</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the condition is not a boolean Column expression</p> </li> </ul> Use when/otherwise for conditional logic <pre><code># Create a DataFrame with age and name columns\ndf = session.createDataFrame(\n    {\"age\": [2, 5]}, {\"name\": [\"Alice\", \"Bob\"]}\n)\n\n# Use when/otherwise to create a case result column\ndf.select(\n    col(\"name\"),\n    when(col(\"age\") &gt; 3, 1).otherwise(0).alias(\"case_result\")\n).show()\n# Output:\n# +-----+-----------+\n# | name|case_result|\n# +-----+-----------+\n# |Alice|          0|\n# |  Bob|          1|\n# +-----+-----------+\n</code></pre> Source code in <code>src/fenic/api/column.py</code> <pre><code>def when(self, condition: Column, value: Column) -&gt; Column:\n    \"\"\"Evaluates a list of conditions and returns one of multiple possible result expressions.\n\n    If Column.otherwise() is not invoked, None is returned for unmatched conditions.\n    Otherwise() will return for rows with None inputs.\n\n    Args:\n        condition (Column): A boolean Column expression\n        value (Column): A literal value or Column expression to return if the condition is true\n\n    Returns:\n        Column: A Column expression representing whether each element of Column matches the condition\n\n    Raises:\n        TypeError: If the condition is not a boolean Column expression\n\n    Example: Use when/otherwise for conditional logic\n        ```python\n        # Create a DataFrame with age and name columns\n        df = session.createDataFrame(\n            {\"age\": [2, 5]}, {\"name\": [\"Alice\", \"Bob\"]}\n        )\n\n        # Use when/otherwise to create a case result column\n        df.select(\n            col(\"name\"),\n            when(col(\"age\") &gt; 3, 1).otherwise(0).alias(\"case_result\")\n        ).show()\n        # Output:\n        # +-----+-----------+\n        # | name|case_result|\n        # +-----+-----------+\n        # |Alice|          0|\n        # |  Bob|          1|\n        # +-----+-----------+\n        ```\n    \"\"\"\n    return Column._from_logical_expr(WhenExpr(self._logical_expr, condition._logical_expr, value._logical_expr))\n</code></pre>"},{"location":"reference/fenic/api/dataframe/","title":"fenic.api.dataframe","text":""},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe","title":"fenic.api.dataframe","text":"<p>DataFrame API for Fenic - provides DataFrame and grouped data operations.</p> <p>Classes:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>A data collection organized into named columns.</p> </li> <li> <code>GroupedData</code>           \u2013            <p>Methods for aggregations on a grouped DataFrame.</p> </li> <li> <code>SemGroupedData</code>           \u2013            <p>Methods for aggregations on a semantically clustered DataFrame.</p> </li> <li> <code>SemanticExtensions</code>           \u2013            <p>A namespace for semantic dataframe operators.</p> </li> </ul>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame","title":"DataFrame","text":"<p>A data collection organized into named columns.</p> <p>The DataFrame class represents a lazily evaluated computation on data. Operations on DataFrame build up a logical query plan that is only executed when an action like show(), to_polars(), to_pandas(), to_arrow(), to_pydict(), to_pylist(), or count() is called.</p> <p>The DataFrame supports method chaining for building complex transformations.</p> Create and transform a DataFrame <pre><code># Create a DataFrame from a dictionary\ndf = session.create_dataframe({\"id\": [1, 2, 3], \"value\": [\"a\", \"b\", \"c\"]})\n\n# Chain transformations\nresult = df.filter(col(\"id\") &gt; 1).select(\"id\", \"value\")\n\n# Show results\nresult.show()\n# Output:\n# +---+-----+\n# | id|value|\n# +---+-----+\n# |  2|    b|\n# |  3|    c|\n# +---+-----+\n</code></pre> <p>Methods:</p> <ul> <li> <code>agg</code>             \u2013              <p>Aggregate on the entire DataFrame without groups.</p> </li> <li> <code>cache</code>             \u2013              <p>Alias for persist(). Mark DataFrame for caching after first computation.</p> </li> <li> <code>collect</code>             \u2013              <p>Execute the DataFrame computation and return the result as a QueryResult.</p> </li> <li> <code>count</code>             \u2013              <p>Count the number of rows in the DataFrame.</p> </li> <li> <code>drop</code>             \u2013              <p>Remove one or more columns from this DataFrame.</p> </li> <li> <code>drop_duplicates</code>             \u2013              <p>Return a DataFrame with duplicate rows removed.</p> </li> <li> <code>explain</code>             \u2013              <p>Display the logical plan of the DataFrame.</p> </li> <li> <code>explode</code>             \u2013              <p>Create a new row for each element in an array column.</p> </li> <li> <code>filter</code>             \u2013              <p>Filters rows using the given condition.</p> </li> <li> <code>group_by</code>             \u2013              <p>Groups the DataFrame using the specified columns.</p> </li> <li> <code>join</code>             \u2013              <p>Joins this DataFrame with another DataFrame.</p> </li> <li> <code>limit</code>             \u2013              <p>Limits the number of rows to the specified number.</p> </li> <li> <code>lineage</code>             \u2013              <p>Create a Lineage object to trace data through transformations.</p> </li> <li> <code>order_by</code>             \u2013              <p>Sort the DataFrame by the specified columns. Alias for sort().</p> </li> <li> <code>persist</code>             \u2013              <p>Mark this DataFrame to be persisted after first computation.</p> </li> <li> <code>select</code>             \u2013              <p>Projects a set of Column expressions or column names.</p> </li> <li> <code>show</code>             \u2013              <p>Display the DataFrame content in a tabular form.</p> </li> <li> <code>sort</code>             \u2013              <p>Sort the DataFrame by the specified columns.</p> </li> <li> <code>to_arrow</code>             \u2013              <p>Execute the DataFrame computation and return an Apache Arrow Table.</p> </li> <li> <code>to_pandas</code>             \u2013              <p>Execute the DataFrame computation and return a Pandas DataFrame.</p> </li> <li> <code>to_polars</code>             \u2013              <p>Execute the DataFrame computation and return the result as a Polars DataFrame.</p> </li> <li> <code>to_pydict</code>             \u2013              <p>Execute the DataFrame computation and return a dictionary of column arrays.</p> </li> <li> <code>to_pylist</code>             \u2013              <p>Execute the DataFrame computation and return a list of row dictionaries.</p> </li> <li> <code>union</code>             \u2013              <p>Return a new DataFrame containing the union of rows in this and another DataFrame.</p> </li> <li> <code>unnest</code>             \u2013              <p>Unnest the specified struct columns into separate columns.</p> </li> <li> <code>where</code>             \u2013              <p>Filters rows using the given condition (alias for filter()).</p> </li> <li> <code>with_column</code>             \u2013              <p>Add a new column or replace an existing column.</p> </li> <li> <code>with_column_renamed</code>             \u2013              <p>Rename a column. No-op if the column does not exist.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>columns</code>               (<code>List[str]</code>)           \u2013            <p>Get list of column names.</p> </li> <li> <code>schema</code>               (<code>Schema</code>)           \u2013            <p>Get the schema of this DataFrame.</p> </li> <li> <code>semantic</code>               (<code>SemanticExtensions</code>)           \u2013            <p>Interface for semantic operations on the DataFrame.</p> </li> <li> <code>write</code>               (<code>DataFrameWriter</code>)           \u2013            <p>Interface for saving the content of the DataFrame.</p> </li> </ul>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: List[str]\n</code></pre> <p>Get list of column names.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: List of all column names in the DataFrame</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df.columns\n['name', 'age', 'city']\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.schema","title":"schema  <code>property</code>","text":"<pre><code>schema: Schema\n</code></pre> <p>Get the schema of this DataFrame.</p> <p>Returns:</p> <ul> <li> <code>Schema</code> (              <code>Schema</code> )          \u2013            <p>Schema containing field names and data types</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df.schema\nSchema([\n    ColumnField('name', StringType),\n    ColumnField('age', IntegerType)\n])\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.semantic","title":"semantic  <code>property</code>","text":"<pre><code>semantic: SemanticExtensions\n</code></pre> <p>Interface for semantic operations on the DataFrame.</p>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.write","title":"write  <code>property</code>","text":"<pre><code>write: DataFrameWriter\n</code></pre> <p>Interface for saving the content of the DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrameWriter</code> (              <code>DataFrameWriter</code> )          \u2013            <p>Writer interface to write DataFrame.</p> </li> </ul>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.agg","title":"agg","text":"<pre><code>agg(*exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame\n</code></pre> <p>Aggregate on the entire DataFrame without groups.</p> <p>This is equivalent to group_by() without any grouping columns.</p> <p>Parameters:</p> <ul> <li> <code>*exprs</code>               (<code>Union[Column, Dict[str, str]]</code>, default:                   <code>()</code> )           \u2013            <p>Aggregation expressions or dictionary of aggregations.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Aggregation results.</p> </li> </ul> Multiple aggregations <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"salary\": [80000, 70000, 90000, 75000, 85000],\n    \"age\": [25, 30, 35, 28, 32]\n})\n\n# Multiple aggregations\ndf.agg(\n    count().alias(\"total_rows\"),\n    avg(col(\"salary\")).alias(\"avg_salary\")\n).show()\n# Output:\n# +----------+-----------+\n# |total_rows|avg_salary|\n# +----------+-----------+\n# |         5|   80000.0|\n# +----------+-----------+\n</code></pre> Dictionary style <pre><code># Dictionary style\ndf.agg({col(\"salary\"): \"avg\", col(\"age\"): \"max\"}).show()\n# Output:\n# +-----------+--------+\n# |avg(salary)|max(age)|\n# +-----------+--------+\n# |    80000.0|      35|\n# +-----------+--------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def agg(self, *exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame:\n    \"\"\"Aggregate on the entire DataFrame without groups.\n\n    This is equivalent to group_by() without any grouping columns.\n\n    Args:\n        *exprs: Aggregation expressions or dictionary of aggregations.\n\n    Returns:\n        DataFrame: Aggregation results.\n\n    Example: Multiple aggregations\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"salary\": [80000, 70000, 90000, 75000, 85000],\n            \"age\": [25, 30, 35, 28, 32]\n        })\n\n        # Multiple aggregations\n        df.agg(\n            count().alias(\"total_rows\"),\n            avg(col(\"salary\")).alias(\"avg_salary\")\n        ).show()\n        # Output:\n        # +----------+-----------+\n        # |total_rows|avg_salary|\n        # +----------+-----------+\n        # |         5|   80000.0|\n        # +----------+-----------+\n        ```\n\n    Example: Dictionary style\n        ```python\n        # Dictionary style\n        df.agg({col(\"salary\"): \"avg\", col(\"age\"): \"max\"}).show()\n        # Output:\n        # +-----------+--------+\n        # |avg(salary)|max(age)|\n        # +-----------+--------+\n        # |    80000.0|      35|\n        # +-----------+--------+\n        ```\n    \"\"\"\n    return self.group_by().agg(*exprs)\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.cache","title":"cache","text":"<pre><code>cache() -&gt; DataFrame\n</code></pre> <p>Alias for persist(). Mark DataFrame for caching after first computation.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Same DataFrame, but marked for caching</p> </li> </ul> See Also <p>persist(): Full documentation of caching behavior</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def cache(self) -&gt; DataFrame:\n    \"\"\"Alias for persist(). Mark DataFrame for caching after first computation.\n\n    Returns:\n        DataFrame: Same DataFrame, but marked for caching\n\n    See Also:\n        persist(): Full documentation of caching behavior\n    \"\"\"\n    return self.persist()\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.collect","title":"collect","text":"<pre><code>collect(data_type: DataLikeType = 'polars') -&gt; QueryResult\n</code></pre> <p>Execute the DataFrame computation and return the result as a QueryResult.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a QueryResult, which contains both the result data and the query metrics.</p> <p>Parameters:</p> <ul> <li> <code>data_type</code>               (<code>DataLikeType</code>, default:                   <code>'polars'</code> )           \u2013            <p>The type of data to return</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>QueryResult</code> (              <code>QueryResult</code> )          \u2013            <p>A QueryResult with materialized data and query metrics</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def collect(self, data_type: DataLikeType = \"polars\") -&gt; QueryResult:\n    \"\"\"Execute the DataFrame computation and return the result as a QueryResult.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a QueryResult, which contains both the result data and the query metrics.\n\n    Args:\n        data_type: The type of data to return\n\n    Returns:\n        QueryResult: A QueryResult with materialized data and query metrics\n    \"\"\"\n    result: Tuple[pl.DataFrame, QueryMetrics] = self._logical_plan.session_state.execution.collect(self._logical_plan)\n    df, metrics = result\n    logger.info(metrics.get_summary())\n\n    if data_type == \"polars\":\n        return QueryResult(df, metrics)\n    elif data_type == \"pandas\":\n        return QueryResult(df.to_pandas(use_pyarrow_extension_array=True), metrics)\n    elif data_type == \"arrow\":\n        return QueryResult(df.to_arrow(), metrics)\n    elif data_type == \"pydict\":\n        return QueryResult(df.to_dict(as_series=False), metrics)\n    elif data_type == \"pylist\":\n        return QueryResult(df.to_dicts(), metrics)\n    else:\n        raise ValidationError(f\"Invalid data type: {data_type} in collect(). Valid data types are: polars, pandas, arrow, pydict, pylist\")\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.count","title":"count","text":"<pre><code>count() -&gt; int\n</code></pre> <p>Count the number of rows in the DataFrame.</p> <p>This is an action that triggers computation of the DataFrame. The output is an integer representing the number of rows.</p> <p>Returns:</p> <ul> <li> <code>int</code> (              <code>int</code> )          \u2013            <p>The number of rows in the DataFrame</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def count(self) -&gt; int:\n    \"\"\"Count the number of rows in the DataFrame.\n\n    This is an action that triggers computation of the DataFrame.\n    The output is an integer representing the number of rows.\n\n    Returns:\n        int: The number of rows in the DataFrame\n    \"\"\"\n    return self._logical_plan.session_state.execution.count(self._logical_plan)[0]\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.drop","title":"drop","text":"<pre><code>drop(*col_names: str) -&gt; DataFrame\n</code></pre> <p>Remove one or more columns from this DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>*col_names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>Names of columns to drop.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>New DataFrame without specified columns.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If any specified column doesn't exist in the DataFrame.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If dropping the columns would result in an empty DataFrame.</p> </li> </ul> Drop single column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35]\n})\n\n# Drop single column\ndf.drop(\"age\").show()\n# Output:\n# +---+-------+\n# | id|   name|\n# +---+-------+\n# |  1|  Alice|\n# |  2|    Bob|\n# |  3|Charlie|\n# +---+-------+\n</code></pre> Drop multiple columns <pre><code># Drop multiple columns\ndf.drop(col(\"id\"), \"age\").show()\n# Output:\n# +-------+\n# |   name|\n# +-------+\n# |  Alice|\n# |    Bob|\n# |Charlie|\n# +-------+\n</code></pre> Error when dropping non-existent column <pre><code># This will raise a ValueError\ndf.drop(\"non_existent_column\")\n# ValueError: Column 'non_existent_column' not found in DataFrame\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def drop(self, *col_names: str) -&gt; DataFrame:\n    \"\"\"Remove one or more columns from this DataFrame.\n\n    Args:\n        *col_names: Names of columns to drop.\n\n    Returns:\n        DataFrame: New DataFrame without specified columns.\n\n    Raises:\n        ValueError: If any specified column doesn't exist in the DataFrame.\n        ValueError: If dropping the columns would result in an empty DataFrame.\n\n    Example: Drop single column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"id\": [1, 2, 3],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n            \"age\": [25, 30, 35]\n        })\n\n        # Drop single column\n        df.drop(\"age\").show()\n        # Output:\n        # +---+-------+\n        # | id|   name|\n        # +---+-------+\n        # |  1|  Alice|\n        # |  2|    Bob|\n        # |  3|Charlie|\n        # +---+-------+\n        ```\n\n    Example: Drop multiple columns\n        ```python\n        # Drop multiple columns\n        df.drop(col(\"id\"), \"age\").show()\n        # Output:\n        # +-------+\n        # |   name|\n        # +-------+\n        # |  Alice|\n        # |    Bob|\n        # |Charlie|\n        # +-------+\n        ```\n\n    Example: Error when dropping non-existent column\n        ```python\n        # This will raise a ValueError\n        df.drop(\"non_existent_column\")\n        # ValueError: Column 'non_existent_column' not found in DataFrame\n        ```\n    \"\"\"\n    if not col_names:\n        return self\n\n    current_cols = set(self.columns)\n    to_drop = set(col_names)\n    missing = to_drop - current_cols\n\n    if missing:\n        missing_str = (\n            f\"Column '{next(iter(missing))}'\"\n            if len(missing) == 1\n            else f\"Columns {sorted(missing)}\"\n        )\n        raise ValueError(f\"{missing_str} not found in DataFrame\")\n\n    remaining_cols = [\n        col(c)._logical_expr for c in self.columns if c not in to_drop\n    ]\n\n    if not remaining_cols:\n        raise ValueError(\"Cannot drop all columns from DataFrame\")\n\n    return self._from_logical_plan(\n        Projection(self._logical_plan, remaining_cols)\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.drop_duplicates","title":"drop_duplicates","text":"<pre><code>drop_duplicates(subset: Optional[List[str]] = None) -&gt; DataFrame\n</code></pre> <p>Return a DataFrame with duplicate rows removed.</p> <p>Parameters:</p> <ul> <li> <code>subset</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Column names to consider when identifying duplicates. If not provided, all columns are considered.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with duplicate rows removed.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a specified column is not present in the current DataFrame schema.</p> </li> </ul> Remove duplicates considering specific columns <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"c1\": [1, 2, 3, 1],\n    \"c2\": [\"a\", \"a\", \"a\", \"a\"],\n    \"c3\": [\"b\", \"b\", \"b\", \"b\"]\n})\n\n# Remove duplicates considering all columns\ndf.drop_duplicates([col(\"c1\"), col(\"c2\"), col(\"c3\")]).show()\n# Output:\n# +---+---+---+\n# | c1| c2| c3|\n# +---+---+---+\n# |  1|  a|  b|\n# |  2|  a|  b|\n# |  3|  a|  b|\n# +---+---+---+\n\n# Remove duplicates considering only c1\ndf.drop_duplicates([col(\"c1\")]).show()\n# Output:\n# +---+---+---+\n# | c1| c2| c3|\n# +---+---+---+\n# |  1|  a|  b|\n# |  2|  a|  b|\n# |  3|  a|  b|\n# +---+---+---+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def drop_duplicates(\n    self,\n    subset: Optional[List[str]] = None,\n) -&gt; DataFrame:\n    \"\"\"Return a DataFrame with duplicate rows removed.\n\n    Args:\n        subset: Column names to consider when identifying duplicates. If not provided, all columns are considered.\n\n    Returns:\n        DataFrame: A new DataFrame with duplicate rows removed.\n\n    Raises:\n        ValueError: If a specified column is not present in the current DataFrame schema.\n\n    Example: Remove duplicates considering specific columns\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"c1\": [1, 2, 3, 1],\n            \"c2\": [\"a\", \"a\", \"a\", \"a\"],\n            \"c3\": [\"b\", \"b\", \"b\", \"b\"]\n        })\n\n        # Remove duplicates considering all columns\n        df.drop_duplicates([col(\"c1\"), col(\"c2\"), col(\"c3\")]).show()\n        # Output:\n        # +---+---+---+\n        # | c1| c2| c3|\n        # +---+---+---+\n        # |  1|  a|  b|\n        # |  2|  a|  b|\n        # |  3|  a|  b|\n        # +---+---+---+\n\n        # Remove duplicates considering only c1\n        df.drop_duplicates([col(\"c1\")]).show()\n        # Output:\n        # +---+---+---+\n        # | c1| c2| c3|\n        # +---+---+---+\n        # |  1|  a|  b|\n        # |  2|  a|  b|\n        # |  3|  a|  b|\n        # +---+---+---+\n        ```\n    \"\"\"\n    exprs = []\n    if subset:\n        for c in subset:\n            if c not in self.columns:\n                raise TypeError(f\"Column {c} not found in DataFrame.\")\n            exprs.append(col(c)._logical_expr)\n\n    return self._from_logical_plan(\n        DropDuplicates(self._logical_plan, exprs),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.explain","title":"explain","text":"<pre><code>explain() -&gt; None\n</code></pre> <p>Display the logical plan of the DataFrame.</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def explain(self) -&gt; None:\n    \"\"\"Display the logical plan of the DataFrame.\"\"\"\n    print(str(self._logical_plan))\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.explode","title":"explode","text":"<pre><code>explode(column: ColumnOrName) -&gt; DataFrame\n</code></pre> <p>Create a new row for each element in an array column.</p> <p>This operation is useful for flattening nested data structures. For each row in the input DataFrame that contains an array/list in the specified column, this method will: 1. Create N new rows, where N is the length of the array 2. Each new row will be identical to the original row, except the array column will    contain just a single element from the original array 3. Rows with NULL values or empty arrays in the specified column are filtered out</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Name of array column to explode (as string) or Column expression.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>New DataFrame with the array column exploded into multiple rows.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column argument is not a string or Column.</p> </li> </ul> Explode array column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"id\": [1, 2, 3, 4],\n    \"tags\": [[\"red\", \"blue\"], [\"green\"], [], None],\n    \"name\": [\"Alice\", \"Bob\", \"Carol\", \"Dave\"]\n})\n\n# Explode the tags column\ndf.explode(\"tags\").show()\n# Output:\n# +---+-----+-----+\n# | id| tags| name|\n# +---+-----+-----+\n# |  1|  red|Alice|\n# |  1| blue|Alice|\n# |  2|green|  Bob|\n# +---+-----+-----+\n</code></pre> Using column expression <pre><code># Explode using column expression\ndf.explode(col(\"tags\")).show()\n# Output:\n# +---+-----+-----+\n# | id| tags| name|\n# +---+-----+-----+\n# |  1|  red|Alice|\n# |  1| blue|Alice|\n# |  2|green|  Bob|\n# +---+-----+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def explode(self, column: ColumnOrName) -&gt; DataFrame:\n    \"\"\"Create a new row for each element in an array column.\n\n    This operation is useful for flattening nested data structures. For each row in the\n    input DataFrame that contains an array/list in the specified column, this method will:\n    1. Create N new rows, where N is the length of the array\n    2. Each new row will be identical to the original row, except the array column will\n       contain just a single element from the original array\n    3. Rows with NULL values or empty arrays in the specified column are filtered out\n\n    Args:\n        column: Name of array column to explode (as string) or Column expression.\n\n    Returns:\n        DataFrame: New DataFrame with the array column exploded into multiple rows.\n\n    Raises:\n        TypeError: If column argument is not a string or Column.\n\n    Example: Explode array column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"id\": [1, 2, 3, 4],\n            \"tags\": [[\"red\", \"blue\"], [\"green\"], [], None],\n            \"name\": [\"Alice\", \"Bob\", \"Carol\", \"Dave\"]\n        })\n\n        # Explode the tags column\n        df.explode(\"tags\").show()\n        # Output:\n        # +---+-----+-----+\n        # | id| tags| name|\n        # +---+-----+-----+\n        # |  1|  red|Alice|\n        # |  1| blue|Alice|\n        # |  2|green|  Bob|\n        # +---+-----+-----+\n        ```\n\n    Example: Using column expression\n        ```python\n        # Explode using column expression\n        df.explode(col(\"tags\")).show()\n        # Output:\n        # +---+-----+-----+\n        # | id| tags| name|\n        # +---+-----+-----+\n        # |  1|  red|Alice|\n        # |  1| blue|Alice|\n        # |  2|green|  Bob|\n        # +---+-----+-----+\n        ```\n    \"\"\"\n    return self._from_logical_plan(\n        Explode(self._logical_plan, Column._from_col_or_name(column)._logical_expr),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.filter","title":"filter","text":"<pre><code>filter(condition: Column) -&gt; DataFrame\n</code></pre> <p>Filters rows using the given condition.</p> <p>Parameters:</p> <ul> <li> <code>condition</code>               (<code>Column</code>)           \u2013            <p>A Column expression that evaluates to a boolean</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Filtered DataFrame</p> </li> </ul> Filter with numeric comparison <pre><code># Create a DataFrame\ndf = session.create_dataframe({\"age\": [25, 30, 35], \"name\": [\"Alice\", \"Bob\", \"Charlie\"]})\n\n# Filter with numeric comparison\ndf.filter(col(\"age\") &gt; 25).show()\n# Output:\n# +---+-------+\n# |age|   name|\n# +---+-------+\n# | 30|    Bob|\n# | 35|Charlie|\n# +---+-------+\n</code></pre> Filter with semantic predicate <pre><code># Filter with semantic predicate\ndf.filter((col(\"age\") &gt; 25) &amp; semantic.predicate(\"This {feedback} mentions problems with the user interface or navigation\")).show()\n# Output:\n# +---+-------+\n# |age|   name|\n# +---+-------+\n# | 30|    Bob|\n# | 35|Charlie|\n# +---+-------+\n</code></pre> Filter with multiple conditions <pre><code># Filter with multiple conditions\ndf.filter((col(\"age\") &gt; 25) &amp; (col(\"age\") &lt;= 35)).show()\n# Output:\n# +---+-------+\n# |age|   name|\n# +---+-------+\n# | 30|    Bob|\n# | 35|Charlie|\n# +---+-------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def filter(self, condition: Column) -&gt; DataFrame:\n    \"\"\"Filters rows using the given condition.\n\n    Args:\n        condition: A Column expression that evaluates to a boolean\n\n    Returns:\n        DataFrame: Filtered DataFrame\n\n    Example: Filter with numeric comparison\n        ```python\n        # Create a DataFrame\n        df = session.create_dataframe({\"age\": [25, 30, 35], \"name\": [\"Alice\", \"Bob\", \"Charlie\"]})\n\n        # Filter with numeric comparison\n        df.filter(col(\"age\") &gt; 25).show()\n        # Output:\n        # +---+-------+\n        # |age|   name|\n        # +---+-------+\n        # | 30|    Bob|\n        # | 35|Charlie|\n        # +---+-------+\n        ```\n\n    Example: Filter with semantic predicate\n        ```python\n        # Filter with semantic predicate\n        df.filter((col(\"age\") &gt; 25) &amp; semantic.predicate(\"This {feedback} mentions problems with the user interface or navigation\")).show()\n        # Output:\n        # +---+-------+\n        # |age|   name|\n        # +---+-------+\n        # | 30|    Bob|\n        # | 35|Charlie|\n        # +---+-------+\n        ```\n\n    Example: Filter with multiple conditions\n        ```python\n        # Filter with multiple conditions\n        df.filter((col(\"age\") &gt; 25) &amp; (col(\"age\") &lt;= 35)).show()\n        # Output:\n        # +---+-------+\n        # |age|   name|\n        # +---+-------+\n        # | 30|    Bob|\n        # | 35|Charlie|\n        # +---+-------+\n        ```\n    \"\"\"\n    return self._from_logical_plan(\n        Filter(self._logical_plan, condition._logical_expr),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.group_by","title":"group_by","text":"<pre><code>group_by(*cols: ColumnOrName) -&gt; GroupedData\n</code></pre> <p>Groups the DataFrame using the specified columns.</p> <p>Parameters:</p> <ul> <li> <code>*cols</code>               (<code>ColumnOrName</code>, default:                   <code>()</code> )           \u2013            <p>Columns to group by. Can be column names as strings or Column expressions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GroupedData</code> (              <code>GroupedData</code> )          \u2013            <p>Object for performing aggregations on the grouped data.</p> </li> </ul> Group by single column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"department\": [\"IT\", \"HR\", \"IT\", \"HR\", \"IT\"],\n    \"salary\": [80000, 70000, 90000, 75000, 85000]\n})\n\n# Group by single column\ndf.group_by(col(\"department\")).count().show()\n# Output:\n# +----------+-----+\n# |department|count|\n# +----------+-----+\n# |        IT|    3|\n# |        HR|    2|\n# +----------+-----+\n</code></pre> Group by multiple columns <pre><code># Group by multiple columns\ndf.group_by(col(\"department\"), col(\"location\")).agg({\"salary\": \"avg\"}).show()\n# Output:\n# +----------+--------+-----------+\n# |department|location|avg(salary)|\n# +----------+--------+-----------+\n# |        IT|    NYC|    85000.0|\n# |        HR|    NYC|    72500.0|\n# +----------+--------+-----------+\n</code></pre> Group by expression <pre><code># Group by expression\ndf.group_by(col(\"age\").cast(\"int\").alias(\"age_group\")).count().show()\n# Output:\n# +---------+-----+\n# |age_group|count|\n# +---------+-----+\n# |       20|    2|\n# |       30|    3|\n# |       40|    1|\n# +---------+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def group_by(self, *cols: ColumnOrName) -&gt; GroupedData:\n    \"\"\"Groups the DataFrame using the specified columns.\n\n    Args:\n        *cols: Columns to group by. Can be column names as strings or Column expressions.\n\n    Returns:\n        GroupedData: Object for performing aggregations on the grouped data.\n\n    Example: Group by single column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"department\": [\"IT\", \"HR\", \"IT\", \"HR\", \"IT\"],\n            \"salary\": [80000, 70000, 90000, 75000, 85000]\n        })\n\n        # Group by single column\n        df.group_by(col(\"department\")).count().show()\n        # Output:\n        # +----------+-----+\n        # |department|count|\n        # +----------+-----+\n        # |        IT|    3|\n        # |        HR|    2|\n        # +----------+-----+\n        ```\n\n    Example: Group by multiple columns\n        ```python\n        # Group by multiple columns\n        df.group_by(col(\"department\"), col(\"location\")).agg({\"salary\": \"avg\"}).show()\n        # Output:\n        # +----------+--------+-----------+\n        # |department|location|avg(salary)|\n        # +----------+--------+-----------+\n        # |        IT|    NYC|    85000.0|\n        # |        HR|    NYC|    72500.0|\n        # +----------+--------+-----------+\n        ```\n\n    Example: Group by expression\n        ```python\n        # Group by expression\n        df.group_by(col(\"age\").cast(\"int\").alias(\"age_group\")).count().show()\n        # Output:\n        # +---------+-----+\n        # |age_group|count|\n        # +---------+-----+\n        # |       20|    2|\n        # |       30|    3|\n        # |       40|    1|\n        # +---------+-----+\n        ```\n    \"\"\"\n    return GroupedData(self, list(cols) if cols else None)\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.join","title":"join","text":"<pre><code>join(other: DataFrame, on: Union[str, List[str]], *, how: JoinType = 'inner') -&gt; DataFrame\n</code></pre><pre><code>join(other: DataFrame, *, left_on: Union[ColumnOrName, List[ColumnOrName]], right_on: Union[ColumnOrName, List[ColumnOrName]], how: JoinType = 'inner') -&gt; DataFrame\n</code></pre> <pre><code>join(other: DataFrame, on: Optional[Union[str, List[str]]] = None, *, left_on: Optional[Union[ColumnOrName, List[ColumnOrName]]] = None, right_on: Optional[Union[ColumnOrName, List[ColumnOrName]]] = None, how: JoinType = 'inner') -&gt; DataFrame\n</code></pre> <p>Joins this DataFrame with another DataFrame.</p> <p>The Dataframes must have no duplicate column names between them. This API only supports equi-joins. For non-equi-joins, use session.sql().</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>DataFrame</code>)           \u2013            <p>DataFrame to join with.</p> </li> <li> <code>on</code>               (<code>Optional[Union[str, List[str]]]</code>, default:                   <code>None</code> )           \u2013            <p>Join condition(s). Can be: - A column name (str) - A list of column names (List[str]) - A Column expression (e.g., col('a')) - A list of Column expressions - <code>None</code> for cross joins</p> </li> <li> <code>left_on</code>               (<code>Optional[Union[ColumnOrName, List[ColumnOrName]]]</code>, default:                   <code>None</code> )           \u2013            <p>Column(s) from the left DataFrame to join on. Can be: - A column name (str) - A Column expression (e.g., col('a'), col('a') + 1) - A list of column names or expressions</p> </li> <li> <code>right_on</code>               (<code>Optional[Union[ColumnOrName, List[ColumnOrName]]]</code>, default:                   <code>None</code> )           \u2013            <p>Column(s) from the right DataFrame to join on. Can be: - A column name (str) - A Column expression (e.g., col('b'), upper(col('b'))) - A list of column names or expressions</p> </li> <li> <code>how</code>               (<code>JoinType</code>, default:                   <code>'inner'</code> )           \u2013            <p>Type of join to perform.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Joined DataFrame.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If cross join is used with an ON clause.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If join condition is invalid.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If both 'on' and 'left_on'/'right_on' parameters are provided.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If only one of 'left_on' or 'right_on' is provided.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If 'left_on' and 'right_on' have different lengths</p> </li> </ul> Inner join on column name <pre><code># Create sample DataFrames\ndf1 = session.create_dataframe({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n})\ndf2 = session.create_dataframe({\n    \"id\": [1, 2, 4],\n    \"age\": [25, 30, 35]\n})\n\n# Join on single column\ndf1.join(df2, on=col(\"id\")).show()\n# Output:\n# +---+-----+---+\n# | id| name|age|\n# +---+-----+---+\n# |  1|Alice| 25|\n# |  2|  Bob| 30|\n# +---+-----+---+\n</code></pre> Join with expression <pre><code># Join with Column expressions\ndf1.join(\n    df2,\n    left_on=col(\"id\"),\n    right_on=col(\"id\"),\n).show()\n# Output:\n# +---+-----+---+\n# | id| name|age|\n# +---+-----+---+\n# |  1|Alice| 25|\n# |  2|  Bob| 30|\n# +---+-----+---+\n</code></pre> Cross join <pre><code># Cross join (cartesian product)\ndf1.join(df2, how=\"cross\").show()\n# Output:\n# +---+-----+---+---+\n# | id| name| id|age|\n# +---+-----+---+---+\n# |  1|Alice|  1| 25|\n# |  1|Alice|  2| 30|\n# |  1|Alice|  4| 35|\n# |  2|  Bob|  1| 25|\n# |  2|  Bob|  2| 30|\n# |  2|  Bob|  4| 35|\n# |  3|Charlie| 1| 25|\n# |  3|Charlie| 2| 30|\n# |  3|Charlie| 4| 35|\n# +---+-----+---+---+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def join(\n    self,\n    other: DataFrame,\n    on: Optional[Union[str, List[str]]] = None,\n    *,\n    left_on: Optional[Union[ColumnOrName, List[ColumnOrName]]] = None,\n    right_on: Optional[Union[ColumnOrName, List[ColumnOrName]]] = None,\n    how: JoinType = \"inner\",\n) -&gt; DataFrame:\n    \"\"\"Joins this DataFrame with another DataFrame.\n\n    The Dataframes must have no duplicate column names between them. This API only supports equi-joins.\n    For non-equi-joins, use session.sql().\n\n    Args:\n        other: DataFrame to join with.\n        on: Join condition(s). Can be:\n            - A column name (str)\n            - A list of column names (List[str])\n            - A Column expression (e.g., col('a'))\n            - A list of Column expressions\n            - `None` for cross joins\n        left_on: Column(s) from the left DataFrame to join on. Can be:\n            - A column name (str)\n            - A Column expression (e.g., col('a'), col('a') + 1)\n            - A list of column names or expressions\n        right_on: Column(s) from the right DataFrame to join on. Can be:\n            - A column name (str)\n            - A Column expression (e.g., col('b'), upper(col('b')))\n            - A list of column names or expressions\n        how: Type of join to perform.\n\n    Returns:\n        Joined DataFrame.\n\n    Raises:\n        ValidationError: If cross join is used with an ON clause.\n        ValidationError: If join condition is invalid.\n        ValidationError: If both 'on' and 'left_on'/'right_on' parameters are provided.\n        ValidationError: If only one of 'left_on' or 'right_on' is provided.\n        ValidationError: If 'left_on' and 'right_on' have different lengths\n\n    Example: Inner join on column name\n        ```python\n        # Create sample DataFrames\n        df1 = session.create_dataframe({\n            \"id\": [1, 2, 3],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n        })\n        df2 = session.create_dataframe({\n            \"id\": [1, 2, 4],\n            \"age\": [25, 30, 35]\n        })\n\n        # Join on single column\n        df1.join(df2, on=col(\"id\")).show()\n        # Output:\n        # +---+-----+---+\n        # | id| name|age|\n        # +---+-----+---+\n        # |  1|Alice| 25|\n        # |  2|  Bob| 30|\n        # +---+-----+---+\n        ```\n\n    Example: Join with expression\n        ```python\n        # Join with Column expressions\n        df1.join(\n            df2,\n            left_on=col(\"id\"),\n            right_on=col(\"id\"),\n        ).show()\n        # Output:\n        # +---+-----+---+\n        # | id| name|age|\n        # +---+-----+---+\n        # |  1|Alice| 25|\n        # |  2|  Bob| 30|\n        # +---+-----+---+\n        ```\n\n    Example: Cross join\n        ```python\n        # Cross join (cartesian product)\n        df1.join(df2, how=\"cross\").show()\n        # Output:\n        # +---+-----+---+---+\n        # | id| name| id|age|\n        # +---+-----+---+---+\n        # |  1|Alice|  1| 25|\n        # |  1|Alice|  2| 30|\n        # |  1|Alice|  4| 35|\n        # |  2|  Bob|  1| 25|\n        # |  2|  Bob|  2| 30|\n        # |  2|  Bob|  4| 35|\n        # |  3|Charlie| 1| 25|\n        # |  3|Charlie| 2| 30|\n        # |  3|Charlie| 4| 35|\n        # +---+-----+---+---+\n        ```\n    \"\"\"\n    validate_join_parameters(self, on, left_on, right_on, how)\n\n    # Build join conditions\n    left_conditions, right_conditions = build_join_conditions(on, left_on, right_on)\n\n    return self._from_logical_plan(\n        Join(self._logical_plan, other._logical_plan, left_conditions, right_conditions, how),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.limit","title":"limit","text":"<pre><code>limit(n: int) -&gt; DataFrame\n</code></pre> <p>Limits the number of rows to the specified number.</p> <p>Parameters:</p> <ul> <li> <code>n</code>               (<code>int</code>)           \u2013            <p>Maximum number of rows to return.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>DataFrame with at most n rows.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If n is not an integer.</p> </li> </ul> Limit rows <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"id\": [1, 2, 3, 4, 5],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Eve\"]\n})\n\n# Get first 3 rows\ndf.limit(3).show()\n# Output:\n# +---+-------+\n# | id|   name|\n# +---+-------+\n# |  1|  Alice|\n# |  2|    Bob|\n# |  3|Charlie|\n# +---+-------+\n</code></pre> Limit with other operations <pre><code># Limit after filtering\ndf.filter(col(\"id\") &gt; 2).limit(2).show()\n# Output:\n# +---+-------+\n# | id|   name|\n# +---+-------+\n# |  3|Charlie|\n# |  4|   Dave|\n# +---+-------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def limit(self, n: int) -&gt; DataFrame:\n    \"\"\"Limits the number of rows to the specified number.\n\n    Args:\n        n: Maximum number of rows to return.\n\n    Returns:\n        DataFrame: DataFrame with at most n rows.\n\n    Raises:\n        TypeError: If n is not an integer.\n\n    Example: Limit rows\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"id\": [1, 2, 3, 4, 5],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Eve\"]\n        })\n\n        # Get first 3 rows\n        df.limit(3).show()\n        # Output:\n        # +---+-------+\n        # | id|   name|\n        # +---+-------+\n        # |  1|  Alice|\n        # |  2|    Bob|\n        # |  3|Charlie|\n        # +---+-------+\n        ```\n\n    Example: Limit with other operations\n        ```python\n        # Limit after filtering\n        df.filter(col(\"id\") &gt; 2).limit(2).show()\n        # Output:\n        # +---+-------+\n        # | id|   name|\n        # +---+-------+\n        # |  3|Charlie|\n        # |  4|   Dave|\n        # +---+-------+\n        ```\n    \"\"\"\n    return self._from_logical_plan(Limit(self._logical_plan, n))\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.lineage","title":"lineage","text":"<pre><code>lineage() -&gt; Lineage\n</code></pre> <p>Create a Lineage object to trace data through transformations.</p> <p>The Lineage interface allows you to trace how specific rows are transformed through your DataFrame operations, both forwards and backwards through the computation graph.</p> <p>Returns:</p> <ul> <li> <code>Lineage</code> (              <code>Lineage</code> )          \u2013            <p>Interface for querying data lineage</p> </li> </ul> Example <pre><code># Create lineage query\nlineage = df.lineage()\n\n# Trace specific rows backwards through transformations\nsource_rows = lineage.backward([\"result_uuid1\", \"result_uuid2\"])\n\n# Or trace forwards to see outputs\nresult_rows = lineage.forward([\"source_uuid1\"])\n</code></pre> See Also <p>LineageQuery: Full documentation of lineage querying capabilities</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def lineage(self) -&gt; Lineage:\n    \"\"\"Create a Lineage object to trace data through transformations.\n\n    The Lineage interface allows you to trace how specific rows are transformed\n    through your DataFrame operations, both forwards and backwards through the\n    computation graph.\n\n    Returns:\n        Lineage: Interface for querying data lineage\n\n    Example:\n        ```python\n        # Create lineage query\n        lineage = df.lineage()\n\n        # Trace specific rows backwards through transformations\n        source_rows = lineage.backward([\"result_uuid1\", \"result_uuid2\"])\n\n        # Or trace forwards to see outputs\n        result_rows = lineage.forward([\"source_uuid1\"])\n        ```\n\n    See Also:\n        LineageQuery: Full documentation of lineage querying capabilities\n    \"\"\"\n    return Lineage(self._logical_plan.session_state.execution.build_lineage(self._logical_plan))\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.order_by","title":"order_by","text":"<pre><code>order_by(cols: Union[ColumnOrName, List[ColumnOrName], None] = None, ascending: Optional[Union[bool, List[bool]]] = None) -&gt; 'DataFrame'\n</code></pre> <p>Sort the DataFrame by the specified columns. Alias for sort().</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>'DataFrame'</code> )          \u2013            <p>sorted Dataframe.</p> </li> </ul> See Also <p>sort(): Full documentation of sorting behavior and parameters.</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def order_by(\n    self,\n    cols: Union[ColumnOrName, List[ColumnOrName], None] = None,\n    ascending: Optional[Union[bool, List[bool]]] = None,\n) -&gt; \"DataFrame\":\n    \"\"\"Sort the DataFrame by the specified columns. Alias for sort().\n\n    Returns:\n        DataFrame: sorted Dataframe.\n\n    See Also:\n        sort(): Full documentation of sorting behavior and parameters.\n    \"\"\"\n    return self.sort(cols, ascending)\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.persist","title":"persist","text":"<pre><code>persist() -&gt; DataFrame\n</code></pre> <p>Mark this DataFrame to be persisted after first computation.</p> <p>The persisted DataFrame will be cached after its first computation, avoiding recomputation in subsequent operations. This is useful for DataFrames that are reused multiple times in your workflow.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Same DataFrame, but marked for persistence</p> </li> </ul> Example <pre><code># Cache intermediate results for reuse\nfiltered_df = (df\n    .filter(col(\"age\") &gt; 25)\n    .persist()  # Cache these results\n)\n\n# Both operations will use cached results\nresult1 = filtered_df.group_by(\"department\").count()\nresult2 = filtered_df.select(\"name\", \"salary\")\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def persist(self) -&gt; DataFrame:\n    \"\"\"Mark this DataFrame to be persisted after first computation.\n\n    The persisted DataFrame will be cached after its first computation,\n    avoiding recomputation in subsequent operations. This is useful for DataFrames\n    that are reused multiple times in your workflow.\n\n    Returns:\n        DataFrame: Same DataFrame, but marked for persistence\n\n    Example:\n        ```python\n        # Cache intermediate results for reuse\n        filtered_df = (df\n            .filter(col(\"age\") &gt; 25)\n            .persist()  # Cache these results\n        )\n\n        # Both operations will use cached results\n        result1 = filtered_df.group_by(\"department\").count()\n        result2 = filtered_df.select(\"name\", \"salary\")\n        ```\n    \"\"\"\n    table_name = f\"cache_{uuid.uuid4().hex}\"\n    cache_info = CacheInfo(duckdb_table_name=table_name)\n    self._logical_plan.set_cache_info(cache_info)\n    return self._from_logical_plan(self._logical_plan)\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.select","title":"select","text":"<pre><code>select(*cols: ColumnOrName) -&gt; DataFrame\n</code></pre> <p>Projects a set of Column expressions or column names.</p> <p>Parameters:</p> <ul> <li> <code>*cols</code>               (<code>ColumnOrName</code>, default:                   <code>()</code> )           \u2013            <p>Column expressions to select. Can be: - String column names (e.g., \"id\", \"name\") - Column objects (e.g., col(\"id\"), col(\"age\") + 1)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with selected columns</p> </li> </ul> Select by column names <pre><code># Create a DataFrame\ndf = session.create_dataframe({\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]})\n\n# Select by column names\ndf.select(col(\"name\"), col(\"age\")).show()\n# Output:\n# +-----+---+\n# | name|age|\n# +-----+---+\n# |Alice| 25|\n# |  Bob| 30|\n# +-----+---+\n</code></pre> Select with expressions <pre><code># Select with expressions\ndf.select(col(\"name\"), col(\"age\") + 1).show()\n# Output:\n# +-----+-------+\n# | name|age + 1|\n# +-----+-------+\n# |Alice|     26|\n# |  Bob|     31|\n# +-----+-------+\n</code></pre> Mix strings and expressions <pre><code># Mix strings and expressions\ndf.select(col(\"name\"), col(\"age\") * 2).show()\n# Output:\n# +-----+-------+\n# | name|age * 2|\n# +-----+-------+\n# |Alice|     50|\n# |  Bob|     60|\n# +-----+-------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def select(self, *cols: ColumnOrName) -&gt; DataFrame:\n    \"\"\"Projects a set of Column expressions or column names.\n\n    Args:\n        *cols: Column expressions to select. Can be:\n            - String column names (e.g., \"id\", \"name\")\n            - Column objects (e.g., col(\"id\"), col(\"age\") + 1)\n\n    Returns:\n        DataFrame: A new DataFrame with selected columns\n\n    Example: Select by column names\n        ```python\n        # Create a DataFrame\n        df = session.create_dataframe({\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]})\n\n        # Select by column names\n        df.select(col(\"name\"), col(\"age\")).show()\n        # Output:\n        # +-----+---+\n        # | name|age|\n        # +-----+---+\n        # |Alice| 25|\n        # |  Bob| 30|\n        # +-----+---+\n        ```\n\n    Example: Select with expressions\n        ```python\n        # Select with expressions\n        df.select(col(\"name\"), col(\"age\") + 1).show()\n        # Output:\n        # +-----+-------+\n        # | name|age + 1|\n        # +-----+-------+\n        # |Alice|     26|\n        # |  Bob|     31|\n        # +-----+-------+\n        ```\n\n    Example: Mix strings and expressions\n        ```python\n        # Mix strings and expressions\n        df.select(col(\"name\"), col(\"age\") * 2).show()\n        # Output:\n        # +-----+-------+\n        # | name|age * 2|\n        # +-----+-------+\n        # |Alice|     50|\n        # |  Bob|     60|\n        # +-----+-------+\n        ```\n    \"\"\"\n    exprs = []\n    if not cols:\n        return self\n    for c in cols:\n        if isinstance(c, str):\n            if c == \"*\":\n                exprs.extend(col(field)._logical_expr for field in self.columns)\n            else:\n                exprs.append(col(c)._logical_expr)\n        else:\n            exprs.append(c._logical_expr)\n\n    return self._from_logical_plan(\n        Projection(self._logical_plan, exprs)\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.show","title":"show","text":"<pre><code>show(n: int = 10, explain_analyze: bool = False) -&gt; None\n</code></pre> <p>Display the DataFrame content in a tabular form.</p> <p>This is an action that triggers computation of the DataFrame. The output is printed to stdout in a formatted table.</p> <p>Parameters:</p> <ul> <li> <code>n</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of rows to display</p> </li> <li> <code>explain_analyze</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to print the explain analyze plan</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def show(self, n: int = 10, explain_analyze: bool = False) -&gt; None:\n    \"\"\"Display the DataFrame content in a tabular form.\n\n    This is an action that triggers computation of the DataFrame.\n    The output is printed to stdout in a formatted table.\n\n    Args:\n        n: Number of rows to display\n        explain_analyze: Whether to print the explain analyze plan\n    \"\"\"\n    output, metrics = self._logical_plan.session_state.execution.show(self._logical_plan, n)\n    logger.info(metrics.get_summary())\n    print(output)\n    if explain_analyze:\n        print(metrics.get_execution_plan_details())\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.sort","title":"sort","text":"<pre><code>sort(cols: Union[ColumnOrName, List[ColumnOrName], None] = None, ascending: Optional[Union[bool, List[bool]]] = None) -&gt; DataFrame\n</code></pre> <p>Sort the DataFrame by the specified columns.</p> <p>Parameters:</p> <ul> <li> <code>cols</code>               (<code>Union[ColumnOrName, List[ColumnOrName], None]</code>, default:                   <code>None</code> )           \u2013            <p>Columns to sort by. This can be: - A single column name (str) - A Column expression (e.g., <code>col(\"name\")</code>) - A list of column names or Column expressions - Column expressions may include sorting directives such as <code>asc(\"col\")</code>, <code>desc(\"col\")</code>, <code>asc_nulls_last(\"col\")</code>, etc. - If no columns are provided, the operation is a no-op.</p> </li> <li> <code>ascending</code>               (<code>Optional[Union[bool, List[bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>A boolean or list of booleans indicating sort order. - If <code>True</code>, sorts in ascending order; if <code>False</code>, descending. - If a list is provided, its length must match the number of columns. - Cannot be used if any of the columns use <code>asc()</code>/<code>desc()</code> expressions. - If not specified and no sort expressions are used, columns will be sorted in ascending order by default.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame sorted by the specified columns.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <ul> <li>If <code>ascending</code> is provided and its length does not match <code>cols</code></li> <li>If both <code>ascending</code> and column expressions like <code>asc()</code>/<code>desc()</code> are used</li> </ul> </li> <li> <code>TypeError</code>             \u2013            <ul> <li>If <code>cols</code> is not a column name, Column, or list of column names/Columns</li> <li>If <code>ascending</code> is not a boolean or list of booleans</li> </ul> </li> </ul> Sort in ascending order <pre><code># Create sample DataFrame\ndf = session.create_dataframe([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n\n# Sort by age in ascending order\ndf.sort(asc(col(\"age\"))).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  2|Alice|\n# |  5|  Bob|\n# +---+-----+\n</code></pre> Sort in descending order <pre><code># Sort by age in descending order\ndf.sort(col(\"age\").desc()).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  5|  Bob|\n# |  2|Alice|\n# +---+-----+\n</code></pre> Sort with boolean ascending parameter <pre><code># Sort by age in descending order using boolean\ndf.sort(col(\"age\"), ascending=False).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  5|  Bob|\n# |  2|Alice|\n# +---+-----+\n</code></pre> Multiple columns with different sort orders <pre><code># Create sample DataFrame\ndf = session.create_dataframe([(2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n\n# Sort by age descending, then name ascending\ndf.sort(desc(col(\"age\")), col(\"name\")).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  5|  Bob|\n# |  2|Alice|\n# |  2|  Bob|\n# +---+-----+\n</code></pre> Multiple columns with list of ascending strategies <pre><code># Sort both columns in descending order\ndf.sort([col(\"age\"), col(\"name\")], ascending=[False, False]).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  5|  Bob|\n# |  2|  Bob|\n# |  2|Alice|\n# +---+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def sort(\n    self,\n    cols: Union[ColumnOrName, List[ColumnOrName], None] = None,\n    ascending: Optional[Union[bool, List[bool]]] = None,\n) -&gt; DataFrame:\n    \"\"\"Sort the DataFrame by the specified columns.\n\n    Args:\n        cols: Columns to sort by. This can be:\n            - A single column name (str)\n            - A Column expression (e.g., `col(\"name\")`)\n            - A list of column names or Column expressions\n            - Column expressions may include sorting directives such as `asc(\"col\")`, `desc(\"col\")`,\n            `asc_nulls_last(\"col\")`, etc.\n            - If no columns are provided, the operation is a no-op.\n\n        ascending: A boolean or list of booleans indicating sort order.\n            - If `True`, sorts in ascending order; if `False`, descending.\n            - If a list is provided, its length must match the number of columns.\n            - Cannot be used if any of the columns use `asc()`/`desc()` expressions.\n            - If not specified and no sort expressions are used, columns will be sorted in ascending order by default.\n\n    Returns:\n        DataFrame: A new DataFrame sorted by the specified columns.\n\n    Raises:\n        ValueError:\n            - If `ascending` is provided and its length does not match `cols`\n            - If both `ascending` and column expressions like `asc()`/`desc()` are used\n        TypeError:\n            - If `cols` is not a column name, Column, or list of column names/Columns\n            - If `ascending` is not a boolean or list of booleans\n\n    Example: Sort in ascending order\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n\n        # Sort by age in ascending order\n        df.sort(asc(col(\"age\"))).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  2|Alice|\n        # |  5|  Bob|\n        # +---+-----+\n        ```\n\n    Example: Sort in descending order\n        ```python\n        # Sort by age in descending order\n        df.sort(col(\"age\").desc()).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  5|  Bob|\n        # |  2|Alice|\n        # +---+-----+\n        ```\n\n    Example: Sort with boolean ascending parameter\n        ```python\n        # Sort by age in descending order using boolean\n        df.sort(col(\"age\"), ascending=False).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  5|  Bob|\n        # |  2|Alice|\n        # +---+-----+\n        ```\n\n    Example: Multiple columns with different sort orders\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe([(2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n\n        # Sort by age descending, then name ascending\n        df.sort(desc(col(\"age\")), col(\"name\")).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  5|  Bob|\n        # |  2|Alice|\n        # |  2|  Bob|\n        # +---+-----+\n        ```\n\n    Example: Multiple columns with list of ascending strategies\n        ```python\n        # Sort both columns in descending order\n        df.sort([col(\"age\"), col(\"name\")], ascending=[False, False]).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  5|  Bob|\n        # |  2|  Bob|\n        # |  2|Alice|\n        # +---+-----+\n        ```\n    \"\"\"\n    col_args = cols\n    if cols is None:\n        return self._from_logical_plan(\n            Sort(self._logical_plan, [])\n        )\n    elif not isinstance(cols, List):\n        col_args = [cols]\n\n    # parse the ascending arguments\n    bool_ascending = []\n    using_default_ascending = False\n    if ascending is None:\n        using_default_ascending = True\n        bool_ascending = [True] * len(col_args)\n    elif isinstance(ascending, bool):\n        bool_ascending = [ascending] * len(col_args)\n    elif isinstance(ascending, List):\n        bool_ascending = ascending\n        if len(bool_ascending) != len(cols):\n            raise ValueError(\n                f\"the list length of ascending sort strategies must match the specified sort columns\"\n                f\"Got {len(cols)} column expressions and {len(bool_ascending)} ascending strategies. \"\n            )\n    else:\n        raise TypeError(\n            f\"Invalid ascending strategy type: {type(ascending)}.  Must be a boolean or list of booleans.\"\n        )\n\n    # create our list of sort expressions, for each column expression\n    # that isn't already provided as a asc()/desc() SortExpr\n    sort_exprs = []\n    for c, asc_bool in zip(col_args, bool_ascending, strict=True):\n        if isinstance(c, ColumnOrName):\n            c_expr = Column._from_col_or_name(c)._logical_expr\n        else:\n            raise TypeError(\n                f\"Invalid column type: {type(c).__name__}.  Must be a string or Column Expression.\"\n            )\n        if not isinstance(asc_bool, bool):\n            raise TypeError(\n                f\"Invalid ascending strategy type: {type(asc_bool).__name__}.  Must be a boolean.\"\n            )\n        if isinstance(c_expr, SortExpr):\n            if not using_default_ascending:\n                raise TypeError(\n                    \"Cannot specify both asc()/desc() expressions and boolean ascending strategies.\"\n                    f\"Got expression: {c_expr} and ascending argument: {bool_ascending}\"\n                )\n            sort_exprs.append(c_expr)\n        else:\n            sort_exprs.append(SortExpr(c_expr, ascending=asc_bool))\n\n    return self._from_logical_plan(\n        Sort(self._logical_plan, sort_exprs),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.to_arrow","title":"to_arrow","text":"<pre><code>to_arrow() -&gt; pa.Table\n</code></pre> <p>Execute the DataFrame computation and return an Apache Arrow Table.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into an Apache Arrow Table with columnar memory layout optimized for analytics and zero-copy data exchange.</p> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>pa.Table: An Apache Arrow Table containing the computed results</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_arrow(self) -&gt; pa.Table:\n    \"\"\"Execute the DataFrame computation and return an Apache Arrow Table.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into an Apache Arrow Table with columnar memory layout\n    optimized for analytics and zero-copy data exchange.\n\n    Returns:\n        pa.Table: An Apache Arrow Table containing the computed results\n    \"\"\"\n    return self.collect(\"arrow\").data\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; pd.DataFrame\n</code></pre> <p>Execute the DataFrame computation and return a Pandas DataFrame.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a Pandas DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A Pandas DataFrame containing the computed results with</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"Execute the DataFrame computation and return a Pandas DataFrame.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a Pandas DataFrame.\n\n    Returns:\n        pd.DataFrame: A Pandas DataFrame containing the computed results with\n    \"\"\"\n    return self.collect(\"pandas\").data\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.to_polars","title":"to_polars","text":"<pre><code>to_polars() -&gt; pl.DataFrame\n</code></pre> <p>Execute the DataFrame computation and return the result as a Polars DataFrame.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a Polars DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pl.DataFrame: A Polars DataFrame with materialized results</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_polars(self) -&gt; pl.DataFrame:\n    \"\"\"Execute the DataFrame computation and return the result as a Polars DataFrame.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a Polars DataFrame.\n\n    Returns:\n        pl.DataFrame: A Polars DataFrame with materialized results\n    \"\"\"\n    return self.collect(\"polars\").data\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.to_pydict","title":"to_pydict","text":"<pre><code>to_pydict() -&gt; Dict[str, List[Any]]\n</code></pre> <p>Execute the DataFrame computation and return a dictionary of column arrays.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a Python dictionary where each column becomes a list of values.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, List[Any]]</code>           \u2013            <p>Dict[str, List[Any]]: A dictionary containing the computed results with: - Keys: Column names as strings - Values: Lists containing all values for each column</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_pydict(self) -&gt; Dict[str, List[Any]]:\n    \"\"\"Execute the DataFrame computation and return a dictionary of column arrays.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a Python dictionary where each column becomes a list of values.\n\n    Returns:\n        Dict[str, List[Any]]: A dictionary containing the computed results with:\n            - Keys: Column names as strings\n            - Values: Lists containing all values for each column\n    \"\"\"\n    return self.collect(\"pydict\").data\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.to_pylist","title":"to_pylist","text":"<pre><code>to_pylist() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Execute the DataFrame computation and return a list of row dictionaries.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a Python list where each element is a dictionary representing one row with column names as keys.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>List[Dict[str, Any]]: A list containing the computed results with: - Each element: A dictionary representing one row - Dictionary keys: Column names as strings - Dictionary values: Cell values in Python native types - List length equals number of rows in the result</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_pylist(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Execute the DataFrame computation and return a list of row dictionaries.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a Python list where each element is a dictionary\n    representing one row with column names as keys.\n\n    Returns:\n        List[Dict[str, Any]]: A list containing the computed results with:\n            - Each element: A dictionary representing one row\n            - Dictionary keys: Column names as strings\n            - Dictionary values: Cell values in Python native types\n            - List length equals number of rows in the result\n    \"\"\"\n    return self.collect(\"pylist\").data\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.union","title":"union","text":"<pre><code>union(other: DataFrame) -&gt; DataFrame\n</code></pre> <p>Return a new DataFrame containing the union of rows in this and another DataFrame.</p> <p>This is equivalent to UNION ALL in SQL. To remove duplicates, use drop_duplicates() after union().</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>DataFrame</code>)           \u2013            <p>Another DataFrame with the same schema.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame containing rows from both DataFrames.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the DataFrames have different schemas.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If other is not a DataFrame.</p> </li> </ul> Union two DataFrames <pre><code># Create two DataFrames\ndf1 = session.create_dataframe({\n    \"id\": [1, 2],\n    \"value\": [\"a\", \"b\"]\n})\ndf2 = session.create_dataframe({\n    \"id\": [3, 4],\n    \"value\": [\"c\", \"d\"]\n})\n\n# Union the DataFrames\ndf1.union(df2).show()\n# Output:\n# +---+-----+\n# | id|value|\n# +---+-----+\n# |  1|    a|\n# |  2|    b|\n# |  3|    c|\n# |  4|    d|\n# +---+-----+\n</code></pre> Union with duplicates <pre><code># Create DataFrames with overlapping data\ndf1 = session.create_dataframe({\n    \"id\": [1, 2],\n    \"value\": [\"a\", \"b\"]\n})\ndf2 = session.create_dataframe({\n    \"id\": [2, 3],\n    \"value\": [\"b\", \"c\"]\n})\n\n# Union with duplicates\ndf1.union(df2).show()\n# Output:\n# +---+-----+\n# | id|value|\n# +---+-----+\n# |  1|    a|\n# |  2|    b|\n# |  2|    b|\n# |  3|    c|\n# +---+-----+\n\n# Remove duplicates after union\ndf1.union(df2).drop_duplicates().show()\n# Output:\n# +---+-----+\n# | id|value|\n# +---+-----+\n# |  1|    a|\n# |  2|    b|\n# |  3|    c|\n# +---+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def union(self, other: DataFrame) -&gt; DataFrame:\n    \"\"\"Return a new DataFrame containing the union of rows in this and another DataFrame.\n\n    This is equivalent to UNION ALL in SQL. To remove duplicates, use drop_duplicates() after union().\n\n    Args:\n        other: Another DataFrame with the same schema.\n\n    Returns:\n        DataFrame: A new DataFrame containing rows from both DataFrames.\n\n    Raises:\n        ValueError: If the DataFrames have different schemas.\n        TypeError: If other is not a DataFrame.\n\n    Example: Union two DataFrames\n        ```python\n        # Create two DataFrames\n        df1 = session.create_dataframe({\n            \"id\": [1, 2],\n            \"value\": [\"a\", \"b\"]\n        })\n        df2 = session.create_dataframe({\n            \"id\": [3, 4],\n            \"value\": [\"c\", \"d\"]\n        })\n\n        # Union the DataFrames\n        df1.union(df2).show()\n        # Output:\n        # +---+-----+\n        # | id|value|\n        # +---+-----+\n        # |  1|    a|\n        # |  2|    b|\n        # |  3|    c|\n        # |  4|    d|\n        # +---+-----+\n        ```\n\n    Example: Union with duplicates\n        ```python\n        # Create DataFrames with overlapping data\n        df1 = session.create_dataframe({\n            \"id\": [1, 2],\n            \"value\": [\"a\", \"b\"]\n        })\n        df2 = session.create_dataframe({\n            \"id\": [2, 3],\n            \"value\": [\"b\", \"c\"]\n        })\n\n        # Union with duplicates\n        df1.union(df2).show()\n        # Output:\n        # +---+-----+\n        # | id|value|\n        # +---+-----+\n        # |  1|    a|\n        # |  2|    b|\n        # |  2|    b|\n        # |  3|    c|\n        # +---+-----+\n\n        # Remove duplicates after union\n        df1.union(df2).drop_duplicates().show()\n        # Output:\n        # +---+-----+\n        # | id|value|\n        # +---+-----+\n        # |  1|    a|\n        # |  2|    b|\n        # |  3|    c|\n        # +---+-----+\n        ```\n    \"\"\"\n    return self._from_logical_plan(\n        UnionLogicalPlan([self._logical_plan, other._logical_plan]),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.unnest","title":"unnest","text":"<pre><code>unnest(*col_names: str) -&gt; DataFrame\n</code></pre> <p>Unnest the specified struct columns into separate columns.</p> <p>This operation flattens nested struct data by expanding each field of a struct into its own top-level column.</p> <p>For each specified column containing a struct: 1. Each field in the struct becomes a separate column. 2. New columns are named after the corresponding struct fields. 3. The new columns are inserted into the DataFrame in place of the original struct column. 4. The overall column order is preserved.</p> <p>Parameters:</p> <ul> <li> <code>*col_names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>One or more struct columns to unnest. Each can be a string (column name) or a Column expression.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with the specified struct columns expanded.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If any argument is not a string or Column.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If a specified column does not contain struct data.</p> </li> </ul> Unnest struct column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"id\": [1, 2],\n    \"tags\": [{\"red\": 1, \"blue\": 2}, {\"red\": 3}],\n    \"name\": [\"Alice\", \"Bob\"]\n})\n\n# Unnest the tags column\ndf.unnest(col(\"tags\")).show()\n# Output:\n# +---+---+----+-----+\n# | id| red|blue| name|\n# +---+---+----+-----+\n# |  1|  1|   2|Alice|\n# |  2|  3|null|  Bob|\n# +---+---+----+-----+\n</code></pre> Unnest multiple struct columns <pre><code># Create sample DataFrame with multiple struct columns\ndf = session.create_dataframe({\n    \"id\": [1, 2],\n    \"tags\": [{\"red\": 1, \"blue\": 2}, {\"red\": 3}],\n    \"info\": [{\"age\": 25, \"city\": \"NY\"}, {\"age\": 30, \"city\": \"LA\"}],\n    \"name\": [\"Alice\", \"Bob\"]\n})\n\n# Unnest multiple struct columns\ndf.unnest(col(\"tags\"), col(\"info\")).show()\n# Output:\n# +---+---+----+---+----+-----+\n# | id| red|blue|age|city| name|\n# +---+---+----+---+----+-----+\n# |  1|  1|   2| 25|  NY|Alice|\n# |  2|  3|null| 30|  LA|  Bob|\n# +---+---+----+---+----+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def unnest(self, *col_names: str) -&gt; DataFrame:\n    \"\"\"Unnest the specified struct columns into separate columns.\n\n    This operation flattens nested struct data by expanding each field of a struct\n    into its own top-level column.\n\n    For each specified column containing a struct:\n    1. Each field in the struct becomes a separate column.\n    2. New columns are named after the corresponding struct fields.\n    3. The new columns are inserted into the DataFrame in place of the original struct column.\n    4. The overall column order is preserved.\n\n    Args:\n        *col_names: One or more struct columns to unnest. Each can be a string (column name)\n            or a Column expression.\n\n    Returns:\n        DataFrame: A new DataFrame with the specified struct columns expanded.\n\n    Raises:\n        TypeError: If any argument is not a string or Column.\n        ValueError: If a specified column does not contain struct data.\n\n    Example: Unnest struct column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"id\": [1, 2],\n            \"tags\": [{\"red\": 1, \"blue\": 2}, {\"red\": 3}],\n            \"name\": [\"Alice\", \"Bob\"]\n        })\n\n        # Unnest the tags column\n        df.unnest(col(\"tags\")).show()\n        # Output:\n        # +---+---+----+-----+\n        # | id| red|blue| name|\n        # +---+---+----+-----+\n        # |  1|  1|   2|Alice|\n        # |  2|  3|null|  Bob|\n        # +---+---+----+-----+\n        ```\n\n    Example: Unnest multiple struct columns\n        ```python\n        # Create sample DataFrame with multiple struct columns\n        df = session.create_dataframe({\n            \"id\": [1, 2],\n            \"tags\": [{\"red\": 1, \"blue\": 2}, {\"red\": 3}],\n            \"info\": [{\"age\": 25, \"city\": \"NY\"}, {\"age\": 30, \"city\": \"LA\"}],\n            \"name\": [\"Alice\", \"Bob\"]\n        })\n\n        # Unnest multiple struct columns\n        df.unnest(col(\"tags\"), col(\"info\")).show()\n        # Output:\n        # +---+---+----+---+----+-----+\n        # | id| red|blue|age|city| name|\n        # +---+---+----+---+----+-----+\n        # |  1|  1|   2| 25|  NY|Alice|\n        # |  2|  3|null| 30|  LA|  Bob|\n        # +---+---+----+---+----+-----+\n        ```\n    \"\"\"\n    if not col_names:\n        return self\n    exprs = []\n    for c in col_names:\n        if c not in self.columns:\n            raise TypeError(f\"Column {c} not found in DataFrame.\")\n        exprs.append(col(c)._logical_expr)\n    return self._from_logical_plan(\n        Unnest(self._logical_plan, exprs),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.where","title":"where","text":"<pre><code>where(condition: Column) -&gt; DataFrame\n</code></pre> <p>Filters rows using the given condition (alias for filter()).</p> <p>Parameters:</p> <ul> <li> <code>condition</code>               (<code>Column</code>)           \u2013            <p>A Column expression that evaluates to a boolean</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Filtered DataFrame</p> </li> </ul> See Also <p>filter(): Full documentation of filtering behavior</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def where(self, condition: Column) -&gt; DataFrame:\n    \"\"\"Filters rows using the given condition (alias for filter()).\n\n    Args:\n        condition: A Column expression that evaluates to a boolean\n\n    Returns:\n        DataFrame: Filtered DataFrame\n\n    See Also:\n        filter(): Full documentation of filtering behavior\n    \"\"\"\n    return self.filter(condition)\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.with_column","title":"with_column","text":"<pre><code>with_column(col_name: str, col: Union[Any, Column]) -&gt; DataFrame\n</code></pre> <p>Add a new column or replace an existing column.</p> <p>Parameters:</p> <ul> <li> <code>col_name</code>               (<code>str</code>)           \u2013            <p>Name of the new column</p> </li> <li> <code>col</code>               (<code>Union[Any, Column]</code>)           \u2013            <p>Column expression or value to assign to the column. If not a Column, it will be treated as a literal value.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>New DataFrame with added/replaced column</p> </li> </ul> Add literal column <pre><code># Create a DataFrame\ndf = session.create_dataframe({\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]})\n\n# Add literal column\ndf.with_column(\"constant\", lit(1)).show()\n# Output:\n# +-----+---+--------+\n# | name|age|constant|\n# +-----+---+--------+\n# |Alice| 25|       1|\n# |  Bob| 30|       1|\n# +-----+---+--------+\n</code></pre> Add computed column <pre><code># Add computed column\ndf.with_column(\"double_age\", col(\"age\") * 2).show()\n# Output:\n# +-----+---+----------+\n# | name|age|double_age|\n# +-----+---+----------+\n# |Alice| 25|        50|\n# |  Bob| 30|        60|\n# +-----+---+----------+\n</code></pre> Replace existing column <pre><code># Replace existing column\ndf.with_column(\"age\", col(\"age\") + 1).show()\n# Output:\n# +-----+---+\n# | name|age|\n# +-----+---+\n# |Alice| 26|\n# |  Bob| 31|\n# +-----+---+\n</code></pre> Add column with complex expression <pre><code># Add column with complex expression\ndf.with_column(\n    \"age_category\",\n    when(col(\"age\") &lt; 30, \"young\")\n    .when(col(\"age\") &lt; 50, \"middle\")\n    .otherwise(\"senior\")\n).show()\n# Output:\n# +-----+---+------------+\n# | name|age|age_category|\n# +-----+---+------------+\n# |Alice| 25|       young|\n# |  Bob| 30|     middle|\n# +-----+---+------------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def with_column(self, col_name: str, col: Union[Any, Column]) -&gt; DataFrame:\n    \"\"\"Add a new column or replace an existing column.\n\n    Args:\n        col_name: Name of the new column\n        col: Column expression or value to assign to the column. If not a Column,\n            it will be treated as a literal value.\n\n    Returns:\n        DataFrame: New DataFrame with added/replaced column\n\n    Example: Add literal column\n        ```python\n        # Create a DataFrame\n        df = session.create_dataframe({\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]})\n\n        # Add literal column\n        df.with_column(\"constant\", lit(1)).show()\n        # Output:\n        # +-----+---+--------+\n        # | name|age|constant|\n        # +-----+---+--------+\n        # |Alice| 25|       1|\n        # |  Bob| 30|       1|\n        # +-----+---+--------+\n        ```\n\n    Example: Add computed column\n        ```python\n        # Add computed column\n        df.with_column(\"double_age\", col(\"age\") * 2).show()\n        # Output:\n        # +-----+---+----------+\n        # | name|age|double_age|\n        # +-----+---+----------+\n        # |Alice| 25|        50|\n        # |  Bob| 30|        60|\n        # +-----+---+----------+\n        ```\n\n    Example: Replace existing column\n        ```python\n        # Replace existing column\n        df.with_column(\"age\", col(\"age\") + 1).show()\n        # Output:\n        # +-----+---+\n        # | name|age|\n        # +-----+---+\n        # |Alice| 26|\n        # |  Bob| 31|\n        # +-----+---+\n        ```\n\n    Example: Add column with complex expression\n        ```python\n        # Add column with complex expression\n        df.with_column(\n            \"age_category\",\n            when(col(\"age\") &lt; 30, \"young\")\n            .when(col(\"age\") &lt; 50, \"middle\")\n            .otherwise(\"senior\")\n        ).show()\n        # Output:\n        # +-----+---+------------+\n        # | name|age|age_category|\n        # +-----+---+------------+\n        # |Alice| 25|       young|\n        # |  Bob| 30|     middle|\n        # +-----+---+------------+\n        ```\n    \"\"\"\n    exprs = []\n    if not isinstance(col, Column):\n        col = lit(col)\n\n    for field in self.columns:\n        if field != col_name:\n            exprs.append(Column._from_column_name(field)._logical_expr)\n\n    # Add the new column with alias\n    exprs.append(col.alias(col_name)._logical_expr)\n\n    return self._from_logical_plan(\n        Projection(self._logical_plan, exprs)\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.DataFrame.with_column_renamed","title":"with_column_renamed","text":"<pre><code>with_column_renamed(col_name: str, new_col_name: str) -&gt; DataFrame\n</code></pre> <p>Rename a column. No-op if the column does not exist.</p> <p>Parameters:</p> <ul> <li> <code>col_name</code>               (<code>str</code>)           \u2013            <p>Name of the column to rename.</p> </li> <li> <code>new_col_name</code>               (<code>str</code>)           \u2013            <p>New name for the column.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>New DataFrame with the column renamed.</p> </li> </ul> Rename a column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"age\": [25, 30, 35],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n})\n\n# Rename a column\ndf.with_column_renamed(\"age\", \"age_in_years\").show()\n# Output:\n# +------------+-------+\n# |age_in_years|   name|\n# +------------+-------+\n# |         25|  Alice|\n# |         30|    Bob|\n# |         35|Charlie|\n# +------------+-------+\n</code></pre> Rename multiple columns <pre><code># Rename multiple columns\ndf = (df\n    .with_column_renamed(\"age\", \"age_in_years\")\n    .with_column_renamed(\"name\", \"full_name\")\n).show()\n# Output:\n# +------------+----------+\n# |age_in_years|full_name |\n# +------------+----------+\n# |         25|     Alice|\n# |         30|       Bob|\n# |         35|   Charlie|\n# +------------+----------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def with_column_renamed(self, col_name: str, new_col_name: str) -&gt; DataFrame:\n    \"\"\"Rename a column. No-op if the column does not exist.\n\n    Args:\n        col_name: Name of the column to rename.\n        new_col_name: New name for the column.\n\n    Returns:\n        DataFrame: New DataFrame with the column renamed.\n\n    Example: Rename a column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"age\": [25, 30, 35],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n        })\n\n        # Rename a column\n        df.with_column_renamed(\"age\", \"age_in_years\").show()\n        # Output:\n        # +------------+-------+\n        # |age_in_years|   name|\n        # +------------+-------+\n        # |         25|  Alice|\n        # |         30|    Bob|\n        # |         35|Charlie|\n        # +------------+-------+\n        ```\n\n    Example: Rename multiple columns\n        ```python\n        # Rename multiple columns\n        df = (df\n            .with_column_renamed(\"age\", \"age_in_years\")\n            .with_column_renamed(\"name\", \"full_name\")\n        ).show()\n        # Output:\n        # +------------+----------+\n        # |age_in_years|full_name |\n        # +------------+----------+\n        # |         25|     Alice|\n        # |         30|       Bob|\n        # |         35|   Charlie|\n        # +------------+----------+\n        ```\n    \"\"\"\n    exprs = []\n    renamed = False\n\n    for field in self.schema.column_fields:\n        name = field.name\n        if name == col_name:\n            exprs.append(col(name).alias(new_col_name)._logical_expr)\n            renamed = True\n        else:\n            exprs.append(col(name)._logical_expr)\n\n    if not renamed:\n        return self\n\n    return self._from_logical_plan(\n        Projection(self._logical_plan, exprs)\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.GroupedData","title":"GroupedData","text":"<pre><code>GroupedData(df: DataFrame, by: Optional[List[ColumnOrName]] = None)\n</code></pre> <p>               Bases: <code>BaseGroupedData</code></p> <p>Methods for aggregations on a grouped DataFrame.</p> <p>Initialize grouped data.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to group.</p> </li> <li> <code>by</code>               (<code>Optional[List[ColumnOrName]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of columns to group by.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>agg</code>             \u2013              <p>Compute aggregations on grouped data and return the result as a DataFrame.</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/grouped_data.py</code> <pre><code>def __init__(self, df: DataFrame, by: Optional[List[ColumnOrName]] = None):\n    \"\"\"Initialize grouped data.\n\n    Args:\n        df: The DataFrame to group.\n        by: Optional list of columns to group by.\n    \"\"\"\n    super().__init__(df)\n    self._by: List[Column] = []\n    for c in by or []:\n        if isinstance(c, str):\n            self._by.append(col(c))\n        elif isinstance(c, Column):\n            # Allow any expression except literals\n            if isinstance(c._logical_expr, LiteralExpr):\n                raise ValueError(f\"Cannot group by literal value: {c}\")\n            self._by.append(c)\n        else:\n            raise TypeError(\n                f\"Group by expressions must be string or Column, got {type(c)}\"\n            )\n    self._by_exprs = [c._logical_expr for c in self._by]\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.GroupedData.agg","title":"agg","text":"<pre><code>agg(*exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame\n</code></pre> <p>Compute aggregations on grouped data and return the result as a DataFrame.</p> <p>This method applies aggregate functions to the grouped data.</p> <p>Parameters:</p> <ul> <li> <code>*exprs</code>               (<code>Union[Column, Dict[str, str]]</code>, default:                   <code>()</code> )           \u2013            <p>Aggregation expressions. Can be:</p> <ul> <li>Column expressions with aggregate functions (e.g., <code>count(\"*\")</code>, <code>sum(\"amount\")</code>)</li> <li>A dictionary mapping column names to aggregate function names (e.g., <code>{\"amount\": \"sum\", \"age\": \"avg\"}</code>)</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with one row per group and columns for group keys and aggregated values</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If arguments are not Column expressions or a dictionary</p> </li> <li> <code>ValueError</code>             \u2013            <p>If dictionary values are not valid aggregate function names</p> </li> </ul> Count employees by department <pre><code># Group by department and count employees\ndf.group_by(\"department\").agg(count(\"*\").alias(\"employee_count\"))\n</code></pre> Multiple aggregations <pre><code># Multiple aggregations\ndf.group_by(\"department\").agg(\n    count(\"*\").alias(\"employee_count\"),\n    avg(\"salary\").alias(\"avg_salary\"),\n    max(\"age\").alias(\"max_age\")\n)\n</code></pre> Dictionary style aggregations <pre><code># Dictionary style for simple aggregations\ndf.group_by(\"department\", \"location\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n</code></pre> Source code in <code>src/fenic/api/dataframe/grouped_data.py</code> <pre><code>def agg(self, *exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame:\n    \"\"\"Compute aggregations on grouped data and return the result as a DataFrame.\n\n    This method applies aggregate functions to the grouped data.\n\n    Args:\n        *exprs: Aggregation expressions. Can be:\n\n            - Column expressions with aggregate functions (e.g., `count(\"*\")`, `sum(\"amount\")`)\n            - A dictionary mapping column names to aggregate function names (e.g., `{\"amount\": \"sum\", \"age\": \"avg\"}`)\n\n    Returns:\n        DataFrame: A new DataFrame with one row per group and columns for group keys and aggregated values\n\n    Raises:\n        ValueError: If arguments are not Column expressions or a dictionary\n        ValueError: If dictionary values are not valid aggregate function names\n\n    Example: Count employees by department\n        ```python\n        # Group by department and count employees\n        df.group_by(\"department\").agg(count(\"*\").alias(\"employee_count\"))\n        ```\n\n    Example: Multiple aggregations\n        ```python\n        # Multiple aggregations\n        df.group_by(\"department\").agg(\n            count(\"*\").alias(\"employee_count\"),\n            avg(\"salary\").alias(\"avg_salary\"),\n            max(\"age\").alias(\"max_age\")\n        )\n        ```\n\n    Example: Dictionary style aggregations\n        ```python\n        # Dictionary style for simple aggregations\n        df.group_by(\"department\", \"location\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n        ```\n    \"\"\"\n    self._validate_agg_exprs(*exprs)\n    if len(exprs) == 1 and isinstance(exprs[0], dict):\n        agg_dict = exprs[0]\n        return self.agg(*self._process_agg_dict(agg_dict))\n\n    agg_exprs = self._process_agg_exprs(exprs)\n    return self._df._from_logical_plan(\n        Aggregate(self._df._logical_plan, self._by_exprs, agg_exprs),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.SemGroupedData","title":"SemGroupedData","text":"<pre><code>SemGroupedData(df: DataFrame, by: ColumnOrName, num_clusters: int)\n</code></pre> <p>               Bases: <code>BaseGroupedData</code></p> <p>Methods for aggregations on a semantically clustered DataFrame.</p> <p>Initialize semantic grouped data.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to group.</p> </li> <li> <code>by</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column containing embeddings to cluster.</p> </li> <li> <code>num_clusters</code>               (<code>int</code>)           \u2013            <p>Number of semantic clusters to create.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>agg</code>             \u2013              <p>Compute aggregations on semantically clustered data and return the result as a DataFrame.</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def __init__(self, df: DataFrame, by: ColumnOrName, num_clusters: int):\n    \"\"\"Initialize semantic grouped data.\n\n    Args:\n        df: The DataFrame to group.\n        by: Column containing embeddings to cluster.\n        num_clusters: Number of semantic clusters to create.\n    \"\"\"\n    super().__init__(df)\n    if not isinstance(num_clusters, int) or num_clusters &lt;= 0:\n        raise ValidationError(\n            \"`num_clusters` must be a positive integer greater than 0.\"\n        )\n    if not isinstance(by, ColumnOrName):\n        raise ValidationError(\n            f\"Invalid group by: expected a column name (str) or Column object, but got {type(by).__name__}.\"\n        )\n\n    self._num_clusters = num_clusters\n    self._by_expr = Column._from_col_or_name(by)._logical_expr\n\n    if isinstance(self._by_expr, LiteralExpr):\n        raise ValidationError(\n            f\"Invalid group by: Cannot group by a literal value: {self._by_expr}. Group by a column name or a valid expression instead.\"\n        )\n\n    if not isinstance(self._by_expr.to_column_field(self._df._logical_plan).data_type, EmbeddingType):\n        raise TypeMismatchError.from_message(\n            f\"semantic.group_by grouping expression must be an embedding column type (EmbeddingType); \"\n            f\"got: {self._by_expr.to_column_field(self._df._logical_plan).data_type}\"\n        )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.SemGroupedData.agg","title":"agg","text":"<pre><code>agg(*exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame\n</code></pre> <p>Compute aggregations on semantically clustered data and return the result as a DataFrame.</p> <p>This method applies aggregate functions to data that has been grouped by semantic similarity, allowing you to discover patterns and insights across natural language clusters.</p> <p>Parameters:</p> <ul> <li> <code>*exprs</code>               (<code>Union[Column, Dict[str, str]]</code>, default:                   <code>()</code> )           \u2013            <p>Aggregation expressions. Can be:</p> <ul> <li>Column expressions with aggregate functions (e.g., <code>count(\"*\")</code>, <code>avg(\"sentiment\")</code>)</li> <li>A dictionary mapping column names to aggregate function names (e.g., {\"sentiment\": \"avg\", \"count\": \"sum\"})</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with one row per semantic cluster and columns for aggregated values</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If arguments are not Column expressions or a dictionary</p> </li> <li> <code>ValueError</code>             \u2013            <p>If dictionary values are not valid aggregate function names</p> </li> </ul> Count items per cluster <pre><code># Group customer feedback into 5 clusters and count items per cluster\ndf.semantic.group_by(\"feedback_embeddings\", 5).agg(count(\"*\").alias(\"feedback_count\"))\n</code></pre> Analyze multiple metrics across clusters <pre><code># Analyze multiple metrics across semantic clusters\ndf.semantic.group_by(\"product_review_embeddings\", 3).agg(\n    count(\"*\").alias(\"review_count\"),\n    avg(\"rating\").alias(\"avg_rating\"),\n    avg(\"sentiment_score\").alias(\"avg_sentiment\")\n)\n</code></pre> Dictionary style aggregations <pre><code># Dictionary style for simple aggregations\ndf.semantic.group_by(\"support_ticket_embeddings\", 4).agg({\"priority\": \"avg\", \"resolution_time\": \"max\"})\n</code></pre> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def agg(self, *exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame:\n    \"\"\"Compute aggregations on semantically clustered data and return the result as a DataFrame.\n\n    This method applies aggregate functions to data that has been grouped by semantic similarity,\n    allowing you to discover patterns and insights across natural language clusters.\n\n    Args:\n        *exprs: Aggregation expressions. Can be:\n\n            - Column expressions with aggregate functions (e.g., `count(\"*\")`, `avg(\"sentiment\")`)\n            - A dictionary mapping column names to aggregate function names (e.g., {\"sentiment\": \"avg\", \"count\": \"sum\"})\n\n    Returns:\n        DataFrame: A new DataFrame with one row per semantic cluster and columns for aggregated values\n\n    Raises:\n        ValueError: If arguments are not Column expressions or a dictionary\n        ValueError: If dictionary values are not valid aggregate function names\n\n    Example: Count items per cluster\n        ```python\n        # Group customer feedback into 5 clusters and count items per cluster\n        df.semantic.group_by(\"feedback_embeddings\", 5).agg(count(\"*\").alias(\"feedback_count\"))\n        ```\n\n    Example: Analyze multiple metrics across clusters\n        ```python\n        # Analyze multiple metrics across semantic clusters\n        df.semantic.group_by(\"product_review_embeddings\", 3).agg(\n            count(\"*\").alias(\"review_count\"),\n            avg(\"rating\").alias(\"avg_rating\"),\n            avg(\"sentiment_score\").alias(\"avg_sentiment\")\n        )\n        ```\n\n    Example: Dictionary style aggregations\n        ```python\n        # Dictionary style for simple aggregations\n        df.semantic.group_by(\"support_ticket_embeddings\", 4).agg({\"priority\": \"avg\", \"resolution_time\": \"max\"})\n        ```\n    \"\"\"\n    self._validate_agg_exprs(*exprs)\n    if len(exprs) == 1 and isinstance(exprs[0], dict):\n        return self.agg(*self._process_agg_dict(exprs[0]))\n    agg_exprs = self._process_agg_exprs(exprs)\n    return self._df._from_logical_plan(\n        SemanticAggregate(\n            self._df._logical_plan, self._by_expr, agg_exprs, self._num_clusters\n        ),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.SemanticExtensions","title":"SemanticExtensions","text":"<pre><code>SemanticExtensions(df: DataFrame)\n</code></pre> <p>A namespace for semantic dataframe operators.</p> <p>Initialize semantic extensions.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to extend with semantic operations.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>group_by</code>             \u2013              <p>Semantically group rows by clustering an embedding column into the specified number of centroids.</p> </li> <li> <code>join</code>             \u2013              <p>Performs a semantic join between two DataFrames using a natural language predicate.</p> </li> <li> <code>sim_join</code>             \u2013              <p>Performs a semantic similarity join between two DataFrames using precomputed text embeddings.</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def __init__(self, df: DataFrame):\n    \"\"\"Initialize semantic extensions.\n\n    Args:\n        df: The DataFrame to extend with semantic operations.\n    \"\"\"\n    self._df = df\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.SemanticExtensions.group_by","title":"group_by","text":"<pre><code>group_by(by: ColumnOrName, num_clusters: int) -&gt; SemGroupedData\n</code></pre> <p>Semantically group rows by clustering an embedding column into the specified number of centroids.</p> <p>This method is useful when you want to uncover natural themes, topics, or intent in embedded free-form text, without needing predefined categories.</p> <p>Parameters:</p> <ul> <li> <code>by</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column containing embeddings to cluster</p> </li> <li> <code>num_clusters</code>               (<code>int</code>)           \u2013            <p>Number of semantic clusters to create</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SemGroupedData</code> (              <code>SemGroupedData</code> )          \u2013            <p>Object for performing aggregations on the clustered data.</p> </li> </ul> Basic semantic grouping <pre><code># Group customer feedback into 5 clusters\ndf.semantic.group_by(\"feedback_embeddings\", 5).agg(count(\"*\"))\n</code></pre> Analyze sentiment by semantic group <pre><code># Analyze sentiment by semantic group\ndf.semantic.group_by(\"feedback_embeddings\", 5).agg(\n    count(\"*\").alias(\"count\"),\n    avg(\"sentiment_score\").alias(\"avg_sentiment\")\n)\n</code></pre> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def group_by(self, by: ColumnOrName, num_clusters: int) -&gt; SemGroupedData:\n    \"\"\"Semantically group rows by clustering an embedding column into the specified number of centroids.\n\n    This method is useful when you want to uncover natural themes, topics, or intent in embedded free-form text,\n    without needing predefined categories.\n\n    Args:\n        by: Column containing embeddings to cluster\n        num_clusters: Number of semantic clusters to create\n\n    Returns:\n        SemGroupedData: Object for performing aggregations on the clustered data.\n\n    Example: Basic semantic grouping\n        ```python\n        # Group customer feedback into 5 clusters\n        df.semantic.group_by(\"feedback_embeddings\", 5).agg(count(\"*\"))\n        ```\n\n    Example: Analyze sentiment by semantic group\n        ```python\n        # Analyze sentiment by semantic group\n        df.semantic.group_by(\"feedback_embeddings\", 5).agg(\n            count(\"*\").alias(\"count\"),\n            avg(\"sentiment_score\").alias(\"avg_sentiment\")\n        )\n        ```\n    \"\"\"\n    return SemGroupedData(self._df, by, num_clusters)\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.SemanticExtensions.join","title":"join","text":"<pre><code>join(other: DataFrame, join_instruction: str, examples: Optional[JoinExampleCollection] = None, model_alias: Optional[str] = None) -&gt; DataFrame\n</code></pre> <p>Performs a semantic join between two DataFrames using a natural language predicate.</p> <p>That evaluates to either true or false for each potential row pair.</p> <p>The join works by: 1. Evaluating the provided join_instruction as a boolean predicate for each possible pair of rows 2. Including ONLY the row pairs where the predicate evaluates to True in the result set 3. Excluding all row pairs where the predicate evaluates to False</p> <p>The instruction must reference exactly two columns, one from each DataFrame, using the <code>:left</code> and <code>:right</code> suffixes to indicate column origin.</p> <p>This is useful when row pairing decisions require complex reasoning based on a custom predicate rather than simple equality or similarity matching.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to join with.</p> </li> <li> <code>join_instruction</code>               (<code>str</code>)           \u2013            <p>A natural language description of how to match values.</p> <ul> <li>Must include one placeholder from the left DataFrame (e.g. <code>{resume_summary:left}</code>) and one from the right (e.g. <code>{job_description:right}</code>).</li> <li>This instruction is evaluated as a boolean predicate - pairs where it's <code>True</code> are included, pairs where it's <code>False</code> are excluded.</li> </ul> </li> <li> <code>examples</code>               (<code>Optional[JoinExampleCollection]</code>, default:                   <code>None</code> )           \u2013            <p>Optional JoinExampleCollection containing labeled pairs (<code>left</code>, <code>right</code>, <code>output</code>) to guide the semantic join behavior.</p> </li> <li> <code>model_alias</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame containing only the row pairs where the join_instruction       predicate evaluates to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If <code>other</code> is not a DataFrame or <code>join_instruction</code> is not a string.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the instruction format is invalid or references invalid columns.</p> </li> </ul> Basic semantic join <pre><code># Match job listings with candidate resumes based on title/skills\n# Only includes pairs where the predicate evaluates to True\ndf_jobs.semantic.join(df_resumes,\n    join_instruction=\"Given a candidate's resume_summary: {resume_summary:left} and a job description: {job_description:right}, does the candidate have the appropriate skills for the job?\"\n)\n</code></pre> Semantic join with examples <pre><code># Improve join quality with examples\nexamples = JoinExampleCollection()\nexamples.create_example(JoinExample(\n    left=\"5 years experience building backend services in Python using asyncio, FastAPI, and PostgreSQL\",\n    right=\"Senior Software Engineer - Backend\",\n    output=True))  # This pair WILL be included in similar cases\nexamples.create_example(JoinExample(\n    left=\"5 years experience with growth strategy, private equity due diligence, and M&amp;A\",\n    right=\"Product Manager - Hardware\",\n    output=False))  # This pair will NOT be included in similar cases\ndf_jobs.semantic.join(df_resumes,\n    join_instruction=\"Given a candidate's resume_summary: {resume_summary:left} and a job description: {job_description:right}, does the candidate have the appropriate skills for the job?\",\n    examples=examples)\n</code></pre> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def join(\n    self,\n    other: DataFrame,\n    join_instruction: str,\n    examples: Optional[JoinExampleCollection] = None,\n    model_alias: Optional[str] = None,\n) -&gt; DataFrame:\n    \"\"\"Performs a semantic join between two DataFrames using a natural language predicate.\n\n    That evaluates to either true or false for each potential row pair.\n\n    The join works by:\n    1. Evaluating the provided join_instruction as a boolean predicate for each possible pair of rows\n    2. Including ONLY the row pairs where the predicate evaluates to True in the result set\n    3. Excluding all row pairs where the predicate evaluates to False\n\n    The instruction must reference **exactly two columns**, one from each DataFrame,\n    using the `:left` and `:right` suffixes to indicate column origin.\n\n    This is useful when row pairing decisions require complex reasoning based on a custom predicate rather than simple equality or similarity matching.\n\n    Args:\n        other: The DataFrame to join with.\n        join_instruction: A natural language description of how to match values.\n\n            - Must include one placeholder from the left DataFrame (e.g. `{resume_summary:left}`)\n            and one from the right (e.g. `{job_description:right}`).\n            - This instruction is evaluated as a boolean predicate - pairs where it's `True` are included,\n            pairs where it's `False` are excluded.\n        examples: Optional JoinExampleCollection containing labeled pairs (`left`, `right`, `output`)\n            to guide the semantic join behavior.\n        model_alias: Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.\n\n    Returns:\n        DataFrame: A new DataFrame containing only the row pairs where the join_instruction\n                  predicate evaluates to True.\n\n    Raises:\n        TypeError: If `other` is not a DataFrame or `join_instruction` is not a string.\n        ValueError: If the instruction format is invalid or references invalid columns.\n\n    Example: Basic semantic join\n        ```python\n        # Match job listings with candidate resumes based on title/skills\n        # Only includes pairs where the predicate evaluates to True\n        df_jobs.semantic.join(df_resumes,\n            join_instruction=\"Given a candidate's resume_summary: {resume_summary:left} and a job description: {job_description:right}, does the candidate have the appropriate skills for the job?\"\n        )\n        ```\n\n    Example: Semantic join with examples\n        ```python\n        # Improve join quality with examples\n        examples = JoinExampleCollection()\n        examples.create_example(JoinExample(\n            left=\"5 years experience building backend services in Python using asyncio, FastAPI, and PostgreSQL\",\n            right=\"Senior Software Engineer - Backend\",\n            output=True))  # This pair WILL be included in similar cases\n        examples.create_example(JoinExample(\n            left=\"5 years experience with growth strategy, private equity due diligence, and M&amp;A\",\n            right=\"Product Manager - Hardware\",\n            output=False))  # This pair will NOT be included in similar cases\n        df_jobs.semantic.join(df_resumes,\n            join_instruction=\"Given a candidate's resume_summary: {resume_summary:left} and a job description: {job_description:right}, does the candidate have the appropriate skills for the job?\",\n            examples=examples)\n        ```\n    \"\"\"\n    from fenic.api.dataframe.dataframe import DataFrame\n\n    if not isinstance(other, DataFrame):\n        raise TypeError(f\"other argument must be a DataFrame, got {type(other)}\")\n\n    if not isinstance(join_instruction, str):\n        raise TypeError(\n            f\"join_instruction argument must be a string, got {type(join_instruction)}\"\n        )\n    join_columns = utils.parse_instruction(join_instruction)\n    if len(join_columns) != 2:\n        raise ValueError(\n            f\"join_instruction must contain exactly two columns, got {len(join_columns)}\"\n        )\n    left_on = None\n    right_on = None\n    for join_col in join_columns:\n        if join_col.endswith(\":left\"):\n            if left_on is not None:\n                raise ValueError(\n                    \"join_instruction cannot contain multiple :left columns\"\n                )\n            left_on = col(join_col.split(\":\")[0])\n        elif join_col.endswith(\":right\"):\n            if right_on is not None:\n                raise ValueError(\n                    \"join_instruction cannot contain multiple :right columns\"\n                )\n            right_on = col(join_col.split(\":\")[0])\n        else:\n            raise ValueError(\n                f\"Column '{join_col}' must end with either :left or :right\"\n            )\n\n    if left_on is None or right_on is None:\n        raise ValueError(\n            \"join_instruction must contain exactly one :left and one :right column\"\n        )\n\n    return self._df._from_logical_plan(\n        SemanticJoin(\n            left=self._df._logical_plan,\n            right=other._logical_plan,\n            left_on=left_on._logical_expr,\n            right_on=right_on._logical_expr,\n            join_instruction=join_instruction,\n            examples=examples,\n            model_alias=model_alias,\n        ),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/#fenic.api.dataframe.SemanticExtensions.sim_join","title":"sim_join","text":"<pre><code>sim_join(other: DataFrame, left_on: ColumnOrName, right_on: ColumnOrName, k: int = 1, similarity_metric: SemanticSimilarityMetric = 'cosine', return_similarity_scores: bool = False) -&gt; DataFrame\n</code></pre> <p>Performs a semantic similarity join between two DataFrames using precomputed text embeddings.</p> <p>For each row in the left DataFrame, finds the top <code>k</code> most semantically similar rows in the right DataFrame based on the cosine similarity between their text embeddings. This is useful for fuzzy matching tasks when exact matches aren't possible.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>DataFrame</code>)           \u2013            <p>The right-hand DataFrame to join with.</p> </li> <li> <code>left_on</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column in this DataFrame containing text embeddings to compare.</p> </li> <li> <code>right_on</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column in the other DataFrame containing text embeddings to compare.</p> </li> <li> <code>k</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of most similar matches to return per row from the left DataFrame.</p> </li> <li> <code>similarity_metric</code>               (<code>SemanticSimilarityMetric</code>, default:                   <code>'cosine'</code> )           \u2013            <p>The metric to use for calculating distances between vectors. Supported distance metrics: \"l2\", \"cosine\", \"dot\"</p> </li> <li> <code>return_similarity_scores</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, include a <code>_similarity_score</code> column in the output DataFrame                     representing the match confidence (cosine similarity).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame containing matched rows from both sides and optionally similarity scores.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If argument types are incorrect.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If <code>k</code> is not positive or if the columns are invalid.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If <code>similarity_metric</code> is not one of \"l2\", \"cosine\", \"dot\"</p> </li> </ul> Match queries to FAQ entries <pre><code># Match customer queries to FAQ entries\ndf_queries.semantic.sim_join(\n    df_faqs,\n    left_on=embeddings(col(\"query_text\")),\n    right_on=embeddings(col(\"faq_question\")),\n    k=1\n)\n</code></pre> Link headlines to articles <pre><code># Link news headlines to full articles\ndf_headlines.semantic.sim_join(\n    df_articles,\n    left_on=embeddings(col(\"headline\")),\n    right_on=embeddings(col(\"content\")),\n    k=3,\n    return_similarity_scores=True\n)\n</code></pre> Find similar job postings <pre><code># Find similar job postings across two sources\ndf_linkedin.semantic.sim_join(\n    df_indeed,\n    left_on=embeddings(col(\"job_title\")),\n    right_on=embeddings(col(\"job_description\")),\n    k=2\n)\n</code></pre> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def sim_join(\n    self,\n    other: DataFrame,\n    left_on: ColumnOrName,\n    right_on: ColumnOrName,\n    k: int = 1,\n    similarity_metric: SemanticSimilarityMetric = \"cosine\",\n    return_similarity_scores: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Performs a semantic similarity join between two DataFrames using precomputed text embeddings.\n\n    For each row in the left DataFrame, finds the top `k` most semantically similar rows in the right DataFrame\n    based on the cosine similarity between their text embeddings. This is useful for fuzzy matching tasks when exact matches aren't possible.\n\n    Args:\n        other: The right-hand DataFrame to join with.\n        left_on: Column in this DataFrame containing text embeddings to compare.\n        right_on: Column in the other DataFrame containing text embeddings to compare.\n        k: Number of most similar matches to return per row from the left DataFrame.\n        similarity_metric: The metric to use for calculating distances between vectors.\n            Supported distance metrics: \"l2\", \"cosine\", \"dot\"\n        return_similarity_scores: If True, include a `_similarity_score` column in the output DataFrame\n                                representing the match confidence (cosine similarity).\n\n    Returns:\n        DataFrame: A new DataFrame containing matched rows from both sides and optionally similarity scores.\n\n    Raises:\n        TypeError: If argument types are incorrect.\n        ValueError: If `k` is not positive or if the columns are invalid.\n        ValueError: If `similarity_metric` is not one of \"l2\", \"cosine\", \"dot\"\n\n    Example: Match queries to FAQ entries\n        ```python\n        # Match customer queries to FAQ entries\n        df_queries.semantic.sim_join(\n            df_faqs,\n            left_on=embeddings(col(\"query_text\")),\n            right_on=embeddings(col(\"faq_question\")),\n            k=1\n        )\n        ```\n\n    Example: Link headlines to articles\n        ```python\n        # Link news headlines to full articles\n        df_headlines.semantic.sim_join(\n            df_articles,\n            left_on=embeddings(col(\"headline\")),\n            right_on=embeddings(col(\"content\")),\n            k=3,\n            return_similarity_scores=True\n        )\n        ```\n\n    Example: Find similar job postings\n        ```python\n        # Find similar job postings across two sources\n        df_linkedin.semantic.sim_join(\n            df_indeed,\n            left_on=embeddings(col(\"job_title\")),\n            right_on=embeddings(col(\"job_description\")),\n            k=2\n        )\n        ```\n    \"\"\"\n    from fenic.api.dataframe.dataframe import DataFrame\n\n    if not isinstance(right_on, ColumnOrName):\n        raise ValidationError(\n            f\"The `right_on` argument must be a `Column` or a string representing a column name, \"\n            f\"but got `{type(right_on).__name__}` instead.\"\n        )\n    if not isinstance(other, DataFrame):\n        raise ValidationError(\n                        f\"The `other` argument to `sim_join()` must be a DataFrame`, but got `{type(other).__name__}`.\"\n                    )\n    if not (isinstance(k, int) and k &gt; 0):\n        raise ValidationError(\n            f\"The parameter `k` must be a positive integer, but received `{k}`.\"\n        )\n    args = get_args(SemanticSimilarityMetric)\n    if similarity_metric not in args:\n        raise ValidationError(\n            f\"The `similarity_metric` argument must be one of {args}, but got `{similarity_metric}`.\"\n        )\n\n    def _validate_column(column: ColumnOrName, name: str):\n        if column is None:\n            raise ValidationError(f\"The `{name}` argument must not be None.\")\n        if not isinstance(column, ColumnOrName):\n            raise ValidationError(\n                f\"The `{name}` argument must be a `Column` or a string representing a column name, \"\n                f\"but got `{type(column).__name__}` instead.\"\n            )\n\n    _validate_column(left_on, \"left_on\")\n    _validate_column(right_on, \"right_on\")\n\n    return self._df._from_logical_plan(\n        SemanticSimilarityJoin(\n            self._df._logical_plan,\n            other._logical_plan,\n            Column._from_col_or_name(left_on)._logical_expr,\n            Column._from_col_or_name(right_on)._logical_expr,\n            k,\n            similarity_metric,\n            return_similarity_scores,\n        ),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/","title":"fenic.api.dataframe.dataframe","text":""},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe","title":"fenic.api.dataframe.dataframe","text":"<p>DataFrame class providing PySpark-inspired API for data manipulation.</p> <p>Classes:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>A data collection organized into named columns.</p> </li> </ul>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame","title":"DataFrame","text":"<p>A data collection organized into named columns.</p> <p>The DataFrame class represents a lazily evaluated computation on data. Operations on DataFrame build up a logical query plan that is only executed when an action like show(), to_polars(), to_pandas(), to_arrow(), to_pydict(), to_pylist(), or count() is called.</p> <p>The DataFrame supports method chaining for building complex transformations.</p> Create and transform a DataFrame <pre><code># Create a DataFrame from a dictionary\ndf = session.create_dataframe({\"id\": [1, 2, 3], \"value\": [\"a\", \"b\", \"c\"]})\n\n# Chain transformations\nresult = df.filter(col(\"id\") &gt; 1).select(\"id\", \"value\")\n\n# Show results\nresult.show()\n# Output:\n# +---+-----+\n# | id|value|\n# +---+-----+\n# |  2|    b|\n# |  3|    c|\n# +---+-----+\n</code></pre> <p>Methods:</p> <ul> <li> <code>agg</code>             \u2013              <p>Aggregate on the entire DataFrame without groups.</p> </li> <li> <code>cache</code>             \u2013              <p>Alias for persist(). Mark DataFrame for caching after first computation.</p> </li> <li> <code>collect</code>             \u2013              <p>Execute the DataFrame computation and return the result as a QueryResult.</p> </li> <li> <code>count</code>             \u2013              <p>Count the number of rows in the DataFrame.</p> </li> <li> <code>drop</code>             \u2013              <p>Remove one or more columns from this DataFrame.</p> </li> <li> <code>drop_duplicates</code>             \u2013              <p>Return a DataFrame with duplicate rows removed.</p> </li> <li> <code>explain</code>             \u2013              <p>Display the logical plan of the DataFrame.</p> </li> <li> <code>explode</code>             \u2013              <p>Create a new row for each element in an array column.</p> </li> <li> <code>filter</code>             \u2013              <p>Filters rows using the given condition.</p> </li> <li> <code>group_by</code>             \u2013              <p>Groups the DataFrame using the specified columns.</p> </li> <li> <code>join</code>             \u2013              <p>Joins this DataFrame with another DataFrame.</p> </li> <li> <code>limit</code>             \u2013              <p>Limits the number of rows to the specified number.</p> </li> <li> <code>lineage</code>             \u2013              <p>Create a Lineage object to trace data through transformations.</p> </li> <li> <code>order_by</code>             \u2013              <p>Sort the DataFrame by the specified columns. Alias for sort().</p> </li> <li> <code>persist</code>             \u2013              <p>Mark this DataFrame to be persisted after first computation.</p> </li> <li> <code>select</code>             \u2013              <p>Projects a set of Column expressions or column names.</p> </li> <li> <code>show</code>             \u2013              <p>Display the DataFrame content in a tabular form.</p> </li> <li> <code>sort</code>             \u2013              <p>Sort the DataFrame by the specified columns.</p> </li> <li> <code>to_arrow</code>             \u2013              <p>Execute the DataFrame computation and return an Apache Arrow Table.</p> </li> <li> <code>to_pandas</code>             \u2013              <p>Execute the DataFrame computation and return a Pandas DataFrame.</p> </li> <li> <code>to_polars</code>             \u2013              <p>Execute the DataFrame computation and return the result as a Polars DataFrame.</p> </li> <li> <code>to_pydict</code>             \u2013              <p>Execute the DataFrame computation and return a dictionary of column arrays.</p> </li> <li> <code>to_pylist</code>             \u2013              <p>Execute the DataFrame computation and return a list of row dictionaries.</p> </li> <li> <code>union</code>             \u2013              <p>Return a new DataFrame containing the union of rows in this and another DataFrame.</p> </li> <li> <code>unnest</code>             \u2013              <p>Unnest the specified struct columns into separate columns.</p> </li> <li> <code>where</code>             \u2013              <p>Filters rows using the given condition (alias for filter()).</p> </li> <li> <code>with_column</code>             \u2013              <p>Add a new column or replace an existing column.</p> </li> <li> <code>with_column_renamed</code>             \u2013              <p>Rename a column. No-op if the column does not exist.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>columns</code>               (<code>List[str]</code>)           \u2013            <p>Get list of column names.</p> </li> <li> <code>schema</code>               (<code>Schema</code>)           \u2013            <p>Get the schema of this DataFrame.</p> </li> <li> <code>semantic</code>               (<code>SemanticExtensions</code>)           \u2013            <p>Interface for semantic operations on the DataFrame.</p> </li> <li> <code>write</code>               (<code>DataFrameWriter</code>)           \u2013            <p>Interface for saving the content of the DataFrame.</p> </li> </ul>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: List[str]\n</code></pre> <p>Get list of column names.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: List of all column names in the DataFrame</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df.columns\n['name', 'age', 'city']\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.schema","title":"schema  <code>property</code>","text":"<pre><code>schema: Schema\n</code></pre> <p>Get the schema of this DataFrame.</p> <p>Returns:</p> <ul> <li> <code>Schema</code> (              <code>Schema</code> )          \u2013            <p>Schema containing field names and data types</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df.schema\nSchema([\n    ColumnField('name', StringType),\n    ColumnField('age', IntegerType)\n])\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.semantic","title":"semantic  <code>property</code>","text":"<pre><code>semantic: SemanticExtensions\n</code></pre> <p>Interface for semantic operations on the DataFrame.</p>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.write","title":"write  <code>property</code>","text":"<pre><code>write: DataFrameWriter\n</code></pre> <p>Interface for saving the content of the DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrameWriter</code> (              <code>DataFrameWriter</code> )          \u2013            <p>Writer interface to write DataFrame.</p> </li> </ul>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.agg","title":"agg","text":"<pre><code>agg(*exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame\n</code></pre> <p>Aggregate on the entire DataFrame without groups.</p> <p>This is equivalent to group_by() without any grouping columns.</p> <p>Parameters:</p> <ul> <li> <code>*exprs</code>               (<code>Union[Column, Dict[str, str]]</code>, default:                   <code>()</code> )           \u2013            <p>Aggregation expressions or dictionary of aggregations.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Aggregation results.</p> </li> </ul> Multiple aggregations <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"salary\": [80000, 70000, 90000, 75000, 85000],\n    \"age\": [25, 30, 35, 28, 32]\n})\n\n# Multiple aggregations\ndf.agg(\n    count().alias(\"total_rows\"),\n    avg(col(\"salary\")).alias(\"avg_salary\")\n).show()\n# Output:\n# +----------+-----------+\n# |total_rows|avg_salary|\n# +----------+-----------+\n# |         5|   80000.0|\n# +----------+-----------+\n</code></pre> Dictionary style <pre><code># Dictionary style\ndf.agg({col(\"salary\"): \"avg\", col(\"age\"): \"max\"}).show()\n# Output:\n# +-----------+--------+\n# |avg(salary)|max(age)|\n# +-----------+--------+\n# |    80000.0|      35|\n# +-----------+--------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def agg(self, *exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame:\n    \"\"\"Aggregate on the entire DataFrame without groups.\n\n    This is equivalent to group_by() without any grouping columns.\n\n    Args:\n        *exprs: Aggregation expressions or dictionary of aggregations.\n\n    Returns:\n        DataFrame: Aggregation results.\n\n    Example: Multiple aggregations\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"salary\": [80000, 70000, 90000, 75000, 85000],\n            \"age\": [25, 30, 35, 28, 32]\n        })\n\n        # Multiple aggregations\n        df.agg(\n            count().alias(\"total_rows\"),\n            avg(col(\"salary\")).alias(\"avg_salary\")\n        ).show()\n        # Output:\n        # +----------+-----------+\n        # |total_rows|avg_salary|\n        # +----------+-----------+\n        # |         5|   80000.0|\n        # +----------+-----------+\n        ```\n\n    Example: Dictionary style\n        ```python\n        # Dictionary style\n        df.agg({col(\"salary\"): \"avg\", col(\"age\"): \"max\"}).show()\n        # Output:\n        # +-----------+--------+\n        # |avg(salary)|max(age)|\n        # +-----------+--------+\n        # |    80000.0|      35|\n        # +-----------+--------+\n        ```\n    \"\"\"\n    return self.group_by().agg(*exprs)\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.cache","title":"cache","text":"<pre><code>cache() -&gt; DataFrame\n</code></pre> <p>Alias for persist(). Mark DataFrame for caching after first computation.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Same DataFrame, but marked for caching</p> </li> </ul> See Also <p>persist(): Full documentation of caching behavior</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def cache(self) -&gt; DataFrame:\n    \"\"\"Alias for persist(). Mark DataFrame for caching after first computation.\n\n    Returns:\n        DataFrame: Same DataFrame, but marked for caching\n\n    See Also:\n        persist(): Full documentation of caching behavior\n    \"\"\"\n    return self.persist()\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.collect","title":"collect","text":"<pre><code>collect(data_type: DataLikeType = 'polars') -&gt; QueryResult\n</code></pre> <p>Execute the DataFrame computation and return the result as a QueryResult.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a QueryResult, which contains both the result data and the query metrics.</p> <p>Parameters:</p> <ul> <li> <code>data_type</code>               (<code>DataLikeType</code>, default:                   <code>'polars'</code> )           \u2013            <p>The type of data to return</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>QueryResult</code> (              <code>QueryResult</code> )          \u2013            <p>A QueryResult with materialized data and query metrics</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def collect(self, data_type: DataLikeType = \"polars\") -&gt; QueryResult:\n    \"\"\"Execute the DataFrame computation and return the result as a QueryResult.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a QueryResult, which contains both the result data and the query metrics.\n\n    Args:\n        data_type: The type of data to return\n\n    Returns:\n        QueryResult: A QueryResult with materialized data and query metrics\n    \"\"\"\n    result: Tuple[pl.DataFrame, QueryMetrics] = self._logical_plan.session_state.execution.collect(self._logical_plan)\n    df, metrics = result\n    logger.info(metrics.get_summary())\n\n    if data_type == \"polars\":\n        return QueryResult(df, metrics)\n    elif data_type == \"pandas\":\n        return QueryResult(df.to_pandas(use_pyarrow_extension_array=True), metrics)\n    elif data_type == \"arrow\":\n        return QueryResult(df.to_arrow(), metrics)\n    elif data_type == \"pydict\":\n        return QueryResult(df.to_dict(as_series=False), metrics)\n    elif data_type == \"pylist\":\n        return QueryResult(df.to_dicts(), metrics)\n    else:\n        raise ValidationError(f\"Invalid data type: {data_type} in collect(). Valid data types are: polars, pandas, arrow, pydict, pylist\")\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.count","title":"count","text":"<pre><code>count() -&gt; int\n</code></pre> <p>Count the number of rows in the DataFrame.</p> <p>This is an action that triggers computation of the DataFrame. The output is an integer representing the number of rows.</p> <p>Returns:</p> <ul> <li> <code>int</code> (              <code>int</code> )          \u2013            <p>The number of rows in the DataFrame</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def count(self) -&gt; int:\n    \"\"\"Count the number of rows in the DataFrame.\n\n    This is an action that triggers computation of the DataFrame.\n    The output is an integer representing the number of rows.\n\n    Returns:\n        int: The number of rows in the DataFrame\n    \"\"\"\n    return self._logical_plan.session_state.execution.count(self._logical_plan)[0]\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.drop","title":"drop","text":"<pre><code>drop(*col_names: str) -&gt; DataFrame\n</code></pre> <p>Remove one or more columns from this DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>*col_names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>Names of columns to drop.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>New DataFrame without specified columns.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If any specified column doesn't exist in the DataFrame.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If dropping the columns would result in an empty DataFrame.</p> </li> </ul> Drop single column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35]\n})\n\n# Drop single column\ndf.drop(\"age\").show()\n# Output:\n# +---+-------+\n# | id|   name|\n# +---+-------+\n# |  1|  Alice|\n# |  2|    Bob|\n# |  3|Charlie|\n# +---+-------+\n</code></pre> Drop multiple columns <pre><code># Drop multiple columns\ndf.drop(col(\"id\"), \"age\").show()\n# Output:\n# +-------+\n# |   name|\n# +-------+\n# |  Alice|\n# |    Bob|\n# |Charlie|\n# +-------+\n</code></pre> Error when dropping non-existent column <pre><code># This will raise a ValueError\ndf.drop(\"non_existent_column\")\n# ValueError: Column 'non_existent_column' not found in DataFrame\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def drop(self, *col_names: str) -&gt; DataFrame:\n    \"\"\"Remove one or more columns from this DataFrame.\n\n    Args:\n        *col_names: Names of columns to drop.\n\n    Returns:\n        DataFrame: New DataFrame without specified columns.\n\n    Raises:\n        ValueError: If any specified column doesn't exist in the DataFrame.\n        ValueError: If dropping the columns would result in an empty DataFrame.\n\n    Example: Drop single column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"id\": [1, 2, 3],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n            \"age\": [25, 30, 35]\n        })\n\n        # Drop single column\n        df.drop(\"age\").show()\n        # Output:\n        # +---+-------+\n        # | id|   name|\n        # +---+-------+\n        # |  1|  Alice|\n        # |  2|    Bob|\n        # |  3|Charlie|\n        # +---+-------+\n        ```\n\n    Example: Drop multiple columns\n        ```python\n        # Drop multiple columns\n        df.drop(col(\"id\"), \"age\").show()\n        # Output:\n        # +-------+\n        # |   name|\n        # +-------+\n        # |  Alice|\n        # |    Bob|\n        # |Charlie|\n        # +-------+\n        ```\n\n    Example: Error when dropping non-existent column\n        ```python\n        # This will raise a ValueError\n        df.drop(\"non_existent_column\")\n        # ValueError: Column 'non_existent_column' not found in DataFrame\n        ```\n    \"\"\"\n    if not col_names:\n        return self\n\n    current_cols = set(self.columns)\n    to_drop = set(col_names)\n    missing = to_drop - current_cols\n\n    if missing:\n        missing_str = (\n            f\"Column '{next(iter(missing))}'\"\n            if len(missing) == 1\n            else f\"Columns {sorted(missing)}\"\n        )\n        raise ValueError(f\"{missing_str} not found in DataFrame\")\n\n    remaining_cols = [\n        col(c)._logical_expr for c in self.columns if c not in to_drop\n    ]\n\n    if not remaining_cols:\n        raise ValueError(\"Cannot drop all columns from DataFrame\")\n\n    return self._from_logical_plan(\n        Projection(self._logical_plan, remaining_cols)\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.drop_duplicates","title":"drop_duplicates","text":"<pre><code>drop_duplicates(subset: Optional[List[str]] = None) -&gt; DataFrame\n</code></pre> <p>Return a DataFrame with duplicate rows removed.</p> <p>Parameters:</p> <ul> <li> <code>subset</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Column names to consider when identifying duplicates. If not provided, all columns are considered.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with duplicate rows removed.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a specified column is not present in the current DataFrame schema.</p> </li> </ul> Remove duplicates considering specific columns <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"c1\": [1, 2, 3, 1],\n    \"c2\": [\"a\", \"a\", \"a\", \"a\"],\n    \"c3\": [\"b\", \"b\", \"b\", \"b\"]\n})\n\n# Remove duplicates considering all columns\ndf.drop_duplicates([col(\"c1\"), col(\"c2\"), col(\"c3\")]).show()\n# Output:\n# +---+---+---+\n# | c1| c2| c3|\n# +---+---+---+\n# |  1|  a|  b|\n# |  2|  a|  b|\n# |  3|  a|  b|\n# +---+---+---+\n\n# Remove duplicates considering only c1\ndf.drop_duplicates([col(\"c1\")]).show()\n# Output:\n# +---+---+---+\n# | c1| c2| c3|\n# +---+---+---+\n# |  1|  a|  b|\n# |  2|  a|  b|\n# |  3|  a|  b|\n# +---+---+---+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def drop_duplicates(\n    self,\n    subset: Optional[List[str]] = None,\n) -&gt; DataFrame:\n    \"\"\"Return a DataFrame with duplicate rows removed.\n\n    Args:\n        subset: Column names to consider when identifying duplicates. If not provided, all columns are considered.\n\n    Returns:\n        DataFrame: A new DataFrame with duplicate rows removed.\n\n    Raises:\n        ValueError: If a specified column is not present in the current DataFrame schema.\n\n    Example: Remove duplicates considering specific columns\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"c1\": [1, 2, 3, 1],\n            \"c2\": [\"a\", \"a\", \"a\", \"a\"],\n            \"c3\": [\"b\", \"b\", \"b\", \"b\"]\n        })\n\n        # Remove duplicates considering all columns\n        df.drop_duplicates([col(\"c1\"), col(\"c2\"), col(\"c3\")]).show()\n        # Output:\n        # +---+---+---+\n        # | c1| c2| c3|\n        # +---+---+---+\n        # |  1|  a|  b|\n        # |  2|  a|  b|\n        # |  3|  a|  b|\n        # +---+---+---+\n\n        # Remove duplicates considering only c1\n        df.drop_duplicates([col(\"c1\")]).show()\n        # Output:\n        # +---+---+---+\n        # | c1| c2| c3|\n        # +---+---+---+\n        # |  1|  a|  b|\n        # |  2|  a|  b|\n        # |  3|  a|  b|\n        # +---+---+---+\n        ```\n    \"\"\"\n    exprs = []\n    if subset:\n        for c in subset:\n            if c not in self.columns:\n                raise TypeError(f\"Column {c} not found in DataFrame.\")\n            exprs.append(col(c)._logical_expr)\n\n    return self._from_logical_plan(\n        DropDuplicates(self._logical_plan, exprs),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.explain","title":"explain","text":"<pre><code>explain() -&gt; None\n</code></pre> <p>Display the logical plan of the DataFrame.</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def explain(self) -&gt; None:\n    \"\"\"Display the logical plan of the DataFrame.\"\"\"\n    print(str(self._logical_plan))\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.explode","title":"explode","text":"<pre><code>explode(column: ColumnOrName) -&gt; DataFrame\n</code></pre> <p>Create a new row for each element in an array column.</p> <p>This operation is useful for flattening nested data structures. For each row in the input DataFrame that contains an array/list in the specified column, this method will: 1. Create N new rows, where N is the length of the array 2. Each new row will be identical to the original row, except the array column will    contain just a single element from the original array 3. Rows with NULL values or empty arrays in the specified column are filtered out</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Name of array column to explode (as string) or Column expression.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>New DataFrame with the array column exploded into multiple rows.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column argument is not a string or Column.</p> </li> </ul> Explode array column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"id\": [1, 2, 3, 4],\n    \"tags\": [[\"red\", \"blue\"], [\"green\"], [], None],\n    \"name\": [\"Alice\", \"Bob\", \"Carol\", \"Dave\"]\n})\n\n# Explode the tags column\ndf.explode(\"tags\").show()\n# Output:\n# +---+-----+-----+\n# | id| tags| name|\n# +---+-----+-----+\n# |  1|  red|Alice|\n# |  1| blue|Alice|\n# |  2|green|  Bob|\n# +---+-----+-----+\n</code></pre> Using column expression <pre><code># Explode using column expression\ndf.explode(col(\"tags\")).show()\n# Output:\n# +---+-----+-----+\n# | id| tags| name|\n# +---+-----+-----+\n# |  1|  red|Alice|\n# |  1| blue|Alice|\n# |  2|green|  Bob|\n# +---+-----+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def explode(self, column: ColumnOrName) -&gt; DataFrame:\n    \"\"\"Create a new row for each element in an array column.\n\n    This operation is useful for flattening nested data structures. For each row in the\n    input DataFrame that contains an array/list in the specified column, this method will:\n    1. Create N new rows, where N is the length of the array\n    2. Each new row will be identical to the original row, except the array column will\n       contain just a single element from the original array\n    3. Rows with NULL values or empty arrays in the specified column are filtered out\n\n    Args:\n        column: Name of array column to explode (as string) or Column expression.\n\n    Returns:\n        DataFrame: New DataFrame with the array column exploded into multiple rows.\n\n    Raises:\n        TypeError: If column argument is not a string or Column.\n\n    Example: Explode array column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"id\": [1, 2, 3, 4],\n            \"tags\": [[\"red\", \"blue\"], [\"green\"], [], None],\n            \"name\": [\"Alice\", \"Bob\", \"Carol\", \"Dave\"]\n        })\n\n        # Explode the tags column\n        df.explode(\"tags\").show()\n        # Output:\n        # +---+-----+-----+\n        # | id| tags| name|\n        # +---+-----+-----+\n        # |  1|  red|Alice|\n        # |  1| blue|Alice|\n        # |  2|green|  Bob|\n        # +---+-----+-----+\n        ```\n\n    Example: Using column expression\n        ```python\n        # Explode using column expression\n        df.explode(col(\"tags\")).show()\n        # Output:\n        # +---+-----+-----+\n        # | id| tags| name|\n        # +---+-----+-----+\n        # |  1|  red|Alice|\n        # |  1| blue|Alice|\n        # |  2|green|  Bob|\n        # +---+-----+-----+\n        ```\n    \"\"\"\n    return self._from_logical_plan(\n        Explode(self._logical_plan, Column._from_col_or_name(column)._logical_expr),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.filter","title":"filter","text":"<pre><code>filter(condition: Column) -&gt; DataFrame\n</code></pre> <p>Filters rows using the given condition.</p> <p>Parameters:</p> <ul> <li> <code>condition</code>               (<code>Column</code>)           \u2013            <p>A Column expression that evaluates to a boolean</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Filtered DataFrame</p> </li> </ul> Filter with numeric comparison <pre><code># Create a DataFrame\ndf = session.create_dataframe({\"age\": [25, 30, 35], \"name\": [\"Alice\", \"Bob\", \"Charlie\"]})\n\n# Filter with numeric comparison\ndf.filter(col(\"age\") &gt; 25).show()\n# Output:\n# +---+-------+\n# |age|   name|\n# +---+-------+\n# | 30|    Bob|\n# | 35|Charlie|\n# +---+-------+\n</code></pre> Filter with semantic predicate <pre><code># Filter with semantic predicate\ndf.filter((col(\"age\") &gt; 25) &amp; semantic.predicate(\"This {feedback} mentions problems with the user interface or navigation\")).show()\n# Output:\n# +---+-------+\n# |age|   name|\n# +---+-------+\n# | 30|    Bob|\n# | 35|Charlie|\n# +---+-------+\n</code></pre> Filter with multiple conditions <pre><code># Filter with multiple conditions\ndf.filter((col(\"age\") &gt; 25) &amp; (col(\"age\") &lt;= 35)).show()\n# Output:\n# +---+-------+\n# |age|   name|\n# +---+-------+\n# | 30|    Bob|\n# | 35|Charlie|\n# +---+-------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def filter(self, condition: Column) -&gt; DataFrame:\n    \"\"\"Filters rows using the given condition.\n\n    Args:\n        condition: A Column expression that evaluates to a boolean\n\n    Returns:\n        DataFrame: Filtered DataFrame\n\n    Example: Filter with numeric comparison\n        ```python\n        # Create a DataFrame\n        df = session.create_dataframe({\"age\": [25, 30, 35], \"name\": [\"Alice\", \"Bob\", \"Charlie\"]})\n\n        # Filter with numeric comparison\n        df.filter(col(\"age\") &gt; 25).show()\n        # Output:\n        # +---+-------+\n        # |age|   name|\n        # +---+-------+\n        # | 30|    Bob|\n        # | 35|Charlie|\n        # +---+-------+\n        ```\n\n    Example: Filter with semantic predicate\n        ```python\n        # Filter with semantic predicate\n        df.filter((col(\"age\") &gt; 25) &amp; semantic.predicate(\"This {feedback} mentions problems with the user interface or navigation\")).show()\n        # Output:\n        # +---+-------+\n        # |age|   name|\n        # +---+-------+\n        # | 30|    Bob|\n        # | 35|Charlie|\n        # +---+-------+\n        ```\n\n    Example: Filter with multiple conditions\n        ```python\n        # Filter with multiple conditions\n        df.filter((col(\"age\") &gt; 25) &amp; (col(\"age\") &lt;= 35)).show()\n        # Output:\n        # +---+-------+\n        # |age|   name|\n        # +---+-------+\n        # | 30|    Bob|\n        # | 35|Charlie|\n        # +---+-------+\n        ```\n    \"\"\"\n    return self._from_logical_plan(\n        Filter(self._logical_plan, condition._logical_expr),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.group_by","title":"group_by","text":"<pre><code>group_by(*cols: ColumnOrName) -&gt; GroupedData\n</code></pre> <p>Groups the DataFrame using the specified columns.</p> <p>Parameters:</p> <ul> <li> <code>*cols</code>               (<code>ColumnOrName</code>, default:                   <code>()</code> )           \u2013            <p>Columns to group by. Can be column names as strings or Column expressions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GroupedData</code> (              <code>GroupedData</code> )          \u2013            <p>Object for performing aggregations on the grouped data.</p> </li> </ul> Group by single column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"department\": [\"IT\", \"HR\", \"IT\", \"HR\", \"IT\"],\n    \"salary\": [80000, 70000, 90000, 75000, 85000]\n})\n\n# Group by single column\ndf.group_by(col(\"department\")).count().show()\n# Output:\n# +----------+-----+\n# |department|count|\n# +----------+-----+\n# |        IT|    3|\n# |        HR|    2|\n# +----------+-----+\n</code></pre> Group by multiple columns <pre><code># Group by multiple columns\ndf.group_by(col(\"department\"), col(\"location\")).agg({\"salary\": \"avg\"}).show()\n# Output:\n# +----------+--------+-----------+\n# |department|location|avg(salary)|\n# +----------+--------+-----------+\n# |        IT|    NYC|    85000.0|\n# |        HR|    NYC|    72500.0|\n# +----------+--------+-----------+\n</code></pre> Group by expression <pre><code># Group by expression\ndf.group_by(col(\"age\").cast(\"int\").alias(\"age_group\")).count().show()\n# Output:\n# +---------+-----+\n# |age_group|count|\n# +---------+-----+\n# |       20|    2|\n# |       30|    3|\n# |       40|    1|\n# +---------+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def group_by(self, *cols: ColumnOrName) -&gt; GroupedData:\n    \"\"\"Groups the DataFrame using the specified columns.\n\n    Args:\n        *cols: Columns to group by. Can be column names as strings or Column expressions.\n\n    Returns:\n        GroupedData: Object for performing aggregations on the grouped data.\n\n    Example: Group by single column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"department\": [\"IT\", \"HR\", \"IT\", \"HR\", \"IT\"],\n            \"salary\": [80000, 70000, 90000, 75000, 85000]\n        })\n\n        # Group by single column\n        df.group_by(col(\"department\")).count().show()\n        # Output:\n        # +----------+-----+\n        # |department|count|\n        # +----------+-----+\n        # |        IT|    3|\n        # |        HR|    2|\n        # +----------+-----+\n        ```\n\n    Example: Group by multiple columns\n        ```python\n        # Group by multiple columns\n        df.group_by(col(\"department\"), col(\"location\")).agg({\"salary\": \"avg\"}).show()\n        # Output:\n        # +----------+--------+-----------+\n        # |department|location|avg(salary)|\n        # +----------+--------+-----------+\n        # |        IT|    NYC|    85000.0|\n        # |        HR|    NYC|    72500.0|\n        # +----------+--------+-----------+\n        ```\n\n    Example: Group by expression\n        ```python\n        # Group by expression\n        df.group_by(col(\"age\").cast(\"int\").alias(\"age_group\")).count().show()\n        # Output:\n        # +---------+-----+\n        # |age_group|count|\n        # +---------+-----+\n        # |       20|    2|\n        # |       30|    3|\n        # |       40|    1|\n        # +---------+-----+\n        ```\n    \"\"\"\n    return GroupedData(self, list(cols) if cols else None)\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.join","title":"join","text":"<pre><code>join(other: DataFrame, on: Union[str, List[str]], *, how: JoinType = 'inner') -&gt; DataFrame\n</code></pre><pre><code>join(other: DataFrame, *, left_on: Union[ColumnOrName, List[ColumnOrName]], right_on: Union[ColumnOrName, List[ColumnOrName]], how: JoinType = 'inner') -&gt; DataFrame\n</code></pre> <pre><code>join(other: DataFrame, on: Optional[Union[str, List[str]]] = None, *, left_on: Optional[Union[ColumnOrName, List[ColumnOrName]]] = None, right_on: Optional[Union[ColumnOrName, List[ColumnOrName]]] = None, how: JoinType = 'inner') -&gt; DataFrame\n</code></pre> <p>Joins this DataFrame with another DataFrame.</p> <p>The Dataframes must have no duplicate column names between them. This API only supports equi-joins. For non-equi-joins, use session.sql().</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>DataFrame</code>)           \u2013            <p>DataFrame to join with.</p> </li> <li> <code>on</code>               (<code>Optional[Union[str, List[str]]]</code>, default:                   <code>None</code> )           \u2013            <p>Join condition(s). Can be: - A column name (str) - A list of column names (List[str]) - A Column expression (e.g., col('a')) - A list of Column expressions - <code>None</code> for cross joins</p> </li> <li> <code>left_on</code>               (<code>Optional[Union[ColumnOrName, List[ColumnOrName]]]</code>, default:                   <code>None</code> )           \u2013            <p>Column(s) from the left DataFrame to join on. Can be: - A column name (str) - A Column expression (e.g., col('a'), col('a') + 1) - A list of column names or expressions</p> </li> <li> <code>right_on</code>               (<code>Optional[Union[ColumnOrName, List[ColumnOrName]]]</code>, default:                   <code>None</code> )           \u2013            <p>Column(s) from the right DataFrame to join on. Can be: - A column name (str) - A Column expression (e.g., col('b'), upper(col('b'))) - A list of column names or expressions</p> </li> <li> <code>how</code>               (<code>JoinType</code>, default:                   <code>'inner'</code> )           \u2013            <p>Type of join to perform.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Joined DataFrame.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If cross join is used with an ON clause.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If join condition is invalid.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If both 'on' and 'left_on'/'right_on' parameters are provided.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If only one of 'left_on' or 'right_on' is provided.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If 'left_on' and 'right_on' have different lengths</p> </li> </ul> Inner join on column name <pre><code># Create sample DataFrames\ndf1 = session.create_dataframe({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n})\ndf2 = session.create_dataframe({\n    \"id\": [1, 2, 4],\n    \"age\": [25, 30, 35]\n})\n\n# Join on single column\ndf1.join(df2, on=col(\"id\")).show()\n# Output:\n# +---+-----+---+\n# | id| name|age|\n# +---+-----+---+\n# |  1|Alice| 25|\n# |  2|  Bob| 30|\n# +---+-----+---+\n</code></pre> Join with expression <pre><code># Join with Column expressions\ndf1.join(\n    df2,\n    left_on=col(\"id\"),\n    right_on=col(\"id\"),\n).show()\n# Output:\n# +---+-----+---+\n# | id| name|age|\n# +---+-----+---+\n# |  1|Alice| 25|\n# |  2|  Bob| 30|\n# +---+-----+---+\n</code></pre> Cross join <pre><code># Cross join (cartesian product)\ndf1.join(df2, how=\"cross\").show()\n# Output:\n# +---+-----+---+---+\n# | id| name| id|age|\n# +---+-----+---+---+\n# |  1|Alice|  1| 25|\n# |  1|Alice|  2| 30|\n# |  1|Alice|  4| 35|\n# |  2|  Bob|  1| 25|\n# |  2|  Bob|  2| 30|\n# |  2|  Bob|  4| 35|\n# |  3|Charlie| 1| 25|\n# |  3|Charlie| 2| 30|\n# |  3|Charlie| 4| 35|\n# +---+-----+---+---+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def join(\n    self,\n    other: DataFrame,\n    on: Optional[Union[str, List[str]]] = None,\n    *,\n    left_on: Optional[Union[ColumnOrName, List[ColumnOrName]]] = None,\n    right_on: Optional[Union[ColumnOrName, List[ColumnOrName]]] = None,\n    how: JoinType = \"inner\",\n) -&gt; DataFrame:\n    \"\"\"Joins this DataFrame with another DataFrame.\n\n    The Dataframes must have no duplicate column names between them. This API only supports equi-joins.\n    For non-equi-joins, use session.sql().\n\n    Args:\n        other: DataFrame to join with.\n        on: Join condition(s). Can be:\n            - A column name (str)\n            - A list of column names (List[str])\n            - A Column expression (e.g., col('a'))\n            - A list of Column expressions\n            - `None` for cross joins\n        left_on: Column(s) from the left DataFrame to join on. Can be:\n            - A column name (str)\n            - A Column expression (e.g., col('a'), col('a') + 1)\n            - A list of column names or expressions\n        right_on: Column(s) from the right DataFrame to join on. Can be:\n            - A column name (str)\n            - A Column expression (e.g., col('b'), upper(col('b')))\n            - A list of column names or expressions\n        how: Type of join to perform.\n\n    Returns:\n        Joined DataFrame.\n\n    Raises:\n        ValidationError: If cross join is used with an ON clause.\n        ValidationError: If join condition is invalid.\n        ValidationError: If both 'on' and 'left_on'/'right_on' parameters are provided.\n        ValidationError: If only one of 'left_on' or 'right_on' is provided.\n        ValidationError: If 'left_on' and 'right_on' have different lengths\n\n    Example: Inner join on column name\n        ```python\n        # Create sample DataFrames\n        df1 = session.create_dataframe({\n            \"id\": [1, 2, 3],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n        })\n        df2 = session.create_dataframe({\n            \"id\": [1, 2, 4],\n            \"age\": [25, 30, 35]\n        })\n\n        # Join on single column\n        df1.join(df2, on=col(\"id\")).show()\n        # Output:\n        # +---+-----+---+\n        # | id| name|age|\n        # +---+-----+---+\n        # |  1|Alice| 25|\n        # |  2|  Bob| 30|\n        # +---+-----+---+\n        ```\n\n    Example: Join with expression\n        ```python\n        # Join with Column expressions\n        df1.join(\n            df2,\n            left_on=col(\"id\"),\n            right_on=col(\"id\"),\n        ).show()\n        # Output:\n        # +---+-----+---+\n        # | id| name|age|\n        # +---+-----+---+\n        # |  1|Alice| 25|\n        # |  2|  Bob| 30|\n        # +---+-----+---+\n        ```\n\n    Example: Cross join\n        ```python\n        # Cross join (cartesian product)\n        df1.join(df2, how=\"cross\").show()\n        # Output:\n        # +---+-----+---+---+\n        # | id| name| id|age|\n        # +---+-----+---+---+\n        # |  1|Alice|  1| 25|\n        # |  1|Alice|  2| 30|\n        # |  1|Alice|  4| 35|\n        # |  2|  Bob|  1| 25|\n        # |  2|  Bob|  2| 30|\n        # |  2|  Bob|  4| 35|\n        # |  3|Charlie| 1| 25|\n        # |  3|Charlie| 2| 30|\n        # |  3|Charlie| 4| 35|\n        # +---+-----+---+---+\n        ```\n    \"\"\"\n    validate_join_parameters(self, on, left_on, right_on, how)\n\n    # Build join conditions\n    left_conditions, right_conditions = build_join_conditions(on, left_on, right_on)\n\n    return self._from_logical_plan(\n        Join(self._logical_plan, other._logical_plan, left_conditions, right_conditions, how),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.limit","title":"limit","text":"<pre><code>limit(n: int) -&gt; DataFrame\n</code></pre> <p>Limits the number of rows to the specified number.</p> <p>Parameters:</p> <ul> <li> <code>n</code>               (<code>int</code>)           \u2013            <p>Maximum number of rows to return.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>DataFrame with at most n rows.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If n is not an integer.</p> </li> </ul> Limit rows <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"id\": [1, 2, 3, 4, 5],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Eve\"]\n})\n\n# Get first 3 rows\ndf.limit(3).show()\n# Output:\n# +---+-------+\n# | id|   name|\n# +---+-------+\n# |  1|  Alice|\n# |  2|    Bob|\n# |  3|Charlie|\n# +---+-------+\n</code></pre> Limit with other operations <pre><code># Limit after filtering\ndf.filter(col(\"id\") &gt; 2).limit(2).show()\n# Output:\n# +---+-------+\n# | id|   name|\n# +---+-------+\n# |  3|Charlie|\n# |  4|   Dave|\n# +---+-------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def limit(self, n: int) -&gt; DataFrame:\n    \"\"\"Limits the number of rows to the specified number.\n\n    Args:\n        n: Maximum number of rows to return.\n\n    Returns:\n        DataFrame: DataFrame with at most n rows.\n\n    Raises:\n        TypeError: If n is not an integer.\n\n    Example: Limit rows\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"id\": [1, 2, 3, 4, 5],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Eve\"]\n        })\n\n        # Get first 3 rows\n        df.limit(3).show()\n        # Output:\n        # +---+-------+\n        # | id|   name|\n        # +---+-------+\n        # |  1|  Alice|\n        # |  2|    Bob|\n        # |  3|Charlie|\n        # +---+-------+\n        ```\n\n    Example: Limit with other operations\n        ```python\n        # Limit after filtering\n        df.filter(col(\"id\") &gt; 2).limit(2).show()\n        # Output:\n        # +---+-------+\n        # | id|   name|\n        # +---+-------+\n        # |  3|Charlie|\n        # |  4|   Dave|\n        # +---+-------+\n        ```\n    \"\"\"\n    return self._from_logical_plan(Limit(self._logical_plan, n))\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.lineage","title":"lineage","text":"<pre><code>lineage() -&gt; Lineage\n</code></pre> <p>Create a Lineage object to trace data through transformations.</p> <p>The Lineage interface allows you to trace how specific rows are transformed through your DataFrame operations, both forwards and backwards through the computation graph.</p> <p>Returns:</p> <ul> <li> <code>Lineage</code> (              <code>Lineage</code> )          \u2013            <p>Interface for querying data lineage</p> </li> </ul> Example <pre><code># Create lineage query\nlineage = df.lineage()\n\n# Trace specific rows backwards through transformations\nsource_rows = lineage.backward([\"result_uuid1\", \"result_uuid2\"])\n\n# Or trace forwards to see outputs\nresult_rows = lineage.forward([\"source_uuid1\"])\n</code></pre> See Also <p>LineageQuery: Full documentation of lineage querying capabilities</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def lineage(self) -&gt; Lineage:\n    \"\"\"Create a Lineage object to trace data through transformations.\n\n    The Lineage interface allows you to trace how specific rows are transformed\n    through your DataFrame operations, both forwards and backwards through the\n    computation graph.\n\n    Returns:\n        Lineage: Interface for querying data lineage\n\n    Example:\n        ```python\n        # Create lineage query\n        lineage = df.lineage()\n\n        # Trace specific rows backwards through transformations\n        source_rows = lineage.backward([\"result_uuid1\", \"result_uuid2\"])\n\n        # Or trace forwards to see outputs\n        result_rows = lineage.forward([\"source_uuid1\"])\n        ```\n\n    See Also:\n        LineageQuery: Full documentation of lineage querying capabilities\n    \"\"\"\n    return Lineage(self._logical_plan.session_state.execution.build_lineage(self._logical_plan))\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.order_by","title":"order_by","text":"<pre><code>order_by(cols: Union[ColumnOrName, List[ColumnOrName], None] = None, ascending: Optional[Union[bool, List[bool]]] = None) -&gt; 'DataFrame'\n</code></pre> <p>Sort the DataFrame by the specified columns. Alias for sort().</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>'DataFrame'</code> )          \u2013            <p>sorted Dataframe.</p> </li> </ul> See Also <p>sort(): Full documentation of sorting behavior and parameters.</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def order_by(\n    self,\n    cols: Union[ColumnOrName, List[ColumnOrName], None] = None,\n    ascending: Optional[Union[bool, List[bool]]] = None,\n) -&gt; \"DataFrame\":\n    \"\"\"Sort the DataFrame by the specified columns. Alias for sort().\n\n    Returns:\n        DataFrame: sorted Dataframe.\n\n    See Also:\n        sort(): Full documentation of sorting behavior and parameters.\n    \"\"\"\n    return self.sort(cols, ascending)\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.persist","title":"persist","text":"<pre><code>persist() -&gt; DataFrame\n</code></pre> <p>Mark this DataFrame to be persisted after first computation.</p> <p>The persisted DataFrame will be cached after its first computation, avoiding recomputation in subsequent operations. This is useful for DataFrames that are reused multiple times in your workflow.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Same DataFrame, but marked for persistence</p> </li> </ul> Example <pre><code># Cache intermediate results for reuse\nfiltered_df = (df\n    .filter(col(\"age\") &gt; 25)\n    .persist()  # Cache these results\n)\n\n# Both operations will use cached results\nresult1 = filtered_df.group_by(\"department\").count()\nresult2 = filtered_df.select(\"name\", \"salary\")\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def persist(self) -&gt; DataFrame:\n    \"\"\"Mark this DataFrame to be persisted after first computation.\n\n    The persisted DataFrame will be cached after its first computation,\n    avoiding recomputation in subsequent operations. This is useful for DataFrames\n    that are reused multiple times in your workflow.\n\n    Returns:\n        DataFrame: Same DataFrame, but marked for persistence\n\n    Example:\n        ```python\n        # Cache intermediate results for reuse\n        filtered_df = (df\n            .filter(col(\"age\") &gt; 25)\n            .persist()  # Cache these results\n        )\n\n        # Both operations will use cached results\n        result1 = filtered_df.group_by(\"department\").count()\n        result2 = filtered_df.select(\"name\", \"salary\")\n        ```\n    \"\"\"\n    table_name = f\"cache_{uuid.uuid4().hex}\"\n    cache_info = CacheInfo(duckdb_table_name=table_name)\n    self._logical_plan.set_cache_info(cache_info)\n    return self._from_logical_plan(self._logical_plan)\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.select","title":"select","text":"<pre><code>select(*cols: ColumnOrName) -&gt; DataFrame\n</code></pre> <p>Projects a set of Column expressions or column names.</p> <p>Parameters:</p> <ul> <li> <code>*cols</code>               (<code>ColumnOrName</code>, default:                   <code>()</code> )           \u2013            <p>Column expressions to select. Can be: - String column names (e.g., \"id\", \"name\") - Column objects (e.g., col(\"id\"), col(\"age\") + 1)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with selected columns</p> </li> </ul> Select by column names <pre><code># Create a DataFrame\ndf = session.create_dataframe({\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]})\n\n# Select by column names\ndf.select(col(\"name\"), col(\"age\")).show()\n# Output:\n# +-----+---+\n# | name|age|\n# +-----+---+\n# |Alice| 25|\n# |  Bob| 30|\n# +-----+---+\n</code></pre> Select with expressions <pre><code># Select with expressions\ndf.select(col(\"name\"), col(\"age\") + 1).show()\n# Output:\n# +-----+-------+\n# | name|age + 1|\n# +-----+-------+\n# |Alice|     26|\n# |  Bob|     31|\n# +-----+-------+\n</code></pre> Mix strings and expressions <pre><code># Mix strings and expressions\ndf.select(col(\"name\"), col(\"age\") * 2).show()\n# Output:\n# +-----+-------+\n# | name|age * 2|\n# +-----+-------+\n# |Alice|     50|\n# |  Bob|     60|\n# +-----+-------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def select(self, *cols: ColumnOrName) -&gt; DataFrame:\n    \"\"\"Projects a set of Column expressions or column names.\n\n    Args:\n        *cols: Column expressions to select. Can be:\n            - String column names (e.g., \"id\", \"name\")\n            - Column objects (e.g., col(\"id\"), col(\"age\") + 1)\n\n    Returns:\n        DataFrame: A new DataFrame with selected columns\n\n    Example: Select by column names\n        ```python\n        # Create a DataFrame\n        df = session.create_dataframe({\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]})\n\n        # Select by column names\n        df.select(col(\"name\"), col(\"age\")).show()\n        # Output:\n        # +-----+---+\n        # | name|age|\n        # +-----+---+\n        # |Alice| 25|\n        # |  Bob| 30|\n        # +-----+---+\n        ```\n\n    Example: Select with expressions\n        ```python\n        # Select with expressions\n        df.select(col(\"name\"), col(\"age\") + 1).show()\n        # Output:\n        # +-----+-------+\n        # | name|age + 1|\n        # +-----+-------+\n        # |Alice|     26|\n        # |  Bob|     31|\n        # +-----+-------+\n        ```\n\n    Example: Mix strings and expressions\n        ```python\n        # Mix strings and expressions\n        df.select(col(\"name\"), col(\"age\") * 2).show()\n        # Output:\n        # +-----+-------+\n        # | name|age * 2|\n        # +-----+-------+\n        # |Alice|     50|\n        # |  Bob|     60|\n        # +-----+-------+\n        ```\n    \"\"\"\n    exprs = []\n    if not cols:\n        return self\n    for c in cols:\n        if isinstance(c, str):\n            if c == \"*\":\n                exprs.extend(col(field)._logical_expr for field in self.columns)\n            else:\n                exprs.append(col(c)._logical_expr)\n        else:\n            exprs.append(c._logical_expr)\n\n    return self._from_logical_plan(\n        Projection(self._logical_plan, exprs)\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.show","title":"show","text":"<pre><code>show(n: int = 10, explain_analyze: bool = False) -&gt; None\n</code></pre> <p>Display the DataFrame content in a tabular form.</p> <p>This is an action that triggers computation of the DataFrame. The output is printed to stdout in a formatted table.</p> <p>Parameters:</p> <ul> <li> <code>n</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of rows to display</p> </li> <li> <code>explain_analyze</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to print the explain analyze plan</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def show(self, n: int = 10, explain_analyze: bool = False) -&gt; None:\n    \"\"\"Display the DataFrame content in a tabular form.\n\n    This is an action that triggers computation of the DataFrame.\n    The output is printed to stdout in a formatted table.\n\n    Args:\n        n: Number of rows to display\n        explain_analyze: Whether to print the explain analyze plan\n    \"\"\"\n    output, metrics = self._logical_plan.session_state.execution.show(self._logical_plan, n)\n    logger.info(metrics.get_summary())\n    print(output)\n    if explain_analyze:\n        print(metrics.get_execution_plan_details())\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.sort","title":"sort","text":"<pre><code>sort(cols: Union[ColumnOrName, List[ColumnOrName], None] = None, ascending: Optional[Union[bool, List[bool]]] = None) -&gt; DataFrame\n</code></pre> <p>Sort the DataFrame by the specified columns.</p> <p>Parameters:</p> <ul> <li> <code>cols</code>               (<code>Union[ColumnOrName, List[ColumnOrName], None]</code>, default:                   <code>None</code> )           \u2013            <p>Columns to sort by. This can be: - A single column name (str) - A Column expression (e.g., <code>col(\"name\")</code>) - A list of column names or Column expressions - Column expressions may include sorting directives such as <code>asc(\"col\")</code>, <code>desc(\"col\")</code>, <code>asc_nulls_last(\"col\")</code>, etc. - If no columns are provided, the operation is a no-op.</p> </li> <li> <code>ascending</code>               (<code>Optional[Union[bool, List[bool]]]</code>, default:                   <code>None</code> )           \u2013            <p>A boolean or list of booleans indicating sort order. - If <code>True</code>, sorts in ascending order; if <code>False</code>, descending. - If a list is provided, its length must match the number of columns. - Cannot be used if any of the columns use <code>asc()</code>/<code>desc()</code> expressions. - If not specified and no sort expressions are used, columns will be sorted in ascending order by default.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame sorted by the specified columns.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <ul> <li>If <code>ascending</code> is provided and its length does not match <code>cols</code></li> <li>If both <code>ascending</code> and column expressions like <code>asc()</code>/<code>desc()</code> are used</li> </ul> </li> <li> <code>TypeError</code>             \u2013            <ul> <li>If <code>cols</code> is not a column name, Column, or list of column names/Columns</li> <li>If <code>ascending</code> is not a boolean or list of booleans</li> </ul> </li> </ul> Sort in ascending order <pre><code># Create sample DataFrame\ndf = session.create_dataframe([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n\n# Sort by age in ascending order\ndf.sort(asc(col(\"age\"))).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  2|Alice|\n# |  5|  Bob|\n# +---+-----+\n</code></pre> Sort in descending order <pre><code># Sort by age in descending order\ndf.sort(col(\"age\").desc()).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  5|  Bob|\n# |  2|Alice|\n# +---+-----+\n</code></pre> Sort with boolean ascending parameter <pre><code># Sort by age in descending order using boolean\ndf.sort(col(\"age\"), ascending=False).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  5|  Bob|\n# |  2|Alice|\n# +---+-----+\n</code></pre> Multiple columns with different sort orders <pre><code># Create sample DataFrame\ndf = session.create_dataframe([(2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n\n# Sort by age descending, then name ascending\ndf.sort(desc(col(\"age\")), col(\"name\")).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  5|  Bob|\n# |  2|Alice|\n# |  2|  Bob|\n# +---+-----+\n</code></pre> Multiple columns with list of ascending strategies <pre><code># Sort both columns in descending order\ndf.sort([col(\"age\"), col(\"name\")], ascending=[False, False]).show()\n# Output:\n# +---+-----+\n# |age| name|\n# +---+-----+\n# |  5|  Bob|\n# |  2|  Bob|\n# |  2|Alice|\n# +---+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def sort(\n    self,\n    cols: Union[ColumnOrName, List[ColumnOrName], None] = None,\n    ascending: Optional[Union[bool, List[bool]]] = None,\n) -&gt; DataFrame:\n    \"\"\"Sort the DataFrame by the specified columns.\n\n    Args:\n        cols: Columns to sort by. This can be:\n            - A single column name (str)\n            - A Column expression (e.g., `col(\"name\")`)\n            - A list of column names or Column expressions\n            - Column expressions may include sorting directives such as `asc(\"col\")`, `desc(\"col\")`,\n            `asc_nulls_last(\"col\")`, etc.\n            - If no columns are provided, the operation is a no-op.\n\n        ascending: A boolean or list of booleans indicating sort order.\n            - If `True`, sorts in ascending order; if `False`, descending.\n            - If a list is provided, its length must match the number of columns.\n            - Cannot be used if any of the columns use `asc()`/`desc()` expressions.\n            - If not specified and no sort expressions are used, columns will be sorted in ascending order by default.\n\n    Returns:\n        DataFrame: A new DataFrame sorted by the specified columns.\n\n    Raises:\n        ValueError:\n            - If `ascending` is provided and its length does not match `cols`\n            - If both `ascending` and column expressions like `asc()`/`desc()` are used\n        TypeError:\n            - If `cols` is not a column name, Column, or list of column names/Columns\n            - If `ascending` is not a boolean or list of booleans\n\n    Example: Sort in ascending order\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n\n        # Sort by age in ascending order\n        df.sort(asc(col(\"age\"))).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  2|Alice|\n        # |  5|  Bob|\n        # +---+-----+\n        ```\n\n    Example: Sort in descending order\n        ```python\n        # Sort by age in descending order\n        df.sort(col(\"age\").desc()).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  5|  Bob|\n        # |  2|Alice|\n        # +---+-----+\n        ```\n\n    Example: Sort with boolean ascending parameter\n        ```python\n        # Sort by age in descending order using boolean\n        df.sort(col(\"age\"), ascending=False).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  5|  Bob|\n        # |  2|Alice|\n        # +---+-----+\n        ```\n\n    Example: Multiple columns with different sort orders\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe([(2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n\n        # Sort by age descending, then name ascending\n        df.sort(desc(col(\"age\")), col(\"name\")).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  5|  Bob|\n        # |  2|Alice|\n        # |  2|  Bob|\n        # +---+-----+\n        ```\n\n    Example: Multiple columns with list of ascending strategies\n        ```python\n        # Sort both columns in descending order\n        df.sort([col(\"age\"), col(\"name\")], ascending=[False, False]).show()\n        # Output:\n        # +---+-----+\n        # |age| name|\n        # +---+-----+\n        # |  5|  Bob|\n        # |  2|  Bob|\n        # |  2|Alice|\n        # +---+-----+\n        ```\n    \"\"\"\n    col_args = cols\n    if cols is None:\n        return self._from_logical_plan(\n            Sort(self._logical_plan, [])\n        )\n    elif not isinstance(cols, List):\n        col_args = [cols]\n\n    # parse the ascending arguments\n    bool_ascending = []\n    using_default_ascending = False\n    if ascending is None:\n        using_default_ascending = True\n        bool_ascending = [True] * len(col_args)\n    elif isinstance(ascending, bool):\n        bool_ascending = [ascending] * len(col_args)\n    elif isinstance(ascending, List):\n        bool_ascending = ascending\n        if len(bool_ascending) != len(cols):\n            raise ValueError(\n                f\"the list length of ascending sort strategies must match the specified sort columns\"\n                f\"Got {len(cols)} column expressions and {len(bool_ascending)} ascending strategies. \"\n            )\n    else:\n        raise TypeError(\n            f\"Invalid ascending strategy type: {type(ascending)}.  Must be a boolean or list of booleans.\"\n        )\n\n    # create our list of sort expressions, for each column expression\n    # that isn't already provided as a asc()/desc() SortExpr\n    sort_exprs = []\n    for c, asc_bool in zip(col_args, bool_ascending, strict=True):\n        if isinstance(c, ColumnOrName):\n            c_expr = Column._from_col_or_name(c)._logical_expr\n        else:\n            raise TypeError(\n                f\"Invalid column type: {type(c).__name__}.  Must be a string or Column Expression.\"\n            )\n        if not isinstance(asc_bool, bool):\n            raise TypeError(\n                f\"Invalid ascending strategy type: {type(asc_bool).__name__}.  Must be a boolean.\"\n            )\n        if isinstance(c_expr, SortExpr):\n            if not using_default_ascending:\n                raise TypeError(\n                    \"Cannot specify both asc()/desc() expressions and boolean ascending strategies.\"\n                    f\"Got expression: {c_expr} and ascending argument: {bool_ascending}\"\n                )\n            sort_exprs.append(c_expr)\n        else:\n            sort_exprs.append(SortExpr(c_expr, ascending=asc_bool))\n\n    return self._from_logical_plan(\n        Sort(self._logical_plan, sort_exprs),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.to_arrow","title":"to_arrow","text":"<pre><code>to_arrow() -&gt; pa.Table\n</code></pre> <p>Execute the DataFrame computation and return an Apache Arrow Table.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into an Apache Arrow Table with columnar memory layout optimized for analytics and zero-copy data exchange.</p> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>pa.Table: An Apache Arrow Table containing the computed results</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_arrow(self) -&gt; pa.Table:\n    \"\"\"Execute the DataFrame computation and return an Apache Arrow Table.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into an Apache Arrow Table with columnar memory layout\n    optimized for analytics and zero-copy data exchange.\n\n    Returns:\n        pa.Table: An Apache Arrow Table containing the computed results\n    \"\"\"\n    return self.collect(\"arrow\").data\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; pd.DataFrame\n</code></pre> <p>Execute the DataFrame computation and return a Pandas DataFrame.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a Pandas DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A Pandas DataFrame containing the computed results with</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"Execute the DataFrame computation and return a Pandas DataFrame.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a Pandas DataFrame.\n\n    Returns:\n        pd.DataFrame: A Pandas DataFrame containing the computed results with\n    \"\"\"\n    return self.collect(\"pandas\").data\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.to_polars","title":"to_polars","text":"<pre><code>to_polars() -&gt; pl.DataFrame\n</code></pre> <p>Execute the DataFrame computation and return the result as a Polars DataFrame.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a Polars DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pl.DataFrame: A Polars DataFrame with materialized results</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_polars(self) -&gt; pl.DataFrame:\n    \"\"\"Execute the DataFrame computation and return the result as a Polars DataFrame.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a Polars DataFrame.\n\n    Returns:\n        pl.DataFrame: A Polars DataFrame with materialized results\n    \"\"\"\n    return self.collect(\"polars\").data\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.to_pydict","title":"to_pydict","text":"<pre><code>to_pydict() -&gt; Dict[str, List[Any]]\n</code></pre> <p>Execute the DataFrame computation and return a dictionary of column arrays.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a Python dictionary where each column becomes a list of values.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, List[Any]]</code>           \u2013            <p>Dict[str, List[Any]]: A dictionary containing the computed results with: - Keys: Column names as strings - Values: Lists containing all values for each column</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_pydict(self) -&gt; Dict[str, List[Any]]:\n    \"\"\"Execute the DataFrame computation and return a dictionary of column arrays.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a Python dictionary where each column becomes a list of values.\n\n    Returns:\n        Dict[str, List[Any]]: A dictionary containing the computed results with:\n            - Keys: Column names as strings\n            - Values: Lists containing all values for each column\n    \"\"\"\n    return self.collect(\"pydict\").data\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.to_pylist","title":"to_pylist","text":"<pre><code>to_pylist() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Execute the DataFrame computation and return a list of row dictionaries.</p> <p>This is an action that triggers computation of the DataFrame query plan. All transformations and operations are executed, and the results are materialized into a Python list where each element is a dictionary representing one row with column names as keys.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>List[Dict[str, Any]]: A list containing the computed results with: - Each element: A dictionary representing one row - Dictionary keys: Column names as strings - Dictionary values: Cell values in Python native types - List length equals number of rows in the result</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def to_pylist(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Execute the DataFrame computation and return a list of row dictionaries.\n\n    This is an action that triggers computation of the DataFrame query plan.\n    All transformations and operations are executed, and the results are\n    materialized into a Python list where each element is a dictionary\n    representing one row with column names as keys.\n\n    Returns:\n        List[Dict[str, Any]]: A list containing the computed results with:\n            - Each element: A dictionary representing one row\n            - Dictionary keys: Column names as strings\n            - Dictionary values: Cell values in Python native types\n            - List length equals number of rows in the result\n    \"\"\"\n    return self.collect(\"pylist\").data\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.union","title":"union","text":"<pre><code>union(other: DataFrame) -&gt; DataFrame\n</code></pre> <p>Return a new DataFrame containing the union of rows in this and another DataFrame.</p> <p>This is equivalent to UNION ALL in SQL. To remove duplicates, use drop_duplicates() after union().</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>DataFrame</code>)           \u2013            <p>Another DataFrame with the same schema.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame containing rows from both DataFrames.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the DataFrames have different schemas.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If other is not a DataFrame.</p> </li> </ul> Union two DataFrames <pre><code># Create two DataFrames\ndf1 = session.create_dataframe({\n    \"id\": [1, 2],\n    \"value\": [\"a\", \"b\"]\n})\ndf2 = session.create_dataframe({\n    \"id\": [3, 4],\n    \"value\": [\"c\", \"d\"]\n})\n\n# Union the DataFrames\ndf1.union(df2).show()\n# Output:\n# +---+-----+\n# | id|value|\n# +---+-----+\n# |  1|    a|\n# |  2|    b|\n# |  3|    c|\n# |  4|    d|\n# +---+-----+\n</code></pre> Union with duplicates <pre><code># Create DataFrames with overlapping data\ndf1 = session.create_dataframe({\n    \"id\": [1, 2],\n    \"value\": [\"a\", \"b\"]\n})\ndf2 = session.create_dataframe({\n    \"id\": [2, 3],\n    \"value\": [\"b\", \"c\"]\n})\n\n# Union with duplicates\ndf1.union(df2).show()\n# Output:\n# +---+-----+\n# | id|value|\n# +---+-----+\n# |  1|    a|\n# |  2|    b|\n# |  2|    b|\n# |  3|    c|\n# +---+-----+\n\n# Remove duplicates after union\ndf1.union(df2).drop_duplicates().show()\n# Output:\n# +---+-----+\n# | id|value|\n# +---+-----+\n# |  1|    a|\n# |  2|    b|\n# |  3|    c|\n# +---+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def union(self, other: DataFrame) -&gt; DataFrame:\n    \"\"\"Return a new DataFrame containing the union of rows in this and another DataFrame.\n\n    This is equivalent to UNION ALL in SQL. To remove duplicates, use drop_duplicates() after union().\n\n    Args:\n        other: Another DataFrame with the same schema.\n\n    Returns:\n        DataFrame: A new DataFrame containing rows from both DataFrames.\n\n    Raises:\n        ValueError: If the DataFrames have different schemas.\n        TypeError: If other is not a DataFrame.\n\n    Example: Union two DataFrames\n        ```python\n        # Create two DataFrames\n        df1 = session.create_dataframe({\n            \"id\": [1, 2],\n            \"value\": [\"a\", \"b\"]\n        })\n        df2 = session.create_dataframe({\n            \"id\": [3, 4],\n            \"value\": [\"c\", \"d\"]\n        })\n\n        # Union the DataFrames\n        df1.union(df2).show()\n        # Output:\n        # +---+-----+\n        # | id|value|\n        # +---+-----+\n        # |  1|    a|\n        # |  2|    b|\n        # |  3|    c|\n        # |  4|    d|\n        # +---+-----+\n        ```\n\n    Example: Union with duplicates\n        ```python\n        # Create DataFrames with overlapping data\n        df1 = session.create_dataframe({\n            \"id\": [1, 2],\n            \"value\": [\"a\", \"b\"]\n        })\n        df2 = session.create_dataframe({\n            \"id\": [2, 3],\n            \"value\": [\"b\", \"c\"]\n        })\n\n        # Union with duplicates\n        df1.union(df2).show()\n        # Output:\n        # +---+-----+\n        # | id|value|\n        # +---+-----+\n        # |  1|    a|\n        # |  2|    b|\n        # |  2|    b|\n        # |  3|    c|\n        # +---+-----+\n\n        # Remove duplicates after union\n        df1.union(df2).drop_duplicates().show()\n        # Output:\n        # +---+-----+\n        # | id|value|\n        # +---+-----+\n        # |  1|    a|\n        # |  2|    b|\n        # |  3|    c|\n        # +---+-----+\n        ```\n    \"\"\"\n    return self._from_logical_plan(\n        UnionLogicalPlan([self._logical_plan, other._logical_plan]),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.unnest","title":"unnest","text":"<pre><code>unnest(*col_names: str) -&gt; DataFrame\n</code></pre> <p>Unnest the specified struct columns into separate columns.</p> <p>This operation flattens nested struct data by expanding each field of a struct into its own top-level column.</p> <p>For each specified column containing a struct: 1. Each field in the struct becomes a separate column. 2. New columns are named after the corresponding struct fields. 3. The new columns are inserted into the DataFrame in place of the original struct column. 4. The overall column order is preserved.</p> <p>Parameters:</p> <ul> <li> <code>*col_names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>One or more struct columns to unnest. Each can be a string (column name) or a Column expression.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with the specified struct columns expanded.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If any argument is not a string or Column.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If a specified column does not contain struct data.</p> </li> </ul> Unnest struct column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"id\": [1, 2],\n    \"tags\": [{\"red\": 1, \"blue\": 2}, {\"red\": 3}],\n    \"name\": [\"Alice\", \"Bob\"]\n})\n\n# Unnest the tags column\ndf.unnest(col(\"tags\")).show()\n# Output:\n# +---+---+----+-----+\n# | id| red|blue| name|\n# +---+---+----+-----+\n# |  1|  1|   2|Alice|\n# |  2|  3|null|  Bob|\n# +---+---+----+-----+\n</code></pre> Unnest multiple struct columns <pre><code># Create sample DataFrame with multiple struct columns\ndf = session.create_dataframe({\n    \"id\": [1, 2],\n    \"tags\": [{\"red\": 1, \"blue\": 2}, {\"red\": 3}],\n    \"info\": [{\"age\": 25, \"city\": \"NY\"}, {\"age\": 30, \"city\": \"LA\"}],\n    \"name\": [\"Alice\", \"Bob\"]\n})\n\n# Unnest multiple struct columns\ndf.unnest(col(\"tags\"), col(\"info\")).show()\n# Output:\n# +---+---+----+---+----+-----+\n# | id| red|blue|age|city| name|\n# +---+---+----+---+----+-----+\n# |  1|  1|   2| 25|  NY|Alice|\n# |  2|  3|null| 30|  LA|  Bob|\n# +---+---+----+---+----+-----+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def unnest(self, *col_names: str) -&gt; DataFrame:\n    \"\"\"Unnest the specified struct columns into separate columns.\n\n    This operation flattens nested struct data by expanding each field of a struct\n    into its own top-level column.\n\n    For each specified column containing a struct:\n    1. Each field in the struct becomes a separate column.\n    2. New columns are named after the corresponding struct fields.\n    3. The new columns are inserted into the DataFrame in place of the original struct column.\n    4. The overall column order is preserved.\n\n    Args:\n        *col_names: One or more struct columns to unnest. Each can be a string (column name)\n            or a Column expression.\n\n    Returns:\n        DataFrame: A new DataFrame with the specified struct columns expanded.\n\n    Raises:\n        TypeError: If any argument is not a string or Column.\n        ValueError: If a specified column does not contain struct data.\n\n    Example: Unnest struct column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"id\": [1, 2],\n            \"tags\": [{\"red\": 1, \"blue\": 2}, {\"red\": 3}],\n            \"name\": [\"Alice\", \"Bob\"]\n        })\n\n        # Unnest the tags column\n        df.unnest(col(\"tags\")).show()\n        # Output:\n        # +---+---+----+-----+\n        # | id| red|blue| name|\n        # +---+---+----+-----+\n        # |  1|  1|   2|Alice|\n        # |  2|  3|null|  Bob|\n        # +---+---+----+-----+\n        ```\n\n    Example: Unnest multiple struct columns\n        ```python\n        # Create sample DataFrame with multiple struct columns\n        df = session.create_dataframe({\n            \"id\": [1, 2],\n            \"tags\": [{\"red\": 1, \"blue\": 2}, {\"red\": 3}],\n            \"info\": [{\"age\": 25, \"city\": \"NY\"}, {\"age\": 30, \"city\": \"LA\"}],\n            \"name\": [\"Alice\", \"Bob\"]\n        })\n\n        # Unnest multiple struct columns\n        df.unnest(col(\"tags\"), col(\"info\")).show()\n        # Output:\n        # +---+---+----+---+----+-----+\n        # | id| red|blue|age|city| name|\n        # +---+---+----+---+----+-----+\n        # |  1|  1|   2| 25|  NY|Alice|\n        # |  2|  3|null| 30|  LA|  Bob|\n        # +---+---+----+---+----+-----+\n        ```\n    \"\"\"\n    if not col_names:\n        return self\n    exprs = []\n    for c in col_names:\n        if c not in self.columns:\n            raise TypeError(f\"Column {c} not found in DataFrame.\")\n        exprs.append(col(c)._logical_expr)\n    return self._from_logical_plan(\n        Unnest(self._logical_plan, exprs),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.where","title":"where","text":"<pre><code>where(condition: Column) -&gt; DataFrame\n</code></pre> <p>Filters rows using the given condition (alias for filter()).</p> <p>Parameters:</p> <ul> <li> <code>condition</code>               (<code>Column</code>)           \u2013            <p>A Column expression that evaluates to a boolean</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>Filtered DataFrame</p> </li> </ul> See Also <p>filter(): Full documentation of filtering behavior</p> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def where(self, condition: Column) -&gt; DataFrame:\n    \"\"\"Filters rows using the given condition (alias for filter()).\n\n    Args:\n        condition: A Column expression that evaluates to a boolean\n\n    Returns:\n        DataFrame: Filtered DataFrame\n\n    See Also:\n        filter(): Full documentation of filtering behavior\n    \"\"\"\n    return self.filter(condition)\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.with_column","title":"with_column","text":"<pre><code>with_column(col_name: str, col: Union[Any, Column]) -&gt; DataFrame\n</code></pre> <p>Add a new column or replace an existing column.</p> <p>Parameters:</p> <ul> <li> <code>col_name</code>               (<code>str</code>)           \u2013            <p>Name of the new column</p> </li> <li> <code>col</code>               (<code>Union[Any, Column]</code>)           \u2013            <p>Column expression or value to assign to the column. If not a Column, it will be treated as a literal value.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>New DataFrame with added/replaced column</p> </li> </ul> Add literal column <pre><code># Create a DataFrame\ndf = session.create_dataframe({\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]})\n\n# Add literal column\ndf.with_column(\"constant\", lit(1)).show()\n# Output:\n# +-----+---+--------+\n# | name|age|constant|\n# +-----+---+--------+\n# |Alice| 25|       1|\n# |  Bob| 30|       1|\n# +-----+---+--------+\n</code></pre> Add computed column <pre><code># Add computed column\ndf.with_column(\"double_age\", col(\"age\") * 2).show()\n# Output:\n# +-----+---+----------+\n# | name|age|double_age|\n# +-----+---+----------+\n# |Alice| 25|        50|\n# |  Bob| 30|        60|\n# +-----+---+----------+\n</code></pre> Replace existing column <pre><code># Replace existing column\ndf.with_column(\"age\", col(\"age\") + 1).show()\n# Output:\n# +-----+---+\n# | name|age|\n# +-----+---+\n# |Alice| 26|\n# |  Bob| 31|\n# +-----+---+\n</code></pre> Add column with complex expression <pre><code># Add column with complex expression\ndf.with_column(\n    \"age_category\",\n    when(col(\"age\") &lt; 30, \"young\")\n    .when(col(\"age\") &lt; 50, \"middle\")\n    .otherwise(\"senior\")\n).show()\n# Output:\n# +-----+---+------------+\n# | name|age|age_category|\n# +-----+---+------------+\n# |Alice| 25|       young|\n# |  Bob| 30|     middle|\n# +-----+---+------------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def with_column(self, col_name: str, col: Union[Any, Column]) -&gt; DataFrame:\n    \"\"\"Add a new column or replace an existing column.\n\n    Args:\n        col_name: Name of the new column\n        col: Column expression or value to assign to the column. If not a Column,\n            it will be treated as a literal value.\n\n    Returns:\n        DataFrame: New DataFrame with added/replaced column\n\n    Example: Add literal column\n        ```python\n        # Create a DataFrame\n        df = session.create_dataframe({\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]})\n\n        # Add literal column\n        df.with_column(\"constant\", lit(1)).show()\n        # Output:\n        # +-----+---+--------+\n        # | name|age|constant|\n        # +-----+---+--------+\n        # |Alice| 25|       1|\n        # |  Bob| 30|       1|\n        # +-----+---+--------+\n        ```\n\n    Example: Add computed column\n        ```python\n        # Add computed column\n        df.with_column(\"double_age\", col(\"age\") * 2).show()\n        # Output:\n        # +-----+---+----------+\n        # | name|age|double_age|\n        # +-----+---+----------+\n        # |Alice| 25|        50|\n        # |  Bob| 30|        60|\n        # +-----+---+----------+\n        ```\n\n    Example: Replace existing column\n        ```python\n        # Replace existing column\n        df.with_column(\"age\", col(\"age\") + 1).show()\n        # Output:\n        # +-----+---+\n        # | name|age|\n        # +-----+---+\n        # |Alice| 26|\n        # |  Bob| 31|\n        # +-----+---+\n        ```\n\n    Example: Add column with complex expression\n        ```python\n        # Add column with complex expression\n        df.with_column(\n            \"age_category\",\n            when(col(\"age\") &lt; 30, \"young\")\n            .when(col(\"age\") &lt; 50, \"middle\")\n            .otherwise(\"senior\")\n        ).show()\n        # Output:\n        # +-----+---+------------+\n        # | name|age|age_category|\n        # +-----+---+------------+\n        # |Alice| 25|       young|\n        # |  Bob| 30|     middle|\n        # +-----+---+------------+\n        ```\n    \"\"\"\n    exprs = []\n    if not isinstance(col, Column):\n        col = lit(col)\n\n    for field in self.columns:\n        if field != col_name:\n            exprs.append(Column._from_column_name(field)._logical_expr)\n\n    # Add the new column with alias\n    exprs.append(col.alias(col_name)._logical_expr)\n\n    return self._from_logical_plan(\n        Projection(self._logical_plan, exprs)\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/dataframe/#fenic.api.dataframe.dataframe.DataFrame.with_column_renamed","title":"with_column_renamed","text":"<pre><code>with_column_renamed(col_name: str, new_col_name: str) -&gt; DataFrame\n</code></pre> <p>Rename a column. No-op if the column does not exist.</p> <p>Parameters:</p> <ul> <li> <code>col_name</code>               (<code>str</code>)           \u2013            <p>Name of the column to rename.</p> </li> <li> <code>new_col_name</code>               (<code>str</code>)           \u2013            <p>New name for the column.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>New DataFrame with the column renamed.</p> </li> </ul> Rename a column <pre><code># Create sample DataFrame\ndf = session.create_dataframe({\n    \"age\": [25, 30, 35],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n})\n\n# Rename a column\ndf.with_column_renamed(\"age\", \"age_in_years\").show()\n# Output:\n# +------------+-------+\n# |age_in_years|   name|\n# +------------+-------+\n# |         25|  Alice|\n# |         30|    Bob|\n# |         35|Charlie|\n# +------------+-------+\n</code></pre> Rename multiple columns <pre><code># Rename multiple columns\ndf = (df\n    .with_column_renamed(\"age\", \"age_in_years\")\n    .with_column_renamed(\"name\", \"full_name\")\n).show()\n# Output:\n# +------------+----------+\n# |age_in_years|full_name |\n# +------------+----------+\n# |         25|     Alice|\n# |         30|       Bob|\n# |         35|   Charlie|\n# +------------+----------+\n</code></pre> Source code in <code>src/fenic/api/dataframe/dataframe.py</code> <pre><code>def with_column_renamed(self, col_name: str, new_col_name: str) -&gt; DataFrame:\n    \"\"\"Rename a column. No-op if the column does not exist.\n\n    Args:\n        col_name: Name of the column to rename.\n        new_col_name: New name for the column.\n\n    Returns:\n        DataFrame: New DataFrame with the column renamed.\n\n    Example: Rename a column\n        ```python\n        # Create sample DataFrame\n        df = session.create_dataframe({\n            \"age\": [25, 30, 35],\n            \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\n        })\n\n        # Rename a column\n        df.with_column_renamed(\"age\", \"age_in_years\").show()\n        # Output:\n        # +------------+-------+\n        # |age_in_years|   name|\n        # +------------+-------+\n        # |         25|  Alice|\n        # |         30|    Bob|\n        # |         35|Charlie|\n        # +------------+-------+\n        ```\n\n    Example: Rename multiple columns\n        ```python\n        # Rename multiple columns\n        df = (df\n            .with_column_renamed(\"age\", \"age_in_years\")\n            .with_column_renamed(\"name\", \"full_name\")\n        ).show()\n        # Output:\n        # +------------+----------+\n        # |age_in_years|full_name |\n        # +------------+----------+\n        # |         25|     Alice|\n        # |         30|       Bob|\n        # |         35|   Charlie|\n        # +------------+----------+\n        ```\n    \"\"\"\n    exprs = []\n    renamed = False\n\n    for field in self.schema.column_fields:\n        name = field.name\n        if name == col_name:\n            exprs.append(col(name).alias(new_col_name)._logical_expr)\n            renamed = True\n        else:\n            exprs.append(col(name)._logical_expr)\n\n    if not renamed:\n        return self\n\n    return self._from_logical_plan(\n        Projection(self._logical_plan, exprs)\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/grouped_data/","title":"fenic.api.dataframe.grouped_data","text":""},{"location":"reference/fenic/api/dataframe/grouped_data/#fenic.api.dataframe.grouped_data","title":"fenic.api.dataframe.grouped_data","text":"<p>GroupedData class for aggregations on grouped DataFrames.</p> <p>Classes:</p> <ul> <li> <code>GroupedData</code>           \u2013            <p>Methods for aggregations on a grouped DataFrame.</p> </li> </ul>"},{"location":"reference/fenic/api/dataframe/grouped_data/#fenic.api.dataframe.grouped_data.GroupedData","title":"GroupedData","text":"<pre><code>GroupedData(df: DataFrame, by: Optional[List[ColumnOrName]] = None)\n</code></pre> <p>               Bases: <code>BaseGroupedData</code></p> <p>Methods for aggregations on a grouped DataFrame.</p> <p>Initialize grouped data.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to group.</p> </li> <li> <code>by</code>               (<code>Optional[List[ColumnOrName]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of columns to group by.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>agg</code>             \u2013              <p>Compute aggregations on grouped data and return the result as a DataFrame.</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/grouped_data.py</code> <pre><code>def __init__(self, df: DataFrame, by: Optional[List[ColumnOrName]] = None):\n    \"\"\"Initialize grouped data.\n\n    Args:\n        df: The DataFrame to group.\n        by: Optional list of columns to group by.\n    \"\"\"\n    super().__init__(df)\n    self._by: List[Column] = []\n    for c in by or []:\n        if isinstance(c, str):\n            self._by.append(col(c))\n        elif isinstance(c, Column):\n            # Allow any expression except literals\n            if isinstance(c._logical_expr, LiteralExpr):\n                raise ValueError(f\"Cannot group by literal value: {c}\")\n            self._by.append(c)\n        else:\n            raise TypeError(\n                f\"Group by expressions must be string or Column, got {type(c)}\"\n            )\n    self._by_exprs = [c._logical_expr for c in self._by]\n</code></pre>"},{"location":"reference/fenic/api/dataframe/grouped_data/#fenic.api.dataframe.grouped_data.GroupedData.agg","title":"agg","text":"<pre><code>agg(*exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame\n</code></pre> <p>Compute aggregations on grouped data and return the result as a DataFrame.</p> <p>This method applies aggregate functions to the grouped data.</p> <p>Parameters:</p> <ul> <li> <code>*exprs</code>               (<code>Union[Column, Dict[str, str]]</code>, default:                   <code>()</code> )           \u2013            <p>Aggregation expressions. Can be:</p> <ul> <li>Column expressions with aggregate functions (e.g., <code>count(\"*\")</code>, <code>sum(\"amount\")</code>)</li> <li>A dictionary mapping column names to aggregate function names (e.g., <code>{\"amount\": \"sum\", \"age\": \"avg\"}</code>)</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with one row per group and columns for group keys and aggregated values</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If arguments are not Column expressions or a dictionary</p> </li> <li> <code>ValueError</code>             \u2013            <p>If dictionary values are not valid aggregate function names</p> </li> </ul> Count employees by department <pre><code># Group by department and count employees\ndf.group_by(\"department\").agg(count(\"*\").alias(\"employee_count\"))\n</code></pre> Multiple aggregations <pre><code># Multiple aggregations\ndf.group_by(\"department\").agg(\n    count(\"*\").alias(\"employee_count\"),\n    avg(\"salary\").alias(\"avg_salary\"),\n    max(\"age\").alias(\"max_age\")\n)\n</code></pre> Dictionary style aggregations <pre><code># Dictionary style for simple aggregations\ndf.group_by(\"department\", \"location\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n</code></pre> Source code in <code>src/fenic/api/dataframe/grouped_data.py</code> <pre><code>def agg(self, *exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame:\n    \"\"\"Compute aggregations on grouped data and return the result as a DataFrame.\n\n    This method applies aggregate functions to the grouped data.\n\n    Args:\n        *exprs: Aggregation expressions. Can be:\n\n            - Column expressions with aggregate functions (e.g., `count(\"*\")`, `sum(\"amount\")`)\n            - A dictionary mapping column names to aggregate function names (e.g., `{\"amount\": \"sum\", \"age\": \"avg\"}`)\n\n    Returns:\n        DataFrame: A new DataFrame with one row per group and columns for group keys and aggregated values\n\n    Raises:\n        ValueError: If arguments are not Column expressions or a dictionary\n        ValueError: If dictionary values are not valid aggregate function names\n\n    Example: Count employees by department\n        ```python\n        # Group by department and count employees\n        df.group_by(\"department\").agg(count(\"*\").alias(\"employee_count\"))\n        ```\n\n    Example: Multiple aggregations\n        ```python\n        # Multiple aggregations\n        df.group_by(\"department\").agg(\n            count(\"*\").alias(\"employee_count\"),\n            avg(\"salary\").alias(\"avg_salary\"),\n            max(\"age\").alias(\"max_age\")\n        )\n        ```\n\n    Example: Dictionary style aggregations\n        ```python\n        # Dictionary style for simple aggregations\n        df.group_by(\"department\", \"location\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n        ```\n    \"\"\"\n    self._validate_agg_exprs(*exprs)\n    if len(exprs) == 1 and isinstance(exprs[0], dict):\n        agg_dict = exprs[0]\n        return self.agg(*self._process_agg_dict(agg_dict))\n\n    agg_exprs = self._process_agg_exprs(exprs)\n    return self._df._from_logical_plan(\n        Aggregate(self._df._logical_plan, self._by_exprs, agg_exprs),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/semantic_extensions/","title":"fenic.api.dataframe.semantic_extensions","text":""},{"location":"reference/fenic/api/dataframe/semantic_extensions/#fenic.api.dataframe.semantic_extensions","title":"fenic.api.dataframe.semantic_extensions","text":"<p>Semantic extensions for DataFrames providing clustering and semantic join operations.</p> <p>Classes:</p> <ul> <li> <code>SemGroupedData</code>           \u2013            <p>Methods for aggregations on a semantically clustered DataFrame.</p> </li> <li> <code>SemanticExtensions</code>           \u2013            <p>A namespace for semantic dataframe operators.</p> </li> </ul>"},{"location":"reference/fenic/api/dataframe/semantic_extensions/#fenic.api.dataframe.semantic_extensions.SemGroupedData","title":"SemGroupedData","text":"<pre><code>SemGroupedData(df: DataFrame, by: ColumnOrName, num_clusters: int)\n</code></pre> <p>               Bases: <code>BaseGroupedData</code></p> <p>Methods for aggregations on a semantically clustered DataFrame.</p> <p>Initialize semantic grouped data.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to group.</p> </li> <li> <code>by</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column containing embeddings to cluster.</p> </li> <li> <code>num_clusters</code>               (<code>int</code>)           \u2013            <p>Number of semantic clusters to create.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>agg</code>             \u2013              <p>Compute aggregations on semantically clustered data and return the result as a DataFrame.</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def __init__(self, df: DataFrame, by: ColumnOrName, num_clusters: int):\n    \"\"\"Initialize semantic grouped data.\n\n    Args:\n        df: The DataFrame to group.\n        by: Column containing embeddings to cluster.\n        num_clusters: Number of semantic clusters to create.\n    \"\"\"\n    super().__init__(df)\n    if not isinstance(num_clusters, int) or num_clusters &lt;= 0:\n        raise ValidationError(\n            \"`num_clusters` must be a positive integer greater than 0.\"\n        )\n    if not isinstance(by, ColumnOrName):\n        raise ValidationError(\n            f\"Invalid group by: expected a column name (str) or Column object, but got {type(by).__name__}.\"\n        )\n\n    self._num_clusters = num_clusters\n    self._by_expr = Column._from_col_or_name(by)._logical_expr\n\n    if isinstance(self._by_expr, LiteralExpr):\n        raise ValidationError(\n            f\"Invalid group by: Cannot group by a literal value: {self._by_expr}. Group by a column name or a valid expression instead.\"\n        )\n\n    if not isinstance(self._by_expr.to_column_field(self._df._logical_plan).data_type, EmbeddingType):\n        raise TypeMismatchError.from_message(\n            f\"semantic.group_by grouping expression must be an embedding column type (EmbeddingType); \"\n            f\"got: {self._by_expr.to_column_field(self._df._logical_plan).data_type}\"\n        )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/semantic_extensions/#fenic.api.dataframe.semantic_extensions.SemGroupedData.agg","title":"agg","text":"<pre><code>agg(*exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame\n</code></pre> <p>Compute aggregations on semantically clustered data and return the result as a DataFrame.</p> <p>This method applies aggregate functions to data that has been grouped by semantic similarity, allowing you to discover patterns and insights across natural language clusters.</p> <p>Parameters:</p> <ul> <li> <code>*exprs</code>               (<code>Union[Column, Dict[str, str]]</code>, default:                   <code>()</code> )           \u2013            <p>Aggregation expressions. Can be:</p> <ul> <li>Column expressions with aggregate functions (e.g., <code>count(\"*\")</code>, <code>avg(\"sentiment\")</code>)</li> <li>A dictionary mapping column names to aggregate function names (e.g., {\"sentiment\": \"avg\", \"count\": \"sum\"})</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame with one row per semantic cluster and columns for aggregated values</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If arguments are not Column expressions or a dictionary</p> </li> <li> <code>ValueError</code>             \u2013            <p>If dictionary values are not valid aggregate function names</p> </li> </ul> Count items per cluster <pre><code># Group customer feedback into 5 clusters and count items per cluster\ndf.semantic.group_by(\"feedback_embeddings\", 5).agg(count(\"*\").alias(\"feedback_count\"))\n</code></pre> Analyze multiple metrics across clusters <pre><code># Analyze multiple metrics across semantic clusters\ndf.semantic.group_by(\"product_review_embeddings\", 3).agg(\n    count(\"*\").alias(\"review_count\"),\n    avg(\"rating\").alias(\"avg_rating\"),\n    avg(\"sentiment_score\").alias(\"avg_sentiment\")\n)\n</code></pre> Dictionary style aggregations <pre><code># Dictionary style for simple aggregations\ndf.semantic.group_by(\"support_ticket_embeddings\", 4).agg({\"priority\": \"avg\", \"resolution_time\": \"max\"})\n</code></pre> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def agg(self, *exprs: Union[Column, Dict[str, str]]) -&gt; DataFrame:\n    \"\"\"Compute aggregations on semantically clustered data and return the result as a DataFrame.\n\n    This method applies aggregate functions to data that has been grouped by semantic similarity,\n    allowing you to discover patterns and insights across natural language clusters.\n\n    Args:\n        *exprs: Aggregation expressions. Can be:\n\n            - Column expressions with aggregate functions (e.g., `count(\"*\")`, `avg(\"sentiment\")`)\n            - A dictionary mapping column names to aggregate function names (e.g., {\"sentiment\": \"avg\", \"count\": \"sum\"})\n\n    Returns:\n        DataFrame: A new DataFrame with one row per semantic cluster and columns for aggregated values\n\n    Raises:\n        ValueError: If arguments are not Column expressions or a dictionary\n        ValueError: If dictionary values are not valid aggregate function names\n\n    Example: Count items per cluster\n        ```python\n        # Group customer feedback into 5 clusters and count items per cluster\n        df.semantic.group_by(\"feedback_embeddings\", 5).agg(count(\"*\").alias(\"feedback_count\"))\n        ```\n\n    Example: Analyze multiple metrics across clusters\n        ```python\n        # Analyze multiple metrics across semantic clusters\n        df.semantic.group_by(\"product_review_embeddings\", 3).agg(\n            count(\"*\").alias(\"review_count\"),\n            avg(\"rating\").alias(\"avg_rating\"),\n            avg(\"sentiment_score\").alias(\"avg_sentiment\")\n        )\n        ```\n\n    Example: Dictionary style aggregations\n        ```python\n        # Dictionary style for simple aggregations\n        df.semantic.group_by(\"support_ticket_embeddings\", 4).agg({\"priority\": \"avg\", \"resolution_time\": \"max\"})\n        ```\n    \"\"\"\n    self._validate_agg_exprs(*exprs)\n    if len(exprs) == 1 and isinstance(exprs[0], dict):\n        return self.agg(*self._process_agg_dict(exprs[0]))\n    agg_exprs = self._process_agg_exprs(exprs)\n    return self._df._from_logical_plan(\n        SemanticAggregate(\n            self._df._logical_plan, self._by_expr, agg_exprs, self._num_clusters\n        ),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/semantic_extensions/#fenic.api.dataframe.semantic_extensions.SemanticExtensions","title":"SemanticExtensions","text":"<pre><code>SemanticExtensions(df: DataFrame)\n</code></pre> <p>A namespace for semantic dataframe operators.</p> <p>Initialize semantic extensions.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to extend with semantic operations.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>group_by</code>             \u2013              <p>Semantically group rows by clustering an embedding column into the specified number of centroids.</p> </li> <li> <code>join</code>             \u2013              <p>Performs a semantic join between two DataFrames using a natural language predicate.</p> </li> <li> <code>sim_join</code>             \u2013              <p>Performs a semantic similarity join between two DataFrames using precomputed text embeddings.</p> </li> </ul> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def __init__(self, df: DataFrame):\n    \"\"\"Initialize semantic extensions.\n\n    Args:\n        df: The DataFrame to extend with semantic operations.\n    \"\"\"\n    self._df = df\n</code></pre>"},{"location":"reference/fenic/api/dataframe/semantic_extensions/#fenic.api.dataframe.semantic_extensions.SemanticExtensions.group_by","title":"group_by","text":"<pre><code>group_by(by: ColumnOrName, num_clusters: int) -&gt; SemGroupedData\n</code></pre> <p>Semantically group rows by clustering an embedding column into the specified number of centroids.</p> <p>This method is useful when you want to uncover natural themes, topics, or intent in embedded free-form text, without needing predefined categories.</p> <p>Parameters:</p> <ul> <li> <code>by</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column containing embeddings to cluster</p> </li> <li> <code>num_clusters</code>               (<code>int</code>)           \u2013            <p>Number of semantic clusters to create</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SemGroupedData</code> (              <code>SemGroupedData</code> )          \u2013            <p>Object for performing aggregations on the clustered data.</p> </li> </ul> Basic semantic grouping <pre><code># Group customer feedback into 5 clusters\ndf.semantic.group_by(\"feedback_embeddings\", 5).agg(count(\"*\"))\n</code></pre> Analyze sentiment by semantic group <pre><code># Analyze sentiment by semantic group\ndf.semantic.group_by(\"feedback_embeddings\", 5).agg(\n    count(\"*\").alias(\"count\"),\n    avg(\"sentiment_score\").alias(\"avg_sentiment\")\n)\n</code></pre> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def group_by(self, by: ColumnOrName, num_clusters: int) -&gt; SemGroupedData:\n    \"\"\"Semantically group rows by clustering an embedding column into the specified number of centroids.\n\n    This method is useful when you want to uncover natural themes, topics, or intent in embedded free-form text,\n    without needing predefined categories.\n\n    Args:\n        by: Column containing embeddings to cluster\n        num_clusters: Number of semantic clusters to create\n\n    Returns:\n        SemGroupedData: Object for performing aggregations on the clustered data.\n\n    Example: Basic semantic grouping\n        ```python\n        # Group customer feedback into 5 clusters\n        df.semantic.group_by(\"feedback_embeddings\", 5).agg(count(\"*\"))\n        ```\n\n    Example: Analyze sentiment by semantic group\n        ```python\n        # Analyze sentiment by semantic group\n        df.semantic.group_by(\"feedback_embeddings\", 5).agg(\n            count(\"*\").alias(\"count\"),\n            avg(\"sentiment_score\").alias(\"avg_sentiment\")\n        )\n        ```\n    \"\"\"\n    return SemGroupedData(self._df, by, num_clusters)\n</code></pre>"},{"location":"reference/fenic/api/dataframe/semantic_extensions/#fenic.api.dataframe.semantic_extensions.SemanticExtensions.join","title":"join","text":"<pre><code>join(other: DataFrame, join_instruction: str, examples: Optional[JoinExampleCollection] = None, model_alias: Optional[str] = None) -&gt; DataFrame\n</code></pre> <p>Performs a semantic join between two DataFrames using a natural language predicate.</p> <p>That evaluates to either true or false for each potential row pair.</p> <p>The join works by: 1. Evaluating the provided join_instruction as a boolean predicate for each possible pair of rows 2. Including ONLY the row pairs where the predicate evaluates to True in the result set 3. Excluding all row pairs where the predicate evaluates to False</p> <p>The instruction must reference exactly two columns, one from each DataFrame, using the <code>:left</code> and <code>:right</code> suffixes to indicate column origin.</p> <p>This is useful when row pairing decisions require complex reasoning based on a custom predicate rather than simple equality or similarity matching.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to join with.</p> </li> <li> <code>join_instruction</code>               (<code>str</code>)           \u2013            <p>A natural language description of how to match values.</p> <ul> <li>Must include one placeholder from the left DataFrame (e.g. <code>{resume_summary:left}</code>) and one from the right (e.g. <code>{job_description:right}</code>).</li> <li>This instruction is evaluated as a boolean predicate - pairs where it's <code>True</code> are included, pairs where it's <code>False</code> are excluded.</li> </ul> </li> <li> <code>examples</code>               (<code>Optional[JoinExampleCollection]</code>, default:                   <code>None</code> )           \u2013            <p>Optional JoinExampleCollection containing labeled pairs (<code>left</code>, <code>right</code>, <code>output</code>) to guide the semantic join behavior.</p> </li> <li> <code>model_alias</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame containing only the row pairs where the join_instruction       predicate evaluates to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If <code>other</code> is not a DataFrame or <code>join_instruction</code> is not a string.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the instruction format is invalid or references invalid columns.</p> </li> </ul> Basic semantic join <pre><code># Match job listings with candidate resumes based on title/skills\n# Only includes pairs where the predicate evaluates to True\ndf_jobs.semantic.join(df_resumes,\n    join_instruction=\"Given a candidate's resume_summary: {resume_summary:left} and a job description: {job_description:right}, does the candidate have the appropriate skills for the job?\"\n)\n</code></pre> Semantic join with examples <pre><code># Improve join quality with examples\nexamples = JoinExampleCollection()\nexamples.create_example(JoinExample(\n    left=\"5 years experience building backend services in Python using asyncio, FastAPI, and PostgreSQL\",\n    right=\"Senior Software Engineer - Backend\",\n    output=True))  # This pair WILL be included in similar cases\nexamples.create_example(JoinExample(\n    left=\"5 years experience with growth strategy, private equity due diligence, and M&amp;A\",\n    right=\"Product Manager - Hardware\",\n    output=False))  # This pair will NOT be included in similar cases\ndf_jobs.semantic.join(df_resumes,\n    join_instruction=\"Given a candidate's resume_summary: {resume_summary:left} and a job description: {job_description:right}, does the candidate have the appropriate skills for the job?\",\n    examples=examples)\n</code></pre> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def join(\n    self,\n    other: DataFrame,\n    join_instruction: str,\n    examples: Optional[JoinExampleCollection] = None,\n    model_alias: Optional[str] = None,\n) -&gt; DataFrame:\n    \"\"\"Performs a semantic join between two DataFrames using a natural language predicate.\n\n    That evaluates to either true or false for each potential row pair.\n\n    The join works by:\n    1. Evaluating the provided join_instruction as a boolean predicate for each possible pair of rows\n    2. Including ONLY the row pairs where the predicate evaluates to True in the result set\n    3. Excluding all row pairs where the predicate evaluates to False\n\n    The instruction must reference **exactly two columns**, one from each DataFrame,\n    using the `:left` and `:right` suffixes to indicate column origin.\n\n    This is useful when row pairing decisions require complex reasoning based on a custom predicate rather than simple equality or similarity matching.\n\n    Args:\n        other: The DataFrame to join with.\n        join_instruction: A natural language description of how to match values.\n\n            - Must include one placeholder from the left DataFrame (e.g. `{resume_summary:left}`)\n            and one from the right (e.g. `{job_description:right}`).\n            - This instruction is evaluated as a boolean predicate - pairs where it's `True` are included,\n            pairs where it's `False` are excluded.\n        examples: Optional JoinExampleCollection containing labeled pairs (`left`, `right`, `output`)\n            to guide the semantic join behavior.\n        model_alias: Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.\n\n    Returns:\n        DataFrame: A new DataFrame containing only the row pairs where the join_instruction\n                  predicate evaluates to True.\n\n    Raises:\n        TypeError: If `other` is not a DataFrame or `join_instruction` is not a string.\n        ValueError: If the instruction format is invalid or references invalid columns.\n\n    Example: Basic semantic join\n        ```python\n        # Match job listings with candidate resumes based on title/skills\n        # Only includes pairs where the predicate evaluates to True\n        df_jobs.semantic.join(df_resumes,\n            join_instruction=\"Given a candidate's resume_summary: {resume_summary:left} and a job description: {job_description:right}, does the candidate have the appropriate skills for the job?\"\n        )\n        ```\n\n    Example: Semantic join with examples\n        ```python\n        # Improve join quality with examples\n        examples = JoinExampleCollection()\n        examples.create_example(JoinExample(\n            left=\"5 years experience building backend services in Python using asyncio, FastAPI, and PostgreSQL\",\n            right=\"Senior Software Engineer - Backend\",\n            output=True))  # This pair WILL be included in similar cases\n        examples.create_example(JoinExample(\n            left=\"5 years experience with growth strategy, private equity due diligence, and M&amp;A\",\n            right=\"Product Manager - Hardware\",\n            output=False))  # This pair will NOT be included in similar cases\n        df_jobs.semantic.join(df_resumes,\n            join_instruction=\"Given a candidate's resume_summary: {resume_summary:left} and a job description: {job_description:right}, does the candidate have the appropriate skills for the job?\",\n            examples=examples)\n        ```\n    \"\"\"\n    from fenic.api.dataframe.dataframe import DataFrame\n\n    if not isinstance(other, DataFrame):\n        raise TypeError(f\"other argument must be a DataFrame, got {type(other)}\")\n\n    if not isinstance(join_instruction, str):\n        raise TypeError(\n            f\"join_instruction argument must be a string, got {type(join_instruction)}\"\n        )\n    join_columns = utils.parse_instruction(join_instruction)\n    if len(join_columns) != 2:\n        raise ValueError(\n            f\"join_instruction must contain exactly two columns, got {len(join_columns)}\"\n        )\n    left_on = None\n    right_on = None\n    for join_col in join_columns:\n        if join_col.endswith(\":left\"):\n            if left_on is not None:\n                raise ValueError(\n                    \"join_instruction cannot contain multiple :left columns\"\n                )\n            left_on = col(join_col.split(\":\")[0])\n        elif join_col.endswith(\":right\"):\n            if right_on is not None:\n                raise ValueError(\n                    \"join_instruction cannot contain multiple :right columns\"\n                )\n            right_on = col(join_col.split(\":\")[0])\n        else:\n            raise ValueError(\n                f\"Column '{join_col}' must end with either :left or :right\"\n            )\n\n    if left_on is None or right_on is None:\n        raise ValueError(\n            \"join_instruction must contain exactly one :left and one :right column\"\n        )\n\n    return self._df._from_logical_plan(\n        SemanticJoin(\n            left=self._df._logical_plan,\n            right=other._logical_plan,\n            left_on=left_on._logical_expr,\n            right_on=right_on._logical_expr,\n            join_instruction=join_instruction,\n            examples=examples,\n            model_alias=model_alias,\n        ),\n    )\n</code></pre>"},{"location":"reference/fenic/api/dataframe/semantic_extensions/#fenic.api.dataframe.semantic_extensions.SemanticExtensions.sim_join","title":"sim_join","text":"<pre><code>sim_join(other: DataFrame, left_on: ColumnOrName, right_on: ColumnOrName, k: int = 1, similarity_metric: SemanticSimilarityMetric = 'cosine', return_similarity_scores: bool = False) -&gt; DataFrame\n</code></pre> <p>Performs a semantic similarity join between two DataFrames using precomputed text embeddings.</p> <p>For each row in the left DataFrame, finds the top <code>k</code> most semantically similar rows in the right DataFrame based on the cosine similarity between their text embeddings. This is useful for fuzzy matching tasks when exact matches aren't possible.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>DataFrame</code>)           \u2013            <p>The right-hand DataFrame to join with.</p> </li> <li> <code>left_on</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column in this DataFrame containing text embeddings to compare.</p> </li> <li> <code>right_on</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column in the other DataFrame containing text embeddings to compare.</p> </li> <li> <code>k</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of most similar matches to return per row from the left DataFrame.</p> </li> <li> <code>similarity_metric</code>               (<code>SemanticSimilarityMetric</code>, default:                   <code>'cosine'</code> )           \u2013            <p>The metric to use for calculating distances between vectors. Supported distance metrics: \"l2\", \"cosine\", \"dot\"</p> </li> <li> <code>return_similarity_scores</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, include a <code>_similarity_score</code> column in the output DataFrame                     representing the match confidence (cosine similarity).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>A new DataFrame containing matched rows from both sides and optionally similarity scores.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If argument types are incorrect.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If <code>k</code> is not positive or if the columns are invalid.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If <code>similarity_metric</code> is not one of \"l2\", \"cosine\", \"dot\"</p> </li> </ul> Match queries to FAQ entries <pre><code># Match customer queries to FAQ entries\ndf_queries.semantic.sim_join(\n    df_faqs,\n    left_on=embeddings(col(\"query_text\")),\n    right_on=embeddings(col(\"faq_question\")),\n    k=1\n)\n</code></pre> Link headlines to articles <pre><code># Link news headlines to full articles\ndf_headlines.semantic.sim_join(\n    df_articles,\n    left_on=embeddings(col(\"headline\")),\n    right_on=embeddings(col(\"content\")),\n    k=3,\n    return_similarity_scores=True\n)\n</code></pre> Find similar job postings <pre><code># Find similar job postings across two sources\ndf_linkedin.semantic.sim_join(\n    df_indeed,\n    left_on=embeddings(col(\"job_title\")),\n    right_on=embeddings(col(\"job_description\")),\n    k=2\n)\n</code></pre> Source code in <code>src/fenic/api/dataframe/semantic_extensions.py</code> <pre><code>def sim_join(\n    self,\n    other: DataFrame,\n    left_on: ColumnOrName,\n    right_on: ColumnOrName,\n    k: int = 1,\n    similarity_metric: SemanticSimilarityMetric = \"cosine\",\n    return_similarity_scores: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Performs a semantic similarity join between two DataFrames using precomputed text embeddings.\n\n    For each row in the left DataFrame, finds the top `k` most semantically similar rows in the right DataFrame\n    based on the cosine similarity between their text embeddings. This is useful for fuzzy matching tasks when exact matches aren't possible.\n\n    Args:\n        other: The right-hand DataFrame to join with.\n        left_on: Column in this DataFrame containing text embeddings to compare.\n        right_on: Column in the other DataFrame containing text embeddings to compare.\n        k: Number of most similar matches to return per row from the left DataFrame.\n        similarity_metric: The metric to use for calculating distances between vectors.\n            Supported distance metrics: \"l2\", \"cosine\", \"dot\"\n        return_similarity_scores: If True, include a `_similarity_score` column in the output DataFrame\n                                representing the match confidence (cosine similarity).\n\n    Returns:\n        DataFrame: A new DataFrame containing matched rows from both sides and optionally similarity scores.\n\n    Raises:\n        TypeError: If argument types are incorrect.\n        ValueError: If `k` is not positive or if the columns are invalid.\n        ValueError: If `similarity_metric` is not one of \"l2\", \"cosine\", \"dot\"\n\n    Example: Match queries to FAQ entries\n        ```python\n        # Match customer queries to FAQ entries\n        df_queries.semantic.sim_join(\n            df_faqs,\n            left_on=embeddings(col(\"query_text\")),\n            right_on=embeddings(col(\"faq_question\")),\n            k=1\n        )\n        ```\n\n    Example: Link headlines to articles\n        ```python\n        # Link news headlines to full articles\n        df_headlines.semantic.sim_join(\n            df_articles,\n            left_on=embeddings(col(\"headline\")),\n            right_on=embeddings(col(\"content\")),\n            k=3,\n            return_similarity_scores=True\n        )\n        ```\n\n    Example: Find similar job postings\n        ```python\n        # Find similar job postings across two sources\n        df_linkedin.semantic.sim_join(\n            df_indeed,\n            left_on=embeddings(col(\"job_title\")),\n            right_on=embeddings(col(\"job_description\")),\n            k=2\n        )\n        ```\n    \"\"\"\n    from fenic.api.dataframe.dataframe import DataFrame\n\n    if not isinstance(right_on, ColumnOrName):\n        raise ValidationError(\n            f\"The `right_on` argument must be a `Column` or a string representing a column name, \"\n            f\"but got `{type(right_on).__name__}` instead.\"\n        )\n    if not isinstance(other, DataFrame):\n        raise ValidationError(\n                        f\"The `other` argument to `sim_join()` must be a DataFrame`, but got `{type(other).__name__}`.\"\n                    )\n    if not (isinstance(k, int) and k &gt; 0):\n        raise ValidationError(\n            f\"The parameter `k` must be a positive integer, but received `{k}`.\"\n        )\n    args = get_args(SemanticSimilarityMetric)\n    if similarity_metric not in args:\n        raise ValidationError(\n            f\"The `similarity_metric` argument must be one of {args}, but got `{similarity_metric}`.\"\n        )\n\n    def _validate_column(column: ColumnOrName, name: str):\n        if column is None:\n            raise ValidationError(f\"The `{name}` argument must not be None.\")\n        if not isinstance(column, ColumnOrName):\n            raise ValidationError(\n                f\"The `{name}` argument must be a `Column` or a string representing a column name, \"\n                f\"but got `{type(column).__name__}` instead.\"\n            )\n\n    _validate_column(left_on, \"left_on\")\n    _validate_column(right_on, \"right_on\")\n\n    return self._df._from_logical_plan(\n        SemanticSimilarityJoin(\n            self._df._logical_plan,\n            other._logical_plan,\n            Column._from_col_or_name(left_on)._logical_expr,\n            Column._from_col_or_name(right_on)._logical_expr,\n            k,\n            similarity_metric,\n            return_similarity_scores,\n        ),\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/","title":"fenic.api.functions","text":""},{"location":"reference/fenic/api/functions/#fenic.api.functions","title":"fenic.api.functions","text":"<p>Functions for working with DataFrame columns.</p> <p>Functions:</p> <ul> <li> <code>array</code>             \u2013              <p>Creates a new array column from multiple input columns.</p> </li> <li> <code>array_agg</code>             \u2013              <p>Alias for collect_list().</p> </li> <li> <code>array_contains</code>             \u2013              <p>Checks if array column contains a specific value.</p> </li> <li> <code>array_size</code>             \u2013              <p>Returns the number of elements in an array column.</p> </li> <li> <code>asc</code>             \u2013              <p>Creates a Column expression representing an ascending sort order.</p> </li> <li> <code>asc_nulls_first</code>             \u2013              <p>Creates a Column expression representing an ascending sort order with nulls first.</p> </li> <li> <code>asc_nulls_last</code>             \u2013              <p>Creates a Column expression representing an ascending sort order with nulls last.</p> </li> <li> <code>avg</code>             \u2013              <p>Aggregate function: returns the average (mean) of all values in the specified column.</p> </li> <li> <code>coalesce</code>             \u2013              <p>Returns the first non-null value from the given columns for each row.</p> </li> <li> <code>col</code>             \u2013              <p>Creates a Column expression referencing a column in the DataFrame.</p> </li> <li> <code>collect_list</code>             \u2013              <p>Aggregate function: collects all values from the specified column into a list.</p> </li> <li> <code>count</code>             \u2013              <p>Aggregate function: returns the count of non-null values in the specified column.</p> </li> <li> <code>desc</code>             \u2013              <p>Creates a Column expression representing a descending sort order.</p> </li> <li> <code>desc_nulls_first</code>             \u2013              <p>Creates a Column expression representing a descending sort order with nulls first.</p> </li> <li> <code>desc_nulls_last</code>             \u2013              <p>Creates a Column expression representing a descending sort order with nulls last.</p> </li> <li> <code>lit</code>             \u2013              <p>Creates a Column expression representing a literal value.</p> </li> <li> <code>max</code>             \u2013              <p>Aggregate function: returns the maximum value in the specified column.</p> </li> <li> <code>mean</code>             \u2013              <p>Aggregate function: returns the mean (average) of all values in the specified column.</p> </li> <li> <code>min</code>             \u2013              <p>Aggregate function: returns the minimum value in the specified column.</p> </li> <li> <code>struct</code>             \u2013              <p>Creates a new struct column from multiple input columns.</p> </li> <li> <code>sum</code>             \u2013              <p>Aggregate function: returns the sum of all values in the specified column.</p> </li> <li> <code>udf</code>             \u2013              <p>A decorator or function for creating user-defined functions (UDFs) that can be applied to DataFrame rows.</p> </li> <li> <code>when</code>             \u2013              <p>Evaluates a condition and returns a value if true.</p> </li> </ul>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.array","title":"array","text":"<pre><code>array(*args: Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]) -&gt; Column\n</code></pre> <p>Creates a new array column from multiple input columns.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]</code>, default:                   <code>()</code> )           \u2013            <p>Columns or column names to combine into an array. Can be:</p> <ul> <li>Individual arguments</li> <li>Lists of columns/column names</li> <li>Tuples of columns/column names</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing an array containing values from the input columns</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If any argument is not a Column, string, or collection of Columns/strings</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef array(\n    *args: Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]\n) -&gt; Column:\n    \"\"\"Creates a new array column from multiple input columns.\n\n    Args:\n        *args: Columns or column names to combine into an array. Can be:\n\n            - Individual arguments\n            - Lists of columns/column names\n            - Tuples of columns/column names\n\n    Returns:\n        A Column expression representing an array containing values from the input columns\n\n    Raises:\n        TypeError: If any argument is not a Column, string, or collection of\n            Columns/strings\n    \"\"\"\n    flattened_args = []\n    for arg in args:\n        if isinstance(arg, (list, tuple)):\n            flattened_args.extend(arg)\n        else:\n            flattened_args.append(arg)\n\n    expr_columns = [Column._from_col_or_name(c)._logical_expr for c in flattened_args]\n\n    return Column._from_logical_expr(ArrayExpr(expr_columns))\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.array_agg","title":"array_agg","text":"<pre><code>array_agg(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Alias for collect_list().</p> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef array_agg(column: ColumnOrName) -&gt; Column:\n    \"\"\"Alias for collect_list().\"\"\"\n    return collect_list(column)\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.array_contains","title":"array_contains","text":"<pre><code>array_contains(column: ColumnOrName, value: Union[str, int, float, bool, Column]) -&gt; Column\n</code></pre> <p>Checks if array column contains a specific value.</p> <p>This function returns True if the array in the specified column contains the given value, and False otherwise. Returns False if the array is None.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name containing the arrays to check.</p> </li> <li> <code>value</code>               (<code>Union[str, int, float, bool, Column]</code>)           \u2013            <p>Value to search for in the arrays. Can be: - A literal value (string, number, boolean) - A Column expression</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A boolean Column expression (True if value is found, False otherwise).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If value type is incompatible with the array element type.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If the column does not contain array data.</p> </li> </ul> Check for values in arrays <pre><code># Check if 'python' exists in arrays in the 'tags' column\ndf.select(array_contains(\"tags\", \"python\"))\n\n# Check using a value from another column\ndf.select(array_contains(\"tags\", col(\"search_term\")))\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef array_contains(\n    column: ColumnOrName, value: Union[str, int, float, bool, Column]\n) -&gt; Column:\n    \"\"\"Checks if array column contains a specific value.\n\n    This function returns True if the array in the specified column contains the given value,\n    and False otherwise. Returns False if the array is None.\n\n    Args:\n        column: Column or column name containing the arrays to check.\n\n        value: Value to search for in the arrays. Can be:\n            - A literal value (string, number, boolean)\n            - A Column expression\n\n    Returns:\n        A boolean Column expression (True if value is found, False otherwise).\n\n    Raises:\n        TypeError: If value type is incompatible with the array element type.\n        TypeError: If the column does not contain array data.\n\n    Example: Check for values in arrays\n        ```python\n        # Check if 'python' exists in arrays in the 'tags' column\n        df.select(array_contains(\"tags\", \"python\"))\n\n        # Check using a value from another column\n        df.select(array_contains(\"tags\", col(\"search_term\")))\n        ```\n    \"\"\"\n    value_column = None\n    if isinstance(value, Column):\n        value_column = value\n    else:\n        value_column = lit(value)\n    return Column._from_logical_expr(\n        ArrayContainsExpr(\n            Column._from_col_or_name(column)._logical_expr, value_column._logical_expr\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.array_size","title":"array_size","text":"<pre><code>array_size(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Returns the number of elements in an array column.</p> <p>This function computes the length of arrays stored in the specified column. Returns None for None arrays.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name containing arrays whose length to compute.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the array length.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the column does not contain array data.</p> </li> </ul> Get array sizes <pre><code># Get the size of arrays in 'tags' column\ndf.select(array_size(\"tags\"))\n\n# Use with column reference\ndf.select(array_size(col(\"tags\")))\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef array_size(column: ColumnOrName) -&gt; Column:\n    \"\"\"Returns the number of elements in an array column.\n\n    This function computes the length of arrays stored in the specified column.\n    Returns None for None arrays.\n\n    Args:\n        column: Column or column name containing arrays whose length to compute.\n\n    Returns:\n        A Column expression representing the array length.\n\n    Raises:\n        TypeError: If the column does not contain array data.\n\n    Example: Get array sizes\n        ```python\n        # Get the size of arrays in 'tags' column\n        df.select(array_size(\"tags\"))\n\n        # Use with column reference\n        df.select(array_size(col(\"tags\")))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        ArrayLengthExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.asc","title":"asc","text":"<pre><code>asc(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing an ascending sort order.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the ascending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the ascending sort order.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef asc(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing an ascending sort order.\n\n    Args:\n        column: The column to apply the ascending ordering to.\n\n    Returns:\n        A Column expression representing the column and the ascending sort order.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).asc()\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.asc_nulls_first","title":"asc_nulls_first","text":"<pre><code>asc_nulls_first(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing an ascending sort order with nulls first.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the ascending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the ascending sort order with nulls first.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef asc_nulls_first(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing an ascending sort order with nulls first.\n\n    Args:\n        column: The column to apply the ascending ordering to.\n\n    Returns:\n        A Column expression representing the column and the ascending sort order with nulls first.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).asc_nulls_first()\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.asc_nulls_last","title":"asc_nulls_last","text":"<pre><code>asc_nulls_last(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing an ascending sort order with nulls last.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the ascending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the ascending sort order with nulls last.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef asc_nulls_last(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing an ascending sort order with nulls last.\n\n    Args:\n        column: The column to apply the ascending ordering to.\n\n    Returns:\n        A Column expression representing the column and the ascending sort order with nulls last.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).asc_nulls_last()\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.avg","title":"avg","text":"<pre><code>avg(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the average (mean) of all values in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the average of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the average aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef avg(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the average (mean) of all values in the specified column.\n\n    Args:\n        column: Column or column name to compute the average of\n\n    Returns:\n        A Column expression representing the average aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        AvgExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.coalesce","title":"coalesce","text":"<pre><code>coalesce(*cols: ColumnOrName) -&gt; Column\n</code></pre> <p>Returns the first non-null value from the given columns for each row.</p> <p>This function mimics the behavior of SQL's COALESCE function. It evaluates the input columns in order and returns the first non-null value encountered. If all values are null, returns null.</p> <p>Parameters:</p> <ul> <li> <code>*cols</code>               (<code>ColumnOrName</code>, default:                   <code>()</code> )           \u2013            <p>Column expressions or column names to evaluate. Can be:</p> <ul> <li>Individual arguments</li> <li>Lists of columns/column names</li> <li>Tuples of columns/column names</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression containing the first non-null value from the input columns.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no columns are provided.</p> </li> </ul> Basic coalesce usage <pre><code># Basic usage\ndf.select(coalesce(\"col1\", \"col2\", \"col3\"))\n\n# With nested collections\ndf.select(coalesce([\"col1\", \"col2\"], \"col3\"))\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef coalesce(*cols: ColumnOrName) -&gt; Column:\n    \"\"\"Returns the first non-null value from the given columns for each row.\n\n    This function mimics the behavior of SQL's COALESCE function. It evaluates the input columns\n    in order and returns the first non-null value encountered. If all values are null, returns null.\n\n    Args:\n        *cols: Column expressions or column names to evaluate. Can be:\n\n            - Individual arguments\n            - Lists of columns/column names\n            - Tuples of columns/column names\n\n    Returns:\n        A Column expression containing the first non-null value from the input columns.\n\n    Raises:\n        ValueError: If no columns are provided.\n\n    Example: Basic coalesce usage\n        ```python\n        # Basic usage\n        df.select(coalesce(\"col1\", \"col2\", \"col3\"))\n\n        # With nested collections\n        df.select(coalesce([\"col1\", \"col2\"], \"col3\"))\n        ```\n    \"\"\"\n    if not cols:\n        raise ValueError(\"At least one column must be provided to coalesce method\")\n\n    flattened_args = []\n    for arg in cols:\n        if isinstance(arg, (list, tuple)):\n            flattened_args.extend(arg)\n        else:\n            flattened_args.append(arg)\n\n    flattened_exprs = [\n        Column._from_col_or_name(c)._logical_expr for c in flattened_args\n    ]\n    return Column._from_logical_expr(CoalesceExpr(flattened_exprs))\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.col","title":"col","text":"<pre><code>col(col_name: str) -&gt; Column\n</code></pre> <p>Creates a Column expression referencing a column in the DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>col_name</code>               (<code>str</code>)           \u2013            <p>Name of the column to reference</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression for the specified column</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If colName is not a string</p> </li> </ul> Source code in <code>src/fenic/api/functions/core.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef col(col_name: str) -&gt; Column:\n    \"\"\"Creates a Column expression referencing a column in the DataFrame.\n\n    Args:\n        col_name: Name of the column to reference\n\n    Returns:\n        A Column expression for the specified column\n\n    Raises:\n        TypeError: If colName is not a string\n    \"\"\"\n    return Column._from_column_name(col_name)\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.collect_list","title":"collect_list","text":"<pre><code>collect_list(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: collects all values from the specified column into a list.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to collect values from</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the list aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef collect_list(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: collects all values from the specified column into a list.\n\n    Args:\n        column: Column or column name to collect values from\n\n    Returns:\n        A Column expression representing the list aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        ListExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.count","title":"count","text":"<pre><code>count(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the count of non-null values in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to count values in</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the count aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef count(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the count of non-null values in the specified column.\n\n    Args:\n        column: Column or column name to count values in\n\n    Returns:\n        A Column expression representing the count aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    if isinstance(column, str) and column == \"*\":\n        return Column._from_logical_expr(CountExpr(lit(\"*\")._logical_expr))\n    return Column._from_logical_expr(\n        CountExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.desc","title":"desc","text":"<pre><code>desc(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing a descending sort order.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the descending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the descending sort order.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef desc(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing a descending sort order.\n\n    Args:\n        column: The column to apply the descending ordering to.\n\n    Returns:\n        A Column expression representing the column and the descending sort order.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).desc()\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.desc_nulls_first","title":"desc_nulls_first","text":"<pre><code>desc_nulls_first(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing a descending sort order with nulls first.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the descending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the descending sort order with nulls first.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef desc_nulls_first(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing a descending sort order with nulls first.\n\n    Args:\n        column: The column to apply the descending ordering to.\n\n    Returns:\n        A Column expression representing the column and the descending sort order with nulls first.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).desc_nulls_first()\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.desc_nulls_last","title":"desc_nulls_last","text":"<pre><code>desc_nulls_last(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing a descending sort order with nulls last.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the descending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the descending sort order with nulls last.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef desc_nulls_last(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing a descending sort order with nulls last.\n\n    Args:\n        column: The column to apply the descending ordering to.\n\n    Returns:\n        A Column expression representing the column and the descending sort order with nulls last.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).desc_nulls_last()\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.lit","title":"lit","text":"<pre><code>lit(value: Any) -&gt; Column\n</code></pre> <p>Creates a Column expression representing a literal value.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>The literal value to create a column for</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the literal value</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the value cannot be inferred</p> </li> </ul> Source code in <code>src/fenic/api/functions/core.py</code> <pre><code>def lit(value: Any) -&gt; Column:\n    \"\"\"Creates a Column expression representing a literal value.\n\n    Args:\n        value: The literal value to create a column for\n\n    Returns:\n        A Column expression representing the literal value\n\n    Raises:\n        ValueError: If the type of the value cannot be inferred\n    \"\"\"\n    try:\n        inferred_type = infer_dtype_from_pyobj(value)\n    except TypeInferenceError as e:\n        raise ValidationError(f\"`lit` failed to infer type for value `{value}`\") from e\n    literal_expr = LiteralExpr(value, inferred_type)\n    return Column._from_logical_expr(literal_expr)\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.max","title":"max","text":"<pre><code>max(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the maximum value in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the maximum of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the maximum aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef max(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the maximum value in the specified column.\n\n    Args:\n        column: Column or column name to compute the maximum of\n\n    Returns:\n        A Column expression representing the maximum aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        MaxExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.mean","title":"mean","text":"<pre><code>mean(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the mean (average) of all values in the specified column.</p> <p>Alias for avg().</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the mean of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the mean aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef mean(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the mean (average) of all values in the specified column.\n\n    Alias for avg().\n\n    Args:\n        column: Column or column name to compute the mean of\n\n    Returns:\n        A Column expression representing the mean aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        AvgExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.min","title":"min","text":"<pre><code>min(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the minimum value in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the minimum of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the minimum aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef min(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the minimum value in the specified column.\n\n    Args:\n        column: Column or column name to compute the minimum of\n\n    Returns:\n        A Column expression representing the minimum aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        MinExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.struct","title":"struct","text":"<pre><code>struct(*args: Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]) -&gt; Column\n</code></pre> <p>Creates a new struct column from multiple input columns.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]</code>, default:                   <code>()</code> )           \u2013            <p>Columns or column names to combine into a struct. Can be:</p> <ul> <li>Individual arguments</li> <li>Lists of columns/column names</li> <li>Tuples of columns/column names</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing a struct containing the input columns</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If any argument is not a Column, string, or collection of Columns/strings</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef struct(\n    *args: Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]\n) -&gt; Column:\n    \"\"\"Creates a new struct column from multiple input columns.\n\n    Args:\n        *args: Columns or column names to combine into a struct. Can be:\n\n            - Individual arguments\n            - Lists of columns/column names\n            - Tuples of columns/column names\n\n    Returns:\n        A Column expression representing a struct containing the input columns\n\n    Raises:\n        TypeError: If any argument is not a Column, string, or collection of\n            Columns/strings\n    \"\"\"\n    flattened_args = []\n    for arg in args:\n        if isinstance(arg, (list, tuple)):\n            flattened_args.extend(arg)\n        else:\n            flattened_args.append(arg)\n\n    expr_columns = [Column._from_col_or_name(c)._logical_expr for c in flattened_args]\n\n    return Column._from_logical_expr(StructExpr(expr_columns))\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.sum","title":"sum","text":"<pre><code>sum(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the sum of all values in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the sum of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the sum aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef sum(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the sum of all values in the specified column.\n\n    Args:\n        column: Column or column name to compute the sum of\n\n    Returns:\n        A Column expression representing the sum aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        SumExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.udf","title":"udf","text":"<pre><code>udf(f: Optional[Callable] = None, *, return_type: DataType)\n</code></pre> <p>A decorator or function for creating user-defined functions (UDFs) that can be applied to DataFrame rows.</p> <p>When applied, UDFs will: - Access <code>StructType</code> columns as Python dictionaries (<code>dict[str, Any]</code>). - Access <code>ArrayType</code> columns as Python lists (<code>list[Any]</code>). - Access primitive types (e.g., <code>int</code>, <code>float</code>, <code>str</code>) as their respective Python types.</p> <p>Parameters:</p> <ul> <li> <code>f</code>               (<code>Optional[Callable]</code>, default:                   <code>None</code> )           \u2013            <p>Python function to convert to UDF</p> </li> <li> <code>return_type</code>               (<code>DataType</code>)           \u2013            <p>Expected return type of the UDF. Required parameter.</p> </li> </ul> UDF with primitive types <pre><code># UDF with primitive types\n@udf(return_type=IntegerType)\ndef add_one(x: int):\n    return x + 1\n\n# Or\nadd_one = udf(lambda x: x + 1, return_type=IntegerType)\n</code></pre> UDF with nested types <pre><code># UDF with nested types\n@udf(return_type=StructType([StructField(\"value1\", IntegerType), StructField(\"value2\", IntegerType)]))\ndef example_udf(x: dict[str, int], y: list[int]):\n    return {\n        \"value1\": x[\"value1\"] + x[\"value2\"] + y[0],\n        \"value2\": x[\"value1\"] + x[\"value2\"] + y[1],\n    }\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef udf(f: Optional[Callable] = None, *, return_type: DataType):\n    \"\"\"A decorator or function for creating user-defined functions (UDFs) that can be applied to DataFrame rows.\n\n    When applied, UDFs will:\n    - Access `StructType` columns as Python dictionaries (`dict[str, Any]`).\n    - Access `ArrayType` columns as Python lists (`list[Any]`).\n    - Access primitive types (e.g., `int`, `float`, `str`) as their respective Python types.\n\n    Args:\n        f: Python function to convert to UDF\n\n        return_type: Expected return type of the UDF. Required parameter.\n\n    Example: UDF with primitive types\n        ```python\n        # UDF with primitive types\n        @udf(return_type=IntegerType)\n        def add_one(x: int):\n            return x + 1\n\n        # Or\n        add_one = udf(lambda x: x + 1, return_type=IntegerType)\n        ```\n\n    Example: UDF with nested types\n        ```python\n        # UDF with nested types\n        @udf(return_type=StructType([StructField(\"value1\", IntegerType), StructField(\"value2\", IntegerType)]))\n        def example_udf(x: dict[str, int], y: list[int]):\n            return {\n                \"value1\": x[\"value1\"] + x[\"value2\"] + y[0],\n                \"value2\": x[\"value1\"] + x[\"value2\"] + y[1],\n            }\n        ```\n    \"\"\"\n\n    def _create_udf(func: Callable) -&gt; Callable:\n        @wraps(func)\n        def _udf_wrapper(*cols: ColumnOrName) -&gt; Column:\n            col_exprs = [Column._from_col_or_name(c)._logical_expr for c in cols]\n            return Column._from_logical_expr(UDFExpr(func, col_exprs, return_type))\n\n        return _udf_wrapper\n\n    if f is not None:\n        return _create_udf(f)\n    return _create_udf\n</code></pre>"},{"location":"reference/fenic/api/functions/#fenic.api.functions.when","title":"when","text":"<pre><code>when(condition: Column, value: Column) -&gt; Column\n</code></pre> <p>Evaluates a condition and returns a value if true.</p> <p>This function is used to create conditional expressions. If Column.otherwise() is not invoked, None is returned for unmatched conditions.</p> <p>Parameters:</p> <ul> <li> <code>condition</code>               (<code>Column</code>)           \u2013            <p>A boolean Column expression to evaluate.</p> </li> <li> <code>value</code>               (<code>Column</code>)           \u2013            <p>A Column expression to return if the condition is true.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression that evaluates the condition and returns the specified value when true,</p> </li> <li> <code>Column</code>           \u2013            <p>and None otherwise.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the condition is not a boolean Column expression.</p> </li> </ul> Basic conditional expression <pre><code># Basic usage\ndf.select(when(col(\"age\") &gt; 18, lit(\"adult\")))\n\n# With otherwise\ndf.select(when(col(\"age\") &gt; 18, lit(\"adult\")).otherwise(lit(\"minor\")))\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef when(condition: Column, value: Column) -&gt; Column:\n    \"\"\"Evaluates a condition and returns a value if true.\n\n    This function is used to create conditional expressions. If Column.otherwise() is not invoked,\n    None is returned for unmatched conditions.\n\n    Args:\n        condition: A boolean Column expression to evaluate.\n\n        value: A Column expression to return if the condition is true.\n\n    Returns:\n        A Column expression that evaluates the condition and returns the specified value when true,\n        and None otherwise.\n\n    Raises:\n        TypeError: If the condition is not a boolean Column expression.\n\n    Example: Basic conditional expression\n        ```python\n        # Basic usage\n        df.select(when(col(\"age\") &gt; 18, lit(\"adult\")))\n\n        # With otherwise\n        df.select(when(col(\"age\") &gt; 18, lit(\"adult\")).otherwise(lit(\"minor\")))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        WhenExpr(None, condition._logical_expr, value._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/","title":"fenic.api.functions.builtin","text":""},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin","title":"fenic.api.functions.builtin","text":"<p>Built-in functions for Fenic DataFrames.</p> <p>Functions:</p> <ul> <li> <code>array</code>             \u2013              <p>Creates a new array column from multiple input columns.</p> </li> <li> <code>array_agg</code>             \u2013              <p>Alias for collect_list().</p> </li> <li> <code>array_contains</code>             \u2013              <p>Checks if array column contains a specific value.</p> </li> <li> <code>array_size</code>             \u2013              <p>Returns the number of elements in an array column.</p> </li> <li> <code>asc</code>             \u2013              <p>Creates a Column expression representing an ascending sort order.</p> </li> <li> <code>asc_nulls_first</code>             \u2013              <p>Creates a Column expression representing an ascending sort order with nulls first.</p> </li> <li> <code>asc_nulls_last</code>             \u2013              <p>Creates a Column expression representing an ascending sort order with nulls last.</p> </li> <li> <code>avg</code>             \u2013              <p>Aggregate function: returns the average (mean) of all values in the specified column.</p> </li> <li> <code>coalesce</code>             \u2013              <p>Returns the first non-null value from the given columns for each row.</p> </li> <li> <code>collect_list</code>             \u2013              <p>Aggregate function: collects all values from the specified column into a list.</p> </li> <li> <code>count</code>             \u2013              <p>Aggregate function: returns the count of non-null values in the specified column.</p> </li> <li> <code>desc</code>             \u2013              <p>Creates a Column expression representing a descending sort order.</p> </li> <li> <code>desc_nulls_first</code>             \u2013              <p>Creates a Column expression representing a descending sort order with nulls first.</p> </li> <li> <code>desc_nulls_last</code>             \u2013              <p>Creates a Column expression representing a descending sort order with nulls last.</p> </li> <li> <code>max</code>             \u2013              <p>Aggregate function: returns the maximum value in the specified column.</p> </li> <li> <code>mean</code>             \u2013              <p>Aggregate function: returns the mean (average) of all values in the specified column.</p> </li> <li> <code>min</code>             \u2013              <p>Aggregate function: returns the minimum value in the specified column.</p> </li> <li> <code>struct</code>             \u2013              <p>Creates a new struct column from multiple input columns.</p> </li> <li> <code>sum</code>             \u2013              <p>Aggregate function: returns the sum of all values in the specified column.</p> </li> <li> <code>udf</code>             \u2013              <p>A decorator or function for creating user-defined functions (UDFs) that can be applied to DataFrame rows.</p> </li> <li> <code>when</code>             \u2013              <p>Evaluates a condition and returns a value if true.</p> </li> </ul>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.array","title":"array","text":"<pre><code>array(*args: Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]) -&gt; Column\n</code></pre> <p>Creates a new array column from multiple input columns.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]</code>, default:                   <code>()</code> )           \u2013            <p>Columns or column names to combine into an array. Can be:</p> <ul> <li>Individual arguments</li> <li>Lists of columns/column names</li> <li>Tuples of columns/column names</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing an array containing values from the input columns</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If any argument is not a Column, string, or collection of Columns/strings</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef array(\n    *args: Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]\n) -&gt; Column:\n    \"\"\"Creates a new array column from multiple input columns.\n\n    Args:\n        *args: Columns or column names to combine into an array. Can be:\n\n            - Individual arguments\n            - Lists of columns/column names\n            - Tuples of columns/column names\n\n    Returns:\n        A Column expression representing an array containing values from the input columns\n\n    Raises:\n        TypeError: If any argument is not a Column, string, or collection of\n            Columns/strings\n    \"\"\"\n    flattened_args = []\n    for arg in args:\n        if isinstance(arg, (list, tuple)):\n            flattened_args.extend(arg)\n        else:\n            flattened_args.append(arg)\n\n    expr_columns = [Column._from_col_or_name(c)._logical_expr for c in flattened_args]\n\n    return Column._from_logical_expr(ArrayExpr(expr_columns))\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.array_agg","title":"array_agg","text":"<pre><code>array_agg(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Alias for collect_list().</p> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef array_agg(column: ColumnOrName) -&gt; Column:\n    \"\"\"Alias for collect_list().\"\"\"\n    return collect_list(column)\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.array_contains","title":"array_contains","text":"<pre><code>array_contains(column: ColumnOrName, value: Union[str, int, float, bool, Column]) -&gt; Column\n</code></pre> <p>Checks if array column contains a specific value.</p> <p>This function returns True if the array in the specified column contains the given value, and False otherwise. Returns False if the array is None.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name containing the arrays to check.</p> </li> <li> <code>value</code>               (<code>Union[str, int, float, bool, Column]</code>)           \u2013            <p>Value to search for in the arrays. Can be: - A literal value (string, number, boolean) - A Column expression</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A boolean Column expression (True if value is found, False otherwise).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If value type is incompatible with the array element type.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If the column does not contain array data.</p> </li> </ul> Check for values in arrays <pre><code># Check if 'python' exists in arrays in the 'tags' column\ndf.select(array_contains(\"tags\", \"python\"))\n\n# Check using a value from another column\ndf.select(array_contains(\"tags\", col(\"search_term\")))\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef array_contains(\n    column: ColumnOrName, value: Union[str, int, float, bool, Column]\n) -&gt; Column:\n    \"\"\"Checks if array column contains a specific value.\n\n    This function returns True if the array in the specified column contains the given value,\n    and False otherwise. Returns False if the array is None.\n\n    Args:\n        column: Column or column name containing the arrays to check.\n\n        value: Value to search for in the arrays. Can be:\n            - A literal value (string, number, boolean)\n            - A Column expression\n\n    Returns:\n        A boolean Column expression (True if value is found, False otherwise).\n\n    Raises:\n        TypeError: If value type is incompatible with the array element type.\n        TypeError: If the column does not contain array data.\n\n    Example: Check for values in arrays\n        ```python\n        # Check if 'python' exists in arrays in the 'tags' column\n        df.select(array_contains(\"tags\", \"python\"))\n\n        # Check using a value from another column\n        df.select(array_contains(\"tags\", col(\"search_term\")))\n        ```\n    \"\"\"\n    value_column = None\n    if isinstance(value, Column):\n        value_column = value\n    else:\n        value_column = lit(value)\n    return Column._from_logical_expr(\n        ArrayContainsExpr(\n            Column._from_col_or_name(column)._logical_expr, value_column._logical_expr\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.array_size","title":"array_size","text":"<pre><code>array_size(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Returns the number of elements in an array column.</p> <p>This function computes the length of arrays stored in the specified column. Returns None for None arrays.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name containing arrays whose length to compute.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the array length.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the column does not contain array data.</p> </li> </ul> Get array sizes <pre><code># Get the size of arrays in 'tags' column\ndf.select(array_size(\"tags\"))\n\n# Use with column reference\ndf.select(array_size(col(\"tags\")))\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef array_size(column: ColumnOrName) -&gt; Column:\n    \"\"\"Returns the number of elements in an array column.\n\n    This function computes the length of arrays stored in the specified column.\n    Returns None for None arrays.\n\n    Args:\n        column: Column or column name containing arrays whose length to compute.\n\n    Returns:\n        A Column expression representing the array length.\n\n    Raises:\n        TypeError: If the column does not contain array data.\n\n    Example: Get array sizes\n        ```python\n        # Get the size of arrays in 'tags' column\n        df.select(array_size(\"tags\"))\n\n        # Use with column reference\n        df.select(array_size(col(\"tags\")))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        ArrayLengthExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.asc","title":"asc","text":"<pre><code>asc(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing an ascending sort order.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the ascending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the ascending sort order.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef asc(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing an ascending sort order.\n\n    Args:\n        column: The column to apply the ascending ordering to.\n\n    Returns:\n        A Column expression representing the column and the ascending sort order.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).asc()\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.asc_nulls_first","title":"asc_nulls_first","text":"<pre><code>asc_nulls_first(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing an ascending sort order with nulls first.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the ascending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the ascending sort order with nulls first.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef asc_nulls_first(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing an ascending sort order with nulls first.\n\n    Args:\n        column: The column to apply the ascending ordering to.\n\n    Returns:\n        A Column expression representing the column and the ascending sort order with nulls first.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).asc_nulls_first()\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.asc_nulls_last","title":"asc_nulls_last","text":"<pre><code>asc_nulls_last(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing an ascending sort order with nulls last.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the ascending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the ascending sort order with nulls last.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef asc_nulls_last(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing an ascending sort order with nulls last.\n\n    Args:\n        column: The column to apply the ascending ordering to.\n\n    Returns:\n        A Column expression representing the column and the ascending sort order with nulls last.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).asc_nulls_last()\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.avg","title":"avg","text":"<pre><code>avg(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the average (mean) of all values in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the average of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the average aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef avg(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the average (mean) of all values in the specified column.\n\n    Args:\n        column: Column or column name to compute the average of\n\n    Returns:\n        A Column expression representing the average aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        AvgExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.coalesce","title":"coalesce","text":"<pre><code>coalesce(*cols: ColumnOrName) -&gt; Column\n</code></pre> <p>Returns the first non-null value from the given columns for each row.</p> <p>This function mimics the behavior of SQL's COALESCE function. It evaluates the input columns in order and returns the first non-null value encountered. If all values are null, returns null.</p> <p>Parameters:</p> <ul> <li> <code>*cols</code>               (<code>ColumnOrName</code>, default:                   <code>()</code> )           \u2013            <p>Column expressions or column names to evaluate. Can be:</p> <ul> <li>Individual arguments</li> <li>Lists of columns/column names</li> <li>Tuples of columns/column names</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression containing the first non-null value from the input columns.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no columns are provided.</p> </li> </ul> Basic coalesce usage <pre><code># Basic usage\ndf.select(coalesce(\"col1\", \"col2\", \"col3\"))\n\n# With nested collections\ndf.select(coalesce([\"col1\", \"col2\"], \"col3\"))\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef coalesce(*cols: ColumnOrName) -&gt; Column:\n    \"\"\"Returns the first non-null value from the given columns for each row.\n\n    This function mimics the behavior of SQL's COALESCE function. It evaluates the input columns\n    in order and returns the first non-null value encountered. If all values are null, returns null.\n\n    Args:\n        *cols: Column expressions or column names to evaluate. Can be:\n\n            - Individual arguments\n            - Lists of columns/column names\n            - Tuples of columns/column names\n\n    Returns:\n        A Column expression containing the first non-null value from the input columns.\n\n    Raises:\n        ValueError: If no columns are provided.\n\n    Example: Basic coalesce usage\n        ```python\n        # Basic usage\n        df.select(coalesce(\"col1\", \"col2\", \"col3\"))\n\n        # With nested collections\n        df.select(coalesce([\"col1\", \"col2\"], \"col3\"))\n        ```\n    \"\"\"\n    if not cols:\n        raise ValueError(\"At least one column must be provided to coalesce method\")\n\n    flattened_args = []\n    for arg in cols:\n        if isinstance(arg, (list, tuple)):\n            flattened_args.extend(arg)\n        else:\n            flattened_args.append(arg)\n\n    flattened_exprs = [\n        Column._from_col_or_name(c)._logical_expr for c in flattened_args\n    ]\n    return Column._from_logical_expr(CoalesceExpr(flattened_exprs))\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.collect_list","title":"collect_list","text":"<pre><code>collect_list(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: collects all values from the specified column into a list.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to collect values from</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the list aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef collect_list(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: collects all values from the specified column into a list.\n\n    Args:\n        column: Column or column name to collect values from\n\n    Returns:\n        A Column expression representing the list aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        ListExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.count","title":"count","text":"<pre><code>count(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the count of non-null values in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to count values in</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the count aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef count(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the count of non-null values in the specified column.\n\n    Args:\n        column: Column or column name to count values in\n\n    Returns:\n        A Column expression representing the count aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    if isinstance(column, str) and column == \"*\":\n        return Column._from_logical_expr(CountExpr(lit(\"*\")._logical_expr))\n    return Column._from_logical_expr(\n        CountExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.desc","title":"desc","text":"<pre><code>desc(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing a descending sort order.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the descending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the descending sort order.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef desc(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing a descending sort order.\n\n    Args:\n        column: The column to apply the descending ordering to.\n\n    Returns:\n        A Column expression representing the column and the descending sort order.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).desc()\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.desc_nulls_first","title":"desc_nulls_first","text":"<pre><code>desc_nulls_first(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing a descending sort order with nulls first.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the descending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the descending sort order with nulls first.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef desc_nulls_first(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing a descending sort order with nulls first.\n\n    Args:\n        column: The column to apply the descending ordering to.\n\n    Returns:\n        A Column expression representing the column and the descending sort order with nulls first.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).desc_nulls_first()\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.desc_nulls_last","title":"desc_nulls_last","text":"<pre><code>desc_nulls_last(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Creates a Column expression representing a descending sort order with nulls last.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to apply the descending ordering to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the column and the descending sort order with nulls last.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the column cannot be inferred.</p> </li> <li> <code>Error</code>             \u2013            <p>If this expression is passed to a dataframe operation besides sort() and order_by().</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef desc_nulls_last(column: ColumnOrName) -&gt; Column:\n    \"\"\"Creates a Column expression representing a descending sort order with nulls last.\n\n    Args:\n        column: The column to apply the descending ordering to.\n\n    Returns:\n        A Column expression representing the column and the descending sort order with nulls last.\n\n    Raises:\n        ValueError: If the type of the column cannot be inferred.\n        Error: If this expression is passed to a dataframe operation besides sort() and order_by().\n    \"\"\"\n    return Column._from_col_or_name(column).desc_nulls_last()\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.max","title":"max","text":"<pre><code>max(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the maximum value in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the maximum of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the maximum aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef max(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the maximum value in the specified column.\n\n    Args:\n        column: Column or column name to compute the maximum of\n\n    Returns:\n        A Column expression representing the maximum aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        MaxExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.mean","title":"mean","text":"<pre><code>mean(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the mean (average) of all values in the specified column.</p> <p>Alias for avg().</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the mean of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the mean aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef mean(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the mean (average) of all values in the specified column.\n\n    Alias for avg().\n\n    Args:\n        column: Column or column name to compute the mean of\n\n    Returns:\n        A Column expression representing the mean aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        AvgExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.min","title":"min","text":"<pre><code>min(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the minimum value in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the minimum of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the minimum aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef min(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the minimum value in the specified column.\n\n    Args:\n        column: Column or column name to compute the minimum of\n\n    Returns:\n        A Column expression representing the minimum aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        MinExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.struct","title":"struct","text":"<pre><code>struct(*args: Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]) -&gt; Column\n</code></pre> <p>Creates a new struct column from multiple input columns.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]</code>, default:                   <code>()</code> )           \u2013            <p>Columns or column names to combine into a struct. Can be:</p> <ul> <li>Individual arguments</li> <li>Lists of columns/column names</li> <li>Tuples of columns/column names</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing a struct containing the input columns</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If any argument is not a Column, string, or collection of Columns/strings</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef struct(\n    *args: Union[ColumnOrName, List[ColumnOrName], Tuple[ColumnOrName, ...]]\n) -&gt; Column:\n    \"\"\"Creates a new struct column from multiple input columns.\n\n    Args:\n        *args: Columns or column names to combine into a struct. Can be:\n\n            - Individual arguments\n            - Lists of columns/column names\n            - Tuples of columns/column names\n\n    Returns:\n        A Column expression representing a struct containing the input columns\n\n    Raises:\n        TypeError: If any argument is not a Column, string, or collection of\n            Columns/strings\n    \"\"\"\n    flattened_args = []\n    for arg in args:\n        if isinstance(arg, (list, tuple)):\n            flattened_args.extend(arg)\n        else:\n            flattened_args.append(arg)\n\n    expr_columns = [Column._from_col_or_name(c)._logical_expr for c in flattened_args]\n\n    return Column._from_logical_expr(StructExpr(expr_columns))\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.sum","title":"sum","text":"<pre><code>sum(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Aggregate function: returns the sum of all values in the specified column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name to compute the sum of</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the sum aggregation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If column is not a Column or string</p> </li> </ul> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef sum(column: ColumnOrName) -&gt; Column:\n    \"\"\"Aggregate function: returns the sum of all values in the specified column.\n\n    Args:\n        column: Column or column name to compute the sum of\n\n    Returns:\n        A Column expression representing the sum aggregation\n\n    Raises:\n        TypeError: If column is not a Column or string\n    \"\"\"\n    return Column._from_logical_expr(\n        SumExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.udf","title":"udf","text":"<pre><code>udf(f: Optional[Callable] = None, *, return_type: DataType)\n</code></pre> <p>A decorator or function for creating user-defined functions (UDFs) that can be applied to DataFrame rows.</p> <p>When applied, UDFs will: - Access <code>StructType</code> columns as Python dictionaries (<code>dict[str, Any]</code>). - Access <code>ArrayType</code> columns as Python lists (<code>list[Any]</code>). - Access primitive types (e.g., <code>int</code>, <code>float</code>, <code>str</code>) as their respective Python types.</p> <p>Parameters:</p> <ul> <li> <code>f</code>               (<code>Optional[Callable]</code>, default:                   <code>None</code> )           \u2013            <p>Python function to convert to UDF</p> </li> <li> <code>return_type</code>               (<code>DataType</code>)           \u2013            <p>Expected return type of the UDF. Required parameter.</p> </li> </ul> UDF with primitive types <pre><code># UDF with primitive types\n@udf(return_type=IntegerType)\ndef add_one(x: int):\n    return x + 1\n\n# Or\nadd_one = udf(lambda x: x + 1, return_type=IntegerType)\n</code></pre> UDF with nested types <pre><code># UDF with nested types\n@udf(return_type=StructType([StructField(\"value1\", IntegerType), StructField(\"value2\", IntegerType)]))\ndef example_udf(x: dict[str, int], y: list[int]):\n    return {\n        \"value1\": x[\"value1\"] + x[\"value2\"] + y[0],\n        \"value2\": x[\"value1\"] + x[\"value2\"] + y[1],\n    }\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef udf(f: Optional[Callable] = None, *, return_type: DataType):\n    \"\"\"A decorator or function for creating user-defined functions (UDFs) that can be applied to DataFrame rows.\n\n    When applied, UDFs will:\n    - Access `StructType` columns as Python dictionaries (`dict[str, Any]`).\n    - Access `ArrayType` columns as Python lists (`list[Any]`).\n    - Access primitive types (e.g., `int`, `float`, `str`) as their respective Python types.\n\n    Args:\n        f: Python function to convert to UDF\n\n        return_type: Expected return type of the UDF. Required parameter.\n\n    Example: UDF with primitive types\n        ```python\n        # UDF with primitive types\n        @udf(return_type=IntegerType)\n        def add_one(x: int):\n            return x + 1\n\n        # Or\n        add_one = udf(lambda x: x + 1, return_type=IntegerType)\n        ```\n\n    Example: UDF with nested types\n        ```python\n        # UDF with nested types\n        @udf(return_type=StructType([StructField(\"value1\", IntegerType), StructField(\"value2\", IntegerType)]))\n        def example_udf(x: dict[str, int], y: list[int]):\n            return {\n                \"value1\": x[\"value1\"] + x[\"value2\"] + y[0],\n                \"value2\": x[\"value1\"] + x[\"value2\"] + y[1],\n            }\n        ```\n    \"\"\"\n\n    def _create_udf(func: Callable) -&gt; Callable:\n        @wraps(func)\n        def _udf_wrapper(*cols: ColumnOrName) -&gt; Column:\n            col_exprs = [Column._from_col_or_name(c)._logical_expr for c in cols]\n            return Column._from_logical_expr(UDFExpr(func, col_exprs, return_type))\n\n        return _udf_wrapper\n\n    if f is not None:\n        return _create_udf(f)\n    return _create_udf\n</code></pre>"},{"location":"reference/fenic/api/functions/builtin/#fenic.api.functions.builtin.when","title":"when","text":"<pre><code>when(condition: Column, value: Column) -&gt; Column\n</code></pre> <p>Evaluates a condition and returns a value if true.</p> <p>This function is used to create conditional expressions. If Column.otherwise() is not invoked, None is returned for unmatched conditions.</p> <p>Parameters:</p> <ul> <li> <code>condition</code>               (<code>Column</code>)           \u2013            <p>A boolean Column expression to evaluate.</p> </li> <li> <code>value</code>               (<code>Column</code>)           \u2013            <p>A Column expression to return if the condition is true.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression that evaluates the condition and returns the specified value when true,</p> </li> <li> <code>Column</code>           \u2013            <p>and None otherwise.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the condition is not a boolean Column expression.</p> </li> </ul> Basic conditional expression <pre><code># Basic usage\ndf.select(when(col(\"age\") &gt; 18, lit(\"adult\")))\n\n# With otherwise\ndf.select(when(col(\"age\") &gt; 18, lit(\"adult\")).otherwise(lit(\"minor\")))\n</code></pre> Source code in <code>src/fenic/api/functions/builtin.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef when(condition: Column, value: Column) -&gt; Column:\n    \"\"\"Evaluates a condition and returns a value if true.\n\n    This function is used to create conditional expressions. If Column.otherwise() is not invoked,\n    None is returned for unmatched conditions.\n\n    Args:\n        condition: A boolean Column expression to evaluate.\n\n        value: A Column expression to return if the condition is true.\n\n    Returns:\n        A Column expression that evaluates the condition and returns the specified value when true,\n        and None otherwise.\n\n    Raises:\n        TypeError: If the condition is not a boolean Column expression.\n\n    Example: Basic conditional expression\n        ```python\n        # Basic usage\n        df.select(when(col(\"age\") &gt; 18, lit(\"adult\")))\n\n        # With otherwise\n        df.select(when(col(\"age\") &gt; 18, lit(\"adult\")).otherwise(lit(\"minor\")))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        WhenExpr(None, condition._logical_expr, value._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/core/","title":"fenic.api.functions.core","text":""},{"location":"reference/fenic/api/functions/core/#fenic.api.functions.core","title":"fenic.api.functions.core","text":"<p>Core functions for Fenic DataFrames.</p> <p>Functions:</p> <ul> <li> <code>col</code>             \u2013              <p>Creates a Column expression referencing a column in the DataFrame.</p> </li> <li> <code>lit</code>             \u2013              <p>Creates a Column expression representing a literal value.</p> </li> </ul>"},{"location":"reference/fenic/api/functions/core/#fenic.api.functions.core.col","title":"col","text":"<pre><code>col(col_name: str) -&gt; Column\n</code></pre> <p>Creates a Column expression referencing a column in the DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>col_name</code>               (<code>str</code>)           \u2013            <p>Name of the column to reference</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression for the specified column</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If colName is not a string</p> </li> </ul> Source code in <code>src/fenic/api/functions/core.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef col(col_name: str) -&gt; Column:\n    \"\"\"Creates a Column expression referencing a column in the DataFrame.\n\n    Args:\n        col_name: Name of the column to reference\n\n    Returns:\n        A Column expression for the specified column\n\n    Raises:\n        TypeError: If colName is not a string\n    \"\"\"\n    return Column._from_column_name(col_name)\n</code></pre>"},{"location":"reference/fenic/api/functions/core/#fenic.api.functions.core.lit","title":"lit","text":"<pre><code>lit(value: Any) -&gt; Column\n</code></pre> <p>Creates a Column expression representing a literal value.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>The literal value to create a column for</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression representing the literal value</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the type of the value cannot be inferred</p> </li> </ul> Source code in <code>src/fenic/api/functions/core.py</code> <pre><code>def lit(value: Any) -&gt; Column:\n    \"\"\"Creates a Column expression representing a literal value.\n\n    Args:\n        value: The literal value to create a column for\n\n    Returns:\n        A Column expression representing the literal value\n\n    Raises:\n        ValueError: If the type of the value cannot be inferred\n    \"\"\"\n    try:\n        inferred_type = infer_dtype_from_pyobj(value)\n    except TypeInferenceError as e:\n        raise ValidationError(f\"`lit` failed to infer type for value `{value}`\") from e\n    literal_expr = LiteralExpr(value, inferred_type)\n    return Column._from_logical_expr(literal_expr)\n</code></pre>"},{"location":"reference/fenic/api/functions/embedding/","title":"fenic.api.functions.embedding","text":""},{"location":"reference/fenic/api/functions/embedding/#fenic.api.functions.embedding","title":"fenic.api.functions.embedding","text":"<p>Embedding functions.</p> <p>Functions:</p> <ul> <li> <code>compute_similarity</code>             \u2013              <p>Compute similarity between embedding vectors using specified metric.</p> </li> <li> <code>normalize</code>             \u2013              <p>Normalize embedding vectors to unit length.</p> </li> </ul>"},{"location":"reference/fenic/api/functions/embedding/#fenic.api.functions.embedding.compute_similarity","title":"compute_similarity","text":"<pre><code>compute_similarity(column: ColumnOrName, other: Union[ColumnOrName, List[float], ndarray], metric: SemanticSimilarityMetric = 'cosine') -&gt; Column\n</code></pre> <p>Compute similarity between embedding vectors using specified metric.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column containing embedding vectors.</p> </li> <li> <code>other</code>               (<code>Union[ColumnOrName, List[float], ndarray]</code>)           \u2013            <p>Either:</p> <ul> <li>Another column containing embedding vectors for pairwise similarity</li> <li>A query vector (list of floats or numpy array) for similarity with each embedding</li> </ul> </li> <li> <code>metric</code>               (<code>SemanticSimilarityMetric</code>, default:                   <code>'cosine'</code> )           \u2013            <p>The similarity metric to use. Options:</p> <ul> <li><code>cosine</code>: Cosine similarity (range: -1 to 1, higher is more similar)</li> <li><code>dot</code>: Dot product similarity (raw inner product)</li> <li><code>l2</code>: L2 (Euclidean) distance (lower is more similar)</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column of float values representing similarity scores.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If query vector contains NaN values or has invalid dimensions.</p> </li> </ul> Notes <ul> <li>Cosine similarity normalizes vectors internally, so pre-normalization is not required</li> <li>Dot product does not normalize, useful when vectors are already normalized</li> <li>L2 distance measures the straight-line distance between vectors</li> <li>When using two columns, dimensions must match between embeddings</li> </ul> Compute dot product with a query vector <pre><code># Compute dot product with a query vector\nquery = [0.1, 0.2, 0.3]\ndf.select(\n    embedding.compute_similarity(col(\"embeddings\"), query).alias(\"similarity\")\n)\n</code></pre> Compute cosine similarity with a query vector <pre><code>query = [0.6, ... 0.8]  # Already normalized\ndf.select(\n    embedding.compute_similarity(col(\"embeddings\"), query, metric=\"cosine\").alias(\"cosine_sim\")\n)\n</code></pre> Compute pairwise dot products between columns <pre><code># Compute L2 distance between two columns of embeddings\ndf.select(\n    embedding.compute_similarity(col(\"embeddings1\"), col(\"embeddings2\"), metric=\"l2\").alias(\"distance\")\n)\n</code></pre> Using numpy array as query vector <pre><code># Use numpy array as query vector\nimport numpy as np\nquery = np.array([0.1, 0.2, 0.3])\ndf.select(embedding.compute_similarity(\"embeddings\", query))\n</code></pre> Source code in <code>src/fenic/api/functions/embedding.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef compute_similarity(\n    column: ColumnOrName,\n    other: Union[ColumnOrName, List[float], np.ndarray],\n    metric: SemanticSimilarityMetric = \"cosine\"\n) -&gt; Column:\n    \"\"\"Compute similarity between embedding vectors using specified metric.\n\n    Args:\n        column: Column containing embedding vectors.\n\n        other: Either:\n\n            - Another column containing embedding vectors for pairwise similarity\n            - A query vector (list of floats or numpy array) for similarity with each embedding\n\n        metric: The similarity metric to use. Options:\n\n            - `cosine`: Cosine similarity (range: -1 to 1, higher is more similar)\n            - `dot`: Dot product similarity (raw inner product)\n            - `l2`: L2 (Euclidean) distance (lower is more similar)\n\n    Returns:\n        Column: A column of float values representing similarity scores.\n\n    Raises:\n        ValidationError: If query vector contains NaN values or has invalid dimensions.\n\n    Notes:\n        - Cosine similarity normalizes vectors internally, so pre-normalization is not required\n        - Dot product does not normalize, useful when vectors are already normalized\n        - L2 distance measures the straight-line distance between vectors\n        - When using two columns, dimensions must match between embeddings\n\n    Example: Compute dot product with a query vector\n        ```python\n        # Compute dot product with a query vector\n        query = [0.1, 0.2, 0.3]\n        df.select(\n            embedding.compute_similarity(col(\"embeddings\"), query).alias(\"similarity\")\n        )\n        ```\n\n    Example: Compute cosine similarity with a query vector\n        ```python\n        query = [0.6, ... 0.8]  # Already normalized\n        df.select(\n            embedding.compute_similarity(col(\"embeddings\"), query, metric=\"cosine\").alias(\"cosine_sim\")\n        )\n        ```\n\n    Example: Compute pairwise dot products between columns\n        ```python\n        # Compute L2 distance between two columns of embeddings\n        df.select(\n            embedding.compute_similarity(col(\"embeddings1\"), col(\"embeddings2\"), metric=\"l2\").alias(\"distance\")\n        )\n        ```\n\n    Example: Using numpy array as query vector\n        ```python\n        # Use numpy array as query vector\n        import numpy as np\n        query = np.array([0.1, 0.2, 0.3])\n        df.select(embedding.compute_similarity(\"embeddings\", query))\n        ```\n    \"\"\"\n    column_expr = Column._from_col_or_name(column)._logical_expr\n\n    # Check if other is a column\n    if isinstance(other, ColumnOrName):\n        other_expr = Column._from_col_or_name(other)._logical_expr\n        return Column._from_logical_expr(\n            EmbeddingSimilarityExpr(column_expr, other_expr, metric)\n        )\n\n    # Otherwise it's a query vector\n    if isinstance(other, list):\n        query_array = np.array(other, dtype=np.float32)\n    else:\n        query_array = other.astype(np.float32)\n\n    # Check for NaNs\n    if np.any(np.isnan(query_array)):\n        raise ValidationError(\"Query vector cannot contain NaN values\")\n\n    return Column._from_logical_expr(\n        EmbeddingSimilarityExpr(column_expr, query_array, metric)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/embedding/#fenic.api.functions.embedding.normalize","title":"normalize","text":"<pre><code>normalize(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Normalize embedding vectors to unit length.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column containing embedding vectors.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column of normalized embedding vectors with the same embedding type.</p> </li> </ul> Notes <ul> <li>Normalizes each embedding vector to have unit length (L2 norm = 1)</li> <li>Preserves the original embedding model in the type</li> <li>Null values are preserved as null</li> <li>Zero vectors become NaN after normalization</li> </ul> Normalize embeddings for dot product similarity <pre><code># Normalize embeddings for dot product similarity comparisons\ndf.select(\n    embedding.normalize(col(\"embeddings\")).alias(\"unit_embeddings\")\n)\n</code></pre> Compare normalized embeddings using dot product <pre><code># Compare normalized embeddings using dot product (equivalent to cosine similarity)\nnormalized_df = df.select(embedding.normalize(col(\"embeddings\")).alias(\"norm_emb\"))\nquery = [0.6, 0.8]  # Already normalized\nnormalized_df.select(\n    embedding.compute_similarity(col(\"norm_emb\"), query, metric=\"dot\").alias(\"dot_product_sim\")\n)\n</code></pre> Source code in <code>src/fenic/api/functions/embedding.py</code> <pre><code>def normalize(column: ColumnOrName) -&gt; Column:\n    \"\"\"Normalize embedding vectors to unit length.\n\n    Args:\n        column: Column containing embedding vectors.\n\n    Returns:\n        Column: A column of normalized embedding vectors with the same embedding type.\n\n    Notes:\n        - Normalizes each embedding vector to have unit length (L2 norm = 1)\n        - Preserves the original embedding model in the type\n        - Null values are preserved as null\n        - Zero vectors become NaN after normalization\n\n    Example: Normalize embeddings for dot product similarity\n        ```python\n        # Normalize embeddings for dot product similarity comparisons\n        df.select(\n            embedding.normalize(col(\"embeddings\")).alias(\"unit_embeddings\")\n        )\n        ```\n\n    Example: Compare normalized embeddings using dot product\n        ```python\n        # Compare normalized embeddings using dot product (equivalent to cosine similarity)\n        normalized_df = df.select(embedding.normalize(col(\"embeddings\")).alias(\"norm_emb\"))\n        query = [0.6, 0.8]  # Already normalized\n        normalized_df.select(\n            embedding.compute_similarity(col(\"norm_emb\"), query, metric=\"dot\").alias(\"dot_product_sim\")\n        )\n        ```\n    \"\"\"\n    column_expr = Column._from_col_or_name(column)._logical_expr\n    return Column._from_logical_expr(EmbeddingNormalizeExpr(column_expr))\n</code></pre>"},{"location":"reference/fenic/api/functions/json/","title":"fenic.api.functions.json","text":""},{"location":"reference/fenic/api/functions/json/#fenic.api.functions.json","title":"fenic.api.functions.json","text":"<p>JSON functions.</p> <p>Functions:</p> <ul> <li> <code>contains</code>             \u2013              <p>Check if a JSON value contains the specified value using recursive deep search.</p> </li> <li> <code>get_type</code>             \u2013              <p>Get the JSON type of each value.</p> </li> <li> <code>jq</code>             \u2013              <p>Applies a JQ query to a column containing JSON-formatted strings.</p> </li> </ul>"},{"location":"reference/fenic/api/functions/json/#fenic.api.functions.json.contains","title":"contains","text":"<pre><code>contains(column: ColumnOrName, value: str) -&gt; Column\n</code></pre> <p>Check if a JSON value contains the specified value using recursive deep search.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Input column of type <code>JsonType</code>.</p> </li> <li> <code>value</code>               (<code>str</code>)           \u2013            <p>Valid JSON string to search for.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column of booleans indicating whether the JSON contains the value.</p> </li> </ul> Matching Rules <ul> <li>Objects: Uses partial matching - <code>{\"role\": \"admin\"}</code> matches <code>{\"role\": \"admin\", \"level\": 5}</code></li> <li>Arrays: Uses exact matching - <code>[1, 2]</code> only matches exactly <code>[1, 2]</code>, not <code>[1, 2, 3]</code></li> <li>Primitives: Uses exact matching - <code>42</code> matches <code>42</code> but not <code>\"42\"</code></li> <li>Search is recursive: Searches at all nesting levels throughout the JSON structure</li> <li>Type-aware: Distinguishes between <code>42</code> (number) and <code>\"42\"</code> (string)</li> </ul> Find objects with partial structure match <pre><code># Find objects with partial structure match (at any nesting level)\ndf.select(json.contains(col(\"json_data\"), '{\"name\": \"Alice\"}'))\n# Matches: {\"name\": \"Alice\", \"age\": 30} and {\"user\": {\"name\": \"Alice\"}}\n</code></pre> Find exact array match <pre><code># Find exact array match (at any nesting level)\ndf.select(json.contains(col(\"json_data\"), '[\"read\", \"write\"]'))\n# Matches: {\"permissions\": [\"read\", \"write\"]} but not [\"read\", \"write\", \"admin\"]\n</code></pre> Find exact primitive values <pre><code># Find exact primitive values (at any nesting level)\ndf.select(json.contains(col(\"json_data\"), '\"admin\"'))\n# Matches: {\"role\": \"admin\"} and [\"admin\", \"user\"] but not {\"role\": \"administrator\"}\n</code></pre> Type distinction matters <pre><code># Type distinction matters\ndf.select(json.contains(col(\"json_data\"), '42'))      # number 42\ndf.select(json.contains(col(\"json_data\"), '\"42\"'))    # string \"42\"\n</code></pre> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If <code>value</code> is not valid JSON.</p> </li> </ul> Source code in <code>src/fenic/api/functions/json.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef contains(column: ColumnOrName, value: str) -&gt; Column:\n    \"\"\"Check if a JSON value contains the specified value using recursive deep search.\n\n    Args:\n        column (ColumnOrName): Input column of type `JsonType`.\n        value (str): Valid JSON string to search for.\n\n    Returns:\n        Column: A column of booleans indicating whether the JSON contains the value.\n\n    Matching Rules:\n        - **Objects**: Uses partial matching - `{\"role\": \"admin\"}` matches `{\"role\": \"admin\", \"level\": 5}`\n        - **Arrays**: Uses exact matching - `[1, 2]` only matches exactly `[1, 2]`, not `[1, 2, 3]`\n        - **Primitives**: Uses exact matching - `42` matches `42` but not `\"42\"`\n        - **Search is recursive**: Searches at all nesting levels throughout the JSON structure\n        - **Type-aware**: Distinguishes between `42` (number) and `\"42\"` (string)\n\n    Example: Find objects with partial structure match\n        ```python\n        # Find objects with partial structure match (at any nesting level)\n        df.select(json.contains(col(\"json_data\"), '{\"name\": \"Alice\"}'))\n        # Matches: {\"name\": \"Alice\", \"age\": 30} and {\"user\": {\"name\": \"Alice\"}}\n        ```\n\n    Example: Find exact array match\n        ```python\n        # Find exact array match (at any nesting level)\n        df.select(json.contains(col(\"json_data\"), '[\"read\", \"write\"]'))\n        # Matches: {\"permissions\": [\"read\", \"write\"]} but not [\"read\", \"write\", \"admin\"]\n        ```\n\n    Example: Find exact primitive values\n        ```python\n        # Find exact primitive values (at any nesting level)\n        df.select(json.contains(col(\"json_data\"), '\"admin\"'))\n        # Matches: {\"role\": \"admin\"} and [\"admin\", \"user\"] but not {\"role\": \"administrator\"}\n        ```\n\n    Example: Type distinction matters\n        ```python\n        # Type distinction matters\n        df.select(json.contains(col(\"json_data\"), '42'))      # number 42\n        df.select(json.contains(col(\"json_data\"), '\"42\"'))    # string \"42\"\n        ```\n\n    Raises:\n        ValidationError: If `value` is not valid JSON.\n    \"\"\"\n    return Column._from_logical_expr(\n        JsonContainsExpr(Column._from_col_or_name(column)._logical_expr, value)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/json/#fenic.api.functions.json.get_type","title":"get_type","text":"<pre><code>get_type(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Get the JSON type of each value.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Input column of type <code>JsonType</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column of strings indicating the JSON type     (\"string\", \"number\", \"boolean\", \"array\", \"object\", \"null\").</p> </li> </ul> Get JSON types <pre><code>df.select(json.get_type(col(\"json_data\")))\n</code></pre> Filter by type <pre><code># Filter by type\ndf.filter(json.get_type(col(\"data\")) == \"array\")\n</code></pre> Source code in <code>src/fenic/api/functions/json.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef get_type(column: ColumnOrName) -&gt; Column:\n    \"\"\"Get the JSON type of each value.\n\n    Args:\n        column (ColumnOrName): Input column of type `JsonType`.\n\n    Returns:\n        Column: A column of strings indicating the JSON type\n                (\"string\", \"number\", \"boolean\", \"array\", \"object\", \"null\").\n\n    Example: Get JSON types\n        ```python\n        df.select(json.get_type(col(\"json_data\")))\n        ```\n\n    Example: Filter by type\n        ```python\n        # Filter by type\n        df.filter(json.get_type(col(\"data\")) == \"array\")\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        JsonTypeExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/json/#fenic.api.functions.json.jq","title":"jq","text":"<pre><code>jq(column: ColumnOrName, query: str) -&gt; Column\n</code></pre> <p>Applies a JQ query to a column containing JSON-formatted strings.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Input column of type <code>JsonType</code>.</p> </li> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>A JQ expression used to extract or transform values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the result of applying the JQ query to each row's JSON input.</p> </li> </ul> Notes <ul> <li>The input column must be of type <code>JsonType</code>. Use <code>cast(JsonType)</code> if needed to ensure correct typing.</li> <li>This function supports extracting nested fields, transforming arrays/objects, and other standard JQ operations.</li> </ul> Extract nested field <pre><code># Extract the \"user.name\" field from a JSON column\ndf.select(json.jq(col(\"json_col\"), \".user.name\"))\n</code></pre> Cast to JsonType before querying <pre><code>df.select(json.jq(col(\"raw_json\").cast(JsonType), \".event.type\"))\n</code></pre> Work with arrays <pre><code># Work with arrays using JQ functions\ndf.select(json.jq(col(\"json_array\"), \"map(.id)\"))\n</code></pre> Source code in <code>src/fenic/api/functions/json.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef jq(column: ColumnOrName, query: str) -&gt; Column:\n    \"\"\"Applies a JQ query to a column containing JSON-formatted strings.\n\n    Args:\n        column (ColumnOrName): Input column of type `JsonType`.\n        query (str): A [JQ](https://jqlang.org/) expression used to extract or transform values.\n\n    Returns:\n        Column: A column containing the result of applying the JQ query to each row's JSON input.\n\n    Notes:\n        - The input column *must* be of type `JsonType`. Use `cast(JsonType)` if needed to ensure correct typing.\n        - This function supports extracting nested fields, transforming arrays/objects, and other standard JQ operations.\n\n    Example: Extract nested field\n        ```python\n        # Extract the \"user.name\" field from a JSON column\n        df.select(json.jq(col(\"json_col\"), \".user.name\"))\n        ```\n\n    Example: Cast to JsonType before querying\n        ```python\n        df.select(json.jq(col(\"raw_json\").cast(JsonType), \".event.type\"))\n        ```\n\n    Example: Work with arrays\n        ```python\n        # Work with arrays using JQ functions\n        df.select(json.jq(col(\"json_array\"), \"map(.id)\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        JqExpr(Column._from_col_or_name(column)._logical_expr, query)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/markdown/","title":"fenic.api.functions.markdown","text":""},{"location":"reference/fenic/api/functions/markdown/#fenic.api.functions.markdown","title":"fenic.api.functions.markdown","text":"<p>Markdown functions.</p> <p>Functions:</p> <ul> <li> <code>extract_header_chunks</code>             \u2013              <p>Splits markdown documents into logical chunks based on heading hierarchy.</p> </li> <li> <code>generate_toc</code>             \u2013              <p>Generates a table of contents from markdown headings.</p> </li> <li> <code>get_code_blocks</code>             \u2013              <p>Extracts all code blocks from a column of Markdown-formatted strings.</p> </li> <li> <code>to_json</code>             \u2013              <p>Converts a column of Markdown-formatted strings into a hierarchical JSON representation.</p> </li> </ul>"},{"location":"reference/fenic/api/functions/markdown/#fenic.api.functions.markdown.extract_header_chunks","title":"extract_header_chunks","text":"<pre><code>extract_header_chunks(column: ColumnOrName, header_level: int) -&gt; Column\n</code></pre> <p>Splits markdown documents into logical chunks based on heading hierarchy.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Input column containing Markdown strings.</p> </li> <li> <code>header_level</code>               (<code>int</code>)           \u2013            <p>Heading level to split on (1-6). Creates a new chunk at every                 heading of this level, including all nested content and subsections.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column of arrays containing chunk objects with the following structure:</p> <pre><code>ArrayType(StructType([\n    StructField(\"heading\", StringType),        # Heading text (clean, no markdown)\n    StructField(\"level\", IntegerType),         # Heading level (1-6)\n    StructField(\"content\", StringType),        # All content under this heading (clean text)\n    StructField(\"parent_heading\", StringType), # Parent heading text (or null)\n    StructField(\"full_path\", StringType),      # Full breadcrumb path\n]))\n</code></pre> </li> </ul> <p>Notes:     - Context-preserving: Each chunk contains all content and subsections under the heading     - Hierarchical awareness: Includes parent heading context for better LLM understanding     - Clean text output: Strips markdown formatting for direct LLM consumption</p> Chunking Behavior <p>With <code>header_level=2</code>, this markdown:</p> <pre><code># Introduction\nOverview text\n\n## Getting Started\nSetup instructions\n\n### Prerequisites\nPython 3.8+ required\n\n## API Reference\nFunction documentation\n</code></pre> <p>Produces 2 chunks:</p> <ol> <li><code>Getting Started</code> chunk (includes <code>Prerequisites</code> subsection)</li> <li><code>API Reference</code> chunk</li> </ol> Split articles into top-level sections <pre><code>df.select(markdown.extract_header_chunks(col(\"articles\"), header_level=1))\n</code></pre> Split documentation into feature sections <pre><code>df.select(markdown.extract_header_chunks(col(\"docs\"), header_level=2))\n</code></pre> Create fine-grained chunks for detailed analysis <pre><code>df.select(markdown.extract_header_chunks(col(\"content\"), header_level=3))\n</code></pre> Explode chunks into individual rows for processing <pre><code>chunks_df = df.select(\n    markdown.extract_header_chunks(col(\"markdown\"), header_level=2).alias(\"chunks\")\n).explode(\"chunks\")\nchunks_df.select(\n    col(\"chunks\").heading,\n    col(\"chunks\").content,\n    col(\"chunks\").full_path\n)\n</code></pre> Source code in <code>src/fenic/api/functions/markdown.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef extract_header_chunks(column: ColumnOrName, header_level: int) -&gt; Column:\n    \"\"\"Splits markdown documents into logical chunks based on heading hierarchy.\n\n    Args:\n        column (ColumnOrName): Input column containing Markdown strings.\n        header_level (int): Heading level to split on (1-6). Creates a new chunk at every\n                            heading of this level, including all nested content and subsections.\n\n    Returns:\n        Column: A column of arrays containing chunk objects with the following structure:\n            ```python\n            ArrayType(StructType([\n                StructField(\"heading\", StringType),        # Heading text (clean, no markdown)\n                StructField(\"level\", IntegerType),         # Heading level (1-6)\n                StructField(\"content\", StringType),        # All content under this heading (clean text)\n                StructField(\"parent_heading\", StringType), # Parent heading text (or null)\n                StructField(\"full_path\", StringType),      # Full breadcrumb path\n            ]))\n            ```\n    Notes:\n        - **Context-preserving**: Each chunk contains all content and subsections under the heading\n        - **Hierarchical awareness**: Includes parent heading context for better LLM understanding\n        - **Clean text output**: Strips markdown formatting for direct LLM consumption\n\n    Chunking Behavior:\n        With `header_level=2`, this markdown:\n        ```markdown\n        # Introduction\n        Overview text\n\n        ## Getting Started\n        Setup instructions\n\n        ### Prerequisites\n        Python 3.8+ required\n\n        ## API Reference\n        Function documentation\n        ```\n        Produces 2 chunks:\n\n        1. `Getting Started` chunk (includes `Prerequisites` subsection)\n        2. `API Reference` chunk\n\n    Example: Split articles into top-level sections\n        ```python\n        df.select(markdown.extract_header_chunks(col(\"articles\"), header_level=1))\n        ```\n\n    Example: Split documentation into feature sections\n        ```python\n        df.select(markdown.extract_header_chunks(col(\"docs\"), header_level=2))\n        ```\n\n    Example: Create fine-grained chunks for detailed analysis\n        ```python\n        df.select(markdown.extract_header_chunks(col(\"content\"), header_level=3))\n        ```\n\n    Example: Explode chunks into individual rows for processing\n        ```python\n        chunks_df = df.select(\n            markdown.extract_header_chunks(col(\"markdown\"), header_level=2).alias(\"chunks\")\n        ).explode(\"chunks\")\n        chunks_df.select(\n            col(\"chunks\").heading,\n            col(\"chunks\").content,\n            col(\"chunks\").full_path\n        )\n        ```\n    \"\"\"\n    if header_level &lt; 1 or header_level &gt; 6:\n        raise ValidationError(f\"split_level must be between 1 and 6 (inclusive), but got {header_level}. Use 1 for # headings, 2 for ## headings, etc.\")\n\n    return Column._from_logical_expr(\n        MdExtractHeaderChunks(Column._from_col_or_name(column)._logical_expr, header_level)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/markdown/#fenic.api.functions.markdown.generate_toc","title":"generate_toc","text":"<pre><code>generate_toc(column: ColumnOrName, max_level: Optional[int] = None) -&gt; Column\n</code></pre> <p>Generates a table of contents from markdown headings.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Input column containing Markdown strings.</p> </li> <li> <code>max_level</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Maximum heading level to include in the TOC (1-6).                      Defaults to 6 (all levels).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column of Markdown-formatted table of contents strings.</p> </li> </ul> Notes <ul> <li>The TOC is generated using markdown heading syntax (# ## ### etc.)</li> <li>Each heading in the source document becomes a line in the TOC</li> <li>The heading level is preserved in the output</li> <li>This creates a valid markdown document that can be rendered or processed further</li> </ul> Generate a complete TOC <pre><code>df.select(markdown.generate_toc(col(\"documentation\")))\n</code></pre> Generate a simplified TOC with only top 2 levels <pre><code>df.select(markdown.generate_toc(col(\"documentation\"), max_level=2))\n</code></pre> Add TOC as a new column <pre><code>df = df.with_column(\"toc\", markdown.generate_toc(col(\"content\"), max_level=3))\n</code></pre> Source code in <code>src/fenic/api/functions/markdown.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef generate_toc(column: ColumnOrName, max_level: Optional[int] = None) -&gt; Column:\n    \"\"\"Generates a table of contents from markdown headings.\n\n    Args:\n        column (ColumnOrName): Input column containing Markdown strings.\n        max_level (Optional[int]): Maximum heading level to include in the TOC (1-6).\n                                 Defaults to 6 (all levels).\n\n    Returns:\n        Column: A column of Markdown-formatted table of contents strings.\n\n    Notes:\n        - The TOC is generated using markdown heading syntax (# ## ### etc.)\n        - Each heading in the source document becomes a line in the TOC\n        - The heading level is preserved in the output\n        - This creates a valid markdown document that can be rendered or processed further\n\n    Example: Generate a complete TOC\n        ```python\n        df.select(markdown.generate_toc(col(\"documentation\")))\n        ```\n\n    Example: Generate a simplified TOC with only top 2 levels\n        ```python\n        df.select(markdown.generate_toc(col(\"documentation\"), max_level=2))\n        ```\n\n    Example: Add TOC as a new column\n        ```python\n        df = df.with_column(\"toc\", markdown.generate_toc(col(\"content\"), max_level=3))\n        ```\n    \"\"\"\n    if max_level and (max_level &lt; 1 or max_level &gt; 6):\n        raise ValidationError(f\"max_level must be between 1 and 6 (inclusive), but got {max_level}. Use 1 for # headings, 2 for ## headings, etc.\")\n    return Column._from_logical_expr(\n        MdGenerateTocExpr(Column._from_col_or_name(column)._logical_expr, max_level)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/markdown/#fenic.api.functions.markdown.get_code_blocks","title":"get_code_blocks","text":"<pre><code>get_code_blocks(column: ColumnOrName, language_filter: Optional[str] = None) -&gt; Column\n</code></pre> <p>Extracts all code blocks from a column of Markdown-formatted strings.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Input column containing Markdown strings.</p> </li> <li> <code>language_filter</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional language filter to extract only code blocks with a specific language. By default, all code blocks are extracted.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column of code blocks. The output column type is: ArrayType(StructType([     StructField(\"language\", StringType),     StructField(\"code\", StringType), ]))</p> </li> </ul> Notes <ul> <li>Code blocks are parsed from fenced Markdown blocks (e.g., triple backticks ```).</li> <li>Language identifiers are optional and may be null if not provided in the original Markdown.</li> <li>Indented code blocks without fences are not currently supported.</li> <li>This function is useful for extracting embedded logic, configuration, or examples   from documentation or notebooks.</li> </ul> Extract all code blocks <pre><code>df.select(markdown.get_code_blocks(col(\"markdown_text\")))\n</code></pre> Explode code blocks into individual rows <pre><code># Explode the list of code blocks into individual rows\ndf = df.explode(df.with_column(\"blocks\", markdown.get_code_blocks(col(\"md\"))))\ndf = df.select(col(\"blocks\")[\"language\"], col(\"blocks\")[\"code\"])\n</code></pre> Source code in <code>src/fenic/api/functions/markdown.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef get_code_blocks(column: ColumnOrName, language_filter: Optional[str] = None) -&gt; Column:\n    \"\"\"Extracts all code blocks from a column of Markdown-formatted strings.\n\n    Args:\n        column (ColumnOrName): Input column containing Markdown strings.\n        language_filter (Optional[str]): Optional language filter to extract only code blocks with a specific language. By default, all code blocks are extracted.\n\n    Returns:\n        Column: A column of code blocks. The output column type is:\n            ArrayType(StructType([\n                StructField(\"language\", StringType),\n                StructField(\"code\", StringType),\n            ]))\n\n    Notes:\n        - Code blocks are parsed from fenced Markdown blocks (e.g., triple backticks ```).\n        - Language identifiers are optional and may be null if not provided in the original Markdown.\n        - Indented code blocks without fences are not currently supported.\n        - This function is useful for extracting embedded logic, configuration, or examples\n          from documentation or notebooks.\n\n    Example: Extract all code blocks\n        ```python\n        df.select(markdown.get_code_blocks(col(\"markdown_text\")))\n        ```\n\n    Example: Explode code blocks into individual rows\n        ```python\n        # Explode the list of code blocks into individual rows\n        df = df.explode(df.with_column(\"blocks\", markdown.get_code_blocks(col(\"md\"))))\n        df = df.select(col(\"blocks\")[\"language\"], col(\"blocks\")[\"code\"])\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        MdGetCodeBlocksExpr(Column._from_col_or_name(column)._logical_expr, language_filter)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/markdown/#fenic.api.functions.markdown.to_json","title":"to_json","text":"<pre><code>to_json(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Converts a column of Markdown-formatted strings into a hierarchical JSON representation.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Input column containing Markdown strings.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column of JSON-formatted strings representing the structured document tree.</p> </li> </ul> Notes <ul> <li>This function parses Markdown into a structured JSON format optimized for document chunking,   semantic analysis, and <code>jq</code> queries.</li> <li>The output conforms to a custom schema that organizes content into nested sections based   on heading levels. This makes it more expressive than flat ASTs like <code>mdast</code>.</li> <li>The full JSON schema is available at: TODO: link from docs.</li> </ul> Supported Markdown Features <ul> <li>Headings with nested hierarchy (e.g., h2 \u2192 h3 \u2192 h4)</li> <li>Paragraphs with inline formatting (bold, italics, links, code, etc.)</li> <li>Lists (ordered, unordered, task lists)</li> <li>Tables with header alignment and inline content</li> <li>Code blocks with language info</li> <li>Blockquotes, horizontal rules, and inline/flow HTML</li> </ul> Convert markdown to JSON <pre><code>df.select(markdown.to_json(col(\"markdown_text\")))\n</code></pre> Extract all level-2 headings with jq <pre><code># Combine with jq to extract all level-2 headings\ndf.select(json.jq(markdown.to_json(col(\"md\")), \".. | select(.type == 'heading' and .level == 2)\"))\n</code></pre> Source code in <code>src/fenic/api/functions/markdown.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef to_json(column: ColumnOrName) -&gt; Column:\n    \"\"\"Converts a column of Markdown-formatted strings into a hierarchical JSON representation.\n\n    Args:\n        column (ColumnOrName): Input column containing Markdown strings.\n\n    Returns:\n        Column: A column of JSON-formatted strings representing the structured document tree.\n\n    Notes:\n        - This function parses Markdown into a structured JSON format optimized for document chunking,\n          semantic analysis, and `jq` queries.\n        - The output conforms to a custom schema that organizes content into nested sections based\n          on heading levels. This makes it more expressive than flat ASTs like `mdast`.\n        - The full JSON schema is available at: TODO: link from docs.\n\n    Supported Markdown Features:\n        - Headings with nested hierarchy (e.g., h2 \u2192 h3 \u2192 h4)\n        - Paragraphs with inline formatting (bold, italics, links, code, etc.)\n        - Lists (ordered, unordered, task lists)\n        - Tables with header alignment and inline content\n        - Code blocks with language info\n        - Blockquotes, horizontal rules, and inline/flow HTML\n\n    Example: Convert markdown to JSON\n        ```python\n        df.select(markdown.to_json(col(\"markdown_text\")))\n        ```\n\n    Example: Extract all level-2 headings with jq\n        ```python\n        # Combine with jq to extract all level-2 headings\n        df.select(json.jq(markdown.to_json(col(\"md\")), \".. | select(.type == 'heading' and .level == 2)\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        MdToJsonExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/semantic/","title":"fenic.api.functions.semantic","text":""},{"location":"reference/fenic/api/functions/semantic/#fenic.api.functions.semantic","title":"fenic.api.functions.semantic","text":"<p>Semantic functions for Fenic DataFrames - LLM-based operations.</p> <p>Functions:</p> <ul> <li> <code>analyze_sentiment</code>             \u2013              <p>Analyzes the sentiment of a string column. Returns one of 'positive', 'negative', or 'neutral'.</p> </li> <li> <code>classify</code>             \u2013              <p>Classifies a string column into one of the provided labels.</p> </li> <li> <code>embed</code>             \u2013              <p>Generate embeddings for the specified string column.</p> </li> <li> <code>extract</code>             \u2013              <p>Extracts structured information from unstructured text using a provided schema.</p> </li> <li> <code>map</code>             \u2013              <p>Applies a natural language instruction to one or more text columns, enabling rich summarization and generation tasks.</p> </li> <li> <code>predicate</code>             \u2013              <p>Applies a natural language predicate to one or more string columns, returning a boolean result.</p> </li> <li> <code>reduce</code>             \u2013              <p>Aggregate function: reduces a set of strings across columns into a single string using a natural language instruction.</p> </li> </ul>"},{"location":"reference/fenic/api/functions/semantic/#fenic.api.functions.semantic.analyze_sentiment","title":"analyze_sentiment","text":"<pre><code>analyze_sentiment(column: ColumnOrName, model_alias: Optional[str] = None, temperature: float = 0) -&gt; Column\n</code></pre> <p>Analyzes the sentiment of a string column. Returns one of 'positive', 'negative', or 'neutral'.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name containing text for sentiment analysis.</p> </li> <li> <code>model_alias</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.</p> </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Optional temperature parameter for the language model. If None, will use the default temperature (0.0).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>Expression containing sentiment results ('positive', 'negative', or 'neutral').</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If column is invalid or cannot be resolved.</p> </li> </ul> Analyzing the sentiment of a user comment <pre><code>semantic.analyze_sentiment(col('user_comment'))\n</code></pre> Source code in <code>src/fenic/api/functions/semantic.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef analyze_sentiment(\n        column: ColumnOrName,\n        model_alias: Optional[str] = None,\n        temperature: float = 0,\n) -&gt; Column:\n    \"\"\"Analyzes the sentiment of a string column. Returns one of 'positive', 'negative', or 'neutral'.\n\n    Args:\n        column: Column or column name containing text for sentiment analysis.\n        model_alias: Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.\n        temperature: Optional temperature parameter for the language model. If None, will use the default temperature (0.0).\n\n    Returns:\n        Column: Expression containing sentiment results ('positive', 'negative', or 'neutral').\n\n    Raises:\n        ValueError: If column is invalid or cannot be resolved.\n\n    Example: Analyzing the sentiment of a user comment\n        ```python\n        semantic.analyze_sentiment(col('user_comment'))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        AnalyzeSentimentExpr(\n            Column._from_col_or_name(column)._logical_expr,\n            model_alias=model_alias,\n            temperature=temperature,\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/semantic/#fenic.api.functions.semantic.classify","title":"classify","text":"<pre><code>classify(column: ColumnOrName, labels: List[str] | type[Enum], examples: Optional[ClassifyExampleCollection] = None, model_alias: Optional[str] = None, temperature: float = 0) -&gt; Column\n</code></pre> <p>Classifies a string column into one of the provided labels.</p> <p>This is useful for tagging incoming documents with predefined categories.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name containing text to classify.</p> </li> <li> <code>labels</code>               (<code>List[str] | type[Enum]</code>)           \u2013            <p>List of category strings or an Enum defining the categories to classify the text into.</p> </li> <li> <code>examples</code>               (<code>Optional[ClassifyExampleCollection]</code>, default:                   <code>None</code> )           \u2013            <p>Optional collection of example classifications to guide the model. Examples should be created using ClassifyExampleCollection.create_example(), with instruction variables mapped to their expected classifications.</p> </li> <li> <code>model_alias</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.</p> </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Optional temperature parameter for the language model. If None, will use the default temperature (0.0).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>Expression containing the classification results.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If column is invalid or categories is not a list of strings.</p> </li> </ul> Categorizing incoming support requests <pre><code># Categorize incoming support requests\nsemantic.classify(\"message\", [\"Account Access\", \"Billing Issue\", \"Technical Problem\"])\n</code></pre> Categorizing incoming support requests with examples <pre><code>examples = ClassifyExampleCollection()\nexamples.create_example(ClassifyExample(\n    input=\"I can't reset my password or access my account.\",\n    output=\"Account Access\"))\nexamples.create_example(ClassifyExample(\n    input=\"You charged me twice for the same month.\",\n    output=\"Billing Issue\"))\nsemantic.classify(\"message\", [\"Account Access\", \"Billing Issue\", \"Technical Problem\"], examples)\n</code></pre> Source code in <code>src/fenic/api/functions/semantic.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef classify(\n        column: ColumnOrName,\n        labels: List[str] | type[Enum],\n        examples: Optional[ClassifyExampleCollection] = None,\n        model_alias: Optional[str] = None,\n        temperature: float = 0,\n) -&gt; Column:\n    \"\"\"Classifies a string column into one of the provided labels.\n\n    This is useful for tagging incoming documents with predefined categories.\n\n    Args:\n        column: Column or column name containing text to classify.\n\n        labels: List of category strings or an Enum defining the categories to classify the text into.\n\n        examples: Optional collection of example classifications to guide the model.\n            Examples should be created using ClassifyExampleCollection.create_example(),\n            with instruction variables mapped to their expected classifications.\n\n        model_alias: Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.\n\n        temperature: Optional temperature parameter for the language model. If None, will use the default temperature (0.0).\n\n    Returns:\n        Column: Expression containing the classification results.\n\n    Raises:\n        ValueError: If column is invalid or categories is not a list of strings.\n\n    Example: Categorizing incoming support requests\n        ```python\n        # Categorize incoming support requests\n        semantic.classify(\"message\", [\"Account Access\", \"Billing Issue\", \"Technical Problem\"])\n        ```\n\n    Example: Categorizing incoming support requests with examples\n        ```python\n        examples = ClassifyExampleCollection()\n        examples.create_example(ClassifyExample(\n            input=\"I can't reset my password or access my account.\",\n            output=\"Account Access\"))\n        examples.create_example(ClassifyExample(\n            input=\"You charged me twice for the same month.\",\n            output=\"Billing Issue\"))\n        semantic.classify(\"message\", [\"Account Access\", \"Billing Issue\", \"Technical Problem\"], examples)\n        ```\n    \"\"\"\n    if isinstance(labels, List) and len(labels) == 0:\n        raise ValueError(\n            f\"Must specify the categories for classification, found: {len(labels)} categories\"\n        )\n    return Column._from_logical_expr(\n        SemanticClassifyExpr(\n            Column._from_col_or_name(column)._logical_expr,\n            labels,\n            examples=examples,\n            model_alias=model_alias,\n            temperature=temperature,\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/semantic/#fenic.api.functions.semantic.embed","title":"embed","text":"<pre><code>embed(column: ColumnOrName, model_alias: Optional[str] = None) -&gt; Column\n</code></pre> <p>Generate embeddings for the specified string column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column or column name containing the values to generate embeddings for.</p> </li> <li> <code>model_alias</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional alias for the embedding model to use for the mapping. If None, will use the embedding model configured as the default.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code>           \u2013            <p>A Column expression that represents the embeddings for each value in the input column</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the input column is not a string column.</p> </li> </ul> Generate embeddings for a text column <pre><code>df.select(semantic.embed(col(\"text_column\")).alias(\"text_embeddings\"))\n</code></pre> Source code in <code>src/fenic/api/functions/semantic.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef embed(\n    column: ColumnOrName,\n    model_alias: Optional[str] = None,\n) -&gt; Column:\n    \"\"\"Generate embeddings for the specified string column.\n\n    Args:\n        column: Column or column name containing the values to generate embeddings for.\n        model_alias: Optional alias for the embedding model to use for the mapping.\n            If None, will use the embedding model configured as the default.\n\n\n    Returns:\n        A Column expression that represents the embeddings for each value in the input column\n\n    Raises:\n        TypeError: If the input column is not a string column.\n\n    Example: Generate embeddings for a text column\n        ```python\n        df.select(semantic.embed(col(\"text_column\")).alias(\"text_embeddings\"))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        EmbeddingsExpr(Column._from_col_or_name(column)._logical_expr, model_alias=model_alias)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/semantic/#fenic.api.functions.semantic.extract","title":"extract","text":"<pre><code>extract(column: ColumnOrName, schema: Union[ExtractSchema, Type[BaseModel]], max_output_tokens: int = 1024, temperature: float = 0, model_alias: Optional[str] = None) -&gt; Column\n</code></pre> <p>Extracts structured information from unstructured text using a provided schema.</p> <p>This function applies an instruction-driven extraction process to text columns, returning structured data based on the fields and descriptions provided. Useful for pulling out key entities, facts, or labels from documents.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Column containing text to extract from.</p> </li> <li> <code>schema</code>               (<code>Union[ExtractSchema, Type[BaseModel]]</code>)           \u2013            <p>An ExtractSchema containing fields of type ExtractSchemaField that define the output structure and field descriptions or a Pydantic model that defines the output structure with descriptions for each field.</p> </li> <li> <code>model_alias</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.</p> </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Optional temperature parameter for the language model. If None, will use the default temperature (0.0).</p> </li> <li> <code>max_output_tokens</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>Optional parameter to constrain the model to generate at most this many tokens. If None, fenic will calculate the expected max tokens, based on the model's context length and other operator-specific parameters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A new column with structured values (a struct) based on the provided schema.</p> </li> </ul> Extracting product metadata from a description using an explict ExtractSchema <pre><code>schema = ExtractSchema([\n     ExtractSchemaField(\n         name=\"brand\",\n         data_type=DataType.STRING,\n         description=\"The brand or manufacturer mentioned in the product description\"\n     ),\n     ExtractSchemaField(\n         name=\"capacity_gb\",\n         data_type=DataType.INTEGER,\n         description=\"The storage capacity of the product in gigabytes, if mentioned\"\n     ),\n     ExtractSchemaField(\n         name=\"connectivity\",\n         data_type=DataType.STRING,\n         description=\"The type of connectivity or ports described (e.g., USB-C, Thunderbolt)\"\n     )\n ])\ndf.select(semantic.extract(\"product_description\", schema))\n</code></pre> Extracting user intent from a support message using a Pydantic model <pre><code>class UserRequest(BaseModel):\n    request_type: str = Field(..., description=\"The type of request (e.g., refund, technical issue, setup help)\")\n    target_product: str = Field(..., description=\"The name or type of product the user is referring to\")\n    preferred_resolution: str = Field(..., description=\"The action the user is expecting (e.g., replacement, callback)\")\n\ndf.select(semantic.extract(\"support_message\", UserRequest))\n</code></pre> <p>Raises:     ValueError: If any input expression is invalid, or if the schema         is empty or invalid, or if the schema contains fields with no descriptions.</p> Source code in <code>src/fenic/api/functions/semantic.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef extract(\n        column: ColumnOrName,\n        schema: Union[ExtractSchema, Type[BaseModel]],\n        max_output_tokens: int = 1024,\n        temperature: float = 0,\n        model_alias: Optional[str] = None,\n) -&gt; Column:\n    \"\"\"Extracts structured information from unstructured text using a provided schema.\n\n    This function applies an instruction-driven extraction process to text columns, returning\n    structured data based on the fields and descriptions provided. Useful for pulling out key entities,\n    facts, or labels from documents.\n\n    Args:\n        column: Column containing text to extract from.\n        schema: An ExtractSchema containing fields of type ExtractSchemaField that define\n            the output structure and field descriptions or a Pydantic model that defines the output structure with\n            descriptions for each field.\n        model_alias: Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.\n        temperature: Optional temperature parameter for the language model. If None, will use the default temperature (0.0).\n        max_output_tokens: Optional parameter to constrain the model to generate at most this many tokens. If None, fenic will calculate the expected max\n            tokens, based on the model's context length and other operator-specific parameters.\n\n    Returns:\n        Column: A new column with structured values (a struct) based on the provided schema.\n\n    Example: Extracting product metadata from a description using an explict ExtractSchema\n        ```python\n        schema = ExtractSchema([\n             ExtractSchemaField(\n                 name=\"brand\",\n                 data_type=DataType.STRING,\n                 description=\"The brand or manufacturer mentioned in the product description\"\n             ),\n             ExtractSchemaField(\n                 name=\"capacity_gb\",\n                 data_type=DataType.INTEGER,\n                 description=\"The storage capacity of the product in gigabytes, if mentioned\"\n             ),\n             ExtractSchemaField(\n                 name=\"connectivity\",\n                 data_type=DataType.STRING,\n                 description=\"The type of connectivity or ports described (e.g., USB-C, Thunderbolt)\"\n             )\n         ])\n        df.select(semantic.extract(\"product_description\", schema))\n        ```\n\n    Example: Extracting user intent from a support message using a Pydantic model\n        ```python\n        class UserRequest(BaseModel):\n            request_type: str = Field(..., description=\"The type of request (e.g., refund, technical issue, setup help)\")\n            target_product: str = Field(..., description=\"The name or type of product the user is referring to\")\n            preferred_resolution: str = Field(..., description=\"The action the user is expecting (e.g., replacement, callback)\")\n\n        df.select(semantic.extract(\"support_message\", UserRequest))\n        ```\n    Raises:\n        ValueError: If any input expression is invalid, or if the schema\n            is empty or invalid, or if the schema contains fields with no descriptions.\n    \"\"\"\n    validate_extract_schema_structure(schema)\n\n    pydantic_model = (\n        convert_extract_schema_to_pydantic_type(schema)\n        if isinstance(schema, ExtractSchema)\n        else schema\n    )\n\n    return Column._from_logical_expr(\n        SemanticExtractExpr(\n            Column._from_col_or_name(column)._logical_expr,\n            max_tokens=max_output_tokens,\n            temperature=temperature,\n            model_alias=model_alias,\n            schema=pydantic_model,\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/semantic/#fenic.api.functions.semantic.map","title":"map","text":"<pre><code>map(instruction: str, examples: Optional[MapExampleCollection] = None, model_alias: Optional[str] = None, temperature: float = 0, max_output_tokens: int = 512) -&gt; Column\n</code></pre> <p>Applies a natural language instruction to one or more text columns, enabling rich summarization and generation tasks.</p> <p>Parameters:</p> <ul> <li> <code>instruction</code>               (<code>str</code>)           \u2013            <p>A string containing the semantic.map prompt. The instruction must include placeholders in curly braces that reference one or more column names. These placeholders will be replaced with actual column values during prompt construction during query execution.</p> </li> <li> <code>examples</code>               (<code>Optional[MapExampleCollection]</code>, default:                   <code>None</code> )           \u2013            <p>Optional collection of examples to guide the semantic mapping operation. Each example should demonstrate the expected input and output for the mapping. The examples should be created using MapExampleCollection.create_example(), providing instruction variables and their expected answers.</p> </li> <li> <code>model_alias</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.</p> </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Optional temperature parameter for the language model. If None, will use the default temperature (0.0).</p> </li> <li> <code>max_output_tokens</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Optional parameter to constrain the model to generate at most this many tokens. If None, fenic will calculate the expected max tokens, based on the model's context length and other operator-specific parameters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column expression representing the semantic mapping operation.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the instruction is not a string.</p> </li> </ul> Mapping without examples <pre><code>semantic.map(\"Given the product name: {name} and its description: {details}, generate a compelling one-line description suitable for a product catalog.\", examples)\n</code></pre> Mapping with few-shot examples <pre><code>examples = MapExampleCollection()\nexamples.create_example(MapExample(\n    input={\"name\": \"GlowMate\", \"details\": \"A rechargeable bedside lamp with adjustable color temperatures, touch controls, and a sleek minimalist design.\"},\n    output=\"The modern touch-controlled lamp for better sleep and style.\"\n))\nexamples.create_example(MapExample(\n    input={\"name\": \"AquaPure\", \"details\": \"A compact water filter that attaches to your faucet, removes over 99% of contaminants, and improves taste instantly.\"},\n    output=\"Clean, great-tasting water straight from your tap.\"\n))\nsemantic.map(\"Given the product name: {name} and its description: {details}, generate a compelling one-line description suitable for a product catalog.\", examples)\n</code></pre> Source code in <code>src/fenic/api/functions/semantic.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True, strict=True))\ndef map(\n        instruction: str,\n        examples: Optional[MapExampleCollection] = None,\n        model_alias: Optional[str] = None,\n        temperature: float = 0,\n        max_output_tokens: int = 512,\n) -&gt; Column:\n    \"\"\"Applies a natural language instruction to one or more text columns, enabling rich summarization and generation tasks.\n\n    Args:\n        instruction: A string containing the semantic.map prompt.\n            The instruction must include placeholders in curly braces that reference one or more column names.\n            These placeholders will be replaced with actual column values during prompt construction during\n            query execution.\n        examples: Optional collection of examples to guide the semantic mapping operation.\n            Each example should demonstrate the expected input and output for the mapping.\n            The examples should be created using MapExampleCollection.create_example(),\n            providing instruction variables and their expected answers.\n        model_alias: Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.\n        temperature: Optional temperature parameter for the language model. If None, will use the default temperature (0.0).\n        max_output_tokens: Optional parameter to constrain the model to generate at most this many tokens. If None, fenic will calculate the expected max\n            tokens, based on the model's context length and other operator-specific parameters.\n\n    Returns:\n        Column: A column expression representing the semantic mapping operation.\n\n    Raises:\n        ValueError: If the instruction is not a string.\n\n    Example: Mapping without examples\n        ```python\n        semantic.map(\"Given the product name: {name} and its description: {details}, generate a compelling one-line description suitable for a product catalog.\", examples)\n        ```\n\n    Example: Mapping with few-shot examples\n        ```python\n        examples = MapExampleCollection()\n        examples.create_example(MapExample(\n            input={\"name\": \"GlowMate\", \"details\": \"A rechargeable bedside lamp with adjustable color temperatures, touch controls, and a sleek minimalist design.\"},\n            output=\"The modern touch-controlled lamp for better sleep and style.\"\n        ))\n        examples.create_example(MapExample(\n            input={\"name\": \"AquaPure\", \"details\": \"A compact water filter that attaches to your faucet, removes over 99% of contaminants, and improves taste instantly.\"},\n            output=\"Clean, great-tasting water straight from your tap.\"\n        ))\n        semantic.map(\"Given the product name: {name} and its description: {details}, generate a compelling one-line description suitable for a product catalog.\", examples)\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        SemanticMapExpr(\n            instruction,\n            examples=examples,\n            max_tokens=max_output_tokens,\n            model_alias=model_alias,\n            temperature=temperature,\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/semantic/#fenic.api.functions.semantic.predicate","title":"predicate","text":"<pre><code>predicate(instruction: str, examples: Optional[PredicateExampleCollection] = None, model_alias: Optional[str] = None, temperature: float = 0) -&gt; Column\n</code></pre> <p>Applies a natural language predicate to one or more string columns, returning a boolean result.</p> <p>This is useful for filtering rows based on user-defined criteria expressed in natural language.</p> <p>Parameters:</p> <ul> <li> <code>instruction</code>               (<code>str</code>)           \u2013            <p>A string containing the semantic.predicate prompt. The instruction must include placeholders in curly braces that reference one or more column names. These placeholders will be replaced with actual column values during prompt construction during query execution.</p> </li> <li> <code>examples</code>               (<code>Optional[PredicateExampleCollection]</code>, default:                   <code>None</code> )           \u2013            <p>Optional collection of examples to guide the semantic predicate operation. Each example should demonstrate the expected boolean output for different inputs. The examples should be created using PredicateExampleCollection.create_example(), providing instruction variables and their expected boolean answers.</p> </li> <li> <code>model_alias</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.</p> </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Optional temperature parameter for the language model. If None, will use the default temperature (0.0).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column expression that returns a boolean value after applying the natural language predicate.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the instruction is not a string.</p> </li> </ul> Identifying product descriptions that mention wireless capability <pre><code>semantic.predicate(\"Does the product description: {product_description} mention that the item is wireless?\")\n</code></pre> Filtering support tickets that describe a billing issue <pre><code>semantic.predicate(\"Does this support message: {ticket_text} describe a billing issue?\")\n</code></pre> Filtering support tickets that describe a billing issue with examples <pre><code>examples = PredicateExampleCollection()\nexamples.create_example(PredicateExample(\n    input={\"ticket_text\": \"I was charged twice for my subscription and need help.\"},\n    output=True))\nexamples.create_example(PredicateExample(\n    input={\"ticket_text\": \"How do I reset my password?\"},\n    output=False))\nsemantic.predicate(\"Does this support ticket describe a billing issue? {ticket_text}\", examples)\n</code></pre> Source code in <code>src/fenic/api/functions/semantic.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True, strict=True))\ndef predicate(\n        instruction: str,\n        examples: Optional[PredicateExampleCollection] = None,\n        model_alias: Optional[str] = None,\n        temperature: float = 0,\n) -&gt; Column:\n    \"\"\"Applies a natural language predicate to one or more string columns, returning a boolean result.\n\n    This is useful for filtering rows based on user-defined criteria expressed in natural language.\n\n    Args:\n        instruction: A string containing the semantic.predicate prompt.\n            The instruction must include placeholders in curly braces that reference one or more column names.\n            These placeholders will be replaced with actual column values during prompt construction during\n            query execution.\n        examples: Optional collection of examples to guide the semantic predicate operation.\n            Each example should demonstrate the expected boolean output for different inputs.\n            The examples should be created using PredicateExampleCollection.create_example(),\n            providing instruction variables and their expected boolean answers.\n        model_alias: Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.\n        temperature: Optional temperature parameter for the language model. If None, will use the default temperature (0.0).\n\n    Returns:\n        Column: A column expression that returns a boolean value after applying the natural language predicate.\n\n    Raises:\n        ValueError: If the instruction is not a string.\n\n    Example: Identifying product descriptions that mention wireless capability\n        ```python\n        semantic.predicate(\"Does the product description: {product_description} mention that the item is wireless?\")\n        ```\n\n    Example: Filtering support tickets that describe a billing issue\n        ```python\n        semantic.predicate(\"Does this support message: {ticket_text} describe a billing issue?\")\n        ```\n\n    Example: Filtering support tickets that describe a billing issue with examples\n        ```python\n        examples = PredicateExampleCollection()\n        examples.create_example(PredicateExample(\n            input={\"ticket_text\": \"I was charged twice for my subscription and need help.\"},\n            output=True))\n        examples.create_example(PredicateExample(\n            input={\"ticket_text\": \"How do I reset my password?\"},\n            output=False))\n        semantic.predicate(\"Does this support ticket describe a billing issue? {ticket_text}\", examples)\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        SemanticPredExpr(\n            instruction,\n            examples=examples,\n            model_alias=model_alias,\n            temperature=temperature,\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/semantic/#fenic.api.functions.semantic.reduce","title":"reduce","text":"<pre><code>reduce(instruction: str, model_alias: Optional[str] = None, temperature: float = 0, max_output_tokens: int = 512) -&gt; Column\n</code></pre> <p>Aggregate function: reduces a set of strings across columns into a single string using a natural language instruction.</p> <p>Parameters:</p> <ul> <li> <code>instruction</code>               (<code>str</code>)           \u2013            <p>A string containing the semantic.reduce prompt. The instruction can include placeholders in curly braces that reference column names. These placeholders will be replaced with actual column values during prompt construction during query execution.</p> </li> <li> <code>model_alias</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.</p> </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Optional temperature parameter for the language model. If None, will use the default temperature (0.0).</p> </li> <li> <code>max_output_tokens</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Optional parameter to constrain the model to generate at most this many tokens. If None, fenic will calculate the expected max tokens, based on the model's context length and other operator-specific parameters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column expression representing the semantic reduction operation.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the instruction is not a string.</p> </li> </ul> Summarizing documents using their titles and bodies <pre><code>semantic.reduce(\"Summarize these documents using each document's title: {title} and body: {body}.\")\n</code></pre> Source code in <code>src/fenic/api/functions/semantic.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef reduce(\n        instruction: str,\n        model_alias: Optional[str] = None,\n        temperature: float = 0,\n        max_output_tokens: int = 512,\n) -&gt; Column:\n    \"\"\"Aggregate function: reduces a set of strings across columns into a single string using a natural language instruction.\n\n    Args:\n        instruction: A string containing the semantic.reduce prompt.\n            The instruction can include placeholders in curly braces that reference column names.\n            These placeholders will be replaced with actual column values during prompt construction during\n            query execution.\n        model_alias: Optional alias for the language model to use for the mapping. If None, will use the language model configured as the default.\n        temperature: Optional temperature parameter for the language model. If None, will use the default temperature (0.0).\n        max_output_tokens: Optional parameter to constrain the model to generate at most this many tokens. If None, fenic will calculate the expected max\n            tokens, based on the model's context length and other operator-specific parameters.\n\n    Returns:\n        Column: A column expression representing the semantic reduction operation.\n\n    Raises:\n        ValueError: If the instruction is not a string.\n\n    Example: Summarizing documents using their titles and bodies\n        ```python\n        semantic.reduce(\"Summarize these documents using each document's title: {title} and body: {body}.\")\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        SemanticReduceExpr(\n            instruction,\n            max_tokens=max_output_tokens,\n            model_alias=model_alias,\n            temperature=temperature,\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/","title":"fenic.api.functions.text","text":""},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text","title":"fenic.api.functions.text","text":"<p>Text manipulation functions for Fenic DataFrames.</p> <p>Functions:</p> <ul> <li> <code>array_join</code>             \u2013              <p>Joins an array of strings into a single string with a delimiter.</p> </li> <li> <code>btrim</code>             \u2013              <p>Remove specified characters from both sides of strings in a column.</p> </li> <li> <code>byte_length</code>             \u2013              <p>Calculate the byte length of each string in the column.</p> </li> <li> <code>character_chunk</code>             \u2013              <p>Chunks a string column into chunks of a specified size (in characters) with an optional overlap.</p> </li> <li> <code>concat</code>             \u2013              <p>Concatenates multiple columns or strings into a single string.</p> </li> <li> <code>concat_ws</code>             \u2013              <p>Concatenates multiple columns or strings into a single string with a separator.</p> </li> <li> <code>count_tokens</code>             \u2013              <p>Returns the number of tokens in a string using OpenAI's cl100k_base encoding (tiktoken).</p> </li> <li> <code>extract</code>             \u2013              <p>Extracts fields from text using a template pattern.</p> </li> <li> <code>length</code>             \u2013              <p>Calculate the character length of each string in the column.</p> </li> <li> <code>lower</code>             \u2013              <p>Convert all characters in a string column to lowercase.</p> </li> <li> <code>ltrim</code>             \u2013              <p>Remove whitespace from the start of strings in a column.</p> </li> <li> <code>parse_transcript</code>             \u2013              <p>Parses a transcript from text to a structured format with unified schema.</p> </li> <li> <code>recursive_character_chunk</code>             \u2013              <p>Chunks a string column into chunks of a specified size (in characters) with an optional overlap.</p> </li> <li> <code>recursive_token_chunk</code>             \u2013              <p>Chunks a string column into chunks of a specified size (in tokens) with an optional overlap.</p> </li> <li> <code>recursive_word_chunk</code>             \u2013              <p>Chunks a string column into chunks of a specified size (in words) with an optional overlap.</p> </li> <li> <code>regexp_replace</code>             \u2013              <p>Replace all occurrences of a pattern with a new string, treating pattern as a regular expression.</p> </li> <li> <code>replace</code>             \u2013              <p>Replace all occurrences of a pattern with a new string, treating pattern as a literal string.</p> </li> <li> <code>rtrim</code>             \u2013              <p>Remove whitespace from the end of strings in a column.</p> </li> <li> <code>split</code>             \u2013              <p>Split a string column into an array using a regular expression pattern.</p> </li> <li> <code>split_part</code>             \u2013              <p>Split a string and return a specific part using 1-based indexing.</p> </li> <li> <code>title_case</code>             \u2013              <p>Convert the first character of each word in a string column to uppercase.</p> </li> <li> <code>token_chunk</code>             \u2013              <p>Chunks a string column into chunks of a specified size (in tokens) with an optional overlap.</p> </li> <li> <code>trim</code>             \u2013              <p>Remove whitespace from both sides of strings in a column.</p> </li> <li> <code>upper</code>             \u2013              <p>Convert all characters in a string column to uppercase.</p> </li> <li> <code>word_chunk</code>             \u2013              <p>Chunks a string column into chunks of a specified size (in words) with an optional overlap.</p> </li> </ul>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.array_join","title":"array_join","text":"<pre><code>array_join(column: ColumnOrName, delimiter: str) -&gt; Column\n</code></pre> <p>Joins an array of strings into a single string with a delimiter.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The column to join</p> </li> <li> <code>delimiter</code>               (<code>str</code>)           \u2013            <p>The delimiter to use</p> </li> </ul> <p>Returns:         Column: A column containing the joined strings</p> Join array with comma <pre><code># Join array elements with comma\ndf.select(text.array_join(col(\"array_column\"), \",\"))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef array_join(column: ColumnOrName, delimiter: str) -&gt; Column:\n    \"\"\"Joins an array of strings into a single string with a delimiter.\n\n    Args:\n        column: The column to join\n        delimiter: The delimiter to use\n    Returns:\n            Column: A column containing the joined strings\n\n    Example: Join array with comma\n        ```python\n        # Join array elements with comma\n        df.select(text.array_join(col(\"array_column\"), \",\"))\n        ```\n    \"\"\"\n    if not isinstance(delimiter, str):\n        raise TypeError(\n            f\"`array_join` expects a string for the delimiter, but got {type(delimiter).__name__}.\"\n        )\n    return Column._from_logical_expr(\n        ArrayJoinExpr(Column._from_col_or_name(column)._logical_expr, delimiter)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.btrim","title":"btrim","text":"<pre><code>btrim(col: ColumnOrName, trim: Optional[Union[Column, str]]) -&gt; Column\n</code></pre> <p>Remove specified characters from both sides of strings in a column.</p> <p>This function removes all occurrences of the specified characters from both the beginning and end of each string in the column. If trim is a column expression, the characters to remove are determined dynamically from the values in that column.</p> <p>Parameters:</p> <ul> <li> <code>col</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column or column name to trim</p> </li> <li> <code>trim</code>               (<code>Optional[Union[Column, str]]</code>)           \u2013            <p>The characters to remove from both sides (Default: whitespace)   Can be a string or column expression.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the trimmed strings</p> </li> </ul> Remove brackets from both sides <pre><code># Remove brackets from both sides of text\ndf.select(text.btrim(col(\"text\"), \"[]\"))\n</code></pre> Remove characters specified in a column <pre><code># Remove characters specified in a column\ndf.select(text.btrim(col(\"text\"), col(\"chars\")))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef btrim(col: ColumnOrName, trim: Optional[Union[Column, str]]) -&gt; Column:\n    \"\"\"Remove specified characters from both sides of strings in a column.\n\n    This function removes all occurrences of the specified characters from\n    both the beginning and end of each string in the column.\n    If trim is a column expression, the characters to remove are determined dynamically\n    from the values in that column.\n\n    Args:\n        col: The input string column or column name to trim\n        trim: The characters to remove from both sides (Default: whitespace)\n              Can be a string or column expression.\n\n    Returns:\n        Column: A column containing the trimmed strings\n\n    Example: Remove brackets from both sides\n        ```python\n        # Remove brackets from both sides of text\n        df.select(text.btrim(col(\"text\"), \"[]\"))\n        ```\n\n    Example: Remove characters specified in a column\n        ```python\n        # Remove characters specified in a column\n        df.select(text.btrim(col(\"text\"), col(\"chars\")))\n        ```\n    \"\"\"\n    if isinstance(trim, Column):\n        trim = trim._logical_expr\n    return Column._from_logical_expr(\n        StripCharsExpr(Column._from_col_or_name(col)._logical_expr, trim, \"both\")\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.byte_length","title":"byte_length","text":"<pre><code>byte_length(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Calculate the byte length of each string in the column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column to calculate byte lengths for</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the byte length of each string</p> </li> </ul> Get byte lengths <pre><code># Get the byte length of each string in the name column\ndf.select(text.byte_length(col(\"name\")))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef byte_length(column: ColumnOrName) -&gt; Column:\n    \"\"\"Calculate the byte length of each string in the column.\n\n    Args:\n        column: The input string column to calculate byte lengths for\n\n    Returns:\n        Column: A column containing the byte length of each string\n\n    Example: Get byte lengths\n        ```python\n        # Get the byte length of each string in the name column\n        df.select(text.byte_length(col(\"name\")))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        ByteLengthExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.character_chunk","title":"character_chunk","text":"<pre><code>character_chunk(column: ColumnOrName, chunk_size: int, chunk_overlap_percentage: int = 0) -&gt; Column\n</code></pre> <p>Chunks a string column into chunks of a specified size (in characters) with an optional overlap.</p> <p>The chunking is done by applying a simple sliding window across the text to create chunks of equal size. This approach does not attempt to preserve the underlying structure of the text.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column or column name to chunk</p> </li> <li> <code>chunk_size</code>               (<code>int</code>)           \u2013            <p>The size of each chunk in characters</p> </li> <li> <code>chunk_overlap_percentage</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The overlap between chunks as a percentage of the chunk size (Default: 0)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the chunks as an array of strings</p> </li> </ul> Create character chunks <pre><code># Create chunks of 100 characters with 20% overlap\ndf.select(text.character_chunk(col(\"text\"), 100, 20))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef character_chunk(\n    column: ColumnOrName, chunk_size: int, chunk_overlap_percentage: int = 0\n) -&gt; Column:\n    \"\"\"Chunks a string column into chunks of a specified size (in characters) with an optional overlap.\n\n    The chunking is done by applying a simple sliding window across the text to create chunks of equal size.\n    This approach does not attempt to preserve the underlying structure of the text.\n\n    Args:\n        column: The input string column or column name to chunk\n        chunk_size: The size of each chunk in characters\n        chunk_overlap_percentage: The overlap between chunks as a percentage of the chunk size (Default: 0)\n\n    Returns:\n        Column: A column containing the chunks as an array of strings\n\n    Example: Create character chunks\n        ```python\n        # Create chunks of 100 characters with 20% overlap\n        df.select(text.character_chunk(col(\"text\"), 100, 20))\n        ```\n    \"\"\"\n    chunk_configuration = TextChunkExprConfiguration(\n        desired_chunk_size=chunk_size,\n        chunk_overlap_percentage=chunk_overlap_percentage,\n        chunk_length_function_name=ChunkLengthFunction.CHARACTER,\n    )\n    return Column._from_logical_expr(\n        TextChunkExpr(\n            Column._from_col_or_name(column)._logical_expr, chunk_configuration\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.concat","title":"concat","text":"<pre><code>concat(*cols: ColumnOrName) -&gt; Column\n</code></pre> <p>Concatenates multiple columns or strings into a single string.</p> <p>Parameters:</p> <ul> <li> <code>*cols</code>               (<code>ColumnOrName</code>, default:                   <code>()</code> )           \u2013            <p>Columns or strings to concatenate</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the concatenated strings</p> </li> </ul> Concatenate columns <pre><code># Concatenate two columns with a space in between\ndf.select(text.concat(col(\"col1\"), lit(\" \"), col(\"col2\")))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef concat(*cols: ColumnOrName) -&gt; Column:\n    \"\"\"Concatenates multiple columns or strings into a single string.\n\n    Args:\n        *cols: Columns or strings to concatenate\n\n    Returns:\n        Column: A column containing the concatenated strings\n\n    Example: Concatenate columns\n        ```python\n        # Concatenate two columns with a space in between\n        df.select(text.concat(col(\"col1\"), lit(\" \"), col(\"col2\")))\n        ```\n    \"\"\"\n    if not cols:\n        raise ValueError(\"At least one column must be provided to concat method\")\n\n    flattened_args = []\n    for arg in cols:\n        if isinstance(arg, (list, tuple)):\n            flattened_args.extend(arg)\n        else:\n            flattened_args.append(arg)\n\n    flattened_exprs = [\n        Column._from_col_or_name(c)._logical_expr for c in flattened_args\n    ]\n    return Column._from_logical_expr(ConcatExpr(flattened_exprs))\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.concat_ws","title":"concat_ws","text":"<pre><code>concat_ws(separator: str, *cols: ColumnOrName) -&gt; Column\n</code></pre> <p>Concatenates multiple columns or strings into a single string with a separator.</p> <p>Parameters:</p> <ul> <li> <code>separator</code>               (<code>str</code>)           \u2013            <p>The separator to use</p> </li> <li> <code>*cols</code>               (<code>ColumnOrName</code>, default:                   <code>()</code> )           \u2013            <p>Columns or strings to concatenate</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the concatenated strings</p> </li> </ul> Concatenate with comma separator <pre><code># Concatenate columns with comma separator\ndf.select(text.concat_ws(\",\", col(\"col1\"), col(\"col2\")))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef concat_ws(separator: str, *cols: ColumnOrName) -&gt; Column:\n    \"\"\"Concatenates multiple columns or strings into a single string with a separator.\n\n    Args:\n        separator: The separator to use\n        *cols: Columns or strings to concatenate\n\n    Returns:\n        Column: A column containing the concatenated strings\n\n    Example: Concatenate with comma separator\n        ```python\n        # Concatenate columns with comma separator\n        df.select(text.concat_ws(\",\", col(\"col1\"), col(\"col2\")))\n        ```\n    \"\"\"\n    if not cols:\n        raise ValueError(\"At least one column must be provided to concat_ws method\")\n\n    flattened_args = []\n    for arg in cols:\n        if isinstance(arg, (list, tuple)):\n            flattened_args.extend(arg)\n        else:\n            flattened_args.append(arg)\n\n    expr_args = []\n    for arg in flattened_args:\n        expr_args.append(Column._from_col_or_name(arg)._logical_expr)\n        expr_args.append(lit(separator)._logical_expr)\n    expr_args.pop()\n    return Column._from_logical_expr(ConcatExpr(expr_args))\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.count_tokens","title":"count_tokens","text":"<pre><code>count_tokens(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Returns the number of tokens in a string using OpenAI's cl100k_base encoding (tiktoken).</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column with the token counts for each input string.</p> </li> </ul> Count tokens in text <pre><code># Count tokens in a text column\ndf.select(text.count_tokens(col(\"text\")))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef count_tokens(\n    column: ColumnOrName,\n) -&gt; Column:\n    r\"\"\"Returns the number of tokens in a string using OpenAI's cl100k_base encoding (tiktoken).\n\n    Args:\n        column: The input string column.\n\n    Returns:\n        Column: A column with the token counts for each input string.\n\n    Example: Count tokens in text\n        ```python\n        # Count tokens in a text column\n        df.select(text.count_tokens(col(\"text\")))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        CountTokensExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.extract","title":"extract","text":"<pre><code>extract(column: ColumnOrName, template: str) -&gt; Column\n</code></pre> <p>Extracts fields from text using a template pattern.</p> <p>Parameters:</p> <ul> <li> <code>template</code>               (<code>str</code>)           \u2013            <p>Template string with fields marked as <code>${field_name:format}</code></p> </li> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>Input text column to extract from</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A struct column containing the extracted fields</p> </li> </ul> Basic field extraction <pre><code># Extract name and age from a text column\ndf.select(text.extract(col(\"text\"), \"Name: ${name:csv}, Age: ${age:none}\"))\n</code></pre> Multiple field extraction with different formats <pre><code># Extract multiple fields with different formats\ndf.select(text.extract(col(\"text\"), \"Product: ${product:csv}, Price: ${price:none}, Tags: ${tags:json}\"))\n</code></pre> Extract and filter based on extracted fields <pre><code># Extract and filter based on extracted fields\ndf = df.select(\n    col(\"text\"),\n    text.extract(col(\"text\"), \"Name: ${name:csv}, Age: ${age:none}\").alias(\"extracted\")\n)\ndf = df.filter(col(\"extracted\")[\"age\"] == \"30\")\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef extract(column: ColumnOrName, template: str) -&gt; Column:\n    \"\"\"Extracts fields from text using a template pattern.\n\n    Args:\n        template: Template string with fields marked as ``${field_name:format}``\n        column: Input text column to extract from\n\n    Returns:\n        Column: A struct column containing the extracted fields\n\n    Example: Basic field extraction\n        ```python\n        # Extract name and age from a text column\n        df.select(text.extract(col(\"text\"), \"Name: ${name:csv}, Age: ${age:none}\"))\n        ```\n\n    Example: Multiple field extraction with different formats\n        ```python\n        # Extract multiple fields with different formats\n        df.select(text.extract(col(\"text\"), \"Product: ${product:csv}, Price: ${price:none}, Tags: ${tags:json}\"))\n        ```\n\n    Example: Extract and filter based on extracted fields\n        ```python\n        # Extract and filter based on extracted fields\n        df = df.select(\n            col(\"text\"),\n            text.extract(col(\"text\"), \"Name: ${name:csv}, Age: ${age:none}\").alias(\"extracted\")\n        )\n        df = df.filter(col(\"extracted\")[\"age\"] == \"30\")\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        TextractExpr(Column._from_col_or_name(column)._logical_expr,template)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.length","title":"length","text":"<pre><code>length(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Calculate the character length of each string in the column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column to calculate lengths for</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the length of each string in characters</p> </li> </ul> Get string lengths <pre><code># Get the length of each string in the name column\ndf.select(text.length(col(\"name\")))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef length(column: ColumnOrName) -&gt; Column:\n    \"\"\"Calculate the character length of each string in the column.\n\n    Args:\n        column: The input string column to calculate lengths for\n\n    Returns:\n        Column: A column containing the length of each string in characters\n\n    Example: Get string lengths\n        ```python\n        # Get the length of each string in the name column\n        df.select(text.length(col(\"name\")))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        StrLengthExpr(Column._from_col_or_name(column)._logical_expr)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.lower","title":"lower","text":"<pre><code>lower(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Convert all characters in a string column to lowercase.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column to convert to lowercase</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the lowercase strings</p> </li> </ul> Convert text to lowercase <pre><code># Convert all text in the name column to lowercase\ndf.select(text.lower(col(\"name\")))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef lower(column: ColumnOrName) -&gt; Column:\n    \"\"\"Convert all characters in a string column to lowercase.\n\n    Args:\n        column: The input string column to convert to lowercase\n\n    Returns:\n        Column: A column containing the lowercase strings\n\n    Example: Convert text to lowercase\n        ```python\n        # Convert all text in the name column to lowercase\n        df.select(text.lower(col(\"name\")))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        StringCasingExpr(Column._from_col_or_name(column)._logical_expr, \"lower\")\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.ltrim","title":"ltrim","text":"<pre><code>ltrim(col: ColumnOrName) -&gt; Column\n</code></pre> <p>Remove whitespace from the start of strings in a column.</p> <p>This function removes all whitespace characters (spaces, tabs, newlines) from the beginning of each string in the column.</p> <p>Parameters:</p> <ul> <li> <code>col</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column or column name to trim</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the left-trimmed strings</p> </li> </ul> Remove leading whitespace <pre><code># Remove whitespace from the start of text\ndf.select(text.ltrim(col(\"text\")))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef ltrim(col: ColumnOrName) -&gt; Column:\n    \"\"\"Remove whitespace from the start of strings in a column.\n\n    This function removes all whitespace characters (spaces, tabs, newlines) from\n    the beginning of each string in the column.\n\n    Args:\n        col: The input string column or column name to trim\n\n    Returns:\n        Column: A column containing the left-trimmed strings\n\n    Example: Remove leading whitespace\n        ```python\n        # Remove whitespace from the start of text\n        df.select(text.ltrim(col(\"text\")))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        StripCharsExpr(Column._from_col_or_name(col)._logical_expr, None, \"left\")\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.parse_transcript","title":"parse_transcript","text":"<pre><code>parse_transcript(column: ColumnOrName, format: TranscriptFormatType) -&gt; Column\n</code></pre> <p>Parses a transcript from text to a structured format with unified schema.</p> <p>Converts transcript text in various formats (srt, generic) to a standardized structure with fields: index, speaker, start_time, end_time, duration, content, format. All timestamps are returned as floating-point seconds from the start.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column or column name containing transcript text</p> </li> <li> <code>format</code>               (<code>TranscriptFormatType</code>)           \u2013            <p>The format of the transcript (\"srt\" or \"generic\")</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing an array of structured transcript entries with unified schema:</p> <ul> <li>index: Optional[int] - Entry index (1-based)</li> <li>speaker: Optional[str] - Speaker name (for generic format)</li> <li>start_time: float - Start time in seconds</li> <li>end_time: Optional[float] - End time in seconds</li> <li>duration: Optional[float] - Duration in seconds</li> <li>content: str - Transcript content/text</li> <li>format: str - Original format (\"srt\" or \"generic\")</li> </ul> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Parse SRT format transcript\n&gt;&gt;&gt; df.select(text.parse_transcript(col(\"transcript\"), \"srt\"))\n&gt;&gt;&gt; # Parse generic conversation transcript\n&gt;&gt;&gt; df.select(text.parse_transcript(col(\"transcript\"), \"generic\"))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef parse_transcript(column: ColumnOrName, format: TranscriptFormatType) -&gt; Column:\n    \"\"\"Parses a transcript from text to a structured format with unified schema.\n\n    Converts transcript text in various formats (srt, generic) to a standardized structure\n    with fields: index, speaker, start_time, end_time, duration, content, format.\n    All timestamps are returned as floating-point seconds from the start.\n\n    Args:\n        column: The input string column or column name containing transcript text\n        format: The format of the transcript (\"srt\" or \"generic\")\n\n    Returns:\n        Column: A column containing an array of structured transcript entries with unified schema:\n\n            - index: Optional[int] - Entry index (1-based)\n            - speaker: Optional[str] - Speaker name (for generic format)\n            - start_time: float - Start time in seconds\n            - end_time: Optional[float] - End time in seconds\n            - duration: Optional[float] - Duration in seconds\n            - content: str - Transcript content/text\n            - format: str - Original format (\"srt\" or \"generic\")\n\n    Examples:\n        &gt;&gt;&gt; # Parse SRT format transcript\n        &gt;&gt;&gt; df.select(text.parse_transcript(col(\"transcript\"), \"srt\"))\n        &gt;&gt;&gt; # Parse generic conversation transcript\n        &gt;&gt;&gt; df.select(text.parse_transcript(col(\"transcript\"), \"generic\"))\n    \"\"\"\n    return Column._from_logical_expr(\n        TsParseExpr(Column._from_col_or_name(column)._logical_expr, format)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.recursive_character_chunk","title":"recursive_character_chunk","text":"<pre><code>recursive_character_chunk(column: ColumnOrName, chunk_size: int, chunk_overlap_percentage: int, chunking_character_set_custom_characters: Optional[list[str]] = None) -&gt; Column\n</code></pre> <p>Chunks a string column into chunks of a specified size (in characters) with an optional overlap.</p> <p>The chunking is performed recursively, attempting to preserve the underlying structure of the text by splitting on natural boundaries (paragraph breaks, sentence breaks, etc.) to maintain context. By default, these characters are ['\\n\\n', '\\n', '.', ';', ':', ' ', '-', ''], but this can be customized.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column or column name to chunk</p> </li> <li> <code>chunk_size</code>               (<code>int</code>)           \u2013            <p>The size of each chunk in characters</p> </li> <li> <code>chunk_overlap_percentage</code>               (<code>int</code>)           \u2013            <p>The overlap between each chunk as a percentage of the chunk size</p> </li> <li> <code>chunking_character_set_custom_characters</code>               (<code>Optional</code>, default:                   <code>None</code> )           \u2013            <p>List of alternative characters to split on. Note that the characters should be ordered from coarsest to finest desired granularity -- earlier characters in the list should result in fewer overall splits than later characters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the chunks as an array of strings</p> </li> </ul> Default character chunking <pre><code># Create chunks of at most 100 characters with 20% overlap\ndf.select(\n    text.recursive_character_chunk(col(\"text\"), 100, 20).alias(\"chunks\")\n)\n</code></pre> Custom character chunking <pre><code># Create chunks with custom split characters\ndf.select(\n    text.recursive_character_chunk(\n        col(\"text\"),\n        100,\n        20,\n        ['\\n\\n', '\\n', '.', ' ', '']\n    ).alias(\"chunks\")\n)\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef recursive_character_chunk(\n    column: ColumnOrName,\n    chunk_size: int,\n    chunk_overlap_percentage: int,\n    chunking_character_set_custom_characters: Optional[list[str]] = None,\n) -&gt; Column:\n    r\"\"\"Chunks a string column into chunks of a specified size (in characters) with an optional overlap.\n\n    The chunking is performed recursively, attempting to preserve the underlying structure of the text\n    by splitting on natural boundaries (paragraph breaks, sentence breaks, etc.) to maintain context.\n    By default, these characters are ['\\n\\n', '\\n', '.', ';', ':', ' ', '-', ''], but this can be customized.\n\n    Args:\n        column: The input string column or column name to chunk\n        chunk_size: The size of each chunk in characters\n        chunk_overlap_percentage: The overlap between each chunk as a percentage of the chunk size\n        chunking_character_set_custom_characters (Optional): List of alternative characters to split on. Note that the characters should be ordered from coarsest to finest desired granularity -- earlier characters in the list should result in fewer overall splits than later characters.\n\n    Returns:\n        Column: A column containing the chunks as an array of strings\n\n    Example: Default character chunking\n        ```python\n        # Create chunks of at most 100 characters with 20% overlap\n        df.select(\n            text.recursive_character_chunk(col(\"text\"), 100, 20).alias(\"chunks\")\n        )\n        ```\n\n    Example: Custom character chunking\n        ```python\n        # Create chunks with custom split characters\n        df.select(\n            text.recursive_character_chunk(\n                col(\"text\"),\n                100,\n                20,\n                ['\\n\\n', '\\n', '.', ' ', '']\n            ).alias(\"chunks\")\n        )\n        ```\n    \"\"\"\n    if chunking_character_set_custom_characters is None:\n        chunking_character_set_name = ChunkCharacterSet.ASCII\n    else:\n        chunking_character_set_name = ChunkCharacterSet.CUSTOM\n\n    chunk_configuration = RecursiveTextChunkExprConfiguration(\n        desired_chunk_size=chunk_size,\n        chunk_overlap_percentage=chunk_overlap_percentage,\n        chunk_length_function_name=ChunkLengthFunction.CHARACTER,\n        chunking_character_set_name=chunking_character_set_name,\n        chunking_character_set_custom_characters=chunking_character_set_custom_characters,\n    )\n    return Column._from_logical_expr(\n        RecursiveTextChunkExpr(\n            Column._from_col_or_name(column)._logical_expr, chunk_configuration\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.recursive_token_chunk","title":"recursive_token_chunk","text":"<pre><code>recursive_token_chunk(column: ColumnOrName, chunk_size: int, chunk_overlap_percentage: int, chunking_character_set_custom_characters: Optional[list[str]] = None) -&gt; Column\n</code></pre> <p>Chunks a string column into chunks of a specified size (in tokens) with an optional overlap.</p> <p>The chunking is performed recursively, attempting to preserve the underlying structure of the text by splitting on natural boundaries (paragraph breaks, sentence breaks, etc.) to maintain context. By default, these characters are ['\\n\\n', '\\n', '.', ';', ':', ' ', '-', ''], but this can be customized.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column or column name to chunk</p> </li> <li> <code>chunk_size</code>               (<code>int</code>)           \u2013            <p>The size of each chunk in tokens</p> </li> <li> <code>chunk_overlap_percentage</code>               (<code>int</code>)           \u2013            <p>The overlap between each chunk as a percentage of the chunk size</p> </li> <li> <code>chunking_character_set_custom_characters</code>               (<code>Optional</code>, default:                   <code>None</code> )           \u2013            <p>List of alternative characters to split on. Note that the characters should be ordered from coarsest to finest desired granularity -- earlier characters in the list should result in fewer overall splits than later characters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the chunks as an array of strings</p> </li> </ul> Default token chunking <pre><code># Create chunks of at most 100 tokens with 20% overlap\ndf.select(\n    text.recursive_token_chunk(col(\"text\"), 100, 20).alias(\"chunks\")\n)\n</code></pre> Custom token chunking <pre><code># Create chunks with custom split characters\ndf.select(\n    text.recursive_token_chunk(\n        col(\"text\"),\n        100,\n        20,\n        ['\\n\\n', '\\n', '.', ' ', '']\n    ).alias(\"chunks\")\n)\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef recursive_token_chunk(\n    column: ColumnOrName,\n    chunk_size: int,\n    chunk_overlap_percentage: int,\n    chunking_character_set_custom_characters: Optional[list[str]] = None,\n) -&gt; Column:\n    r\"\"\"Chunks a string column into chunks of a specified size (in tokens) with an optional overlap.\n\n    The chunking is performed recursively, attempting to preserve the underlying structure of the text\n    by splitting on natural boundaries (paragraph breaks, sentence breaks, etc.) to maintain context.\n    By default, these characters are ['\\n\\n', '\\n', '.', ';', ':', ' ', '-', ''], but this can be customized.\n\n    Args:\n        column: The input string column or column name to chunk\n        chunk_size: The size of each chunk in tokens\n        chunk_overlap_percentage: The overlap between each chunk as a percentage of the chunk size\n        chunking_character_set_custom_characters (Optional): List of alternative characters to split on. Note that the characters should be ordered from coarsest to finest desired granularity -- earlier characters in the list should result in fewer overall splits than later characters.\n\n    Returns:\n        Column: A column containing the chunks as an array of strings\n\n    Example: Default token chunking\n        ```python\n        # Create chunks of at most 100 tokens with 20% overlap\n        df.select(\n            text.recursive_token_chunk(col(\"text\"), 100, 20).alias(\"chunks\")\n        )\n        ```\n\n    Example: Custom token chunking\n        ```python\n        # Create chunks with custom split characters\n        df.select(\n            text.recursive_token_chunk(\n                col(\"text\"),\n                100,\n                20,\n                ['\\n\\n', '\\n', '.', ' ', '']\n            ).alias(\"chunks\")\n        )\n        ```\n    \"\"\"\n    if chunking_character_set_custom_characters is None:\n        chunking_character_set_name = ChunkCharacterSet.ASCII\n    else:\n        chunking_character_set_name = ChunkCharacterSet.CUSTOM\n\n    chunk_configuration = RecursiveTextChunkExprConfiguration(\n        desired_chunk_size=chunk_size,\n        chunk_overlap_percentage=chunk_overlap_percentage,\n        chunk_length_function_name=ChunkLengthFunction.TOKEN,\n        chunking_character_set_name=chunking_character_set_name,\n        chunking_character_set_custom_characters=chunking_character_set_custom_characters,\n    )\n    return Column._from_logical_expr(\n        RecursiveTextChunkExpr(\n            Column._from_col_or_name(column)._logical_expr, chunk_configuration\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.recursive_word_chunk","title":"recursive_word_chunk","text":"<pre><code>recursive_word_chunk(column: ColumnOrName, chunk_size: int, chunk_overlap_percentage: int, chunking_character_set_custom_characters: Optional[list[str]] = None) -&gt; Column\n</code></pre> <p>Chunks a string column into chunks of a specified size (in words) with an optional overlap.</p> <p>The chunking is performed recursively, attempting to preserve the underlying structure of the text by splitting on natural boundaries (paragraph breaks, sentence breaks, etc.) to maintain context. By default, these characters are ['\\n\\n', '\\n', '.', ';', ':', ' ', '-', ''], but this can be customized.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column or column name to chunk</p> </li> <li> <code>chunk_size</code>               (<code>int</code>)           \u2013            <p>The size of each chunk in words</p> </li> <li> <code>chunk_overlap_percentage</code>               (<code>int</code>)           \u2013            <p>The overlap between each chunk as a percentage of the chunk size</p> </li> <li> <code>chunking_character_set_custom_characters</code>               (<code>Optional</code>, default:                   <code>None</code> )           \u2013            <p>List of alternative characters to split on. Note that the characters should be ordered from coarsest to finest desired granularity -- earlier characters in the list should result in fewer overall splits than later characters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the chunks as an array of strings</p> </li> </ul> Default word chunking <pre><code># Create chunks of at most 100 words with 20% overlap\ndf.select(\n    text.recursive_word_chunk(col(\"text\"), 100, 20).alias(\"chunks\")\n)\n</code></pre> Custom word chunking <pre><code># Create chunks with custom split characters\ndf.select(\n    text.recursive_word_chunk(\n        col(\"text\"),\n        100,\n        20,\n        ['\\n\\n', '\\n', '.', ' ', '']\n    ).alias(\"chunks\")\n)\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef recursive_word_chunk(\n    column: ColumnOrName,\n    chunk_size: int,\n    chunk_overlap_percentage: int,\n    chunking_character_set_custom_characters: Optional[list[str]] = None,\n) -&gt; Column:\n    r\"\"\"Chunks a string column into chunks of a specified size (in words) with an optional overlap.\n\n    The chunking is performed recursively, attempting to preserve the underlying structure of the text\n    by splitting on natural boundaries (paragraph breaks, sentence breaks, etc.) to maintain context.\n    By default, these characters are ['\\n\\n', '\\n', '.', ';', ':', ' ', '-', ''], but this can be customized.\n\n    Args:\n        column: The input string column or column name to chunk\n        chunk_size: The size of each chunk in words\n        chunk_overlap_percentage: The overlap between each chunk as a percentage of the chunk size\n        chunking_character_set_custom_characters (Optional): List of alternative characters to split on. Note that the characters should be ordered from coarsest to finest desired granularity -- earlier characters in the list should result in fewer overall splits than later characters.\n\n    Returns:\n        Column: A column containing the chunks as an array of strings\n\n    Example: Default word chunking\n        ```python\n        # Create chunks of at most 100 words with 20% overlap\n        df.select(\n            text.recursive_word_chunk(col(\"text\"), 100, 20).alias(\"chunks\")\n        )\n        ```\n\n    Example: Custom word chunking\n        ```python\n        # Create chunks with custom split characters\n        df.select(\n            text.recursive_word_chunk(\n                col(\"text\"),\n                100,\n                20,\n                ['\\n\\n', '\\n', '.', ' ', '']\n            ).alias(\"chunks\")\n        )\n        ```\n    \"\"\"\n    if chunking_character_set_custom_characters is None:\n        chunking_character_set_name = ChunkCharacterSet.ASCII\n    else:\n        chunking_character_set_name = ChunkCharacterSet.CUSTOM\n\n    chunk_configuration = RecursiveTextChunkExprConfiguration(\n        desired_chunk_size=chunk_size,\n        chunk_overlap_percentage=chunk_overlap_percentage,\n        chunk_length_function_name=ChunkLengthFunction.WORD,\n        chunking_character_set_name=chunking_character_set_name,\n        chunking_character_set_custom_characters=chunking_character_set_custom_characters,\n    )\n    return Column._from_logical_expr(\n        RecursiveTextChunkExpr(\n            Column._from_col_or_name(column)._logical_expr, chunk_configuration\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.regexp_replace","title":"regexp_replace","text":"<pre><code>regexp_replace(src: ColumnOrName, pattern: Union[Column, str], replacement: Union[Column, str]) -&gt; Column\n</code></pre> <p>Replace all occurrences of a pattern with a new string, treating pattern as a regular expression.</p> <p>This method creates a new string column with all occurrences of the specified pattern replaced with a new string. The pattern is treated as a regular expression. If either pattern or replacement is a column expression, the operation is performed dynamically using the values from those columns.</p> <p>Parameters:</p> <ul> <li> <code>src</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column or column name to perform replacements on</p> </li> <li> <code>pattern</code>               (<code>Union[Column, str]</code>)           \u2013            <p>The regular expression pattern to search for (can be a string or column expression)</p> </li> <li> <code>replacement</code>               (<code>Union[Column, str]</code>)           \u2013            <p>The string to replace with (can be a string or column expression)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the strings with replacements applied</p> </li> </ul> Replace digits with dashes <pre><code># Replace all digits with dashes\ndf.select(text.regexp_replace(col(\"text\"), r\"\\d+\", \"--\"))\n</code></pre> Dynamic replacement using column values <pre><code># Replace using patterns from columns\ndf.select(text.regexp_replace(col(\"text\"), col(\"pattern\"), col(\"replacement\")))\n</code></pre> Complex pattern replacement <pre><code># Replace email addresses with [REDACTED]\ndf.select(text.regexp_replace(col(\"text\"), r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", \"[REDACTED]\"))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef regexp_replace(\n    src: ColumnOrName,\n    pattern: Union[Column, str],\n    replacement: Union[Column, str],\n) -&gt; Column:\n    r\"\"\"Replace all occurrences of a pattern with a new string, treating pattern as a regular expression.\n\n    This method creates a new string column with all occurrences of the specified pattern\n    replaced with a new string. The pattern is treated as a regular expression.\n    If either pattern or replacement is a column expression, the operation is performed dynamically\n    using the values from those columns.\n\n    Args:\n        src: The input string column or column name to perform replacements on\n        pattern: The regular expression pattern to search for (can be a string or column expression)\n        replacement: The string to replace with (can be a string or column expression)\n\n    Returns:\n        Column: A column containing the strings with replacements applied\n\n    Example: Replace digits with dashes\n        ```python\n        # Replace all digits with dashes\n        df.select(text.regexp_replace(col(\"text\"), r\"\\d+\", \"--\"))\n        ```\n\n    Example: Dynamic replacement using column values\n        ```python\n        # Replace using patterns from columns\n        df.select(text.regexp_replace(col(\"text\"), col(\"pattern\"), col(\"replacement\")))\n        ```\n\n    Example: Complex pattern replacement\n        ```python\n        # Replace email addresses with [REDACTED]\n        df.select(text.regexp_replace(col(\"text\"), r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", \"[REDACTED]\"))\n        ```\n    \"\"\"\n    if isinstance(pattern, Column):\n        pattern = pattern._logical_expr\n    if isinstance(replacement, Column):\n        replacement = replacement._logical_expr\n    return Column._from_logical_expr(\n        ReplaceExpr(\n            Column._from_col_or_name(src)._logical_expr,\n            pattern,\n            replacement,\n            False,\n            -1,\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.replace","title":"replace","text":"<pre><code>replace(src: ColumnOrName, search: Union[Column, str], replace: Union[Column, str]) -&gt; Column\n</code></pre> <p>Replace all occurrences of a pattern with a new string, treating pattern as a literal string.</p> <p>This method creates a new string column with all occurrences of the specified pattern replaced with a new string. The pattern is treated as a literal string, not a regular expression. If either search or replace is a column expression, the operation is performed dynamically using the values from those columns.</p> <p>Parameters:</p> <ul> <li> <code>src</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column or column name to perform replacements on</p> </li> <li> <code>search</code>               (<code>Union[Column, str]</code>)           \u2013            <p>The pattern to search for (can be a string or column expression)</p> </li> <li> <code>replace</code>               (<code>Union[Column, str]</code>)           \u2013            <p>The string to replace with (can be a string or column expression)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the strings with replacements applied</p> </li> </ul> Replace with literal string <pre><code># Replace all occurrences of \"foo\" in the \"name\" column with \"bar\"\ndf.select(text.replace(col(\"name\"), \"foo\", \"bar\"))\n</code></pre> Replace using column values <pre><code># Replace all occurrences of the value in the \"search\" column with the value in the \"replace\" column, for each row in the \"text\" column\ndf.select(text.replace(col(\"text\"), col(\"search\"), col(\"replace\")))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef replace(\n    src: ColumnOrName, search: Union[Column, str], replace: Union[Column, str]\n) -&gt; Column:\n    \"\"\"Replace all occurrences of a pattern with a new string, treating pattern as a literal string.\n\n    This method creates a new string column with all occurrences of the specified pattern\n    replaced with a new string. The pattern is treated as a literal string, not a regular expression.\n    If either search or replace is a column expression, the operation is performed dynamically\n    using the values from those columns.\n\n    Args:\n        src: The input string column or column name to perform replacements on\n        search: The pattern to search for (can be a string or column expression)\n        replace: The string to replace with (can be a string or column expression)\n\n    Returns:\n        Column: A column containing the strings with replacements applied\n\n    Example: Replace with literal string\n        ```python\n        # Replace all occurrences of \"foo\" in the \"name\" column with \"bar\"\n        df.select(text.replace(col(\"name\"), \"foo\", \"bar\"))\n        ```\n\n    Example: Replace using column values\n        ```python\n        # Replace all occurrences of the value in the \"search\" column with the value in the \"replace\" column, for each row in the \"text\" column\n        df.select(text.replace(col(\"text\"), col(\"search\"), col(\"replace\")))\n        ```\n    \"\"\"\n    if isinstance(search, Column):\n        search = search._logical_expr\n    if isinstance(replace, Column):\n        replace = replace._logical_expr\n    return Column._from_logical_expr(\n        ReplaceExpr(\n            Column._from_col_or_name(src)._logical_expr, search, replace, True, -1\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.rtrim","title":"rtrim","text":"<pre><code>rtrim(col: ColumnOrName) -&gt; Column\n</code></pre> <p>Remove whitespace from the end of strings in a column.</p> <p>This function removes all whitespace characters (spaces, tabs, newlines) from the end of each string in the column.</p> <p>Parameters:</p> <ul> <li> <code>col</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column or column name to trim</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the right-trimmed strings</p> </li> </ul> Remove trailing whitespace <pre><code># Remove whitespace from the end of text\ndf.select(text.rtrim(col(\"text\")))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef rtrim(col: ColumnOrName) -&gt; Column:\n    \"\"\"Remove whitespace from the end of strings in a column.\n\n    This function removes all whitespace characters (spaces, tabs, newlines) from\n    the end of each string in the column.\n\n    Args:\n        col: The input string column or column name to trim\n\n    Returns:\n        Column: A column containing the right-trimmed strings\n\n    Example: Remove trailing whitespace\n        ```python\n        # Remove whitespace from the end of text\n        df.select(text.rtrim(col(\"text\")))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        StripCharsExpr(Column._from_col_or_name(col)._logical_expr, None, \"right\")\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.split","title":"split","text":"<pre><code>split(src: ColumnOrName, pattern: str, limit: int = -1) -&gt; Column\n</code></pre> <p>Split a string column into an array using a regular expression pattern.</p> <p>This method creates an array column by splitting each value in the input string column at matches of the specified regular expression pattern.</p> <p>Parameters:</p> <ul> <li> <code>src</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column or column name to split</p> </li> <li> <code>pattern</code>               (<code>str</code>)           \u2013            <p>The regular expression pattern to split on</p> </li> <li> <code>limit</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>Maximum number of splits to perform (Default: -1 for unlimited).   If &gt; 0, returns at most limit+1 elements, with remainder in last element.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing arrays of substrings</p> </li> </ul> Split on whitespace <pre><code># Split on whitespace\ndf.select(text.split(col(\"text\"), r\"\\s+\"))\n</code></pre> Split with limit <pre><code># Split on whitespace, max 2 splits\ndf.select(text.split(col(\"text\"), r\"\\s+\", limit=2))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef split(src: ColumnOrName, pattern: str, limit: int = -1) -&gt; Column:\n    r\"\"\"Split a string column into an array using a regular expression pattern.\n\n    This method creates an array column by splitting each value in the input string column\n    at matches of the specified regular expression pattern.\n\n    Args:\n        src: The input string column or column name to split\n        pattern: The regular expression pattern to split on\n        limit: Maximum number of splits to perform (Default: -1 for unlimited).\n              If &gt; 0, returns at most limit+1 elements, with remainder in last element.\n\n    Returns:\n        Column: A column containing arrays of substrings\n\n    Example: Split on whitespace\n        ```python\n        # Split on whitespace\n        df.select(text.split(col(\"text\"), r\"\\s+\"))\n        ```\n\n    Example: Split with limit\n        ```python\n        # Split on whitespace, max 2 splits\n        df.select(text.split(col(\"text\"), r\"\\s+\", limit=2))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        RegexpSplitExpr(Column._from_col_or_name(src)._logical_expr, pattern, limit)\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.split_part","title":"split_part","text":"<pre><code>split_part(src: ColumnOrName, delimiter: Union[Column, str], part_number: Union[int, Column]) -&gt; Column\n</code></pre> <p>Split a string and return a specific part using 1-based indexing.</p> <p>Splits each string by a delimiter and returns the specified part. If the delimiter is a column expression, the split operation is performed dynamically using the delimiter values from that column.</p> <p>Behavior: - If any input is null, returns null - If part_number is out of range of split parts, returns empty string - If part_number is 0, throws an error - If part_number is negative, counts from the end of the split parts - If the delimiter is an empty string, the string is not split</p> <p>Parameters:</p> <ul> <li> <code>src</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column or column name to split</p> </li> <li> <code>delimiter</code>               (<code>Union[Column, str]</code>)           \u2013            <p>The delimiter to split on (can be a string or column expression)</p> </li> <li> <code>part_number</code>               (<code>Union[int, Column]</code>)           \u2013            <p>Which part to return (1-based, can be an integer or column expression)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the specified part from each split string</p> </li> </ul> Get second part of comma-separated values <pre><code># Get second part of comma-separated values\ndf.select(text.split_part(col(\"text\"), \",\", 2))\n</code></pre> Get last part using negative index <pre><code># Get last part using negative index\ndf.select(text.split_part(col(\"text\"), \",\", -1))\n</code></pre> Use dynamic delimiter from column <pre><code># Use dynamic delimiter from column\ndf.select(text.split_part(col(\"text\"), col(\"delimiter\"), 1))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef split_part(\n    src: ColumnOrName, delimiter: Union[Column, str], part_number: Union[int, Column]\n) -&gt; Column:\n    \"\"\"Split a string and return a specific part using 1-based indexing.\n\n    Splits each string by a delimiter and returns the specified part.\n    If the delimiter is a column expression, the split operation is performed dynamically\n    using the delimiter values from that column.\n\n    Behavior:\n    - If any input is null, returns null\n    - If part_number is out of range of split parts, returns empty string\n    - If part_number is 0, throws an error\n    - If part_number is negative, counts from the end of the split parts\n    - If the delimiter is an empty string, the string is not split\n\n    Args:\n        src: The input string column or column name to split\n        delimiter: The delimiter to split on (can be a string or column expression)\n        part_number: Which part to return (1-based, can be an integer or column expression)\n\n    Returns:\n        Column: A column containing the specified part from each split string\n\n    Example: Get second part of comma-separated values\n        ```python\n        # Get second part of comma-separated values\n        df.select(text.split_part(col(\"text\"), \",\", 2))\n        ```\n\n    Example: Get last part using negative index\n        ```python\n        # Get last part using negative index\n        df.select(text.split_part(col(\"text\"), \",\", -1))\n        ```\n\n    Example: Use dynamic delimiter from column\n        ```python\n        # Use dynamic delimiter from column\n        df.select(text.split_part(col(\"text\"), col(\"delimiter\"), 1))\n        ```\n    \"\"\"\n    if isinstance(part_number, int) and part_number == 0:\n        raise ValueError(\n            f\"`split_part` expects a non-zero integer for the part_number, but got {part_number}.\"\n        )\n    if isinstance(delimiter, Column):\n        delimiter = delimiter._logical_expr\n    if isinstance(part_number, Column):\n        part_number = part_number._logical_expr\n    return Column._from_logical_expr(\n        SplitPartExpr(\n            Column._from_col_or_name(src)._logical_expr, delimiter, part_number\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.title_case","title":"title_case","text":"<pre><code>title_case(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Convert the first character of each word in a string column to uppercase.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column to convert to title case</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the title case strings</p> </li> </ul> Convert text to title case <pre><code># Convert text in the name column to title case\ndf.select(text.title_case(col(\"name\")))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef title_case(column: ColumnOrName) -&gt; Column:\n    \"\"\"Convert the first character of each word in a string column to uppercase.\n\n    Args:\n        column: The input string column to convert to title case\n\n    Returns:\n        Column: A column containing the title case strings\n\n    Example: Convert text to title case\n        ```python\n        # Convert text in the name column to title case\n        df.select(text.title_case(col(\"name\")))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        StringCasingExpr(Column._from_col_or_name(column)._logical_expr, \"title\")\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.token_chunk","title":"token_chunk","text":"<pre><code>token_chunk(column: ColumnOrName, chunk_size: int, chunk_overlap_percentage: int = 0) -&gt; Column\n</code></pre> <p>Chunks a string column into chunks of a specified size (in tokens) with an optional overlap.</p> <p>The chunking is done by applying a simple sliding window across the text to create chunks of equal size. This approach does not attempt to preserve the underlying structure of the text.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column or column name to chunk</p> </li> <li> <code>chunk_size</code>               (<code>int</code>)           \u2013            <p>The size of each chunk in tokens</p> </li> <li> <code>chunk_overlap_percentage</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The overlap between chunks as a percentage of the chunk size (Default: 0)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the chunks as an array of strings</p> </li> </ul> Create token chunks <pre><code># Create chunks of 100 tokens with 20% overlap\ndf.select(text.token_chunk(col(\"text\"), 100, 20))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef token_chunk(\n    column: ColumnOrName, chunk_size: int, chunk_overlap_percentage: int = 0\n) -&gt; Column:\n    \"\"\"Chunks a string column into chunks of a specified size (in tokens) with an optional overlap.\n\n    The chunking is done by applying a simple sliding window across the text to create chunks of equal size.\n    This approach does not attempt to preserve the underlying structure of the text.\n\n    Args:\n        column: The input string column or column name to chunk\n        chunk_size: The size of each chunk in tokens\n        chunk_overlap_percentage: The overlap between chunks as a percentage of the chunk size (Default: 0)\n\n    Returns:\n        Column: A column containing the chunks as an array of strings\n\n    Example: Create token chunks\n        ```python\n        # Create chunks of 100 tokens with 20% overlap\n        df.select(text.token_chunk(col(\"text\"), 100, 20))\n        ```\n    \"\"\"\n    chunk_configuration = TextChunkExprConfiguration(\n        desired_chunk_size=chunk_size,\n        chunk_overlap_percentage=chunk_overlap_percentage,\n        chunk_length_function_name=ChunkLengthFunction.TOKEN,\n    )\n    return Column._from_logical_expr(\n        TextChunkExpr(\n            Column._from_col_or_name(column)._logical_expr, chunk_configuration\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.trim","title":"trim","text":"<pre><code>trim(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Remove whitespace from both sides of strings in a column.</p> <p>This function removes all whitespace characters (spaces, tabs, newlines) from both the beginning and end of each string in the column.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column or column name to trim</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the trimmed strings</p> </li> </ul> Remove whitespace from both sides <pre><code># Remove whitespace from both sides of text\ndf.select(text.trim(col(\"text\")))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef trim(column: ColumnOrName) -&gt; Column:\n    \"\"\"Remove whitespace from both sides of strings in a column.\n\n    This function removes all whitespace characters (spaces, tabs, newlines) from\n    both the beginning and end of each string in the column.\n\n    Args:\n        column: The input string column or column name to trim\n\n    Returns:\n        Column: A column containing the trimmed strings\n\n    Example: Remove whitespace from both sides\n        ```python\n        # Remove whitespace from both sides of text\n        df.select(text.trim(col(\"text\")))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        StripCharsExpr(Column._from_col_or_name(column)._logical_expr, None, \"both\")\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.upper","title":"upper","text":"<pre><code>upper(column: ColumnOrName) -&gt; Column\n</code></pre> <p>Convert all characters in a string column to uppercase.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column to convert to uppercase</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the uppercase strings</p> </li> </ul> Convert text to uppercase <pre><code># Convert all text in the name column to uppercase\ndf.select(text.upper(col(\"name\")))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef upper(column: ColumnOrName) -&gt; Column:\n    \"\"\"Convert all characters in a string column to uppercase.\n\n    Args:\n        column: The input string column to convert to uppercase\n\n    Returns:\n        Column: A column containing the uppercase strings\n\n    Example: Convert text to uppercase\n        ```python\n        # Convert all text in the name column to uppercase\n        df.select(text.upper(col(\"name\")))\n        ```\n    \"\"\"\n    return Column._from_logical_expr(\n        StringCasingExpr(Column._from_col_or_name(column)._logical_expr, \"upper\")\n    )\n</code></pre>"},{"location":"reference/fenic/api/functions/text/#fenic.api.functions.text.word_chunk","title":"word_chunk","text":"<pre><code>word_chunk(column: ColumnOrName, chunk_size: int, chunk_overlap_percentage: int = 0) -&gt; Column\n</code></pre> <p>Chunks a string column into chunks of a specified size (in words) with an optional overlap.</p> <p>The chunking is done by applying a simple sliding window across the text to create chunks of equal size. This approach does not attempt to preserve the underlying structure of the text.</p> <p>Parameters:</p> <ul> <li> <code>column</code>               (<code>ColumnOrName</code>)           \u2013            <p>The input string column or column name to chunk</p> </li> <li> <code>chunk_size</code>               (<code>int</code>)           \u2013            <p>The size of each chunk in words</p> </li> <li> <code>chunk_overlap_percentage</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The overlap between chunks as a percentage of the chunk size (Default: 0)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Column</code> (              <code>Column</code> )          \u2013            <p>A column containing the chunks as an array of strings</p> </li> </ul> Create word chunks <pre><code># Create chunks of 100 words with 20% overlap\ndf.select(text.word_chunk(col(\"text\"), 100, 20))\n</code></pre> Source code in <code>src/fenic/api/functions/text.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True, arbitrary_types_allowed=True))\ndef word_chunk(\n    column: ColumnOrName, chunk_size: int, chunk_overlap_percentage: int = 0\n) -&gt; Column:\n    \"\"\"Chunks a string column into chunks of a specified size (in words) with an optional overlap.\n\n    The chunking is done by applying a simple sliding window across the text to create chunks of equal size.\n    This approach does not attempt to preserve the underlying structure of the text.\n\n    Args:\n        column: The input string column or column name to chunk\n        chunk_size: The size of each chunk in words\n        chunk_overlap_percentage: The overlap between chunks as a percentage of the chunk size (Default: 0)\n\n    Returns:\n        Column: A column containing the chunks as an array of strings\n\n    Example: Create word chunks\n        ```python\n        # Create chunks of 100 words with 20% overlap\n        df.select(text.word_chunk(col(\"text\"), 100, 20))\n        ```\n    \"\"\"\n    chunk_configuration = TextChunkExprConfiguration(\n        desired_chunk_size=chunk_size,\n        chunk_overlap_percentage=chunk_overlap_percentage,\n        chunk_length_function_name=ChunkLengthFunction.WORD,\n    )\n    return Column._from_logical_expr(\n        TextChunkExpr(\n            Column._from_col_or_name(column)._logical_expr, chunk_configuration\n        )\n    )\n</code></pre>"},{"location":"reference/fenic/api/io/","title":"fenic.api.io","text":""},{"location":"reference/fenic/api/io/#fenic.api.io","title":"fenic.api.io","text":"<p>IO module for reading and writing DataFrames to external storage.</p> <p>Classes:</p> <ul> <li> <code>DataFrameReader</code>           \u2013            <p>Interface used to load a DataFrame from external storage systems.</p> </li> <li> <code>DataFrameWriter</code>           \u2013            <p>Interface used to write a DataFrame to external storage systems.</p> </li> </ul>"},{"location":"reference/fenic/api/io/#fenic.api.io.DataFrameReader","title":"DataFrameReader","text":"<pre><code>DataFrameReader(session_state: BaseSessionState)\n</code></pre> <p>Interface used to load a DataFrame from external storage systems.</p> <p>Similar to PySpark's DataFrameReader.</p> <p>Creates a DataFrameReader.</p> <p>Parameters:</p> <ul> <li> <code>session_state</code>               (<code>BaseSessionState</code>)           \u2013            <p>The session state to use for reading</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>csv</code>             \u2013              <p>Load a DataFrame from one or more CSV files.</p> </li> <li> <code>parquet</code>             \u2013              <p>Load a DataFrame from one or more Parquet files.</p> </li> </ul> Source code in <code>src/fenic/api/io/reader.py</code> <pre><code>def __init__(self, session_state: BaseSessionState):\n    \"\"\"Creates a DataFrameReader.\n\n    Args:\n        session_state: The session state to use for reading\n    \"\"\"\n    self._options: Dict[str, Any] = {}\n    self._session_state = session_state\n</code></pre>"},{"location":"reference/fenic/api/io/#fenic.api.io.DataFrameReader.csv","title":"csv","text":"<pre><code>csv(paths: Union[str, Path, list[Union[str, Path]]], schema: Optional[Schema] = None, merge_schemas: bool = False) -&gt; DataFrame\n</code></pre> <p>Load a DataFrame from one or more CSV files.</p> <p>Parameters:</p> <ul> <li> <code>paths</code>               (<code>Union[str, Path, list[Union[str, Path]]]</code>)           \u2013            <p>A single file path, a glob pattern (e.g., \"data/*.csv\"), or a list of paths.</p> </li> <li> <code>schema</code>               (<code>Optional[Schema]</code>, default:                   <code>None</code> )           \u2013            <p>(optional) A complete schema definition of column names and their types. Only primitive types are supported. - For e.g.:     - Schema([ColumnField(name=\"id\", data_type=IntegerType), ColumnField(name=\"name\", data_type=StringType)]) - If provided, all files must match this schema exactly\u2014all column names must be present, and values must be convertible to the specified types. Partial schemas are not allowed.</p> </li> <li> <code>merge_schemas</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to merge schemas across all files. - If True: Column names are unified across files. Missing columns are filled with nulls. Column types are inferred and widened as needed. - If False (default): Only accepts columns from the first file. Column types from the first file are inferred and applied across all files. If subsequent files do not have the same column name and order as the first file, an error is raised. - The \"first file\" is defined as:     - The first file in lexicographic order (for glob patterns), or     - The first file in the provided list (for lists of paths).</p> </li> </ul> Notes <ul> <li>The first row in each file is assumed to be a header row.</li> <li>Delimiters (e.g., comma, tab) are automatically inferred.</li> <li>You may specify either <code>schema</code> or <code>merge_schemas=True</code>, but not both.</li> <li>Any date/datetime columns are cast to strings during ingestion.</li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If both <code>schema</code> and <code>merge_schemas=True</code> are provided.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If any path does not end with <code>.csv</code>.</p> </li> <li> <code>PlanError</code>             \u2013            <p>If schemas cannot be merged or if there's a schema mismatch when merge_schemas=False.</p> </li> </ul> Read a single CSV file <pre><code>df = session.read.csv(\"file.csv\")\n</code></pre> Read multiple CSV files with schema merging <pre><code>df = session.read.csv(\"data/*.csv\", merge_schemas=True)\n</code></pre> Read CSV files with explicit schema <p><code>python df = session.read.csv(     [\"a.csv\", \"b.csv\"],     schema=Schema([         ColumnField(name=\"id\", data_type=IntegerType),         ColumnField(name=\"value\", data_type=FloatType)     ]) )</code></p> Source code in <code>src/fenic/api/io/reader.py</code> <pre><code>def csv(\n    self,\n    paths: Union[str, Path, list[Union[str, Path]]],\n    schema: Optional[Schema] = None,\n    merge_schemas: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Load a DataFrame from one or more CSV files.\n\n    Args:\n        paths: A single file path, a glob pattern (e.g., \"data/*.csv\"), or a list of paths.\n        schema: (optional) A complete schema definition of column names and their types. Only primitive types are supported.\n            - For e.g.:\n                - Schema([ColumnField(name=\"id\", data_type=IntegerType), ColumnField(name=\"name\", data_type=StringType)])\n            - If provided, all files must match this schema exactly\u2014all column names must be present, and values must be\n            convertible to the specified types. Partial schemas are not allowed.\n        merge_schemas: Whether to merge schemas across all files.\n            - If True: Column names are unified across files. Missing columns are filled with nulls. Column types are\n            inferred and widened as needed.\n            - If False (default): Only accepts columns from the first file. Column types from the first file are\n            inferred and applied across all files. If subsequent files do not have the same column name and order as the first file, an error is raised.\n            - The \"first file\" is defined as:\n                - The first file in lexicographic order (for glob patterns), or\n                - The first file in the provided list (for lists of paths).\n\n    Notes:\n        - The first row in each file is assumed to be a header row.\n        - Delimiters (e.g., comma, tab) are automatically inferred.\n        - You may specify either `schema` or `merge_schemas=True`, but not both.\n        - Any date/datetime columns are cast to strings during ingestion.\n\n    Raises:\n        ValidationError: If both `schema` and `merge_schemas=True` are provided.\n        ValidationError: If any path does not end with `.csv`.\n        PlanError: If schemas cannot be merged or if there's a schema mismatch when merge_schemas=False.\n\n    Example: Read a single CSV file\n        ```python\n        df = session.read.csv(\"file.csv\")\n        ```\n\n    Example: Read multiple CSV files with schema merging\n        ```python\n        df = session.read.csv(\"data/*.csv\", merge_schemas=True)\n        ```\n\n    Example: Read CSV files with explicit schema\n        ```python\n        df = session.read.csv(\n            [\"a.csv\", \"b.csv\"],\n            schema=Schema([\n                ColumnField(name=\"id\", data_type=IntegerType),\n                ColumnField(name=\"value\", data_type=FloatType)\n            ])\n        )            ```\n    \"\"\"\n    if schema is not None and merge_schemas:\n        raise ValidationError(\n            \"Cannot specify both 'schema' and 'merge_schemas=True' - these options conflict. \"\n            \"Choose one approach: \"\n            \"1) Use 'schema' to enforce a specific schema: csv(paths, schema=your_schema), \"\n            \"2) Use 'merge_schemas=True' to automatically merge schemas: csv(paths, merge_schemas=True), \"\n            \"3) Use neither to inherit schema from the first file: csv(paths)\"\n        )\n    if schema is not None:\n        for col_field in schema.column_fields:\n            if not isinstance(\n                col_field.data_type,\n                _PrimitiveType,\n            ):\n                raise ValidationError(\n                    f\"CSV files only support primitive data types in schema definitions. \"\n                    f\"Column '{col_field.name}' has type {type(col_field.data_type).__name__}, but CSV schemas must use: \"\n                    f\"IntegerType, FloatType, DoubleType, BooleanType, or StringType. \"\n                    f\"Example: Schema([ColumnField(name='id', data_type=IntegerType), ColumnField(name='name', data_type=StringType)])\"\n                )\n    options = {\n        \"schema\": schema,\n        \"merge_schemas\": merge_schemas,\n    }\n    return self._read_file(\n        paths, file_format=\"csv\", file_extension=\".csv\", **options\n    )\n</code></pre>"},{"location":"reference/fenic/api/io/#fenic.api.io.DataFrameReader.parquet","title":"parquet","text":"<pre><code>parquet(paths: Union[str, Path, list[Union[str, Path]]], merge_schemas: bool = False) -&gt; DataFrame\n</code></pre> <p>Load a DataFrame from one or more Parquet files.</p> <p>Parameters:</p> <ul> <li> <code>paths</code>               (<code>Union[str, Path, list[Union[str, Path]]]</code>)           \u2013            <p>A single file path, a glob pattern (e.g., \"data/*.parquet\"), or a list of paths.</p> </li> <li> <code>merge_schemas</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, infers and merges schemas across all files. Missing columns are filled with nulls, and differing types are widened to a common supertype.</p> </li> </ul> Behavior <ul> <li>If <code>merge_schemas=False</code> (default), all files must match the schema of the first file exactly. Subsequent files must contain all columns from the first file with compatible data types. If any column is missing or has incompatible types, an error is raised.</li> <li>If <code>merge_schemas=True</code>, column names are unified across all files, and data types are automatically widened to accommodate all values.</li> <li>The \"first file\" is defined as:<ul> <li>The first file in lexicographic order (for glob patterns), or</li> <li>The first file in the provided list (for lists of paths).</li> </ul> </li> </ul> Notes <ul> <li>Date and datetime columns are cast to strings during ingestion.</li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If any file does not have a <code>.parquet</code> extension.</p> </li> <li> <code>PlanError</code>             \u2013            <p>If schemas cannot be merged or if there's a schema mismatch when merge_schemas=False.</p> </li> </ul> Read a single Parquet file <pre><code>df = session.read.parquet(\"file.parquet\")\n</code></pre> Read multiple Parquet files <pre><code>df = session.read.parquet(\"data/*.parquet\")\n</code></pre> Read Parquet files with schema merging <pre><code>df = session.read.parquet([\"a.parquet\", \"b.parquet\"], merge_schemas=True)\n</code></pre> Source code in <code>src/fenic/api/io/reader.py</code> <pre><code>def parquet(\n    self,\n    paths: Union[str, Path, list[Union[str, Path]]],\n    merge_schemas: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Load a DataFrame from one or more Parquet files.\n\n    Args:\n        paths: A single file path, a glob pattern (e.g., \"data/*.parquet\"), or a list of paths.\n        merge_schemas: If True, infers and merges schemas across all files.\n            Missing columns are filled with nulls, and differing types are widened to a common supertype.\n\n    Behavior:\n        - If `merge_schemas=False` (default), all files must match the schema of the first file exactly.\n        Subsequent files must contain all columns from the first file with compatible data types.\n        If any column is missing or has incompatible types, an error is raised.\n        - If `merge_schemas=True`, column names are unified across all files, and data types are automatically\n        widened to accommodate all values.\n        - The \"first file\" is defined as:\n            - The first file in lexicographic order (for glob patterns), or\n            - The first file in the provided list (for lists of paths).\n\n    Notes:\n        - Date and datetime columns are cast to strings during ingestion.\n\n    Raises:\n        ValidationError: If any file does not have a `.parquet` extension.\n        PlanError: If schemas cannot be merged or if there's a schema mismatch when merge_schemas=False.\n\n    Example: Read a single Parquet file\n        ```python\n        df = session.read.parquet(\"file.parquet\")\n        ```\n\n    Example: Read multiple Parquet files\n        ```python\n        df = session.read.parquet(\"data/*.parquet\")\n        ```\n\n    Example: Read Parquet files with schema merging\n        ```python\n        df = session.read.parquet([\"a.parquet\", \"b.parquet\"], merge_schemas=True)\n        ```\n    \"\"\"\n    options = {\n        \"merge_schemas\": merge_schemas,\n    }\n    return self._read_file(\n        paths, file_format=\"parquet\", file_extension=\".parquet\", **options\n    )\n</code></pre>"},{"location":"reference/fenic/api/io/#fenic.api.io.DataFrameWriter","title":"DataFrameWriter","text":"<pre><code>DataFrameWriter(dataframe: DataFrame)\n</code></pre> <p>Interface used to write a DataFrame to external storage systems.</p> <p>Similar to PySpark's DataFrameWriter.</p> <p>Initialize a DataFrameWriter.</p> <p>Parameters:</p> <ul> <li> <code>dataframe</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to write.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>csv</code>             \u2013              <p>Saves the content of the DataFrame as a single CSV file with comma as the delimiter and headers in the first row.</p> </li> <li> <code>parquet</code>             \u2013              <p>Saves the content of the DataFrame as a single Parquet file.</p> </li> <li> <code>save_as_table</code>             \u2013              <p>Saves the content of the DataFrame as the specified table.</p> </li> </ul> Source code in <code>src/fenic/api/io/writer.py</code> <pre><code>def __init__(self, dataframe: DataFrame):\n    \"\"\"Initialize a DataFrameWriter.\n\n    Args:\n        dataframe: The DataFrame to write.\n    \"\"\"\n    self._dataframe = dataframe\n</code></pre>"},{"location":"reference/fenic/api/io/#fenic.api.io.DataFrameWriter.csv","title":"csv","text":"<pre><code>csv(file_path: Union[str, Path], mode: Literal['error', 'overwrite', 'ignore'] = 'overwrite') -&gt; QueryMetrics\n</code></pre> <p>Saves the content of the DataFrame as a single CSV file with comma as the delimiter and headers in the first row.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path to save the CSV file to</p> </li> <li> <code>mode</code>               (<code>Literal['error', 'overwrite', 'ignore']</code>, default:                   <code>'overwrite'</code> )           \u2013            <p>Write mode. Default is \"overwrite\".  - error: Raises an error if file exists  - overwrite: Overwrites the file if it exists  - ignore: Silently ignores operation if file exists</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>QueryMetrics</code> (              <code>QueryMetrics</code> )          \u2013            <p>The query metrics</p> </li> </ul> Save with overwrite mode (default) <pre><code>df.write.csv(\"output.csv\")  # Overwrites if exists\n</code></pre> Save with error mode <pre><code>df.write.csv(\"output.csv\", mode=\"error\")  # Raises error if exists\n</code></pre> Save with ignore mode <pre><code>df.write.csv(\"output.csv\", mode=\"ignore\")  # Skips if exists\n</code></pre> Source code in <code>src/fenic/api/io/writer.py</code> <pre><code>def csv(\n    self,\n    file_path: Union[str, Path],\n    mode: Literal[\"error\", \"overwrite\", \"ignore\"] = \"overwrite\",\n) -&gt; QueryMetrics:\n    \"\"\"Saves the content of the DataFrame as a single CSV file with comma as the delimiter and headers in the first row.\n\n    Args:\n        file_path: Path to save the CSV file to\n        mode: Write mode. Default is \"overwrite\".\n             - error: Raises an error if file exists\n             - overwrite: Overwrites the file if it exists\n             - ignore: Silently ignores operation if file exists\n\n    Returns:\n        QueryMetrics: The query metrics\n\n    Example: Save with overwrite mode (default)\n        ```python\n        df.write.csv(\"output.csv\")  # Overwrites if exists\n        ```\n\n    Example: Save with error mode\n        ```python\n        df.write.csv(\"output.csv\", mode=\"error\")  # Raises error if exists\n        ```\n\n    Example: Save with ignore mode\n        ```python\n        df.write.csv(\"output.csv\", mode=\"ignore\")  # Skips if exists\n        ```\n    \"\"\"\n    file_path = str(file_path)\n    if not file_path.endswith(\".csv\"):\n        raise ValidationError(\n            f\"CSV writer requires a '.csv' file extension. \"\n            f\"Your path '{file_path}' is missing the extension.\"\n        )\n\n    sink_plan = FileSink(\n        child=self._dataframe._logical_plan,\n        sink_type=\"csv\",\n        path=file_path,\n        mode=mode,\n    )\n\n    metrics = self._dataframe._logical_plan.session_state.execution.save_to_file(\n        sink_plan, file_path=file_path, mode=mode\n    )\n    logger.info(metrics.get_summary())\n    return metrics\n</code></pre>"},{"location":"reference/fenic/api/io/#fenic.api.io.DataFrameWriter.parquet","title":"parquet","text":"<pre><code>parquet(file_path: Union[str, Path], mode: Literal['error', 'overwrite', 'ignore'] = 'overwrite') -&gt; QueryMetrics\n</code></pre> <p>Saves the content of the DataFrame as a single Parquet file.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path to save the Parquet file to</p> </li> <li> <code>mode</code>               (<code>Literal['error', 'overwrite', 'ignore']</code>, default:                   <code>'overwrite'</code> )           \u2013            <p>Write mode. Default is \"overwrite\".  - error: Raises an error if file exists  - overwrite: Overwrites the file if it exists  - ignore: Silently ignores operation if file exists</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>QueryMetrics</code> (              <code>QueryMetrics</code> )          \u2013            <p>The query metrics</p> </li> </ul> Save with overwrite mode (default) <pre><code>df.write.parquet(\"output.parquet\")  # Overwrites if exists\n</code></pre> Save with error mode <pre><code>df.write.parquet(\"output.parquet\", mode=\"error\")  # Raises error if exists\n</code></pre> Save with ignore mode <pre><code>df.write.parquet(\"output.parquet\", mode=\"ignore\")  # Skips if exists\n</code></pre> Source code in <code>src/fenic/api/io/writer.py</code> <pre><code>def parquet(\n    self,\n    file_path: Union[str, Path],\n    mode: Literal[\"error\", \"overwrite\", \"ignore\"] = \"overwrite\",\n) -&gt; QueryMetrics:\n    \"\"\"Saves the content of the DataFrame as a single Parquet file.\n\n    Args:\n        file_path: Path to save the Parquet file to\n        mode: Write mode. Default is \"overwrite\".\n             - error: Raises an error if file exists\n             - overwrite: Overwrites the file if it exists\n             - ignore: Silently ignores operation if file exists\n\n    Returns:\n        QueryMetrics: The query metrics\n\n    Example: Save with overwrite mode (default)\n        ```python\n        df.write.parquet(\"output.parquet\")  # Overwrites if exists\n        ```\n\n    Example: Save with error mode\n        ```python\n        df.write.parquet(\"output.parquet\", mode=\"error\")  # Raises error if exists\n        ```\n\n    Example: Save with ignore mode\n        ```python\n        df.write.parquet(\"output.parquet\", mode=\"ignore\")  # Skips if exists\n        ```\n    \"\"\"\n    file_path = str(file_path)\n    if not file_path.endswith(\".parquet\"):\n        raise ValidationError(\n            f\"Parquet writer requires a '.parquet' file extension. \"\n            f\"Your path '{file_path}' is missing the extension.\"\n        )\n\n    sink_plan = FileSink(\n        child=self._dataframe._logical_plan,\n        sink_type=\"parquet\",\n        path=file_path,\n        mode=mode,\n    )\n\n    metrics = self._dataframe._logical_plan.session_state.execution.save_to_file(\n        sink_plan, file_path=file_path, mode=mode\n    )\n    logger.info(metrics.get_summary())\n    return metrics\n</code></pre>"},{"location":"reference/fenic/api/io/#fenic.api.io.DataFrameWriter.save_as_table","title":"save_as_table","text":"<pre><code>save_as_table(table_name: str, mode: Literal['error', 'append', 'overwrite', 'ignore'] = 'error') -&gt; QueryMetrics\n</code></pre> <p>Saves the content of the DataFrame as the specified table.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Name of the table to save to</p> </li> <li> <code>mode</code>               (<code>Literal['error', 'append', 'overwrite', 'ignore']</code>, default:                   <code>'error'</code> )           \u2013            <p>Write mode. Default is \"error\".  - error: Raises an error if table exists  - append: Appends data to table if it exists  - overwrite: Overwrites existing table  - ignore: Silently ignores operation if table exists</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>QueryMetrics</code> (              <code>QueryMetrics</code> )          \u2013            <p>The query metrics</p> </li> </ul> Save with error mode (default) <pre><code>df.write.save_as_table(\"my_table\")  # Raises error if table exists\n</code></pre> Save with append mode <pre><code>df.write.save_as_table(\"my_table\", mode=\"append\")  # Adds to existing table\n</code></pre> Save with overwrite mode <pre><code>df.write.save_as_table(\"my_table\", mode=\"overwrite\")  # Replaces existing table\n</code></pre> Source code in <code>src/fenic/api/io/writer.py</code> <pre><code>def save_as_table(\n    self,\n    table_name: str,\n    mode: Literal[\"error\", \"append\", \"overwrite\", \"ignore\"] = \"error\",\n) -&gt; QueryMetrics:\n    \"\"\"Saves the content of the DataFrame as the specified table.\n\n    Args:\n        table_name: Name of the table to save to\n        mode: Write mode. Default is \"error\".\n             - error: Raises an error if table exists\n             - append: Appends data to table if it exists\n             - overwrite: Overwrites existing table\n             - ignore: Silently ignores operation if table exists\n\n    Returns:\n        QueryMetrics: The query metrics\n\n    Example: Save with error mode (default)\n        ```python\n        df.write.save_as_table(\"my_table\")  # Raises error if table exists\n        ```\n\n    Example: Save with append mode\n        ```python\n        df.write.save_as_table(\"my_table\", mode=\"append\")  # Adds to existing table\n        ```\n\n    Example: Save with overwrite mode\n        ```python\n        df.write.save_as_table(\"my_table\", mode=\"overwrite\")  # Replaces existing table\n        ```\n    \"\"\"\n    sink_plan = TableSink(\n        child=self._dataframe._logical_plan, table_name=table_name, mode=mode\n    )\n\n    metrics = self._dataframe._logical_plan.session_state.execution.save_as_table(\n        sink_plan, table_name=table_name, mode=mode\n    )\n    logger.info(metrics.get_summary())\n    return metrics\n</code></pre>"},{"location":"reference/fenic/api/io/reader/","title":"fenic.api.io.reader","text":""},{"location":"reference/fenic/api/io/reader/#fenic.api.io.reader","title":"fenic.api.io.reader","text":"<p>Reader interface for loading DataFrames from external storage systems.</p> <p>Classes:</p> <ul> <li> <code>DataFrameReader</code>           \u2013            <p>Interface used to load a DataFrame from external storage systems.</p> </li> </ul>"},{"location":"reference/fenic/api/io/reader/#fenic.api.io.reader.DataFrameReader","title":"DataFrameReader","text":"<pre><code>DataFrameReader(session_state: BaseSessionState)\n</code></pre> <p>Interface used to load a DataFrame from external storage systems.</p> <p>Similar to PySpark's DataFrameReader.</p> <p>Creates a DataFrameReader.</p> <p>Parameters:</p> <ul> <li> <code>session_state</code>               (<code>BaseSessionState</code>)           \u2013            <p>The session state to use for reading</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>csv</code>             \u2013              <p>Load a DataFrame from one or more CSV files.</p> </li> <li> <code>parquet</code>             \u2013              <p>Load a DataFrame from one or more Parquet files.</p> </li> </ul> Source code in <code>src/fenic/api/io/reader.py</code> <pre><code>def __init__(self, session_state: BaseSessionState):\n    \"\"\"Creates a DataFrameReader.\n\n    Args:\n        session_state: The session state to use for reading\n    \"\"\"\n    self._options: Dict[str, Any] = {}\n    self._session_state = session_state\n</code></pre>"},{"location":"reference/fenic/api/io/reader/#fenic.api.io.reader.DataFrameReader.csv","title":"csv","text":"<pre><code>csv(paths: Union[str, Path, list[Union[str, Path]]], schema: Optional[Schema] = None, merge_schemas: bool = False) -&gt; DataFrame\n</code></pre> <p>Load a DataFrame from one or more CSV files.</p> <p>Parameters:</p> <ul> <li> <code>paths</code>               (<code>Union[str, Path, list[Union[str, Path]]]</code>)           \u2013            <p>A single file path, a glob pattern (e.g., \"data/*.csv\"), or a list of paths.</p> </li> <li> <code>schema</code>               (<code>Optional[Schema]</code>, default:                   <code>None</code> )           \u2013            <p>(optional) A complete schema definition of column names and their types. Only primitive types are supported. - For e.g.:     - Schema([ColumnField(name=\"id\", data_type=IntegerType), ColumnField(name=\"name\", data_type=StringType)]) - If provided, all files must match this schema exactly\u2014all column names must be present, and values must be convertible to the specified types. Partial schemas are not allowed.</p> </li> <li> <code>merge_schemas</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to merge schemas across all files. - If True: Column names are unified across files. Missing columns are filled with nulls. Column types are inferred and widened as needed. - If False (default): Only accepts columns from the first file. Column types from the first file are inferred and applied across all files. If subsequent files do not have the same column name and order as the first file, an error is raised. - The \"first file\" is defined as:     - The first file in lexicographic order (for glob patterns), or     - The first file in the provided list (for lists of paths).</p> </li> </ul> Notes <ul> <li>The first row in each file is assumed to be a header row.</li> <li>Delimiters (e.g., comma, tab) are automatically inferred.</li> <li>You may specify either <code>schema</code> or <code>merge_schemas=True</code>, but not both.</li> <li>Any date/datetime columns are cast to strings during ingestion.</li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If both <code>schema</code> and <code>merge_schemas=True</code> are provided.</p> </li> <li> <code>ValidationError</code>             \u2013            <p>If any path does not end with <code>.csv</code>.</p> </li> <li> <code>PlanError</code>             \u2013            <p>If schemas cannot be merged or if there's a schema mismatch when merge_schemas=False.</p> </li> </ul> Read a single CSV file <pre><code>df = session.read.csv(\"file.csv\")\n</code></pre> Read multiple CSV files with schema merging <pre><code>df = session.read.csv(\"data/*.csv\", merge_schemas=True)\n</code></pre> Read CSV files with explicit schema <p><code>python df = session.read.csv(     [\"a.csv\", \"b.csv\"],     schema=Schema([         ColumnField(name=\"id\", data_type=IntegerType),         ColumnField(name=\"value\", data_type=FloatType)     ]) )</code></p> Source code in <code>src/fenic/api/io/reader.py</code> <pre><code>def csv(\n    self,\n    paths: Union[str, Path, list[Union[str, Path]]],\n    schema: Optional[Schema] = None,\n    merge_schemas: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Load a DataFrame from one or more CSV files.\n\n    Args:\n        paths: A single file path, a glob pattern (e.g., \"data/*.csv\"), or a list of paths.\n        schema: (optional) A complete schema definition of column names and their types. Only primitive types are supported.\n            - For e.g.:\n                - Schema([ColumnField(name=\"id\", data_type=IntegerType), ColumnField(name=\"name\", data_type=StringType)])\n            - If provided, all files must match this schema exactly\u2014all column names must be present, and values must be\n            convertible to the specified types. Partial schemas are not allowed.\n        merge_schemas: Whether to merge schemas across all files.\n            - If True: Column names are unified across files. Missing columns are filled with nulls. Column types are\n            inferred and widened as needed.\n            - If False (default): Only accepts columns from the first file. Column types from the first file are\n            inferred and applied across all files. If subsequent files do not have the same column name and order as the first file, an error is raised.\n            - The \"first file\" is defined as:\n                - The first file in lexicographic order (for glob patterns), or\n                - The first file in the provided list (for lists of paths).\n\n    Notes:\n        - The first row in each file is assumed to be a header row.\n        - Delimiters (e.g., comma, tab) are automatically inferred.\n        - You may specify either `schema` or `merge_schemas=True`, but not both.\n        - Any date/datetime columns are cast to strings during ingestion.\n\n    Raises:\n        ValidationError: If both `schema` and `merge_schemas=True` are provided.\n        ValidationError: If any path does not end with `.csv`.\n        PlanError: If schemas cannot be merged or if there's a schema mismatch when merge_schemas=False.\n\n    Example: Read a single CSV file\n        ```python\n        df = session.read.csv(\"file.csv\")\n        ```\n\n    Example: Read multiple CSV files with schema merging\n        ```python\n        df = session.read.csv(\"data/*.csv\", merge_schemas=True)\n        ```\n\n    Example: Read CSV files with explicit schema\n        ```python\n        df = session.read.csv(\n            [\"a.csv\", \"b.csv\"],\n            schema=Schema([\n                ColumnField(name=\"id\", data_type=IntegerType),\n                ColumnField(name=\"value\", data_type=FloatType)\n            ])\n        )            ```\n    \"\"\"\n    if schema is not None and merge_schemas:\n        raise ValidationError(\n            \"Cannot specify both 'schema' and 'merge_schemas=True' - these options conflict. \"\n            \"Choose one approach: \"\n            \"1) Use 'schema' to enforce a specific schema: csv(paths, schema=your_schema), \"\n            \"2) Use 'merge_schemas=True' to automatically merge schemas: csv(paths, merge_schemas=True), \"\n            \"3) Use neither to inherit schema from the first file: csv(paths)\"\n        )\n    if schema is not None:\n        for col_field in schema.column_fields:\n            if not isinstance(\n                col_field.data_type,\n                _PrimitiveType,\n            ):\n                raise ValidationError(\n                    f\"CSV files only support primitive data types in schema definitions. \"\n                    f\"Column '{col_field.name}' has type {type(col_field.data_type).__name__}, but CSV schemas must use: \"\n                    f\"IntegerType, FloatType, DoubleType, BooleanType, or StringType. \"\n                    f\"Example: Schema([ColumnField(name='id', data_type=IntegerType), ColumnField(name='name', data_type=StringType)])\"\n                )\n    options = {\n        \"schema\": schema,\n        \"merge_schemas\": merge_schemas,\n    }\n    return self._read_file(\n        paths, file_format=\"csv\", file_extension=\".csv\", **options\n    )\n</code></pre>"},{"location":"reference/fenic/api/io/reader/#fenic.api.io.reader.DataFrameReader.parquet","title":"parquet","text":"<pre><code>parquet(paths: Union[str, Path, list[Union[str, Path]]], merge_schemas: bool = False) -&gt; DataFrame\n</code></pre> <p>Load a DataFrame from one or more Parquet files.</p> <p>Parameters:</p> <ul> <li> <code>paths</code>               (<code>Union[str, Path, list[Union[str, Path]]]</code>)           \u2013            <p>A single file path, a glob pattern (e.g., \"data/*.parquet\"), or a list of paths.</p> </li> <li> <code>merge_schemas</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, infers and merges schemas across all files. Missing columns are filled with nulls, and differing types are widened to a common supertype.</p> </li> </ul> Behavior <ul> <li>If <code>merge_schemas=False</code> (default), all files must match the schema of the first file exactly. Subsequent files must contain all columns from the first file with compatible data types. If any column is missing or has incompatible types, an error is raised.</li> <li>If <code>merge_schemas=True</code>, column names are unified across all files, and data types are automatically widened to accommodate all values.</li> <li>The \"first file\" is defined as:<ul> <li>The first file in lexicographic order (for glob patterns), or</li> <li>The first file in the provided list (for lists of paths).</li> </ul> </li> </ul> Notes <ul> <li>Date and datetime columns are cast to strings during ingestion.</li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If any file does not have a <code>.parquet</code> extension.</p> </li> <li> <code>PlanError</code>             \u2013            <p>If schemas cannot be merged or if there's a schema mismatch when merge_schemas=False.</p> </li> </ul> Read a single Parquet file <pre><code>df = session.read.parquet(\"file.parquet\")\n</code></pre> Read multiple Parquet files <pre><code>df = session.read.parquet(\"data/*.parquet\")\n</code></pre> Read Parquet files with schema merging <pre><code>df = session.read.parquet([\"a.parquet\", \"b.parquet\"], merge_schemas=True)\n</code></pre> Source code in <code>src/fenic/api/io/reader.py</code> <pre><code>def parquet(\n    self,\n    paths: Union[str, Path, list[Union[str, Path]]],\n    merge_schemas: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Load a DataFrame from one or more Parquet files.\n\n    Args:\n        paths: A single file path, a glob pattern (e.g., \"data/*.parquet\"), or a list of paths.\n        merge_schemas: If True, infers and merges schemas across all files.\n            Missing columns are filled with nulls, and differing types are widened to a common supertype.\n\n    Behavior:\n        - If `merge_schemas=False` (default), all files must match the schema of the first file exactly.\n        Subsequent files must contain all columns from the first file with compatible data types.\n        If any column is missing or has incompatible types, an error is raised.\n        - If `merge_schemas=True`, column names are unified across all files, and data types are automatically\n        widened to accommodate all values.\n        - The \"first file\" is defined as:\n            - The first file in lexicographic order (for glob patterns), or\n            - The first file in the provided list (for lists of paths).\n\n    Notes:\n        - Date and datetime columns are cast to strings during ingestion.\n\n    Raises:\n        ValidationError: If any file does not have a `.parquet` extension.\n        PlanError: If schemas cannot be merged or if there's a schema mismatch when merge_schemas=False.\n\n    Example: Read a single Parquet file\n        ```python\n        df = session.read.parquet(\"file.parquet\")\n        ```\n\n    Example: Read multiple Parquet files\n        ```python\n        df = session.read.parquet(\"data/*.parquet\")\n        ```\n\n    Example: Read Parquet files with schema merging\n        ```python\n        df = session.read.parquet([\"a.parquet\", \"b.parquet\"], merge_schemas=True)\n        ```\n    \"\"\"\n    options = {\n        \"merge_schemas\": merge_schemas,\n    }\n    return self._read_file(\n        paths, file_format=\"parquet\", file_extension=\".parquet\", **options\n    )\n</code></pre>"},{"location":"reference/fenic/api/io/writer/","title":"fenic.api.io.writer","text":""},{"location":"reference/fenic/api/io/writer/#fenic.api.io.writer","title":"fenic.api.io.writer","text":"<p>Writer interface for saving DataFrames to external storage systems.</p> <p>Classes:</p> <ul> <li> <code>DataFrameWriter</code>           \u2013            <p>Interface used to write a DataFrame to external storage systems.</p> </li> </ul>"},{"location":"reference/fenic/api/io/writer/#fenic.api.io.writer.DataFrameWriter","title":"DataFrameWriter","text":"<pre><code>DataFrameWriter(dataframe: DataFrame)\n</code></pre> <p>Interface used to write a DataFrame to external storage systems.</p> <p>Similar to PySpark's DataFrameWriter.</p> <p>Initialize a DataFrameWriter.</p> <p>Parameters:</p> <ul> <li> <code>dataframe</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame to write.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>csv</code>             \u2013              <p>Saves the content of the DataFrame as a single CSV file with comma as the delimiter and headers in the first row.</p> </li> <li> <code>parquet</code>             \u2013              <p>Saves the content of the DataFrame as a single Parquet file.</p> </li> <li> <code>save_as_table</code>             \u2013              <p>Saves the content of the DataFrame as the specified table.</p> </li> </ul> Source code in <code>src/fenic/api/io/writer.py</code> <pre><code>def __init__(self, dataframe: DataFrame):\n    \"\"\"Initialize a DataFrameWriter.\n\n    Args:\n        dataframe: The DataFrame to write.\n    \"\"\"\n    self._dataframe = dataframe\n</code></pre>"},{"location":"reference/fenic/api/io/writer/#fenic.api.io.writer.DataFrameWriter.csv","title":"csv","text":"<pre><code>csv(file_path: Union[str, Path], mode: Literal['error', 'overwrite', 'ignore'] = 'overwrite') -&gt; QueryMetrics\n</code></pre> <p>Saves the content of the DataFrame as a single CSV file with comma as the delimiter and headers in the first row.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path to save the CSV file to</p> </li> <li> <code>mode</code>               (<code>Literal['error', 'overwrite', 'ignore']</code>, default:                   <code>'overwrite'</code> )           \u2013            <p>Write mode. Default is \"overwrite\".  - error: Raises an error if file exists  - overwrite: Overwrites the file if it exists  - ignore: Silently ignores operation if file exists</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>QueryMetrics</code> (              <code>QueryMetrics</code> )          \u2013            <p>The query metrics</p> </li> </ul> Save with overwrite mode (default) <pre><code>df.write.csv(\"output.csv\")  # Overwrites if exists\n</code></pre> Save with error mode <pre><code>df.write.csv(\"output.csv\", mode=\"error\")  # Raises error if exists\n</code></pre> Save with ignore mode <pre><code>df.write.csv(\"output.csv\", mode=\"ignore\")  # Skips if exists\n</code></pre> Source code in <code>src/fenic/api/io/writer.py</code> <pre><code>def csv(\n    self,\n    file_path: Union[str, Path],\n    mode: Literal[\"error\", \"overwrite\", \"ignore\"] = \"overwrite\",\n) -&gt; QueryMetrics:\n    \"\"\"Saves the content of the DataFrame as a single CSV file with comma as the delimiter and headers in the first row.\n\n    Args:\n        file_path: Path to save the CSV file to\n        mode: Write mode. Default is \"overwrite\".\n             - error: Raises an error if file exists\n             - overwrite: Overwrites the file if it exists\n             - ignore: Silently ignores operation if file exists\n\n    Returns:\n        QueryMetrics: The query metrics\n\n    Example: Save with overwrite mode (default)\n        ```python\n        df.write.csv(\"output.csv\")  # Overwrites if exists\n        ```\n\n    Example: Save with error mode\n        ```python\n        df.write.csv(\"output.csv\", mode=\"error\")  # Raises error if exists\n        ```\n\n    Example: Save with ignore mode\n        ```python\n        df.write.csv(\"output.csv\", mode=\"ignore\")  # Skips if exists\n        ```\n    \"\"\"\n    file_path = str(file_path)\n    if not file_path.endswith(\".csv\"):\n        raise ValidationError(\n            f\"CSV writer requires a '.csv' file extension. \"\n            f\"Your path '{file_path}' is missing the extension.\"\n        )\n\n    sink_plan = FileSink(\n        child=self._dataframe._logical_plan,\n        sink_type=\"csv\",\n        path=file_path,\n        mode=mode,\n    )\n\n    metrics = self._dataframe._logical_plan.session_state.execution.save_to_file(\n        sink_plan, file_path=file_path, mode=mode\n    )\n    logger.info(metrics.get_summary())\n    return metrics\n</code></pre>"},{"location":"reference/fenic/api/io/writer/#fenic.api.io.writer.DataFrameWriter.parquet","title":"parquet","text":"<pre><code>parquet(file_path: Union[str, Path], mode: Literal['error', 'overwrite', 'ignore'] = 'overwrite') -&gt; QueryMetrics\n</code></pre> <p>Saves the content of the DataFrame as a single Parquet file.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path to save the Parquet file to</p> </li> <li> <code>mode</code>               (<code>Literal['error', 'overwrite', 'ignore']</code>, default:                   <code>'overwrite'</code> )           \u2013            <p>Write mode. Default is \"overwrite\".  - error: Raises an error if file exists  - overwrite: Overwrites the file if it exists  - ignore: Silently ignores operation if file exists</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>QueryMetrics</code> (              <code>QueryMetrics</code> )          \u2013            <p>The query metrics</p> </li> </ul> Save with overwrite mode (default) <pre><code>df.write.parquet(\"output.parquet\")  # Overwrites if exists\n</code></pre> Save with error mode <pre><code>df.write.parquet(\"output.parquet\", mode=\"error\")  # Raises error if exists\n</code></pre> Save with ignore mode <pre><code>df.write.parquet(\"output.parquet\", mode=\"ignore\")  # Skips if exists\n</code></pre> Source code in <code>src/fenic/api/io/writer.py</code> <pre><code>def parquet(\n    self,\n    file_path: Union[str, Path],\n    mode: Literal[\"error\", \"overwrite\", \"ignore\"] = \"overwrite\",\n) -&gt; QueryMetrics:\n    \"\"\"Saves the content of the DataFrame as a single Parquet file.\n\n    Args:\n        file_path: Path to save the Parquet file to\n        mode: Write mode. Default is \"overwrite\".\n             - error: Raises an error if file exists\n             - overwrite: Overwrites the file if it exists\n             - ignore: Silently ignores operation if file exists\n\n    Returns:\n        QueryMetrics: The query metrics\n\n    Example: Save with overwrite mode (default)\n        ```python\n        df.write.parquet(\"output.parquet\")  # Overwrites if exists\n        ```\n\n    Example: Save with error mode\n        ```python\n        df.write.parquet(\"output.parquet\", mode=\"error\")  # Raises error if exists\n        ```\n\n    Example: Save with ignore mode\n        ```python\n        df.write.parquet(\"output.parquet\", mode=\"ignore\")  # Skips if exists\n        ```\n    \"\"\"\n    file_path = str(file_path)\n    if not file_path.endswith(\".parquet\"):\n        raise ValidationError(\n            f\"Parquet writer requires a '.parquet' file extension. \"\n            f\"Your path '{file_path}' is missing the extension.\"\n        )\n\n    sink_plan = FileSink(\n        child=self._dataframe._logical_plan,\n        sink_type=\"parquet\",\n        path=file_path,\n        mode=mode,\n    )\n\n    metrics = self._dataframe._logical_plan.session_state.execution.save_to_file(\n        sink_plan, file_path=file_path, mode=mode\n    )\n    logger.info(metrics.get_summary())\n    return metrics\n</code></pre>"},{"location":"reference/fenic/api/io/writer/#fenic.api.io.writer.DataFrameWriter.save_as_table","title":"save_as_table","text":"<pre><code>save_as_table(table_name: str, mode: Literal['error', 'append', 'overwrite', 'ignore'] = 'error') -&gt; QueryMetrics\n</code></pre> <p>Saves the content of the DataFrame as the specified table.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Name of the table to save to</p> </li> <li> <code>mode</code>               (<code>Literal['error', 'append', 'overwrite', 'ignore']</code>, default:                   <code>'error'</code> )           \u2013            <p>Write mode. Default is \"error\".  - error: Raises an error if table exists  - append: Appends data to table if it exists  - overwrite: Overwrites existing table  - ignore: Silently ignores operation if table exists</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>QueryMetrics</code> (              <code>QueryMetrics</code> )          \u2013            <p>The query metrics</p> </li> </ul> Save with error mode (default) <pre><code>df.write.save_as_table(\"my_table\")  # Raises error if table exists\n</code></pre> Save with append mode <pre><code>df.write.save_as_table(\"my_table\", mode=\"append\")  # Adds to existing table\n</code></pre> Save with overwrite mode <pre><code>df.write.save_as_table(\"my_table\", mode=\"overwrite\")  # Replaces existing table\n</code></pre> Source code in <code>src/fenic/api/io/writer.py</code> <pre><code>def save_as_table(\n    self,\n    table_name: str,\n    mode: Literal[\"error\", \"append\", \"overwrite\", \"ignore\"] = \"error\",\n) -&gt; QueryMetrics:\n    \"\"\"Saves the content of the DataFrame as the specified table.\n\n    Args:\n        table_name: Name of the table to save to\n        mode: Write mode. Default is \"error\".\n             - error: Raises an error if table exists\n             - append: Appends data to table if it exists\n             - overwrite: Overwrites existing table\n             - ignore: Silently ignores operation if table exists\n\n    Returns:\n        QueryMetrics: The query metrics\n\n    Example: Save with error mode (default)\n        ```python\n        df.write.save_as_table(\"my_table\")  # Raises error if table exists\n        ```\n\n    Example: Save with append mode\n        ```python\n        df.write.save_as_table(\"my_table\", mode=\"append\")  # Adds to existing table\n        ```\n\n    Example: Save with overwrite mode\n        ```python\n        df.write.save_as_table(\"my_table\", mode=\"overwrite\")  # Replaces existing table\n        ```\n    \"\"\"\n    sink_plan = TableSink(\n        child=self._dataframe._logical_plan, table_name=table_name, mode=mode\n    )\n\n    metrics = self._dataframe._logical_plan.session_state.execution.save_as_table(\n        sink_plan, table_name=table_name, mode=mode\n    )\n    logger.info(metrics.get_summary())\n    return metrics\n</code></pre>"},{"location":"reference/fenic/api/lineage/","title":"fenic.api.lineage","text":""},{"location":"reference/fenic/api/lineage/#fenic.api.lineage","title":"fenic.api.lineage","text":"<p>Query interface for tracing data lineage through a query plan.</p> <p>Classes:</p> <ul> <li> <code>Lineage</code>           \u2013            <p>Query interface for tracing data lineage through a query plan.</p> </li> </ul>"},{"location":"reference/fenic/api/lineage/#fenic.api.lineage.Lineage","title":"Lineage","text":"<pre><code>Lineage(lineage: BaseLineage)\n</code></pre> <p>Query interface for tracing data lineage through a query plan.</p> <p>This class allows you to navigate through the query plan both forwards and backwards, tracing how specific rows are transformed through each operation.</p> Example <pre><code># Create a lineage query starting from the root\nquery = LineageQuery(lineage, session.execution)\n\n# Or start from a specific source\nquery.start_from_source(\"my_table\")\n\n# Trace rows backwards through a transformation\nresult = query.backward([\"uuid1\", \"uuid2\"])\n\n# Trace rows forward to see their outputs\nresult = query.forward([\"uuid3\", \"uuid4\"])\n</code></pre> <p>Initialize a Lineage instance.</p> <p>Parameters:</p> <ul> <li> <code>lineage</code>               (<code>BaseLineage</code>)           \u2013            <p>The underlying lineage implementation.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>backwards</code>             \u2013              <p>Trace rows backwards to see which input rows produced them.</p> </li> <li> <code>forwards</code>             \u2013              <p>Trace rows forward to see how they are transformed by the next operation.</p> </li> <li> <code>get_result_df</code>             \u2013              <p>Get the result of the query as a Polars DataFrame.</p> </li> <li> <code>get_source_df</code>             \u2013              <p>Get a query source by name as a Polars DataFrame.</p> </li> <li> <code>get_source_names</code>             \u2013              <p>Get the names of all sources in the query plan. Used to determine where to start the lineage traversal.</p> </li> <li> <code>show</code>             \u2013              <p>Print the operator tree of the query.</p> </li> <li> <code>skip_backwards</code>             \u2013              <p>[Not Implemented] Trace rows backwards through multiple operations at once.</p> </li> <li> <code>skip_forwards</code>             \u2013              <p>[Not Implemented] Trace rows forward through multiple operations at once.</p> </li> <li> <code>start_from_source</code>             \u2013              <p>Set the current position to a specific source in the query plan.</p> </li> </ul> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>def __init__(self, lineage: BaseLineage):\n    \"\"\"Initialize a Lineage instance.\n\n    Args:\n        lineage: The underlying lineage implementation.\n    \"\"\"\n    self.lineage = lineage\n</code></pre>"},{"location":"reference/fenic/api/lineage/#fenic.api.lineage.Lineage.backwards","title":"backwards","text":"<pre><code>backwards(ids: List[str], branch_side: Optional[BranchSide] = None) -&gt; pl.DataFrame\n</code></pre> <p>Trace rows backwards to see which input rows produced them.</p> <p>Parameters:</p> <ul> <li> <code>ids</code>               (<code>List[str]</code>)           \u2013            <p>List of UUIDs identifying the rows to trace back</p> </li> <li> <code>branch_side</code>               (<code>Optional[BranchSide]</code>, default:                   <code>None</code> )           \u2013            <p>For operators with multiple inputs (like joins), specify which input to trace (\"left\" or \"right\"). Not needed for single-input operations.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame containing the source rows that produced the specified outputs</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If invalid ids format or incorrect branch_side specification</p> </li> </ul> Example <pre><code># Simple backward trace\nsource_rows = query.backward([\"result_uuid1\"])\n\n# Trace back through a join\nleft_rows = query.backward([\"join_uuid1\"], branch_side=\"left\")\nright_rows = query.backward([\"join_uuid1\"], branch_side=\"right\")\n</code></pre> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef backwards(\n    self, ids: List[str], branch_side: Optional[BranchSide] = None\n) -&gt; pl.DataFrame:\n    \"\"\"Trace rows backwards to see which input rows produced them.\n\n    Args:\n        ids: List of UUIDs identifying the rows to trace back\n        branch_side: For operators with multiple inputs (like joins), specify which\n            input to trace (\"left\" or \"right\"). Not needed for single-input operations.\n\n    Returns:\n        DataFrame containing the source rows that produced the specified outputs\n\n    Raises:\n        ValueError: If invalid ids format or incorrect branch_side specification\n\n    Example:\n        ```python\n        # Simple backward trace\n        source_rows = query.backward([\"result_uuid1\"])\n\n        # Trace back through a join\n        left_rows = query.backward([\"join_uuid1\"], branch_side=\"left\")\n        right_rows = query.backward([\"join_uuid1\"], branch_side=\"right\")\n        ```\n    \"\"\"\n    return self.lineage.backwards(ids, branch_side)\n</code></pre>"},{"location":"reference/fenic/api/lineage/#fenic.api.lineage.Lineage.forwards","title":"forwards","text":"<pre><code>forwards(row_ids: List[str]) -&gt; pl.DataFrame\n</code></pre> <p>Trace rows forward to see how they are transformed by the next operation.</p> <p>Parameters:</p> <ul> <li> <code>row_ids</code>               (<code>List[str]</code>)           \u2013            <p>List of UUIDs identifying the rows to trace</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame containing the transformed rows in the next operation</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If at root node or if row_ids format is invalid</p> </li> </ul> Example <pre><code># Trace how specific customer rows are transformed\ntransformed = query.forward([\"customer_uuid1\", \"customer_uuid2\"])\n</code></pre> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef forwards(self, row_ids: List[str]) -&gt; pl.DataFrame:\n    \"\"\"Trace rows forward to see how they are transformed by the next operation.\n\n    Args:\n        row_ids: List of UUIDs identifying the rows to trace\n\n    Returns:\n        DataFrame containing the transformed rows in the next operation\n\n    Raises:\n        ValueError: If at root node or if row_ids format is invalid\n\n    Example:\n        ```python\n        # Trace how specific customer rows are transformed\n        transformed = query.forward([\"customer_uuid1\", \"customer_uuid2\"])\n        ```\n    \"\"\"\n    return self.lineage.forwards(row_ids)\n</code></pre>"},{"location":"reference/fenic/api/lineage/#fenic.api.lineage.Lineage.get_result_df","title":"get_result_df","text":"<pre><code>get_result_df() -&gt; pl.DataFrame\n</code></pre> <p>Get the result of the query as a Polars DataFrame.</p> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>def get_result_df(self) -&gt; pl.DataFrame:\n    \"\"\"Get the result of the query as a Polars DataFrame.\"\"\"\n    return self.lineage.get_result_df()\n</code></pre>"},{"location":"reference/fenic/api/lineage/#fenic.api.lineage.Lineage.get_source_df","title":"get_source_df","text":"<pre><code>get_source_df(source_name: str) -&gt; pl.DataFrame\n</code></pre> <p>Get a query source by name as a Polars DataFrame.</p> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef get_source_df(self, source_name: str) -&gt; pl.DataFrame:\n    \"\"\"Get a query source by name as a Polars DataFrame.\"\"\"\n    return self.lineage.get_source_df(source_name)\n</code></pre>"},{"location":"reference/fenic/api/lineage/#fenic.api.lineage.Lineage.get_source_names","title":"get_source_names","text":"<pre><code>get_source_names() -&gt; List[str]\n</code></pre> <p>Get the names of all sources in the query plan. Used to determine where to start the lineage traversal.</p> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef get_source_names(self) -&gt; List[str]:\n    \"\"\"Get the names of all sources in the query plan. Used to determine where to start the lineage traversal.\"\"\"\n    return self.lineage.get_source_names()\n</code></pre>"},{"location":"reference/fenic/api/lineage/#fenic.api.lineage.Lineage.show","title":"show","text":"<pre><code>show() -&gt; None\n</code></pre> <p>Print the operator tree of the query.</p> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>def show(self) -&gt; None:\n    \"\"\"Print the operator tree of the query.\"\"\"\n    print(self.lineage.stringify_graph())\n</code></pre>"},{"location":"reference/fenic/api/lineage/#fenic.api.lineage.Lineage.skip_backwards","title":"skip_backwards","text":"<pre><code>skip_backwards(ids: List[str]) -&gt; Dict[str, pl.DataFrame]\n</code></pre> <p>[Not Implemented] Trace rows backwards through multiple operations at once.</p> <p>This method will allow efficient tracing through multiple operations without intermediate results.</p> <p>Parameters:</p> <ul> <li> <code>ids</code>               (<code>List[str]</code>)           \u2013            <p>List of UUIDs identifying the rows to trace back</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, DataFrame]</code>           \u2013            <p>Dictionary mapping operation names to their source DataFrames</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>This method is not yet implemented</p> </li> </ul> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>def skip_backwards(self, ids: List[str]) -&gt; Dict[str, pl.DataFrame]:\n    \"\"\"[Not Implemented] Trace rows backwards through multiple operations at once.\n\n    This method will allow efficient tracing through multiple operations without\n    intermediate results.\n\n    Args:\n        ids: List of UUIDs identifying the rows to trace back\n\n    Returns:\n        Dictionary mapping operation names to their source DataFrames\n\n    Raises:\n        NotImplementedError: This method is not yet implemented\n    \"\"\"\n    raise NotImplementedError(\"Skip backwards not yet implemented\")\n</code></pre>"},{"location":"reference/fenic/api/lineage/#fenic.api.lineage.Lineage.skip_forwards","title":"skip_forwards","text":"<pre><code>skip_forwards(row_ids: List[str]) -&gt; pl.DataFrame\n</code></pre> <p>[Not Implemented] Trace rows forward through multiple operations at once.</p> <p>This method will allow efficient tracing through multiple operations without intermediate results.</p> <p>Parameters:</p> <ul> <li> <code>row_ids</code>               (<code>List[str]</code>)           \u2013            <p>List of UUIDs identifying the rows to trace</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame containing the final transformed rows</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>This method is not yet implemented</p> </li> </ul> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>def skip_forwards(self, row_ids: List[str]) -&gt; pl.DataFrame:\n    \"\"\"[Not Implemented] Trace rows forward through multiple operations at once.\n\n    This method will allow efficient tracing through multiple operations without\n    intermediate results.\n\n    Args:\n        row_ids: List of UUIDs identifying the rows to trace\n\n    Returns:\n        DataFrame containing the final transformed rows\n\n    Raises:\n        NotImplementedError: This method is not yet implemented\n    \"\"\"\n    raise NotImplementedError(\"Skip forwards not yet implemented\")\n</code></pre>"},{"location":"reference/fenic/api/lineage/#fenic.api.lineage.Lineage.start_from_source","title":"start_from_source","text":"<pre><code>start_from_source(source_name: str) -&gt; None\n</code></pre> <p>Set the current position to a specific source in the query plan.</p> <p>Parameters:</p> <ul> <li> <code>source_name</code>               (<code>str</code>)           \u2013            <p>Name of the source table to start from</p> </li> </ul> Example <pre><code>query.start_from_source(\"customers\")\n# Now you can trace forward from the customers table\n</code></pre> Source code in <code>src/fenic/api/lineage.py</code> <pre><code>@validate_call(config=ConfigDict(strict=True))\ndef start_from_source(self, source_name: str) -&gt; None:\n    \"\"\"Set the current position to a specific source in the query plan.\n\n    Args:\n        source_name: Name of the source table to start from\n\n    Example:\n        ```python\n        query.start_from_source(\"customers\")\n        # Now you can trace forward from the customers table\n        ```\n    \"\"\"\n    self.lineage.start_from_source(source_name)\n</code></pre>"},{"location":"reference/fenic/api/session/","title":"fenic.api.session","text":""},{"location":"reference/fenic/api/session/#fenic.api.session","title":"fenic.api.session","text":"<p>Session module for managing query execution context and state.</p> <p>Classes:</p> <ul> <li> <code>AnthropicModelConfig</code>           \u2013            <p>Configuration for Anthropic models.</p> </li> <li> <code>CloudConfig</code>           \u2013            <p>Configuration for cloud-based execution.</p> </li> <li> <code>CloudExecutorSize</code>           \u2013            <p>Enum defining available cloud executor sizes.</p> </li> <li> <code>GoogleGLAModelConfig</code>           \u2013            <p>Configuration for Google GenerativeLAnguage (GLA) models.</p> </li> <li> <code>OpenAIModelConfig</code>           \u2013            <p>Configuration for OpenAI models.</p> </li> <li> <code>SemanticConfig</code>           \u2013            <p>Configuration for semantic language and embedding models.</p> </li> <li> <code>Session</code>           \u2013            <p>The entry point to programming with the DataFrame API. Similar to PySpark's SparkSession.</p> </li> <li> <code>SessionConfig</code>           \u2013            <p>Configuration for a user session.</p> </li> </ul>"},{"location":"reference/fenic/api/session/#fenic.api.session.AnthropicModelConfig","title":"AnthropicModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Anthropic models.</p> <p>This class defines the configuration settings for Anthropic language models, including model selection and separate rate limiting parameters for input and output tokens.</p> <p>Attributes:</p> <ul> <li> <code>model_name</code>               (<code>ANTHROPIC_AVAILABLE_LANGUAGE_MODELS</code>)           \u2013            <p>The name of the Anthropic model to use.</p> </li> <li> <code>rpm</code>               (<code>int</code>)           \u2013            <p>Requests per minute limit; must be greater than 0.</p> </li> <li> <code>input_tpm</code>               (<code>int</code>)           \u2013            <p>Input tokens per minute limit; must be greater than 0.</p> </li> <li> <code>output_tpm</code>               (<code>int</code>)           \u2013            <p>Output tokens per minute limit; must be greater than 0.</p> </li> </ul> <p>Examples:</p> <p>Configuring an Anthropic model with separate input/output rate limits:</p> <pre><code>config = AnthropicModelConfig(\n    model_name=\"claude-3-5-haiku-latest\",\n    rpm=100,\n    input_tpm=100,\n    output_tpm=100\n)\n</code></pre>"},{"location":"reference/fenic/api/session/#fenic.api.session.CloudConfig","title":"CloudConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for cloud-based execution.</p> <p>This class defines settings for running operations in a cloud environment, allowing for scalable and distributed processing of language model operations.</p> <p>Attributes:</p> <ul> <li> <code>size</code>               (<code>Optional[CloudExecutorSize]</code>)           \u2013            <p>Size of the cloud executor instance. If None, the default size will be used.</p> </li> </ul>"},{"location":"reference/fenic/api/session/#fenic.api.session.CloudExecutorSize","title":"CloudExecutorSize","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum defining available cloud executor sizes.</p> <p>This enum represents the different size options available for cloud-based execution environments.</p> <p>Attributes:</p> <ul> <li> <code>SMALL</code>           \u2013            <p>Small instance size.</p> </li> <li> <code>MEDIUM</code>           \u2013            <p>Medium instance size.</p> </li> <li> <code>LARGE</code>           \u2013            <p>Large instance size.</p> </li> <li> <code>XLARGE</code>           \u2013            <p>Extra large instance size.</p> </li> </ul>"},{"location":"reference/fenic/api/session/#fenic.api.session.GoogleGLAModelConfig","title":"GoogleGLAModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Google GenerativeLAnguage (GLA) models.</p> <p>This class defines the configuration settings for models available in Google Developer AI Studio, including model selection and rate limiting parameters. These models are accessible using a GEMINI_API_KEY environment variable.</p>"},{"location":"reference/fenic/api/session/#fenic.api.session.OpenAIModelConfig","title":"OpenAIModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for OpenAI models.</p> <p>This class defines the configuration settings for OpenAI language and embedding models, including model selection and rate limiting parameters.</p> <p>Attributes:</p> <ul> <li> <code>model_name</code>               (<code>Union[OPENAI_AVAILABLE_LANGUAGE_MODELS, OPENAI_AVAILABLE_EMBEDDING_MODELS]</code>)           \u2013            <p>The name of the OpenAI model to use.</p> </li> <li> <code>rpm</code>               (<code>int</code>)           \u2013            <p>Requests per minute limit; must be greater than 0.</p> </li> <li> <code>tpm</code>               (<code>int</code>)           \u2013            <p>Tokens per minute limit; must be greater than 0.</p> </li> </ul> <p>Examples:</p> <p>Configuring an OpenAI Language model with rate limits:</p> <pre><code>config = OpenAIModelConfig(model_name=\"gpt-4.1-nano\", rpm=100, tpm=100)\n</code></pre> <p>Configuring an OpenAI Embedding model with rate limits:</p> <pre><code>config = OpenAIModelConfig(model_name=\"text-embedding-3-small\", rpm=100, tpm=100)\n</code></pre>"},{"location":"reference/fenic/api/session/#fenic.api.session.SemanticConfig","title":"SemanticConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for semantic language and embedding models.</p> <p>This class defines the configuration for both language models and optional embedding models used in semantic operations. It ensures that all configured models are valid and supported by their respective providers.</p> <p>Attributes:</p> <ul> <li> <code>language_models</code>               (<code>dict[str, ModelConfig]</code>)           \u2013            <p>Mapping of model aliases to language model configurations.</p> </li> <li> <code>default_language_model</code>               (<code>Optional[str]</code>)           \u2013            <p>The alias of the default language model to use for semantic operations. Not required if only one language model is configured.</p> </li> <li> <code>embedding_models</code>               (<code>Optional[dict[str, ModelConfig]]</code>)           \u2013            <p>Optional mapping of model aliases to embedding model configurations.</p> </li> <li> <code>default_embedding_model</code>               (<code>Optional[str]</code>)           \u2013            <p>The alias of the default embedding model to use for semantic operations.</p> </li> </ul> Note <p>The embedding model is optional and only required for operations that need semantic search or embedding capabilities.</p> <p>Methods:</p> <ul> <li> <code>model_post_init</code>             \u2013              <p>Post initialization hook to set defaults.</p> </li> <li> <code>validate_models</code>             \u2013              <p>Validates that the selected models are supported by the system.</p> </li> </ul>"},{"location":"reference/fenic/api/session/#fenic.api.session.SemanticConfig.model_post_init","title":"model_post_init","text":"<pre><code>model_post_init(__context) -&gt; None\n</code></pre> <p>Post initialization hook to set defaults.</p> <p>This hook runs after the model is initialized and validated. It sets the default language and embedding models if they are not set and there is only one model available.</p> Source code in <code>src/fenic/api/session/config.py</code> <pre><code>def model_post_init(self, __context) -&gt; None:\n    \"\"\"Post initialization hook to set defaults.\n\n    This hook runs after the model is initialized and validated.\n    It sets the default language and embedding models if they are not set\n    and there is only one model available.\n    \"\"\"\n    # Set default language model if not set and only one model exists\n    if self.default_language_model is None and len(self.language_models) == 1:\n        self.default_language_model = list(self.language_models.keys())[0]\n    # Set default embedding model if not set and only one model exists\n    if self.embedding_models is not None and self.default_embedding_model is None and len(self.embedding_models) == 1:\n        self.default_embedding_model = list(self.embedding_models.keys())[0]\n</code></pre>"},{"location":"reference/fenic/api/session/#fenic.api.session.SemanticConfig.validate_models","title":"validate_models","text":"<pre><code>validate_models() -&gt; SemanticConfig\n</code></pre> <p>Validates that the selected models are supported by the system.</p> <p>This validator checks that both the language model and embedding model (if provided) are valid and supported by their respective providers.</p> <p>Returns:</p> <ul> <li> <code>SemanticConfig</code>           \u2013            <p>The validated SemanticConfig instance.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ConfigurationError</code>             \u2013            <p>If any of the models are not supported.</p> </li> </ul> Source code in <code>src/fenic/api/session/config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_models(self) -&gt; SemanticConfig:\n    \"\"\"Validates that the selected models are supported by the system.\n\n    This validator checks that both the language model and embedding model (if provided)\n    are valid and supported by their respective providers.\n\n    Returns:\n        The validated SemanticConfig instance.\n\n    Raises:\n        ConfigurationError: If any of the models are not supported.\n    \"\"\"\n    if len(self.language_models) == 0:\n        raise ConfigurationError(\"You must specify at least one language model configuration.\")\n    available_language_model_aliases = list(self.language_models.keys())\n    if self.default_language_model is None and len(self.language_models) &gt; 1:\n        raise ConfigurationError(f\"default_language_model is not set, and multiple language models are configured. Please specify one of: {available_language_model_aliases} as a default_language_model.\")\n\n    if self.default_language_model is not None and self.default_language_model not in self.language_models:\n        raise ConfigurationError(f\"default_language_model {self.default_language_model} is not in configured map of language models. Available models: {available_language_model_aliases} .\")\n\n    for model_alias, language_model in self.language_models.items():\n        if isinstance(language_model, OpenAIModelConfig):\n            language_model_provider = ModelProvider.OPENAI\n            language_model_name = language_model.model_name\n        elif isinstance(language_model, AnthropicModelConfig):\n            language_model_provider = ModelProvider.ANTHROPIC\n            language_model_name = language_model.model_name\n        elif isinstance(language_model, GoogleGLAModelConfig):\n            language_model_provider = ModelProvider.GOOGLE_GLA\n            language_model_name = language_model.model_name\n        else:\n            raise ConfigurationError(\n                f\"Invalid language model: {model_alias}: {language_model} unsupported model type.\")\n\n        completion_model = model_catalog.get_completion_model_parameters(language_model_provider,\n                                                                         language_model_name)\n        if completion_model is None:\n            raise ConfigurationError(\n                model_catalog.generate_unsupported_completion_model_error_message(\n                    language_model_provider,\n                    language_model_name\n                )\n            )\n        if isinstance(language_model, GoogleGLAModelConfig) and completion_model.requires_reasoning_effort:\n            if language_model.reasoning_effort is None:\n                raise ConfigurationError(f\"Reasoning effort level is required for {language_model_provider.value}:{language_model_name} Please specify reasoning_effort for model {model_alias}.\")\n\n    if self.embedding_models is not None:\n        if self.default_embedding_model is None and len(self.embedding_models) &gt; 1:\n            raise ConfigurationError(\"embedding_models is set but default_embedding_model is missing (ambiguous).\")\n\n        if self.default_embedding_model is not None and self.default_embedding_model not in self.embedding_models:\n            raise ConfigurationError(\n                f\"default_embedding_model {self.default_embedding_model} is not in embedding_models\")\n        for model_alias, embedding_model in self.embedding_models.items():\n            if isinstance(embedding_model, OpenAIModelConfig):\n                embedding_model_provider = ModelProvider.OPENAI\n                embedding_model_name = embedding_model.model_name\n            else:\n                raise ConfigurationError(\n                    f\"Invalid embedding model: {model_alias}: {embedding_model} unsupported model type\")\n            embedding_model_parameters = model_catalog.get_embedding_model_parameters(embedding_model_provider,\n                                                                                 embedding_model_name)\n            if embedding_model_parameters is None:\n                raise ConfigurationError(model_catalog.generate_unsupported_embedding_model_error_message(\n                    embedding_model_provider,\n                    embedding_model_name\n                ))\n\n    return self\n</code></pre>"},{"location":"reference/fenic/api/session/#fenic.api.session.Session","title":"Session","text":"<p>The entry point to programming with the DataFrame API. Similar to PySpark's SparkSession.</p> Create a session with default configuration <pre><code>session = Session.get_or_create(SessionConfig(app_name=\"my_app\"))\n</code></pre> Create a session with cloud configuration <pre><code>config = SessionConfig(\n    app_name=\"my_app\",\n    cloud=True,\n    api_key=\"your_api_key\"\n)\nsession = Session.get_or_create(config)\n</code></pre> <p>Methods:</p> <ul> <li> <code>create_dataframe</code>             \u2013              <p>Create a DataFrame from a variety of Python-native data formats.</p> </li> <li> <code>get_or_create</code>             \u2013              <p>Gets an existing Session or creates a new one with the configured settings.</p> </li> <li> <code>sql</code>             \u2013              <p>Execute a read-only SQL query against one or more DataFrames using named placeholders.</p> </li> <li> <code>stop</code>             \u2013              <p>Stops the session and closes all connections.</p> </li> <li> <code>table</code>             \u2013              <p>Returns the specified table as a DataFrame.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>catalog</code>               (<code>Catalog</code>)           \u2013            <p>Interface for catalog operations on the Session.</p> </li> <li> <code>read</code>               (<code>DataFrameReader</code>)           \u2013            <p>Returns a DataFrameReader that can be used to read data in as a DataFrame.</p> </li> </ul>"},{"location":"reference/fenic/api/session/#fenic.api.session.Session.catalog","title":"catalog  <code>property</code>","text":"<pre><code>catalog: Catalog\n</code></pre> <p>Interface for catalog operations on the Session.</p>"},{"location":"reference/fenic/api/session/#fenic.api.session.Session.read","title":"read  <code>property</code>","text":"<pre><code>read: DataFrameReader\n</code></pre> <p>Returns a DataFrameReader that can be used to read data in as a DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrameReader</code> (              <code>DataFrameReader</code> )          \u2013            <p>A reader interface to read data into DataFrame</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the session has been stopped</p> </li> </ul>"},{"location":"reference/fenic/api/session/#fenic.api.session.Session.create_dataframe","title":"create_dataframe","text":"<pre><code>create_dataframe(data: DataLike) -&gt; DataFrame\n</code></pre> <p>Create a DataFrame from a variety of Python-native data formats.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DataLike</code>)           \u2013            <p>Input data. Must be one of: - Polars DataFrame - Pandas DataFrame - dict of column_name -&gt; list of values - list of dicts (each dict representing a row) - pyarrow Table</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>A new DataFrame instance</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input format is unsupported or inconsistent with provided column names.</p> </li> </ul> Create from Polars DataFrame <pre><code>import polars as pl\ndf = pl.DataFrame({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\nsession.create_dataframe(df)\n</code></pre> Create from Pandas DataFrame <pre><code>import pandas as pd\ndf = pd.DataFrame({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\nsession.create_dataframe(df)\n</code></pre> Create from dictionary <pre><code>session.create_dataframe({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n</code></pre> Create from list of dictionaries <pre><code>session.create_dataframe([\n    {\"col1\": 1, \"col2\": \"a\"},\n    {\"col1\": 2, \"col2\": \"b\"}\n])\n</code></pre> Create from pyarrow Table <pre><code>import pyarrow as pa\ntable = pa.Table.from_pydict({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\nsession.create_dataframe(table)\n</code></pre> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>def create_dataframe(\n    self,\n    data: DataLike,\n) -&gt; DataFrame:\n    \"\"\"Create a DataFrame from a variety of Python-native data formats.\n\n    Args:\n        data: Input data. Must be one of:\n            - Polars DataFrame\n            - Pandas DataFrame\n            - dict of column_name -&gt; list of values\n            - list of dicts (each dict representing a row)\n            - pyarrow Table\n\n    Returns:\n        A new DataFrame instance\n\n    Raises:\n        ValueError: If the input format is unsupported or inconsistent with provided column names.\n\n    Example: Create from Polars DataFrame\n        ```python\n        import polars as pl\n        df = pl.DataFrame({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n        session.create_dataframe(df)\n        ```\n\n    Example: Create from Pandas DataFrame\n        ```python\n        import pandas as pd\n        df = pd.DataFrame({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n        session.create_dataframe(df)\n        ```\n\n    Example: Create from dictionary\n        ```python\n        session.create_dataframe({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n        ```\n\n    Example: Create from list of dictionaries\n        ```python\n        session.create_dataframe([\n            {\"col1\": 1, \"col2\": \"a\"},\n            {\"col1\": 2, \"col2\": \"b\"}\n        ])\n        ```\n\n    Example: Create from pyarrow Table\n        ```python\n        import pyarrow as pa\n        table = pa.Table.from_pydict({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n        session.create_dataframe(table)\n        ```\n    \"\"\"\n    try:\n        if isinstance(data, pl.DataFrame):\n            pl_df = data\n        elif isinstance(data, pd.DataFrame):\n            pl_df = pl.from_pandas(data)\n        elif isinstance(data, dict):\n            pl_df = pl.DataFrame(data)\n        elif isinstance(data, list):\n            if not data:\n                raise ValidationError(\n                    \"Cannot create DataFrame from empty list. Provide a non-empty list of dictionaries, lists, or other supported data types.\"\n                )\n\n            if not isinstance(data[0], dict):\n                raise ValidationError(\n                    \"Cannot create DataFrame from list of non-dict values. Provide a list of dictionaries.\"\n                )\n            pl_df = pl.DataFrame(data)\n        elif isinstance(data, pa.Table):\n            pl_df = pl.from_arrow(data)\n\n        else:\n            raise ValidationError(\n                f\"Unsupported data type: {type(data)}. Supported types are: Polars DataFrame, Pandas DataFrame, dict, or list.\"\n            )\n\n    except ValidationError:\n        raise\n    except Exception as e:\n        raise PlanError(f\"Failed to create DataFrame from {data}\") from e\n\n    return DataFrame._from_logical_plan(\n        InMemorySource(pl_df, self._session_state)\n    )\n</code></pre>"},{"location":"reference/fenic/api/session/#fenic.api.session.Session.get_or_create","title":"get_or_create  <code>classmethod</code>","text":"<pre><code>get_or_create(config: SessionConfig) -&gt; Session\n</code></pre> <p>Gets an existing Session or creates a new one with the configured settings.</p> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>A Session instance configured with the provided settings</p> </li> </ul> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>@classmethod\ndef get_or_create(\n    cls,\n    config: SessionConfig,\n) -&gt; Session:\n    \"\"\"Gets an existing Session or creates a new one with the configured settings.\n\n    Returns:\n        A Session instance configured with the provided settings\n    \"\"\"\n    resolved_config = config._to_resolved_config()\n    if config.cloud:\n        from fenic._backends.cloud.manager import CloudSessionManager\n\n        cloud_session_manager = CloudSessionManager()\n        if not cloud_session_manager.initialized:\n            session_manager_dependencies = (\n                CloudSessionManager.create_global_session_dependencies()\n            )\n            cloud_session_manager.configure(session_manager_dependencies)\n        future = asyncio.run_coroutine_threadsafe(\n            cloud_session_manager.get_or_create_session_state(resolved_config),\n            cloud_session_manager._asyncio_loop,\n        )\n        cloud_session_state = future.result()\n        return Session._create_cloud_session(cloud_session_state)\n\n    local_session_state: LocalSessionState = LocalSessionManager().get_or_create_session_state(resolved_config)\n    return Session._create_local_session(local_session_state)\n</code></pre>"},{"location":"reference/fenic/api/session/#fenic.api.session.Session.sql","title":"sql","text":"<pre><code>sql(query: str, /, **tables: DataFrame) -&gt; DataFrame\n</code></pre> <p>Execute a read-only SQL query against one or more DataFrames using named placeholders.</p> <p>This allows you to execute ad hoc SQL queries using familiar syntax when it's more convenient than the DataFrame API. Placeholders in the SQL string (e.g. <code>{df}</code>) should correspond to keyword arguments (e.g. <code>df=my_dataframe</code>).</p> <p>For supported SQL syntax and functions, refer to the DuckDB SQL documentation: https://duckdb.org/docs/sql/introduction.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>A SQL query string with placeholders like <code>{df}</code></p> </li> <li> <code>**tables</code>               (<code>DataFrame</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments mapping placeholder names to DataFrames</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>A lazy DataFrame representing the result of the SQL query</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If a placeholder is used in the query but not passed as a keyword argument</p> </li> </ul> Simple join between two DataFrames <pre><code>df1 = session.create_dataframe({\"id\": [1, 2]})\ndf2 = session.create_dataframe({\"id\": [2, 3]})\nresult = session.sql(\n    \"SELECT * FROM {df1} JOIN {df2} USING (id)\",\n    df1=df1,\n    df2=df2\n)\n</code></pre> Complex query with multiple DataFrames <pre><code>users = session.create_dataframe({\"user_id\": [1, 2], \"name\": [\"Alice\", \"Bob\"]})\norders = session.create_dataframe({\"order_id\": [1, 2], \"user_id\": [1, 2]})\nproducts = session.create_dataframe({\"product_id\": [1, 2], \"name\": [\"Widget\", \"Gadget\"]})\n\nresult = session.sql(\"\"\"\n    SELECT u.name, p.name as product\n    FROM {users} u\n    JOIN {orders} o ON u.user_id = o.user_id\n    JOIN {products} p ON o.product_id = p.product_id\n\"\"\", users=users, orders=orders, products=products)\n</code></pre> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>def sql(self, query: str, /, **tables: DataFrame) -&gt; DataFrame:\n    \"\"\"Execute a read-only SQL query against one or more DataFrames using named placeholders.\n\n    This allows you to execute ad hoc SQL queries using familiar syntax when it's more convenient than the DataFrame API.\n    Placeholders in the SQL string (e.g. `{df}`) should correspond to keyword arguments (e.g. `df=my_dataframe`).\n\n    For supported SQL syntax and functions, refer to the DuckDB SQL documentation:\n    https://duckdb.org/docs/sql/introduction.\n\n    Args:\n        query: A SQL query string with placeholders like `{df}`\n        **tables: Keyword arguments mapping placeholder names to DataFrames\n\n    Returns:\n        A lazy DataFrame representing the result of the SQL query\n\n    Raises:\n        ValidationError: If a placeholder is used in the query but not passed\n            as a keyword argument\n\n    Example: Simple join between two DataFrames\n        ```python\n        df1 = session.create_dataframe({\"id\": [1, 2]})\n        df2 = session.create_dataframe({\"id\": [2, 3]})\n        result = session.sql(\n            \"SELECT * FROM {df1} JOIN {df2} USING (id)\",\n            df1=df1,\n            df2=df2\n        )\n        ```\n\n    Example: Complex query with multiple DataFrames\n        ```python\n        users = session.create_dataframe({\"user_id\": [1, 2], \"name\": [\"Alice\", \"Bob\"]})\n        orders = session.create_dataframe({\"order_id\": [1, 2], \"user_id\": [1, 2]})\n        products = session.create_dataframe({\"product_id\": [1, 2], \"name\": [\"Widget\", \"Gadget\"]})\n\n        result = session.sql(\\\"\\\"\\\"\n            SELECT u.name, p.name as product\n            FROM {users} u\n            JOIN {orders} o ON u.user_id = o.user_id\n            JOIN {products} p ON o.product_id = p.product_id\n        \\\"\\\"\\\", users=users, orders=orders, products=products)\n        ```\n    \"\"\"\n    query = query.strip()\n    if not query:\n        raise ValidationError(\"SQL query must not be empty.\")\n\n    placeholders = set(SQL_PLACEHOLDER_RE.findall(query))\n    missing = placeholders - tables.keys()\n    if missing:\n        raise ValidationError(\n            f\"Missing DataFrames for placeholders in SQL query: {', '.join(sorted(missing))}. \"\n            f\"Make sure to pass them as keyword arguments, e.g., sql(..., {next(iter(missing))}=df).\"\n        )\n\n    logical_plans = []\n    template_names = []\n    for name, table in tables.items():\n        if name in placeholders:\n            template_names.append(name)\n            logical_plans.append(table._logical_plan)\n\n    return DataFrame._from_logical_plan(\n        SQL(logical_plans, template_names, query, self._session_state),\n    )\n</code></pre>"},{"location":"reference/fenic/api/session/#fenic.api.session.Session.stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stops the session and closes all connections.</p> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>def stop(self):\n    \"\"\"Stops the session and closes all connections.\"\"\"\n    self._session_state.stop()\n</code></pre>"},{"location":"reference/fenic/api/session/#fenic.api.session.Session.table","title":"table","text":"<pre><code>table(table_name: str) -&gt; DataFrame\n</code></pre> <p>Returns the specified table as a DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Name of the table</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Table as a DataFrame</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the table does not exist</p> </li> </ul> Load an existing table <pre><code>df = session.table(\"my_table\")\n</code></pre> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>def table(self, table_name: str) -&gt; DataFrame:\n    \"\"\"Returns the specified table as a DataFrame.\n\n    Args:\n        table_name: Name of the table\n\n    Returns:\n        Table as a DataFrame\n\n    Raises:\n        ValueError: If the table does not exist\n\n    Example: Load an existing table\n        ```python\n        df = session.table(\"my_table\")\n        ```\n    \"\"\"\n    if not self._session_state.catalog.does_table_exist(table_name):\n        raise ValueError(f\"Table {table_name} does not exist\")\n    return DataFrame._from_logical_plan(\n        TableSource(table_name, self._session_state),\n    )\n</code></pre>"},{"location":"reference/fenic/api/session/#fenic.api.session.SessionConfig","title":"SessionConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a user session.</p> <p>This class defines the complete configuration for a user session, including application settings, model configurations, and optional cloud settings. It serves as the central configuration object for all language model operations.</p> <p>Attributes:</p> <ul> <li> <code>app_name</code>               (<code>str</code>)           \u2013            <p>Name of the application using this session. Defaults to \"default_app\".</p> </li> <li> <code>db_path</code>               (<code>Optional[Path]</code>)           \u2013            <p>Optional path to a local database file for persistent storage.</p> </li> <li> <code>semantic</code>               (<code>SemanticConfig</code>)           \u2013            <p>Configuration for semantic models (required).</p> </li> <li> <code>cloud</code>               (<code>Optional[CloudConfig]</code>)           \u2013            <p>Optional configuration for cloud execution.</p> </li> </ul> Note <p>The semantic configuration is required as it defines the language models that will be used for processing. The cloud configuration is optional and only needed for distributed processing.</p>"},{"location":"reference/fenic/api/session/config/","title":"fenic.api.session.config","text":""},{"location":"reference/fenic/api/session/config/#fenic.api.session.config","title":"fenic.api.session.config","text":"<p>Session configuration classes for Fenic.</p> <p>Classes:</p> <ul> <li> <code>AnthropicModelConfig</code>           \u2013            <p>Configuration for Anthropic models.</p> </li> <li> <code>CloudConfig</code>           \u2013            <p>Configuration for cloud-based execution.</p> </li> <li> <code>CloudExecutorSize</code>           \u2013            <p>Enum defining available cloud executor sizes.</p> </li> <li> <code>GoogleGLAModelConfig</code>           \u2013            <p>Configuration for Google GenerativeLAnguage (GLA) models.</p> </li> <li> <code>OpenAIModelConfig</code>           \u2013            <p>Configuration for OpenAI models.</p> </li> <li> <code>SemanticConfig</code>           \u2013            <p>Configuration for semantic language and embedding models.</p> </li> <li> <code>SessionConfig</code>           \u2013            <p>Configuration for a user session.</p> </li> </ul>"},{"location":"reference/fenic/api/session/config/#fenic.api.session.config.AnthropicModelConfig","title":"AnthropicModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Anthropic models.</p> <p>This class defines the configuration settings for Anthropic language models, including model selection and separate rate limiting parameters for input and output tokens.</p> <p>Attributes:</p> <ul> <li> <code>model_name</code>               (<code>ANTHROPIC_AVAILABLE_LANGUAGE_MODELS</code>)           \u2013            <p>The name of the Anthropic model to use.</p> </li> <li> <code>rpm</code>               (<code>int</code>)           \u2013            <p>Requests per minute limit; must be greater than 0.</p> </li> <li> <code>input_tpm</code>               (<code>int</code>)           \u2013            <p>Input tokens per minute limit; must be greater than 0.</p> </li> <li> <code>output_tpm</code>               (<code>int</code>)           \u2013            <p>Output tokens per minute limit; must be greater than 0.</p> </li> </ul> <p>Examples:</p> <p>Configuring an Anthropic model with separate input/output rate limits:</p> <pre><code>config = AnthropicModelConfig(\n    model_name=\"claude-3-5-haiku-latest\",\n    rpm=100,\n    input_tpm=100,\n    output_tpm=100\n)\n</code></pre>"},{"location":"reference/fenic/api/session/config/#fenic.api.session.config.CloudConfig","title":"CloudConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for cloud-based execution.</p> <p>This class defines settings for running operations in a cloud environment, allowing for scalable and distributed processing of language model operations.</p> <p>Attributes:</p> <ul> <li> <code>size</code>               (<code>Optional[CloudExecutorSize]</code>)           \u2013            <p>Size of the cloud executor instance. If None, the default size will be used.</p> </li> </ul>"},{"location":"reference/fenic/api/session/config/#fenic.api.session.config.CloudExecutorSize","title":"CloudExecutorSize","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum defining available cloud executor sizes.</p> <p>This enum represents the different size options available for cloud-based execution environments.</p> <p>Attributes:</p> <ul> <li> <code>SMALL</code>           \u2013            <p>Small instance size.</p> </li> <li> <code>MEDIUM</code>           \u2013            <p>Medium instance size.</p> </li> <li> <code>LARGE</code>           \u2013            <p>Large instance size.</p> </li> <li> <code>XLARGE</code>           \u2013            <p>Extra large instance size.</p> </li> </ul>"},{"location":"reference/fenic/api/session/config/#fenic.api.session.config.GoogleGLAModelConfig","title":"GoogleGLAModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Google GenerativeLAnguage (GLA) models.</p> <p>This class defines the configuration settings for models available in Google Developer AI Studio, including model selection and rate limiting parameters. These models are accessible using a GEMINI_API_KEY environment variable.</p>"},{"location":"reference/fenic/api/session/config/#fenic.api.session.config.OpenAIModelConfig","title":"OpenAIModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for OpenAI models.</p> <p>This class defines the configuration settings for OpenAI language and embedding models, including model selection and rate limiting parameters.</p> <p>Attributes:</p> <ul> <li> <code>model_name</code>               (<code>Union[OPENAI_AVAILABLE_LANGUAGE_MODELS, OPENAI_AVAILABLE_EMBEDDING_MODELS]</code>)           \u2013            <p>The name of the OpenAI model to use.</p> </li> <li> <code>rpm</code>               (<code>int</code>)           \u2013            <p>Requests per minute limit; must be greater than 0.</p> </li> <li> <code>tpm</code>               (<code>int</code>)           \u2013            <p>Tokens per minute limit; must be greater than 0.</p> </li> </ul> <p>Examples:</p> <p>Configuring an OpenAI Language model with rate limits:</p> <pre><code>config = OpenAIModelConfig(model_name=\"gpt-4.1-nano\", rpm=100, tpm=100)\n</code></pre> <p>Configuring an OpenAI Embedding model with rate limits:</p> <pre><code>config = OpenAIModelConfig(model_name=\"text-embedding-3-small\", rpm=100, tpm=100)\n</code></pre>"},{"location":"reference/fenic/api/session/config/#fenic.api.session.config.SemanticConfig","title":"SemanticConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for semantic language and embedding models.</p> <p>This class defines the configuration for both language models and optional embedding models used in semantic operations. It ensures that all configured models are valid and supported by their respective providers.</p> <p>Attributes:</p> <ul> <li> <code>language_models</code>               (<code>dict[str, ModelConfig]</code>)           \u2013            <p>Mapping of model aliases to language model configurations.</p> </li> <li> <code>default_language_model</code>               (<code>Optional[str]</code>)           \u2013            <p>The alias of the default language model to use for semantic operations. Not required if only one language model is configured.</p> </li> <li> <code>embedding_models</code>               (<code>Optional[dict[str, ModelConfig]]</code>)           \u2013            <p>Optional mapping of model aliases to embedding model configurations.</p> </li> <li> <code>default_embedding_model</code>               (<code>Optional[str]</code>)           \u2013            <p>The alias of the default embedding model to use for semantic operations.</p> </li> </ul> Note <p>The embedding model is optional and only required for operations that need semantic search or embedding capabilities.</p> <p>Methods:</p> <ul> <li> <code>model_post_init</code>             \u2013              <p>Post initialization hook to set defaults.</p> </li> <li> <code>validate_models</code>             \u2013              <p>Validates that the selected models are supported by the system.</p> </li> </ul>"},{"location":"reference/fenic/api/session/config/#fenic.api.session.config.SemanticConfig.model_post_init","title":"model_post_init","text":"<pre><code>model_post_init(__context) -&gt; None\n</code></pre> <p>Post initialization hook to set defaults.</p> <p>This hook runs after the model is initialized and validated. It sets the default language and embedding models if they are not set and there is only one model available.</p> Source code in <code>src/fenic/api/session/config.py</code> <pre><code>def model_post_init(self, __context) -&gt; None:\n    \"\"\"Post initialization hook to set defaults.\n\n    This hook runs after the model is initialized and validated.\n    It sets the default language and embedding models if they are not set\n    and there is only one model available.\n    \"\"\"\n    # Set default language model if not set and only one model exists\n    if self.default_language_model is None and len(self.language_models) == 1:\n        self.default_language_model = list(self.language_models.keys())[0]\n    # Set default embedding model if not set and only one model exists\n    if self.embedding_models is not None and self.default_embedding_model is None and len(self.embedding_models) == 1:\n        self.default_embedding_model = list(self.embedding_models.keys())[0]\n</code></pre>"},{"location":"reference/fenic/api/session/config/#fenic.api.session.config.SemanticConfig.validate_models","title":"validate_models","text":"<pre><code>validate_models() -&gt; SemanticConfig\n</code></pre> <p>Validates that the selected models are supported by the system.</p> <p>This validator checks that both the language model and embedding model (if provided) are valid and supported by their respective providers.</p> <p>Returns:</p> <ul> <li> <code>SemanticConfig</code>           \u2013            <p>The validated SemanticConfig instance.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ConfigurationError</code>             \u2013            <p>If any of the models are not supported.</p> </li> </ul> Source code in <code>src/fenic/api/session/config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_models(self) -&gt; SemanticConfig:\n    \"\"\"Validates that the selected models are supported by the system.\n\n    This validator checks that both the language model and embedding model (if provided)\n    are valid and supported by their respective providers.\n\n    Returns:\n        The validated SemanticConfig instance.\n\n    Raises:\n        ConfigurationError: If any of the models are not supported.\n    \"\"\"\n    if len(self.language_models) == 0:\n        raise ConfigurationError(\"You must specify at least one language model configuration.\")\n    available_language_model_aliases = list(self.language_models.keys())\n    if self.default_language_model is None and len(self.language_models) &gt; 1:\n        raise ConfigurationError(f\"default_language_model is not set, and multiple language models are configured. Please specify one of: {available_language_model_aliases} as a default_language_model.\")\n\n    if self.default_language_model is not None and self.default_language_model not in self.language_models:\n        raise ConfigurationError(f\"default_language_model {self.default_language_model} is not in configured map of language models. Available models: {available_language_model_aliases} .\")\n\n    for model_alias, language_model in self.language_models.items():\n        if isinstance(language_model, OpenAIModelConfig):\n            language_model_provider = ModelProvider.OPENAI\n            language_model_name = language_model.model_name\n        elif isinstance(language_model, AnthropicModelConfig):\n            language_model_provider = ModelProvider.ANTHROPIC\n            language_model_name = language_model.model_name\n        elif isinstance(language_model, GoogleGLAModelConfig):\n            language_model_provider = ModelProvider.GOOGLE_GLA\n            language_model_name = language_model.model_name\n        else:\n            raise ConfigurationError(\n                f\"Invalid language model: {model_alias}: {language_model} unsupported model type.\")\n\n        completion_model = model_catalog.get_completion_model_parameters(language_model_provider,\n                                                                         language_model_name)\n        if completion_model is None:\n            raise ConfigurationError(\n                model_catalog.generate_unsupported_completion_model_error_message(\n                    language_model_provider,\n                    language_model_name\n                )\n            )\n        if isinstance(language_model, GoogleGLAModelConfig) and completion_model.requires_reasoning_effort:\n            if language_model.reasoning_effort is None:\n                raise ConfigurationError(f\"Reasoning effort level is required for {language_model_provider.value}:{language_model_name} Please specify reasoning_effort for model {model_alias}.\")\n\n    if self.embedding_models is not None:\n        if self.default_embedding_model is None and len(self.embedding_models) &gt; 1:\n            raise ConfigurationError(\"embedding_models is set but default_embedding_model is missing (ambiguous).\")\n\n        if self.default_embedding_model is not None and self.default_embedding_model not in self.embedding_models:\n            raise ConfigurationError(\n                f\"default_embedding_model {self.default_embedding_model} is not in embedding_models\")\n        for model_alias, embedding_model in self.embedding_models.items():\n            if isinstance(embedding_model, OpenAIModelConfig):\n                embedding_model_provider = ModelProvider.OPENAI\n                embedding_model_name = embedding_model.model_name\n            else:\n                raise ConfigurationError(\n                    f\"Invalid embedding model: {model_alias}: {embedding_model} unsupported model type\")\n            embedding_model_parameters = model_catalog.get_embedding_model_parameters(embedding_model_provider,\n                                                                                 embedding_model_name)\n            if embedding_model_parameters is None:\n                raise ConfigurationError(model_catalog.generate_unsupported_embedding_model_error_message(\n                    embedding_model_provider,\n                    embedding_model_name\n                ))\n\n    return self\n</code></pre>"},{"location":"reference/fenic/api/session/config/#fenic.api.session.config.SessionConfig","title":"SessionConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a user session.</p> <p>This class defines the complete configuration for a user session, including application settings, model configurations, and optional cloud settings. It serves as the central configuration object for all language model operations.</p> <p>Attributes:</p> <ul> <li> <code>app_name</code>               (<code>str</code>)           \u2013            <p>Name of the application using this session. Defaults to \"default_app\".</p> </li> <li> <code>db_path</code>               (<code>Optional[Path]</code>)           \u2013            <p>Optional path to a local database file for persistent storage.</p> </li> <li> <code>semantic</code>               (<code>SemanticConfig</code>)           \u2013            <p>Configuration for semantic models (required).</p> </li> <li> <code>cloud</code>               (<code>Optional[CloudConfig]</code>)           \u2013            <p>Optional configuration for cloud execution.</p> </li> </ul> Note <p>The semantic configuration is required as it defines the language models that will be used for processing. The cloud configuration is optional and only needed for distributed processing.</p>"},{"location":"reference/fenic/api/session/session/","title":"fenic.api.session.session","text":""},{"location":"reference/fenic/api/session/session/#fenic.api.session.session","title":"fenic.api.session.session","text":"<p>Main session class for interacting with the DataFrame API.</p> <p>Classes:</p> <ul> <li> <code>Session</code>           \u2013            <p>The entry point to programming with the DataFrame API. Similar to PySpark's SparkSession.</p> </li> </ul>"},{"location":"reference/fenic/api/session/session/#fenic.api.session.session.Session","title":"Session","text":"<p>The entry point to programming with the DataFrame API. Similar to PySpark's SparkSession.</p> Create a session with default configuration <pre><code>session = Session.get_or_create(SessionConfig(app_name=\"my_app\"))\n</code></pre> Create a session with cloud configuration <pre><code>config = SessionConfig(\n    app_name=\"my_app\",\n    cloud=True,\n    api_key=\"your_api_key\"\n)\nsession = Session.get_or_create(config)\n</code></pre> <p>Methods:</p> <ul> <li> <code>create_dataframe</code>             \u2013              <p>Create a DataFrame from a variety of Python-native data formats.</p> </li> <li> <code>get_or_create</code>             \u2013              <p>Gets an existing Session or creates a new one with the configured settings.</p> </li> <li> <code>sql</code>             \u2013              <p>Execute a read-only SQL query against one or more DataFrames using named placeholders.</p> </li> <li> <code>stop</code>             \u2013              <p>Stops the session and closes all connections.</p> </li> <li> <code>table</code>             \u2013              <p>Returns the specified table as a DataFrame.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>catalog</code>               (<code>Catalog</code>)           \u2013            <p>Interface for catalog operations on the Session.</p> </li> <li> <code>read</code>               (<code>DataFrameReader</code>)           \u2013            <p>Returns a DataFrameReader that can be used to read data in as a DataFrame.</p> </li> </ul>"},{"location":"reference/fenic/api/session/session/#fenic.api.session.session.Session.catalog","title":"catalog  <code>property</code>","text":"<pre><code>catalog: Catalog\n</code></pre> <p>Interface for catalog operations on the Session.</p>"},{"location":"reference/fenic/api/session/session/#fenic.api.session.session.Session.read","title":"read  <code>property</code>","text":"<pre><code>read: DataFrameReader\n</code></pre> <p>Returns a DataFrameReader that can be used to read data in as a DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrameReader</code> (              <code>DataFrameReader</code> )          \u2013            <p>A reader interface to read data into DataFrame</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the session has been stopped</p> </li> </ul>"},{"location":"reference/fenic/api/session/session/#fenic.api.session.session.Session.create_dataframe","title":"create_dataframe","text":"<pre><code>create_dataframe(data: DataLike) -&gt; DataFrame\n</code></pre> <p>Create a DataFrame from a variety of Python-native data formats.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DataLike</code>)           \u2013            <p>Input data. Must be one of: - Polars DataFrame - Pandas DataFrame - dict of column_name -&gt; list of values - list of dicts (each dict representing a row) - pyarrow Table</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>A new DataFrame instance</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input format is unsupported or inconsistent with provided column names.</p> </li> </ul> Create from Polars DataFrame <pre><code>import polars as pl\ndf = pl.DataFrame({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\nsession.create_dataframe(df)\n</code></pre> Create from Pandas DataFrame <pre><code>import pandas as pd\ndf = pd.DataFrame({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\nsession.create_dataframe(df)\n</code></pre> Create from dictionary <pre><code>session.create_dataframe({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n</code></pre> Create from list of dictionaries <pre><code>session.create_dataframe([\n    {\"col1\": 1, \"col2\": \"a\"},\n    {\"col1\": 2, \"col2\": \"b\"}\n])\n</code></pre> Create from pyarrow Table <pre><code>import pyarrow as pa\ntable = pa.Table.from_pydict({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\nsession.create_dataframe(table)\n</code></pre> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>def create_dataframe(\n    self,\n    data: DataLike,\n) -&gt; DataFrame:\n    \"\"\"Create a DataFrame from a variety of Python-native data formats.\n\n    Args:\n        data: Input data. Must be one of:\n            - Polars DataFrame\n            - Pandas DataFrame\n            - dict of column_name -&gt; list of values\n            - list of dicts (each dict representing a row)\n            - pyarrow Table\n\n    Returns:\n        A new DataFrame instance\n\n    Raises:\n        ValueError: If the input format is unsupported or inconsistent with provided column names.\n\n    Example: Create from Polars DataFrame\n        ```python\n        import polars as pl\n        df = pl.DataFrame({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n        session.create_dataframe(df)\n        ```\n\n    Example: Create from Pandas DataFrame\n        ```python\n        import pandas as pd\n        df = pd.DataFrame({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n        session.create_dataframe(df)\n        ```\n\n    Example: Create from dictionary\n        ```python\n        session.create_dataframe({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n        ```\n\n    Example: Create from list of dictionaries\n        ```python\n        session.create_dataframe([\n            {\"col1\": 1, \"col2\": \"a\"},\n            {\"col1\": 2, \"col2\": \"b\"}\n        ])\n        ```\n\n    Example: Create from pyarrow Table\n        ```python\n        import pyarrow as pa\n        table = pa.Table.from_pydict({\"col1\": [1, 2], \"col2\": [\"a\", \"b\"]})\n        session.create_dataframe(table)\n        ```\n    \"\"\"\n    try:\n        if isinstance(data, pl.DataFrame):\n            pl_df = data\n        elif isinstance(data, pd.DataFrame):\n            pl_df = pl.from_pandas(data)\n        elif isinstance(data, dict):\n            pl_df = pl.DataFrame(data)\n        elif isinstance(data, list):\n            if not data:\n                raise ValidationError(\n                    \"Cannot create DataFrame from empty list. Provide a non-empty list of dictionaries, lists, or other supported data types.\"\n                )\n\n            if not isinstance(data[0], dict):\n                raise ValidationError(\n                    \"Cannot create DataFrame from list of non-dict values. Provide a list of dictionaries.\"\n                )\n            pl_df = pl.DataFrame(data)\n        elif isinstance(data, pa.Table):\n            pl_df = pl.from_arrow(data)\n\n        else:\n            raise ValidationError(\n                f\"Unsupported data type: {type(data)}. Supported types are: Polars DataFrame, Pandas DataFrame, dict, or list.\"\n            )\n\n    except ValidationError:\n        raise\n    except Exception as e:\n        raise PlanError(f\"Failed to create DataFrame from {data}\") from e\n\n    return DataFrame._from_logical_plan(\n        InMemorySource(pl_df, self._session_state)\n    )\n</code></pre>"},{"location":"reference/fenic/api/session/session/#fenic.api.session.session.Session.get_or_create","title":"get_or_create  <code>classmethod</code>","text":"<pre><code>get_or_create(config: SessionConfig) -&gt; Session\n</code></pre> <p>Gets an existing Session or creates a new one with the configured settings.</p> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>A Session instance configured with the provided settings</p> </li> </ul> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>@classmethod\ndef get_or_create(\n    cls,\n    config: SessionConfig,\n) -&gt; Session:\n    \"\"\"Gets an existing Session or creates a new one with the configured settings.\n\n    Returns:\n        A Session instance configured with the provided settings\n    \"\"\"\n    resolved_config = config._to_resolved_config()\n    if config.cloud:\n        from fenic._backends.cloud.manager import CloudSessionManager\n\n        cloud_session_manager = CloudSessionManager()\n        if not cloud_session_manager.initialized:\n            session_manager_dependencies = (\n                CloudSessionManager.create_global_session_dependencies()\n            )\n            cloud_session_manager.configure(session_manager_dependencies)\n        future = asyncio.run_coroutine_threadsafe(\n            cloud_session_manager.get_or_create_session_state(resolved_config),\n            cloud_session_manager._asyncio_loop,\n        )\n        cloud_session_state = future.result()\n        return Session._create_cloud_session(cloud_session_state)\n\n    local_session_state: LocalSessionState = LocalSessionManager().get_or_create_session_state(resolved_config)\n    return Session._create_local_session(local_session_state)\n</code></pre>"},{"location":"reference/fenic/api/session/session/#fenic.api.session.session.Session.sql","title":"sql","text":"<pre><code>sql(query: str, /, **tables: DataFrame) -&gt; DataFrame\n</code></pre> <p>Execute a read-only SQL query against one or more DataFrames using named placeholders.</p> <p>This allows you to execute ad hoc SQL queries using familiar syntax when it's more convenient than the DataFrame API. Placeholders in the SQL string (e.g. <code>{df}</code>) should correspond to keyword arguments (e.g. <code>df=my_dataframe</code>).</p> <p>For supported SQL syntax and functions, refer to the DuckDB SQL documentation: https://duckdb.org/docs/sql/introduction.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>A SQL query string with placeholders like <code>{df}</code></p> </li> <li> <code>**tables</code>               (<code>DataFrame</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments mapping placeholder names to DataFrames</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>A lazy DataFrame representing the result of the SQL query</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValidationError</code>             \u2013            <p>If a placeholder is used in the query but not passed as a keyword argument</p> </li> </ul> Simple join between two DataFrames <pre><code>df1 = session.create_dataframe({\"id\": [1, 2]})\ndf2 = session.create_dataframe({\"id\": [2, 3]})\nresult = session.sql(\n    \"SELECT * FROM {df1} JOIN {df2} USING (id)\",\n    df1=df1,\n    df2=df2\n)\n</code></pre> Complex query with multiple DataFrames <pre><code>users = session.create_dataframe({\"user_id\": [1, 2], \"name\": [\"Alice\", \"Bob\"]})\norders = session.create_dataframe({\"order_id\": [1, 2], \"user_id\": [1, 2]})\nproducts = session.create_dataframe({\"product_id\": [1, 2], \"name\": [\"Widget\", \"Gadget\"]})\n\nresult = session.sql(\"\"\"\n    SELECT u.name, p.name as product\n    FROM {users} u\n    JOIN {orders} o ON u.user_id = o.user_id\n    JOIN {products} p ON o.product_id = p.product_id\n\"\"\", users=users, orders=orders, products=products)\n</code></pre> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>def sql(self, query: str, /, **tables: DataFrame) -&gt; DataFrame:\n    \"\"\"Execute a read-only SQL query against one or more DataFrames using named placeholders.\n\n    This allows you to execute ad hoc SQL queries using familiar syntax when it's more convenient than the DataFrame API.\n    Placeholders in the SQL string (e.g. `{df}`) should correspond to keyword arguments (e.g. `df=my_dataframe`).\n\n    For supported SQL syntax and functions, refer to the DuckDB SQL documentation:\n    https://duckdb.org/docs/sql/introduction.\n\n    Args:\n        query: A SQL query string with placeholders like `{df}`\n        **tables: Keyword arguments mapping placeholder names to DataFrames\n\n    Returns:\n        A lazy DataFrame representing the result of the SQL query\n\n    Raises:\n        ValidationError: If a placeholder is used in the query but not passed\n            as a keyword argument\n\n    Example: Simple join between two DataFrames\n        ```python\n        df1 = session.create_dataframe({\"id\": [1, 2]})\n        df2 = session.create_dataframe({\"id\": [2, 3]})\n        result = session.sql(\n            \"SELECT * FROM {df1} JOIN {df2} USING (id)\",\n            df1=df1,\n            df2=df2\n        )\n        ```\n\n    Example: Complex query with multiple DataFrames\n        ```python\n        users = session.create_dataframe({\"user_id\": [1, 2], \"name\": [\"Alice\", \"Bob\"]})\n        orders = session.create_dataframe({\"order_id\": [1, 2], \"user_id\": [1, 2]})\n        products = session.create_dataframe({\"product_id\": [1, 2], \"name\": [\"Widget\", \"Gadget\"]})\n\n        result = session.sql(\\\"\\\"\\\"\n            SELECT u.name, p.name as product\n            FROM {users} u\n            JOIN {orders} o ON u.user_id = o.user_id\n            JOIN {products} p ON o.product_id = p.product_id\n        \\\"\\\"\\\", users=users, orders=orders, products=products)\n        ```\n    \"\"\"\n    query = query.strip()\n    if not query:\n        raise ValidationError(\"SQL query must not be empty.\")\n\n    placeholders = set(SQL_PLACEHOLDER_RE.findall(query))\n    missing = placeholders - tables.keys()\n    if missing:\n        raise ValidationError(\n            f\"Missing DataFrames for placeholders in SQL query: {', '.join(sorted(missing))}. \"\n            f\"Make sure to pass them as keyword arguments, e.g., sql(..., {next(iter(missing))}=df).\"\n        )\n\n    logical_plans = []\n    template_names = []\n    for name, table in tables.items():\n        if name in placeholders:\n            template_names.append(name)\n            logical_plans.append(table._logical_plan)\n\n    return DataFrame._from_logical_plan(\n        SQL(logical_plans, template_names, query, self._session_state),\n    )\n</code></pre>"},{"location":"reference/fenic/api/session/session/#fenic.api.session.session.Session.stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stops the session and closes all connections.</p> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>def stop(self):\n    \"\"\"Stops the session and closes all connections.\"\"\"\n    self._session_state.stop()\n</code></pre>"},{"location":"reference/fenic/api/session/session/#fenic.api.session.session.Session.table","title":"table","text":"<pre><code>table(table_name: str) -&gt; DataFrame\n</code></pre> <p>Returns the specified table as a DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>Name of the table</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Table as a DataFrame</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the table does not exist</p> </li> </ul> Load an existing table <pre><code>df = session.table(\"my_table\")\n</code></pre> Source code in <code>src/fenic/api/session/session.py</code> <pre><code>def table(self, table_name: str) -&gt; DataFrame:\n    \"\"\"Returns the specified table as a DataFrame.\n\n    Args:\n        table_name: Name of the table\n\n    Returns:\n        Table as a DataFrame\n\n    Raises:\n        ValueError: If the table does not exist\n\n    Example: Load an existing table\n        ```python\n        df = session.table(\"my_table\")\n        ```\n    \"\"\"\n    if not self._session_state.catalog.does_table_exist(table_name):\n        raise ValueError(f\"Table {table_name} does not exist\")\n    return DataFrame._from_logical_plan(\n        TableSource(table_name, self._session_state),\n    )\n</code></pre>"},{"location":"reference/fenic/core/","title":"fenic.core","text":""},{"location":"reference/fenic/core/#fenic.core","title":"fenic.core","text":"<p>Core module for Fenic.</p> <p>Classes:</p> <ul> <li> <code>ArrayType</code>           \u2013            <p>A type representing a homogeneous variable-length array (list) of elements.</p> </li> <li> <code>ClassifyExample</code>           \u2013            <p>A single semantic example for classification operations.</p> </li> <li> <code>ClassifyExampleCollection</code>           \u2013            <p>Collection of examples for semantic classification operations.</p> </li> <li> <code>ColumnField</code>           \u2013            <p>Represents a typed column in a DataFrame schema.</p> </li> <li> <code>DataType</code>           \u2013            <p>Base class for all data types.</p> </li> <li> <code>DocumentPathType</code>           \u2013            <p>Represents a string containing a a document's local (file system) or remote (URL) path.</p> </li> <li> <code>EmbeddingType</code>           \u2013            <p>A type representing a fixed-length embedding vector.</p> </li> <li> <code>ExtractSchema</code>           \u2013            <p>Represents a structured extraction schema.</p> </li> <li> <code>ExtractSchemaField</code>           \u2013            <p>Represents a field within an structured extraction schema.</p> </li> <li> <code>ExtractSchemaList</code>           \u2013            <p>Represents a list data type for structured extraction schema definitions.</p> </li> <li> <code>JoinExample</code>           \u2013            <p>A single semantic example for semantic join operations.</p> </li> <li> <code>JoinExampleCollection</code>           \u2013            <p>Collection of examples for semantic join operations.</p> </li> <li> <code>LMMetrics</code>           \u2013            <p>Tracks language model usage metrics including token counts and costs.</p> </li> <li> <code>MapExample</code>           \u2013            <p>A single semantic example for semantic mapping operations.</p> </li> <li> <code>MapExampleCollection</code>           \u2013            <p>Collection of examples for semantic mapping operations.</p> </li> <li> <code>OperatorMetrics</code>           \u2013            <p>Metrics for a single operator in the query execution plan.</p> </li> <li> <code>PredicateExample</code>           \u2013            <p>A single semantic example for semantic predicate operations.</p> </li> <li> <code>PredicateExampleCollection</code>           \u2013            <p>Collection of examples for semantic predicate operations.</p> </li> <li> <code>QueryMetrics</code>           \u2013            <p>Comprehensive metrics for an executed query.</p> </li> <li> <code>QueryResult</code>           \u2013            <p>Container for query execution results and associated metadata.</p> </li> <li> <code>RMMetrics</code>           \u2013            <p>Tracks embedding model usage metrics including token counts and costs.</p> </li> <li> <code>Schema</code>           \u2013            <p>Represents the schema of a DataFrame.</p> </li> <li> <code>StructField</code>           \u2013            <p>A field in a StructType. Fields are nullable.</p> </li> <li> <code>StructType</code>           \u2013            <p>A type representing a struct (record) with named fields.</p> </li> <li> <code>TranscriptType</code>           \u2013            <p>Represents a string containing a transcript in a specific format.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>BooleanType</code>           \u2013            <p>Represents a boolean value. (True/False)</p> </li> <li> <code>BranchSide</code>           \u2013            <p>Type alias representing the side of a branch in a lineage graph.</p> </li> <li> <code>DataLike</code>           \u2013            <p>Union type representing any supported data format for both input and output operations.</p> </li> <li> <code>DataLikeType</code>           \u2013            <p>String literal type for specifying data output formats.</p> </li> <li> <code>DoubleType</code>           \u2013            <p>Represents a 64-bit floating-point number.</p> </li> <li> <code>FloatType</code>           \u2013            <p>Represents a 32-bit floating-point number.</p> </li> <li> <code>HtmlType</code>           \u2013            <p>Represents a string containing raw HTML markup.</p> </li> <li> <code>IntegerType</code>           \u2013            <p>Represents a signed integer value.</p> </li> <li> <code>JsonType</code>           \u2013            <p>Represents a string containing JSON data.</p> </li> <li> <code>MarkdownType</code>           \u2013            <p>Represents a string containing Markdown-formatted text.</p> </li> <li> <code>SemanticSimilarityMetric</code>           \u2013            <p>Type alias representing supported semantic similarity metrics.</p> </li> <li> <code>StringType</code>           \u2013            <p>Represents a UTF-8 encoded string value.</p> </li> </ul>"},{"location":"reference/fenic/core/#fenic.core.BooleanType","title":"BooleanType  <code>module-attribute</code>","text":"<pre><code>BooleanType = _BooleanType()\n</code></pre> <p>Represents a boolean value. (True/False)</p>"},{"location":"reference/fenic/core/#fenic.core.BranchSide","title":"BranchSide  <code>module-attribute</code>","text":"<pre><code>BranchSide = Literal['left', 'right']\n</code></pre> <p>Type alias representing the side of a branch in a lineage graph.</p> <p>Valid values:</p> <ul> <li>\"left\": The left branch of a join.</li> <li>\"right\": The right branch of a join.</li> </ul>"},{"location":"reference/fenic/core/#fenic.core.DataLike","title":"DataLike  <code>module-attribute</code>","text":"<pre><code>DataLike = Union[DataFrame, DataFrame, Dict[str, List[Any]], List[Dict[str, Any]], Table]\n</code></pre> <p>Union type representing any supported data format for both input and output operations.</p> <p>This type encompasses all possible data structures that can be: 1. Used as input when creating DataFrames 2. Returned as output from query results</p> Supported formats <ul> <li>pl.DataFrame: Native Polars DataFrame with efficient columnar storage</li> <li>pd.DataFrame: Pandas DataFrame, optionally with PyArrow extension arrays</li> <li>Dict[str, List[Any]]: Column-oriented dictionary where:<ul> <li>Keys are column names (str)</li> <li>Values are lists containing all values for that column</li> </ul> </li> <li>List[Dict[str, Any]]: Row-oriented list where:<ul> <li>Each element is a dictionary representing one row</li> <li>Dictionary keys are column names, values are cell values</li> </ul> </li> <li>pa.Table: Apache Arrow Table with columnar memory layout</li> </ul> Usage <ul> <li>Input: Used in create_dataframe() to accept data in various formats</li> <li>Output: Used in QueryResult.data to return results in requested format</li> </ul> <p>The specific type returned depends on the DataLikeType format specified when collecting query results.</p>"},{"location":"reference/fenic/core/#fenic.core.DataLikeType","title":"DataLikeType  <code>module-attribute</code>","text":"<pre><code>DataLikeType = Literal['polars', 'pandas', 'pydict', 'pylist', 'arrow']\n</code></pre> <p>String literal type for specifying data output formats.</p> Valid values <ul> <li>\"polars\": Native Polars DataFrame format</li> <li>\"pandas\": Pandas DataFrame with PyArrow extension arrays</li> <li>\"pydict\": Python dictionary with column names as keys, lists as values</li> <li>\"pylist\": Python list of dictionaries, each representing one row</li> <li>\"arrow\": Apache Arrow Table format</li> </ul> <p>Used as input parameter for methods that can return data in multiple formats.</p>"},{"location":"reference/fenic/core/#fenic.core.DoubleType","title":"DoubleType  <code>module-attribute</code>","text":"<pre><code>DoubleType = _DoubleType()\n</code></pre> <p>Represents a 64-bit floating-point number.</p>"},{"location":"reference/fenic/core/#fenic.core.FloatType","title":"FloatType  <code>module-attribute</code>","text":"<pre><code>FloatType = _FloatType()\n</code></pre> <p>Represents a 32-bit floating-point number.</p>"},{"location":"reference/fenic/core/#fenic.core.HtmlType","title":"HtmlType  <code>module-attribute</code>","text":"<pre><code>HtmlType = _HtmlType()\n</code></pre> <p>Represents a string containing raw HTML markup.</p>"},{"location":"reference/fenic/core/#fenic.core.IntegerType","title":"IntegerType  <code>module-attribute</code>","text":"<pre><code>IntegerType = _IntegerType()\n</code></pre> <p>Represents a signed integer value.</p>"},{"location":"reference/fenic/core/#fenic.core.JsonType","title":"JsonType  <code>module-attribute</code>","text":"<pre><code>JsonType = _JsonType()\n</code></pre> <p>Represents a string containing JSON data.</p>"},{"location":"reference/fenic/core/#fenic.core.MarkdownType","title":"MarkdownType  <code>module-attribute</code>","text":"<pre><code>MarkdownType = _MarkdownType()\n</code></pre> <p>Represents a string containing Markdown-formatted text.</p>"},{"location":"reference/fenic/core/#fenic.core.SemanticSimilarityMetric","title":"SemanticSimilarityMetric  <code>module-attribute</code>","text":"<pre><code>SemanticSimilarityMetric = Literal['cosine', 'l2', 'dot']\n</code></pre> <p>Type alias representing supported semantic similarity metrics.</p> <p>Valid values:</p> <ul> <li>\"cosine\": Cosine similarity, measures the cosine of the angle between two vectors.</li> <li>\"l2\": Euclidean (L2) distance, measures the straight-line distance between two vectors.</li> <li>\"dot\": Dot product similarity, the raw inner product of two vectors.</li> </ul> <p>These metrics are commonly used for comparing embedding vectors in semantic search and other similarity-based applications.</p>"},{"location":"reference/fenic/core/#fenic.core.StringType","title":"StringType  <code>module-attribute</code>","text":"<pre><code>StringType = _StringType()\n</code></pre> <p>Represents a UTF-8 encoded string value.</p>"},{"location":"reference/fenic/core/#fenic.core.ArrayType","title":"ArrayType","text":"<p>               Bases: <code>DataType</code></p> <p>A type representing a homogeneous variable-length array (list) of elements.</p> <p>Attributes:</p> <ul> <li> <code>element_type</code>               (<code>DataType</code>)           \u2013            <p>The data type of each element in the array.</p> </li> </ul> Create an array of strings <pre><code>ArrayType(StringType)\nArrayType(element_type=StringType)\n</code></pre>"},{"location":"reference/fenic/core/#fenic.core.ClassifyExample","title":"ClassifyExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single semantic example for classification operations.</p> <p>Classify examples demonstrate the classification of an input string into a specific category string, used in a semantic.classify operation.</p>"},{"location":"reference/fenic/core/#fenic.core.ClassifyExampleCollection","title":"ClassifyExampleCollection","text":"<pre><code>ClassifyExampleCollection(examples: List[ExampleType] = None)\n</code></pre> <p>               Bases: <code>BaseExampleCollection[ClassifyExample]</code></p> <p>Collection of examples for semantic classification operations.</p> <p>Classification operations categorize input text into predefined classes. This collection manages examples that demonstrate the expected classification results for different inputs.</p> <p>Examples in this collection have a single input string and an output string representing the classification result.</p> <p>Methods:</p> <ul> <li> <code>from_polars</code>             \u2013              <p>Create collection from a Polars DataFrame. Must have an 'output' column and an 'input' column.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def __init__(self, examples: List[ExampleType] = None):\n    \"\"\"Initialize a collection of semantic examples.\n\n    Args:\n        examples: Optional list of examples to add to the collection. Each example\n            will be processed through create_example() to ensure proper formatting\n            and validation.\n\n    Note:\n        The examples list is initialized as empty if no examples are provided.\n        Each example in the provided list will be processed through create_example()\n        to ensure proper formatting and validation.\n    \"\"\"\n    self.examples: List[ExampleType] = []\n    if examples:\n        for example in examples:\n            self.create_example(example)\n</code></pre>"},{"location":"reference/fenic/core/#fenic.core.ClassifyExampleCollection.from_polars","title":"from_polars  <code>classmethod</code>","text":"<pre><code>from_polars(df: DataFrame) -&gt; ClassifyExampleCollection\n</code></pre> <p>Create collection from a Polars DataFrame. Must have an 'output' column and an 'input' column.</p> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: pl.DataFrame) -&gt; ClassifyExampleCollection:\n    \"\"\"Create collection from a Polars DataFrame. Must have an 'output' column and an 'input' column.\"\"\"\n    collection = cls()\n\n    if EXAMPLE_INPUT_KEY not in df.columns:\n        raise InvalidExampleCollectionError(\n            f\"Classify Examples DataFrame missing required '{EXAMPLE_INPUT_KEY}' column\"\n        )\n    if EXAMPLE_OUTPUT_KEY not in df.columns:\n        raise InvalidExampleCollectionError(\n            f\"Classify Examples DataFrame missing required '{EXAMPLE_OUTPUT_KEY}' column\"\n        )\n\n    for row in df.iter_rows(named=True):\n        if row[EXAMPLE_INPUT_KEY] is None:\n            raise InvalidExampleCollectionError(\n                f\"Classify Examples DataFrame contains null values in '{EXAMPLE_INPUT_KEY}' column\"\n            )\n        if row[EXAMPLE_OUTPUT_KEY] is None:\n            raise InvalidExampleCollectionError(\n                f\"Classify Examples DataFrame contains null values in '{EXAMPLE_OUTPUT_KEY}' column\"\n            )\n\n        example = ClassifyExample(\n            input=row[EXAMPLE_INPUT_KEY],\n            output=row[EXAMPLE_OUTPUT_KEY],\n        )\n        collection.create_example(example)\n\n    return collection\n</code></pre>"},{"location":"reference/fenic/core/#fenic.core.ColumnField","title":"ColumnField","text":"<p>Represents a typed column in a DataFrame schema.</p> <p>A ColumnField defines the structure of a single column by specifying its name and data type. This is used as a building block for DataFrame schemas.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the column.</p> </li> <li> <code>data_type</code>               (<code>DataType</code>)           \u2013            <p>The data type of the column, as a DataType instance.</p> </li> </ul>"},{"location":"reference/fenic/core/#fenic.core.DataType","title":"DataType","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all data types.</p> <p>You won't instantiate this class directly. Instead, use one of the concrete types like <code>StringType</code>, <code>ArrayType</code>, or <code>StructType</code>.</p> <p>Used for casting, type validation, and schema inference in the DataFrame API.</p>"},{"location":"reference/fenic/core/#fenic.core.DocumentPathType","title":"DocumentPathType","text":"<p>               Bases: <code>_StringBackedType</code></p> <p>Represents a string containing a a document's local (file system) or remote (URL) path.</p>"},{"location":"reference/fenic/core/#fenic.core.EmbeddingType","title":"EmbeddingType","text":"<p>               Bases: <code>DataType</code></p> <p>A type representing a fixed-length embedding vector.</p> <p>Attributes:</p> <ul> <li> <code>dimensions</code>               (<code>int</code>)           \u2013            <p>The number of dimensions in the embedding vector.</p> </li> <li> <code>embedding_model</code>               (<code>str</code>)           \u2013            <p>Name of the model used to generate the embedding.</p> </li> </ul> Create an embedding type for text-embedding-3-small <pre><code>EmbeddingType(384, embedding_model=\"text-embedding-3-small\")\n</code></pre>"},{"location":"reference/fenic/core/#fenic.core.ExtractSchema","title":"ExtractSchema","text":"<p>Represents a structured extraction schema.</p> <p>An extract schema contains a collection of named fields with descriptions that define what information should be extracted into each field.</p> <p>Methods:</p> <ul> <li> <code>field_names</code>             \u2013              <p>Get a list of all field names in the schema.</p> </li> </ul>"},{"location":"reference/fenic/core/#fenic.core.ExtractSchema.field_names","title":"field_names","text":"<pre><code>field_names() -&gt; List[str]\n</code></pre> <p>Get a list of all field names in the schema.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>A list of strings containing the names of all fields in the schema.</p> </li> </ul> Source code in <code>src/fenic/core/types/extract_schema.py</code> <pre><code>def field_names(self) -&gt; List[str]:\n    \"\"\"Get a list of all field names in the schema.\n\n    Returns:\n        A list of strings containing the names of all fields in the schema.\n    \"\"\"\n    return [field.name for field in self.struct_fields]\n</code></pre>"},{"location":"reference/fenic/core/#fenic.core.ExtractSchemaField","title":"ExtractSchemaField","text":"<pre><code>ExtractSchemaField(name: str, data_type: Union[DataType, ExtractSchemaList, ExtractSchema], description: str)\n</code></pre> <p>Represents a field within an structured extraction schema.</p> <p>An extract schema field has a name, a data type, and a required description that explains what information should be extracted into this field.</p> <p>Initialize an ExtractSchemaField.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the field.</p> </li> <li> <code>data_type</code>               (<code>Union[DataType, ExtractSchemaList, ExtractSchema]</code>)           \u2013            <p>The data type of the field. Must be either a primitive DataType, ExtractSchemaList, or ExtractSchema.</p> </li> <li> <code>description</code>               (<code>str</code>)           \u2013            <p>A description of what information should be extracted into this field.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If data_type is a non-primitive DataType.</p> </li> </ul> Source code in <code>src/fenic/core/types/extract_schema.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    data_type: Union[DataType, ExtractSchemaList, ExtractSchema],\n    description: str,\n):\n    \"\"\"Initialize an ExtractSchemaField.\n\n    Args:\n        name: The name of the field.\n        data_type: The data type of the field. Must be either a primitive DataType,\n            ExtractSchemaList, or ExtractSchema.\n        description: A description of what information should be extracted into this field.\n\n    Raises:\n        ValueError: If data_type is a non-primitive DataType.\n    \"\"\"\n    self.name = name\n    if isinstance(data_type, DataType) and not isinstance(\n        data_type, _PrimitiveType\n    ):\n        raise ValueError(\n            f\"Invalid data type: {data_type}. Only primitive types are supported directly. \"\n            f\"For complex types, please use ExtractSchemaList or ExtractSchema instead.\"\n        )\n    self.data_type = data_type\n    self.description = description\n</code></pre>"},{"location":"reference/fenic/core/#fenic.core.ExtractSchemaList","title":"ExtractSchemaList","text":"<pre><code>ExtractSchemaList(element_type: Union[DataType, ExtractSchema])\n</code></pre> <p>Represents a list data type for structured extraction schema definitions.</p> <p>A schema list contains elements of a specific data type and is used for defining array-like structures in structured extraction schemas.</p> <p>Initialize an ExtractSchemaList.</p> <p>Parameters:</p> <ul> <li> <code>element_type</code>               (<code>Union[DataType, ExtractSchema]</code>)           \u2013            <p>The data type of elements in the list. Must be either a primitive DataType or another ExtractSchema.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If element_type is a non-primitive DataType.</p> </li> </ul> Source code in <code>src/fenic/core/types/extract_schema.py</code> <pre><code>def __init__(\n    self,\n    element_type: Union[DataType, ExtractSchema],\n):\n    \"\"\"Initialize an ExtractSchemaList.\n\n    Args:\n        element_type: The data type of elements in the list. Must be either a primitive\n            DataType or another ExtractSchema.\n\n    Raises:\n        ValueError: If element_type is a non-primitive DataType.\n    \"\"\"\n    if isinstance(element_type, DataType) and not isinstance(\n        element_type, _PrimitiveType\n    ):\n        raise ValueError(\n            f\"Invalid element type: {element_type}. Only primitive types are supported directly. \"\n            f\"For complex types, please use ExtractSchemaList or ExtractSchema instead.\"\n        )\n    self.element_type = element_type\n</code></pre>"},{"location":"reference/fenic/core/#fenic.core.JoinExample","title":"JoinExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single semantic example for semantic join operations.</p> <p>Join examples demonstrate the evaluation of two input strings across different datasets against a specific condition, used in a semantic.join operation.</p>"},{"location":"reference/fenic/core/#fenic.core.JoinExampleCollection","title":"JoinExampleCollection","text":"<pre><code>JoinExampleCollection(examples: List[ExampleType] = None)\n</code></pre> <p>               Bases: <code>BaseExampleCollection[JoinExample]</code></p> <p>Collection of examples for semantic join operations.</p> <p>Methods:</p> <ul> <li> <code>from_polars</code>             \u2013              <p>Create collection from a Polars DataFrame. Must have 'left', 'right', and 'output' columns.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def __init__(self, examples: List[ExampleType] = None):\n    \"\"\"Initialize a collection of semantic examples.\n\n    Args:\n        examples: Optional list of examples to add to the collection. Each example\n            will be processed through create_example() to ensure proper formatting\n            and validation.\n\n    Note:\n        The examples list is initialized as empty if no examples are provided.\n        Each example in the provided list will be processed through create_example()\n        to ensure proper formatting and validation.\n    \"\"\"\n    self.examples: List[ExampleType] = []\n    if examples:\n        for example in examples:\n            self.create_example(example)\n</code></pre>"},{"location":"reference/fenic/core/#fenic.core.JoinExampleCollection.from_polars","title":"from_polars  <code>classmethod</code>","text":"<pre><code>from_polars(df: DataFrame) -&gt; JoinExampleCollection\n</code></pre> <p>Create collection from a Polars DataFrame. Must have 'left', 'right', and 'output' columns.</p> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: pl.DataFrame) -&gt; JoinExampleCollection:\n    \"\"\"Create collection from a Polars DataFrame. Must have 'left', 'right', and 'output' columns.\"\"\"\n    collection = cls()\n\n    required_columns = [\n        EXAMPLE_LEFT_KEY,\n        EXAMPLE_RIGHT_KEY,\n        EXAMPLE_OUTPUT_KEY,\n    ]\n    for col in required_columns:\n        if col not in df.columns:\n            raise InvalidExampleCollectionError(\n                f\"Join Examples DataFrame missing required '{col}' column\"\n            )\n\n    for row in df.iter_rows(named=True):\n        for col in required_columns:\n            if row[col] is None:\n                raise InvalidExampleCollectionError(\n                    f\"Join Examples DataFrame contains null values in '{col}' column\"\n                )\n\n        example = JoinExample(\n            left=row[EXAMPLE_LEFT_KEY],\n            right=row[EXAMPLE_RIGHT_KEY],\n            output=row[EXAMPLE_OUTPUT_KEY],\n        )\n        collection.create_example(example)\n\n    return collection\n</code></pre>"},{"location":"reference/fenic/core/#fenic.core.LMMetrics","title":"LMMetrics  <code>dataclass</code>","text":"<pre><code>LMMetrics(num_uncached_input_tokens: int = 0, num_cached_input_tokens: int = 0, num_output_tokens: int = 0, cost: float = 0.0, num_requests: int = 0)\n</code></pre> <p>Tracks language model usage metrics including token counts and costs.</p> <p>Attributes:</p> <ul> <li> <code>num_uncached_input_tokens</code>               (<code>int</code>)           \u2013            <p>Number of uncached tokens in the prompt/input</p> </li> <li> <code>num_cached_input_tokens</code>               (<code>int</code>)           \u2013            <p>Number of cached tokens in the prompt/input,</p> </li> <li> <code>num_output_tokens</code>               (<code>int</code>)           \u2013            <p>Number of tokens in the completion/output</p> </li> <li> <code>cost</code>               (<code>float</code>)           \u2013            <p>Total cost in USD for the LM API call</p> </li> </ul>"},{"location":"reference/fenic/core/#fenic.core.MapExample","title":"MapExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single semantic example for semantic mapping operations.</p> <p>Map examples demonstrate the transformation of input variables to a specific output string used in a semantic.map operation.</p>"},{"location":"reference/fenic/core/#fenic.core.MapExampleCollection","title":"MapExampleCollection","text":"<pre><code>MapExampleCollection(examples: List[ExampleType] = None)\n</code></pre> <p>               Bases: <code>BaseExampleCollection[MapExample]</code></p> <p>Collection of examples for semantic mapping operations.</p> <p>Map operations transform input variables into a text output according to specified instructions. This collection manages examples that demonstrate the expected transformations for different inputs.</p> <p>Examples in this collection can have multiple input variables, each mapped to their respective values, with a single output string representing the expected transformation result.</p> <p>Methods:</p> <ul> <li> <code>from_polars</code>             \u2013              <p>Create collection from a Polars DataFrame. Must have an 'output' column and at least one input column.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def __init__(self, examples: List[ExampleType] = None):\n    \"\"\"Initialize a collection of semantic examples.\n\n    Args:\n        examples: Optional list of examples to add to the collection. Each example\n            will be processed through create_example() to ensure proper formatting\n            and validation.\n\n    Note:\n        The examples list is initialized as empty if no examples are provided.\n        Each example in the provided list will be processed through create_example()\n        to ensure proper formatting and validation.\n    \"\"\"\n    self.examples: List[ExampleType] = []\n    if examples:\n        for example in examples:\n            self.create_example(example)\n</code></pre>"},{"location":"reference/fenic/core/#fenic.core.MapExampleCollection.from_polars","title":"from_polars  <code>classmethod</code>","text":"<pre><code>from_polars(df: DataFrame) -&gt; MapExampleCollection\n</code></pre> <p>Create collection from a Polars DataFrame. Must have an 'output' column and at least one input column.</p> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: pl.DataFrame) -&gt; MapExampleCollection:\n    \"\"\"Create collection from a Polars DataFrame. Must have an 'output' column and at least one input column.\"\"\"\n    collection = cls()\n\n    if EXAMPLE_OUTPUT_KEY not in df.columns:\n        raise ValueError(\n            f\"Map Examples DataFrame missing required '{EXAMPLE_OUTPUT_KEY}' column\"\n        )\n\n    input_cols = [col for col in df.columns if col != EXAMPLE_OUTPUT_KEY]\n\n    if not input_cols:\n        raise ValueError(\n            \"Map Examples DataFrame must have at least one input column\"\n        )\n\n    for row in df.iter_rows(named=True):\n        if row[EXAMPLE_OUTPUT_KEY] is None:\n            raise InvalidExampleCollectionError(\n                f\"Map Examples DataFrame contains null values in '{EXAMPLE_OUTPUT_KEY}' column\"\n            )\n\n        input_dict = {\n            col: str(row[col]) for col in input_cols if row[col] is not None\n        }\n\n        example = MapExample(input=input_dict, output=row[EXAMPLE_OUTPUT_KEY])\n        collection.create_example(example)\n\n    return collection\n</code></pre>"},{"location":"reference/fenic/core/#fenic.core.OperatorMetrics","title":"OperatorMetrics  <code>dataclass</code>","text":"<pre><code>OperatorMetrics(operator_id: str, num_output_rows: int = 0, execution_time_ms: float = 0.0, lm_metrics: LMMetrics = LMMetrics(), rm_metrics: RMMetrics = RMMetrics(), is_cache_hit: bool = False)\n</code></pre> <p>Metrics for a single operator in the query execution plan.</p> <p>Attributes:</p> <ul> <li> <code>operator_id</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the operator</p> </li> <li> <code>num_output_rows</code>               (<code>int</code>)           \u2013            <p>Number of rows output by this operator</p> </li> <li> <code>execution_time_ms</code>               (<code>float</code>)           \u2013            <p>Execution time in milliseconds</p> </li> <li> <code>lm_metrics</code>               (<code>LMMetrics</code>)           \u2013            <p>Language model usage metrics for this operator</p> </li> <li> <code>is_cache_hit</code>               (<code>bool</code>)           \u2013            <p>Whether results were retrieved from cache</p> </li> </ul>"},{"location":"reference/fenic/core/#fenic.core.PredicateExample","title":"PredicateExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single semantic example for semantic predicate operations.</p> <p>Predicate examples demonstrate the evaluation of input variables against a specific condition, used in a semantic.predicate operation.</p>"},{"location":"reference/fenic/core/#fenic.core.PredicateExampleCollection","title":"PredicateExampleCollection","text":"<pre><code>PredicateExampleCollection(examples: List[ExampleType] = None)\n</code></pre> <p>               Bases: <code>BaseExampleCollection[PredicateExample]</code></p> <p>Collection of examples for semantic predicate operations.</p> <p>Predicate operations evaluate conditions on input variables to produce boolean (True/False) results. This collection manages examples that demonstrate the expected boolean outcomes for different inputs.</p> <p>Examples in this collection can have multiple input variables, each mapped to their respective values, with a single boolean output representing the evaluation result of the predicate.</p> <p>Methods:</p> <ul> <li> <code>from_polars</code>             \u2013              <p>Create collection from a Polars DataFrame.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def __init__(self, examples: List[ExampleType] = None):\n    \"\"\"Initialize a collection of semantic examples.\n\n    Args:\n        examples: Optional list of examples to add to the collection. Each example\n            will be processed through create_example() to ensure proper formatting\n            and validation.\n\n    Note:\n        The examples list is initialized as empty if no examples are provided.\n        Each example in the provided list will be processed through create_example()\n        to ensure proper formatting and validation.\n    \"\"\"\n    self.examples: List[ExampleType] = []\n    if examples:\n        for example in examples:\n            self.create_example(example)\n</code></pre>"},{"location":"reference/fenic/core/#fenic.core.PredicateExampleCollection.from_polars","title":"from_polars  <code>classmethod</code>","text":"<pre><code>from_polars(df: DataFrame) -&gt; PredicateExampleCollection\n</code></pre> <p>Create collection from a Polars DataFrame.</p> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: pl.DataFrame) -&gt; PredicateExampleCollection:\n    \"\"\"Create collection from a Polars DataFrame.\"\"\"\n    collection = cls()\n\n    # Validate output column exists\n    if EXAMPLE_OUTPUT_KEY not in df.columns:\n        raise InvalidExampleCollectionError(\n            f\"Predicate Examples DataFrame missing required '{EXAMPLE_OUTPUT_KEY}' column\"\n        )\n\n    input_cols = [col for col in df.columns if col != EXAMPLE_OUTPUT_KEY]\n\n    if not input_cols:\n        raise InvalidExampleCollectionError(\n            \"Predicate Examples DataFrame must have at least one input column\"\n        )\n\n    for row in df.iter_rows(named=True):\n        if row[EXAMPLE_OUTPUT_KEY] is None:\n            raise InvalidExampleCollectionError(\n                f\"Predicate Examples DataFrame contains null values in '{EXAMPLE_OUTPUT_KEY}' column\"\n            )\n\n        input_dict = {col: row[col] for col in input_cols if row[col] is not None}\n\n        example = PredicateExample(input=input_dict, output=row[EXAMPLE_OUTPUT_KEY])\n        collection.create_example(example)\n\n    return collection\n</code></pre>"},{"location":"reference/fenic/core/#fenic.core.QueryMetrics","title":"QueryMetrics  <code>dataclass</code>","text":"<pre><code>QueryMetrics(execution_time_ms: float = 0.0, num_output_rows: int = 0, total_lm_metrics: LMMetrics = LMMetrics(), total_rm_metrics: RMMetrics = RMMetrics(), _operator_metrics: Dict[str, OperatorMetrics] = dict(), _plan_repr: PhysicalPlanRepr = lambda: PhysicalPlanRepr(operator_id='empty')())\n</code></pre> <p>Comprehensive metrics for an executed query.</p> <p>Includes overall statistics and detailed metrics for each operator in the execution plan.</p> <p>Attributes:</p> <ul> <li> <code>execution_time_ms</code>               (<code>float</code>)           \u2013            <p>Total query execution time in milliseconds</p> </li> <li> <code>num_output_rows</code>               (<code>int</code>)           \u2013            <p>Total number of rows returned by the query</p> </li> <li> <code>total_lm_metrics</code>               (<code>LMMetrics</code>)           \u2013            <p>Aggregated language model metrics across all operators</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_execution_plan_details</code>             \u2013              <p>Generate a formatted execution plan with detailed metrics.</p> </li> <li> <code>get_summary</code>             \u2013              <p>Summarize the query metrics in a single line.</p> </li> </ul>"},{"location":"reference/fenic/core/#fenic.core.QueryMetrics.get_execution_plan_details","title":"get_execution_plan_details","text":"<pre><code>get_execution_plan_details() -&gt; str\n</code></pre> <p>Generate a formatted execution plan with detailed metrics.</p> <p>Produces a hierarchical representation of the query execution plan, including performance metrics and language model usage for each operator.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A formatted string showing the execution plan with metrics.</p> </li> </ul> Source code in <code>src/fenic/core/metrics.py</code> <pre><code>def get_execution_plan_details(self) -&gt; str:\n    \"\"\"Generate a formatted execution plan with detailed metrics.\n\n    Produces a hierarchical representation of the query execution plan,\n    including performance metrics and language model usage for each operator.\n\n    Returns:\n        str: A formatted string showing the execution plan with metrics.\n    \"\"\"\n\n    def _format_node(node: PhysicalPlanRepr, indent: int = 1) -&gt; str:\n        op = self._operator_metrics[node.operator_id]\n        indent_str = \"  \" * indent\n\n        details = [\n            f\"{indent_str}{op.operator_id}\",\n            f\"{indent_str}  Output Rows: {op.num_output_rows:,}\",\n            f\"{indent_str}  Execution Time: {op.execution_time_ms:.2f}ms\",\n            f\"{indent_str}  Cached: {op.is_cache_hit}\",\n        ]\n\n        if op.lm_metrics.cost &gt; 0:\n            details.extend(\n                [\n                    f\"{indent_str}  Language Model Usage: {op.lm_metrics.num_uncached_input_tokens:,} input tokens, {op.lm_metrics.num_cached_input_tokens:,} cached input tokens, {op.lm_metrics.num_output_tokens:,} output tokens\",\n                    f\"{indent_str}  Language Model Cost: ${op.lm_metrics.cost:.6f}\",\n                ]\n            )\n\n        if op.rm_metrics.cost &gt; 0:\n            details.extend(\n                [\n                    f\"{indent_str}  Embedding Model Usage: {op.rm_metrics.num_input_tokens:,} input tokens\",\n                    f\"{indent_str}  Embedding Model Cost: ${op.rm_metrics.cost:.6f}\",\n                ]\n            )\n        return (\n            \"\\n\".join(details)\n            + \"\\n\"\n            + \"\".join(_format_node(child, indent + 1) for child in node.children)\n        )\n\n    return f\"Execution Plan\\n{_format_node(self._plan_repr)}\"\n</code></pre>"},{"location":"reference/fenic/core/#fenic.core.QueryMetrics.get_summary","title":"get_summary","text":"<pre><code>get_summary() -&gt; str\n</code></pre> <p>Summarize the query metrics in a single line.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A concise summary of execution time, row count, and LM cost.</p> </li> </ul> Source code in <code>src/fenic/core/metrics.py</code> <pre><code>def get_summary(self) -&gt; str:\n    \"\"\"Summarize the query metrics in a single line.\n\n    Returns:\n        str: A concise summary of execution time, row count, and LM cost.\n    \"\"\"\n    return (\n        f\"Query executed in {self.execution_time_ms:.2f}ms, \"\n        f\"returned {self.num_output_rows:,} rows, \"\n        f\"language model cost: ${self.total_lm_metrics.cost:.6f}, \"\n        f\"embedding model cost: ${self.total_rm_metrics.cost:.6f}\"\n    )\n</code></pre>"},{"location":"reference/fenic/core/#fenic.core.QueryResult","title":"QueryResult  <code>dataclass</code>","text":"<pre><code>QueryResult(data: DataLike, metrics: QueryMetrics)\n</code></pre> <p>Container for query execution results and associated metadata.</p> <p>This dataclass bundles together the materialized data from a query execution along with metrics about the execution process. It provides a unified interface for accessing both the computed results and performance information.</p> <p>Attributes:</p> <ul> <li> <code>data</code>               (<code>DataLike</code>)           \u2013            <p>The materialized query results in the requested format. Can be any of the supported data types (Polars/Pandas DataFrame, Arrow Table, or Python dict/list structures).</p> </li> <li> <code>metrics</code>               (<code>QueryMetrics</code>)           \u2013            <p>Execution metadata including timing information, memory usage, rows processed, and other performance metrics collected during query execution.</p> </li> </ul> Access query results and metrics <pre><code># Execute query and get results with metrics\nresult = df.filter(col(\"age\") &gt; 25).collect(\"pandas\")\npandas_df = result.data  # Access the Pandas DataFrame\nprint(result.metrics.execution_time)  # Access execution metrics\nprint(result.metrics.rows_processed)  # Access row count\n</code></pre> Work with different data formats <pre><code># Get results in different formats\npolars_result = df.collect(\"polars\")\narrow_result = df.collect(\"arrow\")\ndict_result = df.collect(\"pydict\")\n\n# All contain the same data, different formats\nprint(type(polars_result.data))  # &lt;class 'polars.DataFrame'&gt;\nprint(type(arrow_result.data))   # &lt;class 'pyarrow.lib.Table'&gt;\nprint(type(dict_result.data))    # &lt;class 'dict'&gt;\n</code></pre> Note <p>The actual type of the <code>data</code> attribute depends on the format requested during collection. Use type checking or isinstance() if you need to handle the data differently based on its format.</p>"},{"location":"reference/fenic/core/#fenic.core.RMMetrics","title":"RMMetrics  <code>dataclass</code>","text":"<pre><code>RMMetrics(num_input_tokens: int = 0, num_requests: int = 0, cost: float = 0.0)\n</code></pre> <p>Tracks embedding model usage metrics including token counts and costs.</p> <p>Attributes:</p> <ul> <li> <code>num_input_tokens</code>               (<code>int</code>)           \u2013            <p>Number of tokens to embed</p> </li> <li> <code>cost</code>               (<code>float</code>)           \u2013            <p>Total cost in USD to embed the tokens</p> </li> </ul>"},{"location":"reference/fenic/core/#fenic.core.Schema","title":"Schema","text":"<p>Represents the schema of a DataFrame.</p> <p>A Schema defines the structure of a DataFrame by specifying an ordered collection of column fields. Each column field defines the name and data type of a column in the DataFrame.</p> <p>Attributes:</p> <ul> <li> <code>column_fields</code>               (<code>List[ColumnField]</code>)           \u2013            <p>An ordered list of ColumnField objects that define the structure of the DataFrame.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>column_names</code>             \u2013              <p>Get a list of all column names in the schema.</p> </li> </ul>"},{"location":"reference/fenic/core/#fenic.core.Schema.column_names","title":"column_names","text":"<pre><code>column_names() -&gt; List[str]\n</code></pre> <p>Get a list of all column names in the schema.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>A list of strings containing the names of all columns in the schema.</p> </li> </ul> Source code in <code>src/fenic/core/types/schema.py</code> <pre><code>def column_names(self) -&gt; List[str]:\n    \"\"\"Get a list of all column names in the schema.\n\n    Returns:\n        A list of strings containing the names of all columns in the schema.\n    \"\"\"\n    return [field.name for field in self.column_fields]\n</code></pre>"},{"location":"reference/fenic/core/#fenic.core.StructField","title":"StructField","text":"<p>A field in a StructType. Fields are nullable.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the field.</p> </li> <li> <code>data_type</code>               (<code>DataType</code>)           \u2013            <p>The data type of the field.</p> </li> </ul>"},{"location":"reference/fenic/core/#fenic.core.StructType","title":"StructType","text":"<p>               Bases: <code>DataType</code></p> <p>A type representing a struct (record) with named fields.</p> <p>Attributes:</p> <ul> <li> <code>fields</code>           \u2013            <p>List of field definitions.</p> </li> </ul> Create a struct with name and age fields <pre><code>StructType([\n    StructField(\"name\", StringType),\n    StructField(\"age\", IntegerType),\n])\n</code></pre>"},{"location":"reference/fenic/core/#fenic.core.TranscriptType","title":"TranscriptType","text":"<p>               Bases: <code>_StringBackedType</code></p> <p>Represents a string containing a transcript in a specific format.</p>"},{"location":"reference/fenic/core/error/","title":"fenic.core.error","text":""},{"location":"reference/fenic/core/error/#fenic.core.error","title":"fenic.core.error","text":"<p>Fenic error hierarchy.</p> <p>Classes:</p> <ul> <li> <code>CatalogAlreadyExistsError</code>           \u2013            <p>Catalog already exists.</p> </li> <li> <code>CatalogError</code>           \u2013            <p>Catalog and table management errors.</p> </li> <li> <code>CatalogNotFoundError</code>           \u2013            <p>Catalog doesn't exist.</p> </li> <li> <code>CloudExecutionError</code>           \u2013            <p>Errors during physical plan execution in a cloud session.</p> </li> <li> <code>CloudSessionError</code>           \u2013            <p>Cloud session lifecycle errors.</p> </li> <li> <code>ColumnNotFoundError</code>           \u2013            <p>Column doesn't exist.</p> </li> <li> <code>ConfigurationError</code>           \u2013            <p>Errors during session configuration or initialization.</p> </li> <li> <code>DatabaseAlreadyExistsError</code>           \u2013            <p>Database already exists.</p> </li> <li> <code>DatabaseNotFoundError</code>           \u2013            <p>Database doesn't exist.</p> </li> <li> <code>ExecutionError</code>           \u2013            <p>Errors during physical plan execution.</p> </li> <li> <code>FenicError</code>           \u2013            <p>Base exception for all fenic errors.</p> </li> <li> <code>InternalError</code>           \u2013            <p>Internal invariant violations.</p> </li> <li> <code>InvalidExampleCollectionError</code>           \u2013            <p>Exception raised when a semantic example collection is invalid.</p> </li> <li> <code>LineageError</code>           \u2013            <p>Errors during lineage traversal.</p> </li> <li> <code>PlanError</code>           \u2013            <p>Errors during logical plan construction and validation.</p> </li> <li> <code>SessionError</code>           \u2013            <p>Session lifecycle errors.</p> </li> <li> <code>TableAlreadyExistsError</code>           \u2013            <p>Table already exists.</p> </li> <li> <code>TableNotFoundError</code>           \u2013            <p>Table doesn't exist.</p> </li> <li> <code>TypeMismatchError</code>           \u2013            <p>Type validation errors.</p> </li> <li> <code>ValidationError</code>           \u2013            <p>Invalid usage of public APIs or incorrect arguments.</p> </li> </ul>"},{"location":"reference/fenic/core/error/#fenic.core.error.CatalogAlreadyExistsError","title":"CatalogAlreadyExistsError","text":"<pre><code>CatalogAlreadyExistsError(catalog_name: str)\n</code></pre> <p>               Bases: <code>CatalogError</code></p> <p>Catalog already exists.</p> <p>Initialize a catalog already exists error.</p> <p>Parameters:</p> <ul> <li> <code>catalog_name</code>               (<code>str</code>)           \u2013            <p>The name of the catalog that already exists.</p> </li> </ul> Source code in <code>src/fenic/core/error.py</code> <pre><code>def __init__(self, catalog_name: str):\n    \"\"\"Initialize a catalog already exists error.\n\n    Args:\n        catalog_name: The name of the catalog that already exists.\n    \"\"\"\n    super().__init__(f\"Catalog '{catalog_name}' already exists\")\n</code></pre>"},{"location":"reference/fenic/core/error/#fenic.core.error.CatalogError","title":"CatalogError","text":"<p>               Bases: <code>FenicError</code></p> <p>Catalog and table management errors.</p>"},{"location":"reference/fenic/core/error/#fenic.core.error.CatalogNotFoundError","title":"CatalogNotFoundError","text":"<pre><code>CatalogNotFoundError(catalog_name: str)\n</code></pre> <p>               Bases: <code>CatalogError</code></p> <p>Catalog doesn't exist.</p> <p>Initialize a catalog not found error.</p> <p>Parameters:</p> <ul> <li> <code>catalog_name</code>               (<code>str</code>)           \u2013            <p>The name of the catalog that was not found.</p> </li> </ul> Source code in <code>src/fenic/core/error.py</code> <pre><code>def __init__(self, catalog_name: str):\n    \"\"\"Initialize a catalog not found error.\n\n    Args:\n        catalog_name: The name of the catalog that was not found.\n    \"\"\"\n    super().__init__(f\"Catalog '{catalog_name}' does not exist\")\n</code></pre>"},{"location":"reference/fenic/core/error/#fenic.core.error.CloudExecutionError","title":"CloudExecutionError","text":"<pre><code>CloudExecutionError(error_message: str)\n</code></pre> <p>               Bases: <code>ExecutionError</code></p> <p>Errors during physical plan execution in a cloud session.</p> <p>Initialize a cloud execution error.</p> <p>Parameters:</p> <ul> <li> <code>error_message</code>               (<code>str</code>)           \u2013            <p>The error message describing what went wrong.</p> </li> </ul> Source code in <code>src/fenic/core/error.py</code> <pre><code>def __init__(self, error_message: str):\n    \"\"\"Initialize a cloud execution error.\n\n    Args:\n        error_message: The error message describing what went wrong.\n    \"\"\"\n    super().__init__(\n        f\"{error_message}. \" \"Please file a ticket with Typedef support.\"\n    )\n</code></pre>"},{"location":"reference/fenic/core/error/#fenic.core.error.CloudSessionError","title":"CloudSessionError","text":"<pre><code>CloudSessionError(error_message: str)\n</code></pre> <p>               Bases: <code>SessionError</code></p> <p>Cloud session lifecycle errors.</p> <p>Initialize a cloud session error.</p> <p>Parameters:</p> <ul> <li> <code>error_message</code>               (<code>str</code>)           \u2013            <p>The error message describing what went wrong.</p> </li> </ul> Source code in <code>src/fenic/core/error.py</code> <pre><code>def __init__(self, error_message: str):\n    \"\"\"Initialize a cloud session error.\n\n    Args:\n        error_message: The error message describing what went wrong.\n    \"\"\"\n    super().__init__(\n        f\"{error_message}. \" \"Please file a ticket with Typedef support.\"\n    )\n</code></pre>"},{"location":"reference/fenic/core/error/#fenic.core.error.ColumnNotFoundError","title":"ColumnNotFoundError","text":"<pre><code>ColumnNotFoundError(column_name: str, available_columns: List[str])\n</code></pre> <p>               Bases: <code>PlanError</code></p> <p>Column doesn't exist.</p> <p>Initialize a column not found error.</p> <p>Parameters:</p> <ul> <li> <code>column_name</code>               (<code>str</code>)           \u2013            <p>The name of the column that was not found.</p> </li> <li> <code>available_columns</code>               (<code>List[str]</code>)           \u2013            <p>List of column names that are available.</p> </li> </ul> Source code in <code>src/fenic/core/error.py</code> <pre><code>def __init__(self, column_name: str, available_columns: List[str]):\n    \"\"\"Initialize a column not found error.\n\n    Args:\n        column_name: The name of the column that was not found.\n        available_columns: List of column names that are available.\n    \"\"\"\n    super().__init__(\n        f\"Column '{column_name}' not found. \"\n        f\"Available columns: {', '.join(sorted(available_columns))}\"\n    )\n</code></pre>"},{"location":"reference/fenic/core/error/#fenic.core.error.ConfigurationError","title":"ConfigurationError","text":"<p>               Bases: <code>FenicError</code></p> <p>Errors during session configuration or initialization.</p>"},{"location":"reference/fenic/core/error/#fenic.core.error.DatabaseAlreadyExistsError","title":"DatabaseAlreadyExistsError","text":"<pre><code>DatabaseAlreadyExistsError(database_name: str)\n</code></pre> <p>               Bases: <code>CatalogError</code></p> <p>Database already exists.</p> <p>Initialize a database already exists error.</p> <p>Parameters:</p> <ul> <li> <code>database_name</code>               (<code>str</code>)           \u2013            <p>The name of the database that already exists.</p> </li> </ul> Source code in <code>src/fenic/core/error.py</code> <pre><code>def __init__(self, database_name: str):\n    \"\"\"Initialize a database already exists error.\n\n    Args:\n        database_name: The name of the database that already exists.\n    \"\"\"\n    super().__init__(f\"Database '{database_name}' already exists\")\n</code></pre>"},{"location":"reference/fenic/core/error/#fenic.core.error.DatabaseNotFoundError","title":"DatabaseNotFoundError","text":"<pre><code>DatabaseNotFoundError(database_name: str)\n</code></pre> <p>               Bases: <code>CatalogError</code></p> <p>Database doesn't exist.</p> <p>Initialize a database not found error.</p> <p>Parameters:</p> <ul> <li> <code>database_name</code>               (<code>str</code>)           \u2013            <p>The name of the database that was not found.</p> </li> </ul> Source code in <code>src/fenic/core/error.py</code> <pre><code>def __init__(self, database_name: str):\n    \"\"\"Initialize a database not found error.\n\n    Args:\n        database_name: The name of the database that was not found.\n    \"\"\"\n    super().__init__(f\"Database '{database_name}' does not exist\")\n</code></pre>"},{"location":"reference/fenic/core/error/#fenic.core.error.ExecutionError","title":"ExecutionError","text":"<p>               Bases: <code>FenicError</code></p> <p>Errors during physical plan execution.</p>"},{"location":"reference/fenic/core/error/#fenic.core.error.FenicError","title":"FenicError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all fenic errors.</p>"},{"location":"reference/fenic/core/error/#fenic.core.error.InternalError","title":"InternalError","text":"<p>               Bases: <code>FenicError</code></p> <p>Internal invariant violations.</p>"},{"location":"reference/fenic/core/error/#fenic.core.error.InvalidExampleCollectionError","title":"InvalidExampleCollectionError","text":"<p>               Bases: <code>ValidationError</code></p> <p>Exception raised when a semantic example collection is invalid.</p>"},{"location":"reference/fenic/core/error/#fenic.core.error.LineageError","title":"LineageError","text":"<p>               Bases: <code>FenicError</code></p> <p>Errors during lineage traversal.</p>"},{"location":"reference/fenic/core/error/#fenic.core.error.PlanError","title":"PlanError","text":"<p>               Bases: <code>FenicError</code></p> <p>Errors during logical plan construction and validation.</p>"},{"location":"reference/fenic/core/error/#fenic.core.error.SessionError","title":"SessionError","text":"<p>               Bases: <code>ConfigurationError</code></p> <p>Session lifecycle errors.</p>"},{"location":"reference/fenic/core/error/#fenic.core.error.TableAlreadyExistsError","title":"TableAlreadyExistsError","text":"<pre><code>TableAlreadyExistsError(table_name: str, database: Optional[str] = None)\n</code></pre> <p>               Bases: <code>CatalogError</code></p> <p>Table already exists.</p> <p>Initialize a table already exists error.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>The name of the table that already exists.</p> </li> <li> <code>database</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional name of the database containing the table.</p> </li> </ul> Source code in <code>src/fenic/core/error.py</code> <pre><code>def __init__(self, table_name: str, database: Optional[str] = None):\n    \"\"\"Initialize a table already exists error.\n\n    Args:\n        table_name: The name of the table that already exists.\n        database: Optional name of the database containing the table.\n    \"\"\"\n    if database:\n        table_ref = f\"{database}.{table_name}\"\n    else:\n        table_ref = table_name\n    super().__init__(\n        f\"Table '{table_ref}' already exists. \"\n        f\"Use mode='overwrite' to replace the existing table.\"\n    )\n</code></pre>"},{"location":"reference/fenic/core/error/#fenic.core.error.TableNotFoundError","title":"TableNotFoundError","text":"<pre><code>TableNotFoundError(table_name: str, database: str)\n</code></pre> <p>               Bases: <code>CatalogError</code></p> <p>Table doesn't exist.</p> <p>Initialize a table not found error.</p> <p>Parameters:</p> <ul> <li> <code>table_name</code>               (<code>str</code>)           \u2013            <p>The name of the table that was not found.</p> </li> <li> <code>database</code>               (<code>str</code>)           \u2013            <p>The name of the database containing the table.</p> </li> </ul> Source code in <code>src/fenic/core/error.py</code> <pre><code>def __init__(self, table_name: str, database: str):\n    \"\"\"Initialize a table not found error.\n\n    Args:\n        table_name: The name of the table that was not found.\n        database: The name of the database containing the table.\n    \"\"\"\n    self.table_name = table_name\n    self.database = database\n    super().__init__(f\"Table '{database}.{table_name}' does not exist\")\n</code></pre>"},{"location":"reference/fenic/core/error/#fenic.core.error.TypeMismatchError","title":"TypeMismatchError","text":"<pre><code>TypeMismatchError(expected: DataType, actual: DataType, context: str)\n</code></pre> <p>               Bases: <code>PlanError</code></p> <p>Type validation errors.</p> <p>Initialize a type mismatch error.</p> <p>Parameters:</p> <ul> <li> <code>expected</code>               (<code>DataType</code>)           \u2013            <p>The expected data type.</p> </li> <li> <code>actual</code>               (<code>DataType</code>)           \u2013            <p>The actual data type that was found.</p> </li> <li> <code>context</code>               (<code>str</code>)           \u2013            <p>Additional context about where the type mismatch occurred.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>from_message</code>             \u2013              <p>Create a TypeMismatchError from a message string.</p> </li> </ul> Source code in <code>src/fenic/core/error.py</code> <pre><code>def __init__(self, expected: DataType, actual: DataType, context: str):\n    \"\"\"Initialize a type mismatch error.\n\n    Args:\n        expected: The expected data type.\n        actual: The actual data type that was found.\n        context: Additional context about where the type mismatch occurred.\n    \"\"\"\n    super().__init__(f\"{context}: expected {expected}, got {actual}\")\n</code></pre>"},{"location":"reference/fenic/core/error/#fenic.core.error.TypeMismatchError.from_message","title":"from_message  <code>classmethod</code>","text":"<pre><code>from_message(msg: str) -&gt; TypeMismatchError\n</code></pre> <p>Create a TypeMismatchError from a message string.</p> <p>Parameters:</p> <ul> <li> <code>msg</code>               (<code>str</code>)           \u2013            <p>The error message.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TypeMismatchError</code>           \u2013            <p>A new TypeMismatchError instance with the given message.</p> </li> </ul> Source code in <code>src/fenic/core/error.py</code> <pre><code>@classmethod\ndef from_message(cls, msg: str) -&gt; TypeMismatchError:\n    \"\"\"Create a TypeMismatchError from a message string.\n\n    Args:\n        msg: The error message.\n\n    Returns:\n        A new TypeMismatchError instance with the given message.\n    \"\"\"\n    instance = cls.__new__(cls)  # Bypass __init__\n    super(TypeMismatchError, instance).__init__(msg)\n    return instance\n</code></pre>"},{"location":"reference/fenic/core/error/#fenic.core.error.ValidationError","title":"ValidationError","text":"<p>               Bases: <code>FenicError</code></p> <p>Invalid usage of public APIs or incorrect arguments.</p>"},{"location":"reference/fenic/core/metrics/","title":"fenic.core.metrics","text":""},{"location":"reference/fenic/core/metrics/#fenic.core.metrics","title":"fenic.core.metrics","text":"<p>Metrics tracking for query execution and model usage.</p> <p>This module defines classes for tracking various metrics during query execution, including language model usage, embedding model usage, operator performance, and overall query statistics.</p> <p>Classes:</p> <ul> <li> <code>LMMetrics</code>           \u2013            <p>Tracks language model usage metrics including token counts and costs.</p> </li> <li> <code>OperatorMetrics</code>           \u2013            <p>Metrics for a single operator in the query execution plan.</p> </li> <li> <code>PhysicalPlanRepr</code>           \u2013            <p>Tree node representing the physical execution plan, used for pretty printing execution plan.</p> </li> <li> <code>QueryMetrics</code>           \u2013            <p>Comprehensive metrics for an executed query.</p> </li> <li> <code>RMMetrics</code>           \u2013            <p>Tracks embedding model usage metrics including token counts and costs.</p> </li> </ul>"},{"location":"reference/fenic/core/metrics/#fenic.core.metrics.LMMetrics","title":"LMMetrics  <code>dataclass</code>","text":"<pre><code>LMMetrics(num_uncached_input_tokens: int = 0, num_cached_input_tokens: int = 0, num_output_tokens: int = 0, cost: float = 0.0, num_requests: int = 0)\n</code></pre> <p>Tracks language model usage metrics including token counts and costs.</p> <p>Attributes:</p> <ul> <li> <code>num_uncached_input_tokens</code>               (<code>int</code>)           \u2013            <p>Number of uncached tokens in the prompt/input</p> </li> <li> <code>num_cached_input_tokens</code>               (<code>int</code>)           \u2013            <p>Number of cached tokens in the prompt/input,</p> </li> <li> <code>num_output_tokens</code>               (<code>int</code>)           \u2013            <p>Number of tokens in the completion/output</p> </li> <li> <code>cost</code>               (<code>float</code>)           \u2013            <p>Total cost in USD for the LM API call</p> </li> </ul>"},{"location":"reference/fenic/core/metrics/#fenic.core.metrics.OperatorMetrics","title":"OperatorMetrics  <code>dataclass</code>","text":"<pre><code>OperatorMetrics(operator_id: str, num_output_rows: int = 0, execution_time_ms: float = 0.0, lm_metrics: LMMetrics = LMMetrics(), rm_metrics: RMMetrics = RMMetrics(), is_cache_hit: bool = False)\n</code></pre> <p>Metrics for a single operator in the query execution plan.</p> <p>Attributes:</p> <ul> <li> <code>operator_id</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the operator</p> </li> <li> <code>num_output_rows</code>               (<code>int</code>)           \u2013            <p>Number of rows output by this operator</p> </li> <li> <code>execution_time_ms</code>               (<code>float</code>)           \u2013            <p>Execution time in milliseconds</p> </li> <li> <code>lm_metrics</code>               (<code>LMMetrics</code>)           \u2013            <p>Language model usage metrics for this operator</p> </li> <li> <code>is_cache_hit</code>               (<code>bool</code>)           \u2013            <p>Whether results were retrieved from cache</p> </li> </ul>"},{"location":"reference/fenic/core/metrics/#fenic.core.metrics.PhysicalPlanRepr","title":"PhysicalPlanRepr  <code>dataclass</code>","text":"<pre><code>PhysicalPlanRepr(operator_id: str, children: List[PhysicalPlanRepr] = list())\n</code></pre> <p>Tree node representing the physical execution plan, used for pretty printing execution plan.</p>"},{"location":"reference/fenic/core/metrics/#fenic.core.metrics.QueryMetrics","title":"QueryMetrics  <code>dataclass</code>","text":"<pre><code>QueryMetrics(execution_time_ms: float = 0.0, num_output_rows: int = 0, total_lm_metrics: LMMetrics = LMMetrics(), total_rm_metrics: RMMetrics = RMMetrics(), _operator_metrics: Dict[str, OperatorMetrics] = dict(), _plan_repr: PhysicalPlanRepr = lambda: PhysicalPlanRepr(operator_id='empty')())\n</code></pre> <p>Comprehensive metrics for an executed query.</p> <p>Includes overall statistics and detailed metrics for each operator in the execution plan.</p> <p>Attributes:</p> <ul> <li> <code>execution_time_ms</code>               (<code>float</code>)           \u2013            <p>Total query execution time in milliseconds</p> </li> <li> <code>num_output_rows</code>               (<code>int</code>)           \u2013            <p>Total number of rows returned by the query</p> </li> <li> <code>total_lm_metrics</code>               (<code>LMMetrics</code>)           \u2013            <p>Aggregated language model metrics across all operators</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_execution_plan_details</code>             \u2013              <p>Generate a formatted execution plan with detailed metrics.</p> </li> <li> <code>get_summary</code>             \u2013              <p>Summarize the query metrics in a single line.</p> </li> </ul>"},{"location":"reference/fenic/core/metrics/#fenic.core.metrics.QueryMetrics.get_execution_plan_details","title":"get_execution_plan_details","text":"<pre><code>get_execution_plan_details() -&gt; str\n</code></pre> <p>Generate a formatted execution plan with detailed metrics.</p> <p>Produces a hierarchical representation of the query execution plan, including performance metrics and language model usage for each operator.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A formatted string showing the execution plan with metrics.</p> </li> </ul> Source code in <code>src/fenic/core/metrics.py</code> <pre><code>def get_execution_plan_details(self) -&gt; str:\n    \"\"\"Generate a formatted execution plan with detailed metrics.\n\n    Produces a hierarchical representation of the query execution plan,\n    including performance metrics and language model usage for each operator.\n\n    Returns:\n        str: A formatted string showing the execution plan with metrics.\n    \"\"\"\n\n    def _format_node(node: PhysicalPlanRepr, indent: int = 1) -&gt; str:\n        op = self._operator_metrics[node.operator_id]\n        indent_str = \"  \" * indent\n\n        details = [\n            f\"{indent_str}{op.operator_id}\",\n            f\"{indent_str}  Output Rows: {op.num_output_rows:,}\",\n            f\"{indent_str}  Execution Time: {op.execution_time_ms:.2f}ms\",\n            f\"{indent_str}  Cached: {op.is_cache_hit}\",\n        ]\n\n        if op.lm_metrics.cost &gt; 0:\n            details.extend(\n                [\n                    f\"{indent_str}  Language Model Usage: {op.lm_metrics.num_uncached_input_tokens:,} input tokens, {op.lm_metrics.num_cached_input_tokens:,} cached input tokens, {op.lm_metrics.num_output_tokens:,} output tokens\",\n                    f\"{indent_str}  Language Model Cost: ${op.lm_metrics.cost:.6f}\",\n                ]\n            )\n\n        if op.rm_metrics.cost &gt; 0:\n            details.extend(\n                [\n                    f\"{indent_str}  Embedding Model Usage: {op.rm_metrics.num_input_tokens:,} input tokens\",\n                    f\"{indent_str}  Embedding Model Cost: ${op.rm_metrics.cost:.6f}\",\n                ]\n            )\n        return (\n            \"\\n\".join(details)\n            + \"\\n\"\n            + \"\".join(_format_node(child, indent + 1) for child in node.children)\n        )\n\n    return f\"Execution Plan\\n{_format_node(self._plan_repr)}\"\n</code></pre>"},{"location":"reference/fenic/core/metrics/#fenic.core.metrics.QueryMetrics.get_summary","title":"get_summary","text":"<pre><code>get_summary() -&gt; str\n</code></pre> <p>Summarize the query metrics in a single line.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A concise summary of execution time, row count, and LM cost.</p> </li> </ul> Source code in <code>src/fenic/core/metrics.py</code> <pre><code>def get_summary(self) -&gt; str:\n    \"\"\"Summarize the query metrics in a single line.\n\n    Returns:\n        str: A concise summary of execution time, row count, and LM cost.\n    \"\"\"\n    return (\n        f\"Query executed in {self.execution_time_ms:.2f}ms, \"\n        f\"returned {self.num_output_rows:,} rows, \"\n        f\"language model cost: ${self.total_lm_metrics.cost:.6f}, \"\n        f\"embedding model cost: ${self.total_rm_metrics.cost:.6f}\"\n    )\n</code></pre>"},{"location":"reference/fenic/core/metrics/#fenic.core.metrics.RMMetrics","title":"RMMetrics  <code>dataclass</code>","text":"<pre><code>RMMetrics(num_input_tokens: int = 0, num_requests: int = 0, cost: float = 0.0)\n</code></pre> <p>Tracks embedding model usage metrics including token counts and costs.</p> <p>Attributes:</p> <ul> <li> <code>num_input_tokens</code>               (<code>int</code>)           \u2013            <p>Number of tokens to embed</p> </li> <li> <code>cost</code>               (<code>float</code>)           \u2013            <p>Total cost in USD to embed the tokens</p> </li> </ul>"},{"location":"reference/fenic/core/types/","title":"fenic.core.types","text":""},{"location":"reference/fenic/core/types/#fenic.core.types","title":"fenic.core.types","text":"<p>Schema module for defining and manipulating DataFrame schemas.</p> <p>Classes:</p> <ul> <li> <code>ArrayType</code>           \u2013            <p>A type representing a homogeneous variable-length array (list) of elements.</p> </li> <li> <code>ClassifyExample</code>           \u2013            <p>A single semantic example for classification operations.</p> </li> <li> <code>ClassifyExampleCollection</code>           \u2013            <p>Collection of examples for semantic classification operations.</p> </li> <li> <code>ColumnField</code>           \u2013            <p>Represents a typed column in a DataFrame schema.</p> </li> <li> <code>DataType</code>           \u2013            <p>Base class for all data types.</p> </li> <li> <code>DocumentPathType</code>           \u2013            <p>Represents a string containing a a document's local (file system) or remote (URL) path.</p> </li> <li> <code>EmbeddingType</code>           \u2013            <p>A type representing a fixed-length embedding vector.</p> </li> <li> <code>ExtractSchema</code>           \u2013            <p>Represents a structured extraction schema.</p> </li> <li> <code>ExtractSchemaField</code>           \u2013            <p>Represents a field within an structured extraction schema.</p> </li> <li> <code>ExtractSchemaList</code>           \u2013            <p>Represents a list data type for structured extraction schema definitions.</p> </li> <li> <code>JoinExample</code>           \u2013            <p>A single semantic example for semantic join operations.</p> </li> <li> <code>JoinExampleCollection</code>           \u2013            <p>Collection of examples for semantic join operations.</p> </li> <li> <code>MapExample</code>           \u2013            <p>A single semantic example for semantic mapping operations.</p> </li> <li> <code>MapExampleCollection</code>           \u2013            <p>Collection of examples for semantic mapping operations.</p> </li> <li> <code>PredicateExample</code>           \u2013            <p>A single semantic example for semantic predicate operations.</p> </li> <li> <code>PredicateExampleCollection</code>           \u2013            <p>Collection of examples for semantic predicate operations.</p> </li> <li> <code>QueryResult</code>           \u2013            <p>Container for query execution results and associated metadata.</p> </li> <li> <code>Schema</code>           \u2013            <p>Represents the schema of a DataFrame.</p> </li> <li> <code>StructField</code>           \u2013            <p>A field in a StructType. Fields are nullable.</p> </li> <li> <code>StructType</code>           \u2013            <p>A type representing a struct (record) with named fields.</p> </li> <li> <code>TranscriptType</code>           \u2013            <p>Represents a string containing a transcript in a specific format.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>BooleanType</code>           \u2013            <p>Represents a boolean value. (True/False)</p> </li> <li> <code>BranchSide</code>           \u2013            <p>Type alias representing the side of a branch in a lineage graph.</p> </li> <li> <code>DataLike</code>           \u2013            <p>Union type representing any supported data format for both input and output operations.</p> </li> <li> <code>DataLikeType</code>           \u2013            <p>String literal type for specifying data output formats.</p> </li> <li> <code>DoubleType</code>           \u2013            <p>Represents a 64-bit floating-point number.</p> </li> <li> <code>FloatType</code>           \u2013            <p>Represents a 32-bit floating-point number.</p> </li> <li> <code>HtmlType</code>           \u2013            <p>Represents a string containing raw HTML markup.</p> </li> <li> <code>IntegerType</code>           \u2013            <p>Represents a signed integer value.</p> </li> <li> <code>JsonType</code>           \u2013            <p>Represents a string containing JSON data.</p> </li> <li> <code>MarkdownType</code>           \u2013            <p>Represents a string containing Markdown-formatted text.</p> </li> <li> <code>SemanticSimilarityMetric</code>           \u2013            <p>Type alias representing supported semantic similarity metrics.</p> </li> <li> <code>StringType</code>           \u2013            <p>Represents a UTF-8 encoded string value.</p> </li> </ul>"},{"location":"reference/fenic/core/types/#fenic.core.types.BooleanType","title":"BooleanType  <code>module-attribute</code>","text":"<pre><code>BooleanType = _BooleanType()\n</code></pre> <p>Represents a boolean value. (True/False)</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.BranchSide","title":"BranchSide  <code>module-attribute</code>","text":"<pre><code>BranchSide = Literal['left', 'right']\n</code></pre> <p>Type alias representing the side of a branch in a lineage graph.</p> <p>Valid values:</p> <ul> <li>\"left\": The left branch of a join.</li> <li>\"right\": The right branch of a join.</li> </ul>"},{"location":"reference/fenic/core/types/#fenic.core.types.DataLike","title":"DataLike  <code>module-attribute</code>","text":"<pre><code>DataLike = Union[DataFrame, DataFrame, Dict[str, List[Any]], List[Dict[str, Any]], Table]\n</code></pre> <p>Union type representing any supported data format for both input and output operations.</p> <p>This type encompasses all possible data structures that can be: 1. Used as input when creating DataFrames 2. Returned as output from query results</p> Supported formats <ul> <li>pl.DataFrame: Native Polars DataFrame with efficient columnar storage</li> <li>pd.DataFrame: Pandas DataFrame, optionally with PyArrow extension arrays</li> <li>Dict[str, List[Any]]: Column-oriented dictionary where:<ul> <li>Keys are column names (str)</li> <li>Values are lists containing all values for that column</li> </ul> </li> <li>List[Dict[str, Any]]: Row-oriented list where:<ul> <li>Each element is a dictionary representing one row</li> <li>Dictionary keys are column names, values are cell values</li> </ul> </li> <li>pa.Table: Apache Arrow Table with columnar memory layout</li> </ul> Usage <ul> <li>Input: Used in create_dataframe() to accept data in various formats</li> <li>Output: Used in QueryResult.data to return results in requested format</li> </ul> <p>The specific type returned depends on the DataLikeType format specified when collecting query results.</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.DataLikeType","title":"DataLikeType  <code>module-attribute</code>","text":"<pre><code>DataLikeType = Literal['polars', 'pandas', 'pydict', 'pylist', 'arrow']\n</code></pre> <p>String literal type for specifying data output formats.</p> Valid values <ul> <li>\"polars\": Native Polars DataFrame format</li> <li>\"pandas\": Pandas DataFrame with PyArrow extension arrays</li> <li>\"pydict\": Python dictionary with column names as keys, lists as values</li> <li>\"pylist\": Python list of dictionaries, each representing one row</li> <li>\"arrow\": Apache Arrow Table format</li> </ul> <p>Used as input parameter for methods that can return data in multiple formats.</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.DoubleType","title":"DoubleType  <code>module-attribute</code>","text":"<pre><code>DoubleType = _DoubleType()\n</code></pre> <p>Represents a 64-bit floating-point number.</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.FloatType","title":"FloatType  <code>module-attribute</code>","text":"<pre><code>FloatType = _FloatType()\n</code></pre> <p>Represents a 32-bit floating-point number.</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.HtmlType","title":"HtmlType  <code>module-attribute</code>","text":"<pre><code>HtmlType = _HtmlType()\n</code></pre> <p>Represents a string containing raw HTML markup.</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.IntegerType","title":"IntegerType  <code>module-attribute</code>","text":"<pre><code>IntegerType = _IntegerType()\n</code></pre> <p>Represents a signed integer value.</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.JsonType","title":"JsonType  <code>module-attribute</code>","text":"<pre><code>JsonType = _JsonType()\n</code></pre> <p>Represents a string containing JSON data.</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.MarkdownType","title":"MarkdownType  <code>module-attribute</code>","text":"<pre><code>MarkdownType = _MarkdownType()\n</code></pre> <p>Represents a string containing Markdown-formatted text.</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.SemanticSimilarityMetric","title":"SemanticSimilarityMetric  <code>module-attribute</code>","text":"<pre><code>SemanticSimilarityMetric = Literal['cosine', 'l2', 'dot']\n</code></pre> <p>Type alias representing supported semantic similarity metrics.</p> <p>Valid values:</p> <ul> <li>\"cosine\": Cosine similarity, measures the cosine of the angle between two vectors.</li> <li>\"l2\": Euclidean (L2) distance, measures the straight-line distance between two vectors.</li> <li>\"dot\": Dot product similarity, the raw inner product of two vectors.</li> </ul> <p>These metrics are commonly used for comparing embedding vectors in semantic search and other similarity-based applications.</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.StringType","title":"StringType  <code>module-attribute</code>","text":"<pre><code>StringType = _StringType()\n</code></pre> <p>Represents a UTF-8 encoded string value.</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.ArrayType","title":"ArrayType","text":"<p>               Bases: <code>DataType</code></p> <p>A type representing a homogeneous variable-length array (list) of elements.</p> <p>Attributes:</p> <ul> <li> <code>element_type</code>               (<code>DataType</code>)           \u2013            <p>The data type of each element in the array.</p> </li> </ul> Create an array of strings <pre><code>ArrayType(StringType)\nArrayType(element_type=StringType)\n</code></pre>"},{"location":"reference/fenic/core/types/#fenic.core.types.ClassifyExample","title":"ClassifyExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single semantic example for classification operations.</p> <p>Classify examples demonstrate the classification of an input string into a specific category string, used in a semantic.classify operation.</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.ClassifyExampleCollection","title":"ClassifyExampleCollection","text":"<pre><code>ClassifyExampleCollection(examples: List[ExampleType] = None)\n</code></pre> <p>               Bases: <code>BaseExampleCollection[ClassifyExample]</code></p> <p>Collection of examples for semantic classification operations.</p> <p>Classification operations categorize input text into predefined classes. This collection manages examples that demonstrate the expected classification results for different inputs.</p> <p>Examples in this collection have a single input string and an output string representing the classification result.</p> <p>Methods:</p> <ul> <li> <code>from_polars</code>             \u2013              <p>Create collection from a Polars DataFrame. Must have an 'output' column and an 'input' column.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def __init__(self, examples: List[ExampleType] = None):\n    \"\"\"Initialize a collection of semantic examples.\n\n    Args:\n        examples: Optional list of examples to add to the collection. Each example\n            will be processed through create_example() to ensure proper formatting\n            and validation.\n\n    Note:\n        The examples list is initialized as empty if no examples are provided.\n        Each example in the provided list will be processed through create_example()\n        to ensure proper formatting and validation.\n    \"\"\"\n    self.examples: List[ExampleType] = []\n    if examples:\n        for example in examples:\n            self.create_example(example)\n</code></pre>"},{"location":"reference/fenic/core/types/#fenic.core.types.ClassifyExampleCollection.from_polars","title":"from_polars  <code>classmethod</code>","text":"<pre><code>from_polars(df: DataFrame) -&gt; ClassifyExampleCollection\n</code></pre> <p>Create collection from a Polars DataFrame. Must have an 'output' column and an 'input' column.</p> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: pl.DataFrame) -&gt; ClassifyExampleCollection:\n    \"\"\"Create collection from a Polars DataFrame. Must have an 'output' column and an 'input' column.\"\"\"\n    collection = cls()\n\n    if EXAMPLE_INPUT_KEY not in df.columns:\n        raise InvalidExampleCollectionError(\n            f\"Classify Examples DataFrame missing required '{EXAMPLE_INPUT_KEY}' column\"\n        )\n    if EXAMPLE_OUTPUT_KEY not in df.columns:\n        raise InvalidExampleCollectionError(\n            f\"Classify Examples DataFrame missing required '{EXAMPLE_OUTPUT_KEY}' column\"\n        )\n\n    for row in df.iter_rows(named=True):\n        if row[EXAMPLE_INPUT_KEY] is None:\n            raise InvalidExampleCollectionError(\n                f\"Classify Examples DataFrame contains null values in '{EXAMPLE_INPUT_KEY}' column\"\n            )\n        if row[EXAMPLE_OUTPUT_KEY] is None:\n            raise InvalidExampleCollectionError(\n                f\"Classify Examples DataFrame contains null values in '{EXAMPLE_OUTPUT_KEY}' column\"\n            )\n\n        example = ClassifyExample(\n            input=row[EXAMPLE_INPUT_KEY],\n            output=row[EXAMPLE_OUTPUT_KEY],\n        )\n        collection.create_example(example)\n\n    return collection\n</code></pre>"},{"location":"reference/fenic/core/types/#fenic.core.types.ColumnField","title":"ColumnField","text":"<p>Represents a typed column in a DataFrame schema.</p> <p>A ColumnField defines the structure of a single column by specifying its name and data type. This is used as a building block for DataFrame schemas.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the column.</p> </li> <li> <code>data_type</code>               (<code>DataType</code>)           \u2013            <p>The data type of the column, as a DataType instance.</p> </li> </ul>"},{"location":"reference/fenic/core/types/#fenic.core.types.DataType","title":"DataType","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all data types.</p> <p>You won't instantiate this class directly. Instead, use one of the concrete types like <code>StringType</code>, <code>ArrayType</code>, or <code>StructType</code>.</p> <p>Used for casting, type validation, and schema inference in the DataFrame API.</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.DocumentPathType","title":"DocumentPathType","text":"<p>               Bases: <code>_StringBackedType</code></p> <p>Represents a string containing a a document's local (file system) or remote (URL) path.</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.EmbeddingType","title":"EmbeddingType","text":"<p>               Bases: <code>DataType</code></p> <p>A type representing a fixed-length embedding vector.</p> <p>Attributes:</p> <ul> <li> <code>dimensions</code>               (<code>int</code>)           \u2013            <p>The number of dimensions in the embedding vector.</p> </li> <li> <code>embedding_model</code>               (<code>str</code>)           \u2013            <p>Name of the model used to generate the embedding.</p> </li> </ul> Create an embedding type for text-embedding-3-small <pre><code>EmbeddingType(384, embedding_model=\"text-embedding-3-small\")\n</code></pre>"},{"location":"reference/fenic/core/types/#fenic.core.types.ExtractSchema","title":"ExtractSchema","text":"<p>Represents a structured extraction schema.</p> <p>An extract schema contains a collection of named fields with descriptions that define what information should be extracted into each field.</p> <p>Methods:</p> <ul> <li> <code>field_names</code>             \u2013              <p>Get a list of all field names in the schema.</p> </li> </ul>"},{"location":"reference/fenic/core/types/#fenic.core.types.ExtractSchema.field_names","title":"field_names","text":"<pre><code>field_names() -&gt; List[str]\n</code></pre> <p>Get a list of all field names in the schema.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>A list of strings containing the names of all fields in the schema.</p> </li> </ul> Source code in <code>src/fenic/core/types/extract_schema.py</code> <pre><code>def field_names(self) -&gt; List[str]:\n    \"\"\"Get a list of all field names in the schema.\n\n    Returns:\n        A list of strings containing the names of all fields in the schema.\n    \"\"\"\n    return [field.name for field in self.struct_fields]\n</code></pre>"},{"location":"reference/fenic/core/types/#fenic.core.types.ExtractSchemaField","title":"ExtractSchemaField","text":"<pre><code>ExtractSchemaField(name: str, data_type: Union[DataType, ExtractSchemaList, ExtractSchema], description: str)\n</code></pre> <p>Represents a field within an structured extraction schema.</p> <p>An extract schema field has a name, a data type, and a required description that explains what information should be extracted into this field.</p> <p>Initialize an ExtractSchemaField.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the field.</p> </li> <li> <code>data_type</code>               (<code>Union[DataType, ExtractSchemaList, ExtractSchema]</code>)           \u2013            <p>The data type of the field. Must be either a primitive DataType, ExtractSchemaList, or ExtractSchema.</p> </li> <li> <code>description</code>               (<code>str</code>)           \u2013            <p>A description of what information should be extracted into this field.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If data_type is a non-primitive DataType.</p> </li> </ul> Source code in <code>src/fenic/core/types/extract_schema.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    data_type: Union[DataType, ExtractSchemaList, ExtractSchema],\n    description: str,\n):\n    \"\"\"Initialize an ExtractSchemaField.\n\n    Args:\n        name: The name of the field.\n        data_type: The data type of the field. Must be either a primitive DataType,\n            ExtractSchemaList, or ExtractSchema.\n        description: A description of what information should be extracted into this field.\n\n    Raises:\n        ValueError: If data_type is a non-primitive DataType.\n    \"\"\"\n    self.name = name\n    if isinstance(data_type, DataType) and not isinstance(\n        data_type, _PrimitiveType\n    ):\n        raise ValueError(\n            f\"Invalid data type: {data_type}. Only primitive types are supported directly. \"\n            f\"For complex types, please use ExtractSchemaList or ExtractSchema instead.\"\n        )\n    self.data_type = data_type\n    self.description = description\n</code></pre>"},{"location":"reference/fenic/core/types/#fenic.core.types.ExtractSchemaList","title":"ExtractSchemaList","text":"<pre><code>ExtractSchemaList(element_type: Union[DataType, ExtractSchema])\n</code></pre> <p>Represents a list data type for structured extraction schema definitions.</p> <p>A schema list contains elements of a specific data type and is used for defining array-like structures in structured extraction schemas.</p> <p>Initialize an ExtractSchemaList.</p> <p>Parameters:</p> <ul> <li> <code>element_type</code>               (<code>Union[DataType, ExtractSchema]</code>)           \u2013            <p>The data type of elements in the list. Must be either a primitive DataType or another ExtractSchema.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If element_type is a non-primitive DataType.</p> </li> </ul> Source code in <code>src/fenic/core/types/extract_schema.py</code> <pre><code>def __init__(\n    self,\n    element_type: Union[DataType, ExtractSchema],\n):\n    \"\"\"Initialize an ExtractSchemaList.\n\n    Args:\n        element_type: The data type of elements in the list. Must be either a primitive\n            DataType or another ExtractSchema.\n\n    Raises:\n        ValueError: If element_type is a non-primitive DataType.\n    \"\"\"\n    if isinstance(element_type, DataType) and not isinstance(\n        element_type, _PrimitiveType\n    ):\n        raise ValueError(\n            f\"Invalid element type: {element_type}. Only primitive types are supported directly. \"\n            f\"For complex types, please use ExtractSchemaList or ExtractSchema instead.\"\n        )\n    self.element_type = element_type\n</code></pre>"},{"location":"reference/fenic/core/types/#fenic.core.types.JoinExample","title":"JoinExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single semantic example for semantic join operations.</p> <p>Join examples demonstrate the evaluation of two input strings across different datasets against a specific condition, used in a semantic.join operation.</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.JoinExampleCollection","title":"JoinExampleCollection","text":"<pre><code>JoinExampleCollection(examples: List[ExampleType] = None)\n</code></pre> <p>               Bases: <code>BaseExampleCollection[JoinExample]</code></p> <p>Collection of examples for semantic join operations.</p> <p>Methods:</p> <ul> <li> <code>from_polars</code>             \u2013              <p>Create collection from a Polars DataFrame. Must have 'left', 'right', and 'output' columns.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def __init__(self, examples: List[ExampleType] = None):\n    \"\"\"Initialize a collection of semantic examples.\n\n    Args:\n        examples: Optional list of examples to add to the collection. Each example\n            will be processed through create_example() to ensure proper formatting\n            and validation.\n\n    Note:\n        The examples list is initialized as empty if no examples are provided.\n        Each example in the provided list will be processed through create_example()\n        to ensure proper formatting and validation.\n    \"\"\"\n    self.examples: List[ExampleType] = []\n    if examples:\n        for example in examples:\n            self.create_example(example)\n</code></pre>"},{"location":"reference/fenic/core/types/#fenic.core.types.JoinExampleCollection.from_polars","title":"from_polars  <code>classmethod</code>","text":"<pre><code>from_polars(df: DataFrame) -&gt; JoinExampleCollection\n</code></pre> <p>Create collection from a Polars DataFrame. Must have 'left', 'right', and 'output' columns.</p> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: pl.DataFrame) -&gt; JoinExampleCollection:\n    \"\"\"Create collection from a Polars DataFrame. Must have 'left', 'right', and 'output' columns.\"\"\"\n    collection = cls()\n\n    required_columns = [\n        EXAMPLE_LEFT_KEY,\n        EXAMPLE_RIGHT_KEY,\n        EXAMPLE_OUTPUT_KEY,\n    ]\n    for col in required_columns:\n        if col not in df.columns:\n            raise InvalidExampleCollectionError(\n                f\"Join Examples DataFrame missing required '{col}' column\"\n            )\n\n    for row in df.iter_rows(named=True):\n        for col in required_columns:\n            if row[col] is None:\n                raise InvalidExampleCollectionError(\n                    f\"Join Examples DataFrame contains null values in '{col}' column\"\n                )\n\n        example = JoinExample(\n            left=row[EXAMPLE_LEFT_KEY],\n            right=row[EXAMPLE_RIGHT_KEY],\n            output=row[EXAMPLE_OUTPUT_KEY],\n        )\n        collection.create_example(example)\n\n    return collection\n</code></pre>"},{"location":"reference/fenic/core/types/#fenic.core.types.MapExample","title":"MapExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single semantic example for semantic mapping operations.</p> <p>Map examples demonstrate the transformation of input variables to a specific output string used in a semantic.map operation.</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.MapExampleCollection","title":"MapExampleCollection","text":"<pre><code>MapExampleCollection(examples: List[ExampleType] = None)\n</code></pre> <p>               Bases: <code>BaseExampleCollection[MapExample]</code></p> <p>Collection of examples for semantic mapping operations.</p> <p>Map operations transform input variables into a text output according to specified instructions. This collection manages examples that demonstrate the expected transformations for different inputs.</p> <p>Examples in this collection can have multiple input variables, each mapped to their respective values, with a single output string representing the expected transformation result.</p> <p>Methods:</p> <ul> <li> <code>from_polars</code>             \u2013              <p>Create collection from a Polars DataFrame. Must have an 'output' column and at least one input column.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def __init__(self, examples: List[ExampleType] = None):\n    \"\"\"Initialize a collection of semantic examples.\n\n    Args:\n        examples: Optional list of examples to add to the collection. Each example\n            will be processed through create_example() to ensure proper formatting\n            and validation.\n\n    Note:\n        The examples list is initialized as empty if no examples are provided.\n        Each example in the provided list will be processed through create_example()\n        to ensure proper formatting and validation.\n    \"\"\"\n    self.examples: List[ExampleType] = []\n    if examples:\n        for example in examples:\n            self.create_example(example)\n</code></pre>"},{"location":"reference/fenic/core/types/#fenic.core.types.MapExampleCollection.from_polars","title":"from_polars  <code>classmethod</code>","text":"<pre><code>from_polars(df: DataFrame) -&gt; MapExampleCollection\n</code></pre> <p>Create collection from a Polars DataFrame. Must have an 'output' column and at least one input column.</p> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: pl.DataFrame) -&gt; MapExampleCollection:\n    \"\"\"Create collection from a Polars DataFrame. Must have an 'output' column and at least one input column.\"\"\"\n    collection = cls()\n\n    if EXAMPLE_OUTPUT_KEY not in df.columns:\n        raise ValueError(\n            f\"Map Examples DataFrame missing required '{EXAMPLE_OUTPUT_KEY}' column\"\n        )\n\n    input_cols = [col for col in df.columns if col != EXAMPLE_OUTPUT_KEY]\n\n    if not input_cols:\n        raise ValueError(\n            \"Map Examples DataFrame must have at least one input column\"\n        )\n\n    for row in df.iter_rows(named=True):\n        if row[EXAMPLE_OUTPUT_KEY] is None:\n            raise InvalidExampleCollectionError(\n                f\"Map Examples DataFrame contains null values in '{EXAMPLE_OUTPUT_KEY}' column\"\n            )\n\n        input_dict = {\n            col: str(row[col]) for col in input_cols if row[col] is not None\n        }\n\n        example = MapExample(input=input_dict, output=row[EXAMPLE_OUTPUT_KEY])\n        collection.create_example(example)\n\n    return collection\n</code></pre>"},{"location":"reference/fenic/core/types/#fenic.core.types.PredicateExample","title":"PredicateExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single semantic example for semantic predicate operations.</p> <p>Predicate examples demonstrate the evaluation of input variables against a specific condition, used in a semantic.predicate operation.</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.PredicateExampleCollection","title":"PredicateExampleCollection","text":"<pre><code>PredicateExampleCollection(examples: List[ExampleType] = None)\n</code></pre> <p>               Bases: <code>BaseExampleCollection[PredicateExample]</code></p> <p>Collection of examples for semantic predicate operations.</p> <p>Predicate operations evaluate conditions on input variables to produce boolean (True/False) results. This collection manages examples that demonstrate the expected boolean outcomes for different inputs.</p> <p>Examples in this collection can have multiple input variables, each mapped to their respective values, with a single boolean output representing the evaluation result of the predicate.</p> <p>Methods:</p> <ul> <li> <code>from_polars</code>             \u2013              <p>Create collection from a Polars DataFrame.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def __init__(self, examples: List[ExampleType] = None):\n    \"\"\"Initialize a collection of semantic examples.\n\n    Args:\n        examples: Optional list of examples to add to the collection. Each example\n            will be processed through create_example() to ensure proper formatting\n            and validation.\n\n    Note:\n        The examples list is initialized as empty if no examples are provided.\n        Each example in the provided list will be processed through create_example()\n        to ensure proper formatting and validation.\n    \"\"\"\n    self.examples: List[ExampleType] = []\n    if examples:\n        for example in examples:\n            self.create_example(example)\n</code></pre>"},{"location":"reference/fenic/core/types/#fenic.core.types.PredicateExampleCollection.from_polars","title":"from_polars  <code>classmethod</code>","text":"<pre><code>from_polars(df: DataFrame) -&gt; PredicateExampleCollection\n</code></pre> <p>Create collection from a Polars DataFrame.</p> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: pl.DataFrame) -&gt; PredicateExampleCollection:\n    \"\"\"Create collection from a Polars DataFrame.\"\"\"\n    collection = cls()\n\n    # Validate output column exists\n    if EXAMPLE_OUTPUT_KEY not in df.columns:\n        raise InvalidExampleCollectionError(\n            f\"Predicate Examples DataFrame missing required '{EXAMPLE_OUTPUT_KEY}' column\"\n        )\n\n    input_cols = [col for col in df.columns if col != EXAMPLE_OUTPUT_KEY]\n\n    if not input_cols:\n        raise InvalidExampleCollectionError(\n            \"Predicate Examples DataFrame must have at least one input column\"\n        )\n\n    for row in df.iter_rows(named=True):\n        if row[EXAMPLE_OUTPUT_KEY] is None:\n            raise InvalidExampleCollectionError(\n                f\"Predicate Examples DataFrame contains null values in '{EXAMPLE_OUTPUT_KEY}' column\"\n            )\n\n        input_dict = {col: row[col] for col in input_cols if row[col] is not None}\n\n        example = PredicateExample(input=input_dict, output=row[EXAMPLE_OUTPUT_KEY])\n        collection.create_example(example)\n\n    return collection\n</code></pre>"},{"location":"reference/fenic/core/types/#fenic.core.types.QueryResult","title":"QueryResult  <code>dataclass</code>","text":"<pre><code>QueryResult(data: DataLike, metrics: QueryMetrics)\n</code></pre> <p>Container for query execution results and associated metadata.</p> <p>This dataclass bundles together the materialized data from a query execution along with metrics about the execution process. It provides a unified interface for accessing both the computed results and performance information.</p> <p>Attributes:</p> <ul> <li> <code>data</code>               (<code>DataLike</code>)           \u2013            <p>The materialized query results in the requested format. Can be any of the supported data types (Polars/Pandas DataFrame, Arrow Table, or Python dict/list structures).</p> </li> <li> <code>metrics</code>               (<code>QueryMetrics</code>)           \u2013            <p>Execution metadata including timing information, memory usage, rows processed, and other performance metrics collected during query execution.</p> </li> </ul> Access query results and metrics <pre><code># Execute query and get results with metrics\nresult = df.filter(col(\"age\") &gt; 25).collect(\"pandas\")\npandas_df = result.data  # Access the Pandas DataFrame\nprint(result.metrics.execution_time)  # Access execution metrics\nprint(result.metrics.rows_processed)  # Access row count\n</code></pre> Work with different data formats <pre><code># Get results in different formats\npolars_result = df.collect(\"polars\")\narrow_result = df.collect(\"arrow\")\ndict_result = df.collect(\"pydict\")\n\n# All contain the same data, different formats\nprint(type(polars_result.data))  # &lt;class 'polars.DataFrame'&gt;\nprint(type(arrow_result.data))   # &lt;class 'pyarrow.lib.Table'&gt;\nprint(type(dict_result.data))    # &lt;class 'dict'&gt;\n</code></pre> Note <p>The actual type of the <code>data</code> attribute depends on the format requested during collection. Use type checking or isinstance() if you need to handle the data differently based on its format.</p>"},{"location":"reference/fenic/core/types/#fenic.core.types.Schema","title":"Schema","text":"<p>Represents the schema of a DataFrame.</p> <p>A Schema defines the structure of a DataFrame by specifying an ordered collection of column fields. Each column field defines the name and data type of a column in the DataFrame.</p> <p>Attributes:</p> <ul> <li> <code>column_fields</code>               (<code>List[ColumnField]</code>)           \u2013            <p>An ordered list of ColumnField objects that define the structure of the DataFrame.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>column_names</code>             \u2013              <p>Get a list of all column names in the schema.</p> </li> </ul>"},{"location":"reference/fenic/core/types/#fenic.core.types.Schema.column_names","title":"column_names","text":"<pre><code>column_names() -&gt; List[str]\n</code></pre> <p>Get a list of all column names in the schema.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>A list of strings containing the names of all columns in the schema.</p> </li> </ul> Source code in <code>src/fenic/core/types/schema.py</code> <pre><code>def column_names(self) -&gt; List[str]:\n    \"\"\"Get a list of all column names in the schema.\n\n    Returns:\n        A list of strings containing the names of all columns in the schema.\n    \"\"\"\n    return [field.name for field in self.column_fields]\n</code></pre>"},{"location":"reference/fenic/core/types/#fenic.core.types.StructField","title":"StructField","text":"<p>A field in a StructType. Fields are nullable.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the field.</p> </li> <li> <code>data_type</code>               (<code>DataType</code>)           \u2013            <p>The data type of the field.</p> </li> </ul>"},{"location":"reference/fenic/core/types/#fenic.core.types.StructType","title":"StructType","text":"<p>               Bases: <code>DataType</code></p> <p>A type representing a struct (record) with named fields.</p> <p>Attributes:</p> <ul> <li> <code>fields</code>           \u2013            <p>List of field definitions.</p> </li> </ul> Create a struct with name and age fields <pre><code>StructType([\n    StructField(\"name\", StringType),\n    StructField(\"age\", IntegerType),\n])\n</code></pre>"},{"location":"reference/fenic/core/types/#fenic.core.types.TranscriptType","title":"TranscriptType","text":"<p>               Bases: <code>_StringBackedType</code></p> <p>Represents a string containing a transcript in a specific format.</p>"},{"location":"reference/fenic/core/types/datatypes/","title":"fenic.core.types.datatypes","text":""},{"location":"reference/fenic/core/types/datatypes/#fenic.core.types.datatypes","title":"fenic.core.types.datatypes","text":"<p>Core data type definitions for the DataFrame API.</p> <p>This module defines the type system used throughout the DataFrame API. It includes: - Base classes for all data types - Primitive types (string, integer, float, etc.) - Composite types (arrays, structs) - Specialized types (embeddings, markdown, etc.)</p> <p>Classes:</p> <ul> <li> <code>ArrayType</code>           \u2013            <p>A type representing a homogeneous variable-length array (list) of elements.</p> </li> <li> <code>DataType</code>           \u2013            <p>Base class for all data types.</p> </li> <li> <code>DocumentPathType</code>           \u2013            <p>Represents a string containing a a document's local (file system) or remote (URL) path.</p> </li> <li> <code>EmbeddingType</code>           \u2013            <p>A type representing a fixed-length embedding vector.</p> </li> <li> <code>StructField</code>           \u2013            <p>A field in a StructType. Fields are nullable.</p> </li> <li> <code>StructType</code>           \u2013            <p>A type representing a struct (record) with named fields.</p> </li> <li> <code>TranscriptType</code>           \u2013            <p>Represents a string containing a transcript in a specific format.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>is_dtype_numeric</code>             \u2013              <p>Check if a data type is a numeric type.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>BooleanType</code>           \u2013            <p>Represents a boolean value. (True/False)</p> </li> <li> <code>DoubleType</code>           \u2013            <p>Represents a 64-bit floating-point number.</p> </li> <li> <code>FloatType</code>           \u2013            <p>Represents a 32-bit floating-point number.</p> </li> <li> <code>HtmlType</code>           \u2013            <p>Represents a string containing raw HTML markup.</p> </li> <li> <code>IntegerType</code>           \u2013            <p>Represents a signed integer value.</p> </li> <li> <code>JsonType</code>           \u2013            <p>Represents a string containing JSON data.</p> </li> <li> <code>MarkdownType</code>           \u2013            <p>Represents a string containing Markdown-formatted text.</p> </li> <li> <code>StringType</code>           \u2013            <p>Represents a UTF-8 encoded string value.</p> </li> </ul>"},{"location":"reference/fenic/core/types/datatypes/#fenic.core.types.datatypes.BooleanType","title":"BooleanType  <code>module-attribute</code>","text":"<pre><code>BooleanType = _BooleanType()\n</code></pre> <p>Represents a boolean value. (True/False)</p>"},{"location":"reference/fenic/core/types/datatypes/#fenic.core.types.datatypes.DoubleType","title":"DoubleType  <code>module-attribute</code>","text":"<pre><code>DoubleType = _DoubleType()\n</code></pre> <p>Represents a 64-bit floating-point number.</p>"},{"location":"reference/fenic/core/types/datatypes/#fenic.core.types.datatypes.FloatType","title":"FloatType  <code>module-attribute</code>","text":"<pre><code>FloatType = _FloatType()\n</code></pre> <p>Represents a 32-bit floating-point number.</p>"},{"location":"reference/fenic/core/types/datatypes/#fenic.core.types.datatypes.HtmlType","title":"HtmlType  <code>module-attribute</code>","text":"<pre><code>HtmlType = _HtmlType()\n</code></pre> <p>Represents a string containing raw HTML markup.</p>"},{"location":"reference/fenic/core/types/datatypes/#fenic.core.types.datatypes.IntegerType","title":"IntegerType  <code>module-attribute</code>","text":"<pre><code>IntegerType = _IntegerType()\n</code></pre> <p>Represents a signed integer value.</p>"},{"location":"reference/fenic/core/types/datatypes/#fenic.core.types.datatypes.JsonType","title":"JsonType  <code>module-attribute</code>","text":"<pre><code>JsonType = _JsonType()\n</code></pre> <p>Represents a string containing JSON data.</p>"},{"location":"reference/fenic/core/types/datatypes/#fenic.core.types.datatypes.MarkdownType","title":"MarkdownType  <code>module-attribute</code>","text":"<pre><code>MarkdownType = _MarkdownType()\n</code></pre> <p>Represents a string containing Markdown-formatted text.</p>"},{"location":"reference/fenic/core/types/datatypes/#fenic.core.types.datatypes.StringType","title":"StringType  <code>module-attribute</code>","text":"<pre><code>StringType = _StringType()\n</code></pre> <p>Represents a UTF-8 encoded string value.</p>"},{"location":"reference/fenic/core/types/datatypes/#fenic.core.types.datatypes.ArrayType","title":"ArrayType","text":"<p>               Bases: <code>DataType</code></p> <p>A type representing a homogeneous variable-length array (list) of elements.</p> <p>Attributes:</p> <ul> <li> <code>element_type</code>               (<code>DataType</code>)           \u2013            <p>The data type of each element in the array.</p> </li> </ul> Create an array of strings <pre><code>ArrayType(StringType)\nArrayType(element_type=StringType)\n</code></pre>"},{"location":"reference/fenic/core/types/datatypes/#fenic.core.types.datatypes.DataType","title":"DataType","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all data types.</p> <p>You won't instantiate this class directly. Instead, use one of the concrete types like <code>StringType</code>, <code>ArrayType</code>, or <code>StructType</code>.</p> <p>Used for casting, type validation, and schema inference in the DataFrame API.</p>"},{"location":"reference/fenic/core/types/datatypes/#fenic.core.types.datatypes.DocumentPathType","title":"DocumentPathType","text":"<p>               Bases: <code>_StringBackedType</code></p> <p>Represents a string containing a a document's local (file system) or remote (URL) path.</p>"},{"location":"reference/fenic/core/types/datatypes/#fenic.core.types.datatypes.EmbeddingType","title":"EmbeddingType","text":"<p>               Bases: <code>DataType</code></p> <p>A type representing a fixed-length embedding vector.</p> <p>Attributes:</p> <ul> <li> <code>dimensions</code>               (<code>int</code>)           \u2013            <p>The number of dimensions in the embedding vector.</p> </li> <li> <code>embedding_model</code>               (<code>str</code>)           \u2013            <p>Name of the model used to generate the embedding.</p> </li> </ul> Create an embedding type for text-embedding-3-small <pre><code>EmbeddingType(384, embedding_model=\"text-embedding-3-small\")\n</code></pre>"},{"location":"reference/fenic/core/types/datatypes/#fenic.core.types.datatypes.StructField","title":"StructField","text":"<p>A field in a StructType. Fields are nullable.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the field.</p> </li> <li> <code>data_type</code>               (<code>DataType</code>)           \u2013            <p>The data type of the field.</p> </li> </ul>"},{"location":"reference/fenic/core/types/datatypes/#fenic.core.types.datatypes.StructType","title":"StructType","text":"<p>               Bases: <code>DataType</code></p> <p>A type representing a struct (record) with named fields.</p> <p>Attributes:</p> <ul> <li> <code>fields</code>           \u2013            <p>List of field definitions.</p> </li> </ul> Create a struct with name and age fields <pre><code>StructType([\n    StructField(\"name\", StringType),\n    StructField(\"age\", IntegerType),\n])\n</code></pre>"},{"location":"reference/fenic/core/types/datatypes/#fenic.core.types.datatypes.TranscriptType","title":"TranscriptType","text":"<p>               Bases: <code>_StringBackedType</code></p> <p>Represents a string containing a transcript in a specific format.</p>"},{"location":"reference/fenic/core/types/datatypes/#fenic.core.types.datatypes.is_dtype_numeric","title":"is_dtype_numeric","text":"<pre><code>is_dtype_numeric(dtype: DataType) -&gt; bool\n</code></pre> <p>Check if a data type is a numeric type.</p> Source code in <code>src/fenic/core/types/datatypes.py</code> <pre><code>def is_dtype_numeric(dtype: DataType) -&gt; bool:\n    \"\"\"Check if a data type is a numeric type.\"\"\"\n    return dtype in (IntegerType, FloatType, DoubleType)\n</code></pre>"},{"location":"reference/fenic/core/types/enums/","title":"fenic.core.types.enums","text":""},{"location":"reference/fenic/core/types/enums/#fenic.core.types.enums","title":"fenic.core.types.enums","text":"<p>Enums used in the DataFrame API.</p> <p>Attributes:</p> <ul> <li> <code>BranchSide</code>           \u2013            <p>Type alias representing the side of a branch in a lineage graph.</p> </li> <li> <code>JoinType</code>           \u2013            <p>Type alias representing supported join types.</p> </li> <li> <code>SemanticSimilarityMetric</code>           \u2013            <p>Type alias representing supported semantic similarity metrics.</p> </li> <li> <code>TranscriptFormatType</code>           \u2013            <p>Type alias representing supported transcript formats.</p> </li> </ul>"},{"location":"reference/fenic/core/types/enums/#fenic.core.types.enums.BranchSide","title":"BranchSide  <code>module-attribute</code>","text":"<pre><code>BranchSide = Literal['left', 'right']\n</code></pre> <p>Type alias representing the side of a branch in a lineage graph.</p> <p>Valid values:</p> <ul> <li>\"left\": The left branch of a join.</li> <li>\"right\": The right branch of a join.</li> </ul>"},{"location":"reference/fenic/core/types/enums/#fenic.core.types.enums.JoinType","title":"JoinType  <code>module-attribute</code>","text":"<pre><code>JoinType = Literal['inner', 'full', 'left', 'right', 'cross']\n</code></pre> <p>Type alias representing supported join types.</p> <p>Valid values:</p> <ul> <li>\"inner\": Inner join, returns only rows that have matching values in both tables.</li> <li>\"outer\": Outer join, returns all rows from both tables, filling missing values with nulls.</li> <li>\"left\": Left join, returns all rows from the left table and matching rows from the right table.</li> <li>\"right\": Right join, returns all rows from the right table and matching rows from the left table.</li> <li>\"cross\": Cross join, returns the Cartesian product of the two tables.</li> </ul>"},{"location":"reference/fenic/core/types/enums/#fenic.core.types.enums.SemanticSimilarityMetric","title":"SemanticSimilarityMetric  <code>module-attribute</code>","text":"<pre><code>SemanticSimilarityMetric = Literal['cosine', 'l2', 'dot']\n</code></pre> <p>Type alias representing supported semantic similarity metrics.</p> <p>Valid values:</p> <ul> <li>\"cosine\": Cosine similarity, measures the cosine of the angle between two vectors.</li> <li>\"l2\": Euclidean (L2) distance, measures the straight-line distance between two vectors.</li> <li>\"dot\": Dot product similarity, the raw inner product of two vectors.</li> </ul> <p>These metrics are commonly used for comparing embedding vectors in semantic search and other similarity-based applications.</p>"},{"location":"reference/fenic/core/types/enums/#fenic.core.types.enums.TranscriptFormatType","title":"TranscriptFormatType  <code>module-attribute</code>","text":"<pre><code>TranscriptFormatType = Literal['srt', 'generic']\n</code></pre> <p>Type alias representing supported transcript formats.</p> <p>Valid values:</p> <ul> <li>\"srt\": SubRip Subtitle format with indexed entries and timestamp ranges</li> <li>\"generic\": Conversation transcript format with speaker names and timestamps</li> </ul> <p>Both formats are parsed into a unified schema with fields: index, speaker, start_time, end_time, duration, content, format.</p>"},{"location":"reference/fenic/core/types/extract_schema/","title":"fenic.core.types.extract_schema","text":""},{"location":"reference/fenic/core/types/extract_schema/#fenic.core.types.extract_schema","title":"fenic.core.types.extract_schema","text":"<p>Type definitions for semantic extraction schemas.</p> <p>Classes:</p> <ul> <li> <code>ExtractSchema</code>           \u2013            <p>Represents a structured extraction schema.</p> </li> <li> <code>ExtractSchemaField</code>           \u2013            <p>Represents a field within an structured extraction schema.</p> </li> <li> <code>ExtractSchemaList</code>           \u2013            <p>Represents a list data type for structured extraction schema definitions.</p> </li> </ul>"},{"location":"reference/fenic/core/types/extract_schema/#fenic.core.types.extract_schema.ExtractSchema","title":"ExtractSchema","text":"<p>Represents a structured extraction schema.</p> <p>An extract schema contains a collection of named fields with descriptions that define what information should be extracted into each field.</p> <p>Methods:</p> <ul> <li> <code>field_names</code>             \u2013              <p>Get a list of all field names in the schema.</p> </li> </ul>"},{"location":"reference/fenic/core/types/extract_schema/#fenic.core.types.extract_schema.ExtractSchema.field_names","title":"field_names","text":"<pre><code>field_names() -&gt; List[str]\n</code></pre> <p>Get a list of all field names in the schema.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>A list of strings containing the names of all fields in the schema.</p> </li> </ul> Source code in <code>src/fenic/core/types/extract_schema.py</code> <pre><code>def field_names(self) -&gt; List[str]:\n    \"\"\"Get a list of all field names in the schema.\n\n    Returns:\n        A list of strings containing the names of all fields in the schema.\n    \"\"\"\n    return [field.name for field in self.struct_fields]\n</code></pre>"},{"location":"reference/fenic/core/types/extract_schema/#fenic.core.types.extract_schema.ExtractSchemaField","title":"ExtractSchemaField","text":"<pre><code>ExtractSchemaField(name: str, data_type: Union[DataType, ExtractSchemaList, ExtractSchema], description: str)\n</code></pre> <p>Represents a field within an structured extraction schema.</p> <p>An extract schema field has a name, a data type, and a required description that explains what information should be extracted into this field.</p> <p>Initialize an ExtractSchemaField.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the field.</p> </li> <li> <code>data_type</code>               (<code>Union[DataType, ExtractSchemaList, ExtractSchema]</code>)           \u2013            <p>The data type of the field. Must be either a primitive DataType, ExtractSchemaList, or ExtractSchema.</p> </li> <li> <code>description</code>               (<code>str</code>)           \u2013            <p>A description of what information should be extracted into this field.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If data_type is a non-primitive DataType.</p> </li> </ul> Source code in <code>src/fenic/core/types/extract_schema.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    data_type: Union[DataType, ExtractSchemaList, ExtractSchema],\n    description: str,\n):\n    \"\"\"Initialize an ExtractSchemaField.\n\n    Args:\n        name: The name of the field.\n        data_type: The data type of the field. Must be either a primitive DataType,\n            ExtractSchemaList, or ExtractSchema.\n        description: A description of what information should be extracted into this field.\n\n    Raises:\n        ValueError: If data_type is a non-primitive DataType.\n    \"\"\"\n    self.name = name\n    if isinstance(data_type, DataType) and not isinstance(\n        data_type, _PrimitiveType\n    ):\n        raise ValueError(\n            f\"Invalid data type: {data_type}. Only primitive types are supported directly. \"\n            f\"For complex types, please use ExtractSchemaList or ExtractSchema instead.\"\n        )\n    self.data_type = data_type\n    self.description = description\n</code></pre>"},{"location":"reference/fenic/core/types/extract_schema/#fenic.core.types.extract_schema.ExtractSchemaList","title":"ExtractSchemaList","text":"<pre><code>ExtractSchemaList(element_type: Union[DataType, ExtractSchema])\n</code></pre> <p>Represents a list data type for structured extraction schema definitions.</p> <p>A schema list contains elements of a specific data type and is used for defining array-like structures in structured extraction schemas.</p> <p>Initialize an ExtractSchemaList.</p> <p>Parameters:</p> <ul> <li> <code>element_type</code>               (<code>Union[DataType, ExtractSchema]</code>)           \u2013            <p>The data type of elements in the list. Must be either a primitive DataType or another ExtractSchema.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If element_type is a non-primitive DataType.</p> </li> </ul> Source code in <code>src/fenic/core/types/extract_schema.py</code> <pre><code>def __init__(\n    self,\n    element_type: Union[DataType, ExtractSchema],\n):\n    \"\"\"Initialize an ExtractSchemaList.\n\n    Args:\n        element_type: The data type of elements in the list. Must be either a primitive\n            DataType or another ExtractSchema.\n\n    Raises:\n        ValueError: If element_type is a non-primitive DataType.\n    \"\"\"\n    if isinstance(element_type, DataType) and not isinstance(\n        element_type, _PrimitiveType\n    ):\n        raise ValueError(\n            f\"Invalid element type: {element_type}. Only primitive types are supported directly. \"\n            f\"For complex types, please use ExtractSchemaList or ExtractSchema instead.\"\n        )\n    self.element_type = element_type\n</code></pre>"},{"location":"reference/fenic/core/types/query_result/","title":"fenic.core.types.query_result","text":""},{"location":"reference/fenic/core/types/query_result/#fenic.core.types.query_result","title":"fenic.core.types.query_result","text":"<p>QueryResult class and related types.</p> <p>Classes:</p> <ul> <li> <code>QueryResult</code>           \u2013            <p>Container for query execution results and associated metadata.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>DataLike</code>           \u2013            <p>Union type representing any supported data format for both input and output operations.</p> </li> <li> <code>DataLikeType</code>           \u2013            <p>String literal type for specifying data output formats.</p> </li> </ul>"},{"location":"reference/fenic/core/types/query_result/#fenic.core.types.query_result.DataLike","title":"DataLike  <code>module-attribute</code>","text":"<pre><code>DataLike = Union[DataFrame, DataFrame, Dict[str, List[Any]], List[Dict[str, Any]], Table]\n</code></pre> <p>Union type representing any supported data format for both input and output operations.</p> <p>This type encompasses all possible data structures that can be: 1. Used as input when creating DataFrames 2. Returned as output from query results</p> Supported formats <ul> <li>pl.DataFrame: Native Polars DataFrame with efficient columnar storage</li> <li>pd.DataFrame: Pandas DataFrame, optionally with PyArrow extension arrays</li> <li>Dict[str, List[Any]]: Column-oriented dictionary where:<ul> <li>Keys are column names (str)</li> <li>Values are lists containing all values for that column</li> </ul> </li> <li>List[Dict[str, Any]]: Row-oriented list where:<ul> <li>Each element is a dictionary representing one row</li> <li>Dictionary keys are column names, values are cell values</li> </ul> </li> <li>pa.Table: Apache Arrow Table with columnar memory layout</li> </ul> Usage <ul> <li>Input: Used in create_dataframe() to accept data in various formats</li> <li>Output: Used in QueryResult.data to return results in requested format</li> </ul> <p>The specific type returned depends on the DataLikeType format specified when collecting query results.</p>"},{"location":"reference/fenic/core/types/query_result/#fenic.core.types.query_result.DataLikeType","title":"DataLikeType  <code>module-attribute</code>","text":"<pre><code>DataLikeType = Literal['polars', 'pandas', 'pydict', 'pylist', 'arrow']\n</code></pre> <p>String literal type for specifying data output formats.</p> Valid values <ul> <li>\"polars\": Native Polars DataFrame format</li> <li>\"pandas\": Pandas DataFrame with PyArrow extension arrays</li> <li>\"pydict\": Python dictionary with column names as keys, lists as values</li> <li>\"pylist\": Python list of dictionaries, each representing one row</li> <li>\"arrow\": Apache Arrow Table format</li> </ul> <p>Used as input parameter for methods that can return data in multiple formats.</p>"},{"location":"reference/fenic/core/types/query_result/#fenic.core.types.query_result.QueryResult","title":"QueryResult  <code>dataclass</code>","text":"<pre><code>QueryResult(data: DataLike, metrics: QueryMetrics)\n</code></pre> <p>Container for query execution results and associated metadata.</p> <p>This dataclass bundles together the materialized data from a query execution along with metrics about the execution process. It provides a unified interface for accessing both the computed results and performance information.</p> <p>Attributes:</p> <ul> <li> <code>data</code>               (<code>DataLike</code>)           \u2013            <p>The materialized query results in the requested format. Can be any of the supported data types (Polars/Pandas DataFrame, Arrow Table, or Python dict/list structures).</p> </li> <li> <code>metrics</code>               (<code>QueryMetrics</code>)           \u2013            <p>Execution metadata including timing information, memory usage, rows processed, and other performance metrics collected during query execution.</p> </li> </ul> Access query results and metrics <pre><code># Execute query and get results with metrics\nresult = df.filter(col(\"age\") &gt; 25).collect(\"pandas\")\npandas_df = result.data  # Access the Pandas DataFrame\nprint(result.metrics.execution_time)  # Access execution metrics\nprint(result.metrics.rows_processed)  # Access row count\n</code></pre> Work with different data formats <pre><code># Get results in different formats\npolars_result = df.collect(\"polars\")\narrow_result = df.collect(\"arrow\")\ndict_result = df.collect(\"pydict\")\n\n# All contain the same data, different formats\nprint(type(polars_result.data))  # &lt;class 'polars.DataFrame'&gt;\nprint(type(arrow_result.data))   # &lt;class 'pyarrow.lib.Table'&gt;\nprint(type(dict_result.data))    # &lt;class 'dict'&gt;\n</code></pre> Note <p>The actual type of the <code>data</code> attribute depends on the format requested during collection. Use type checking or isinstance() if you need to handle the data differently based on its format.</p>"},{"location":"reference/fenic/core/types/schema/","title":"fenic.core.types.schema","text":""},{"location":"reference/fenic/core/types/schema/#fenic.core.types.schema","title":"fenic.core.types.schema","text":"<p>Schema definitions for DataFrame structures.</p> <p>This module provides classes for defining and working with DataFrame schemas. It includes ColumnField for individual column definitions and Schema for complete DataFrame structure definitions.</p> <p>Classes:</p> <ul> <li> <code>ColumnField</code>           \u2013            <p>Represents a typed column in a DataFrame schema.</p> </li> <li> <code>Schema</code>           \u2013            <p>Represents the schema of a DataFrame.</p> </li> </ul>"},{"location":"reference/fenic/core/types/schema/#fenic.core.types.schema.ColumnField","title":"ColumnField","text":"<p>Represents a typed column in a DataFrame schema.</p> <p>A ColumnField defines the structure of a single column by specifying its name and data type. This is used as a building block for DataFrame schemas.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the column.</p> </li> <li> <code>data_type</code>               (<code>DataType</code>)           \u2013            <p>The data type of the column, as a DataType instance.</p> </li> </ul>"},{"location":"reference/fenic/core/types/schema/#fenic.core.types.schema.Schema","title":"Schema","text":"<p>Represents the schema of a DataFrame.</p> <p>A Schema defines the structure of a DataFrame by specifying an ordered collection of column fields. Each column field defines the name and data type of a column in the DataFrame.</p> <p>Attributes:</p> <ul> <li> <code>column_fields</code>               (<code>List[ColumnField]</code>)           \u2013            <p>An ordered list of ColumnField objects that define the structure of the DataFrame.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>column_names</code>             \u2013              <p>Get a list of all column names in the schema.</p> </li> </ul>"},{"location":"reference/fenic/core/types/schema/#fenic.core.types.schema.Schema.column_names","title":"column_names","text":"<pre><code>column_names() -&gt; List[str]\n</code></pre> <p>Get a list of all column names in the schema.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>A list of strings containing the names of all columns in the schema.</p> </li> </ul> Source code in <code>src/fenic/core/types/schema.py</code> <pre><code>def column_names(self) -&gt; List[str]:\n    \"\"\"Get a list of all column names in the schema.\n\n    Returns:\n        A list of strings containing the names of all columns in the schema.\n    \"\"\"\n    return [field.name for field in self.column_fields]\n</code></pre>"},{"location":"reference/fenic/core/types/semantic_examples/","title":"fenic.core.types.semantic_examples","text":""},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples","title":"fenic.core.types.semantic_examples","text":"<p>Module for handling semantic examples in query processing.</p> <p>This module provides classes and utilities for building, managing, and validating semantic examples used in query processing.</p> <p>Classes:</p> <ul> <li> <code>BaseExampleCollection</code>           \u2013            <p>Abstract base class for all semantic example collections.</p> </li> <li> <code>ClassifyExample</code>           \u2013            <p>A single semantic example for classification operations.</p> </li> <li> <code>ClassifyExampleCollection</code>           \u2013            <p>Collection of examples for semantic classification operations.</p> </li> <li> <code>JoinExample</code>           \u2013            <p>A single semantic example for semantic join operations.</p> </li> <li> <code>JoinExampleCollection</code>           \u2013            <p>Collection of examples for semantic join operations.</p> </li> <li> <code>MapExample</code>           \u2013            <p>A single semantic example for semantic mapping operations.</p> </li> <li> <code>MapExampleCollection</code>           \u2013            <p>Collection of examples for semantic mapping operations.</p> </li> <li> <code>PredicateExample</code>           \u2013            <p>A single semantic example for semantic predicate operations.</p> </li> <li> <code>PredicateExampleCollection</code>           \u2013            <p>Collection of examples for semantic predicate operations.</p> </li> </ul>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.BaseExampleCollection","title":"BaseExampleCollection","text":"<pre><code>BaseExampleCollection(examples: List[ExampleType] = None)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[ExampleType]</code></p> <p>Abstract base class for all semantic example collections.</p> <p>Semantic examples demonstrate the expected input-output relationship for a given task, helping guide language models to produce consistent and accurate responses. Each example consists of inputs and the corresponding expected output.</p> <p>These examples are particularly valuable for:</p> <ul> <li>Demonstrating the expected reasoning pattern</li> <li>Showing correct output formats</li> <li>Handling edge cases through demonstration</li> <li>Improving model performance without changing the underlying model</li> </ul> <p>Initialize a collection of semantic examples.</p> <p>Parameters:</p> <ul> <li> <code>examples</code>               (<code>List[ExampleType]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of examples to add to the collection. Each example will be processed through create_example() to ensure proper formatting and validation.</p> </li> </ul> Note <p>The examples list is initialized as empty if no examples are provided. Each example in the provided list will be processed through create_example() to ensure proper formatting and validation.</p> <p>Methods:</p> <ul> <li> <code>create_example</code>             \u2013              <p>Create an example in the collection.</p> </li> <li> <code>from_pandas</code>             \u2013              <p>Create a collection from a Pandas DataFrame.</p> </li> <li> <code>from_polars</code>             \u2013              <p>Create a collection from a Polars DataFrame.</p> </li> <li> <code>to_pandas</code>             \u2013              <p>Convert the collection to a Pandas DataFrame.</p> </li> <li> <code>to_polars</code>             \u2013              <p>Convert the collection to a Polars DataFrame.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def __init__(self, examples: List[ExampleType] = None):\n    \"\"\"Initialize a collection of semantic examples.\n\n    Args:\n        examples: Optional list of examples to add to the collection. Each example\n            will be processed through create_example() to ensure proper formatting\n            and validation.\n\n    Note:\n        The examples list is initialized as empty if no examples are provided.\n        Each example in the provided list will be processed through create_example()\n        to ensure proper formatting and validation.\n    \"\"\"\n    self.examples: List[ExampleType] = []\n    if examples:\n        for example in examples:\n            self.create_example(example)\n</code></pre>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.BaseExampleCollection.create_example","title":"create_example","text":"<pre><code>create_example(example: ExampleType) -&gt; BaseExampleCollection\n</code></pre> <p>Create an example in the collection.</p> <p>example: The semantic example to add. Must be an instance of the         collection's example_class.</p> <p>Returns:</p> <ul> <li> <code>BaseExampleCollection</code>           \u2013            <p>Self for method chaining.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def create_example(self, example: ExampleType) -&gt; BaseExampleCollection:\n    \"\"\"Create an example in the collection.\n\n    Args:\n    example: The semantic example to add. Must be an instance of the\n            collection's example_class.\n\n    Returns:\n        Self for method chaining.\n    \"\"\"\n    if not isinstance(example, self.example_class):\n        raise InvalidExampleCollectionError(\n            f\"Expected example of type {self.example_class.__name__}, got {type(example).__name__}\"\n        )\n    self.examples.append(example)\n    return self\n</code></pre>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.BaseExampleCollection.from_pandas","title":"from_pandas  <code>classmethod</code>","text":"<pre><code>from_pandas(df: DataFrame) -&gt; BaseExampleCollection\n</code></pre> <p>Create a collection from a Pandas DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>The Pandas DataFrame containing example data. The specific column structure requirements depend on the concrete collection type.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>BaseExampleCollection</code>           \u2013            <p>A new example collection populated with examples from the DataFrame.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidExampleCollectionError</code>             \u2013            <p>If the DataFrame's structure doesn't match the expected format for this collection type.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\ndef from_pandas(cls, df: pd.DataFrame) -&gt; BaseExampleCollection:\n    \"\"\"Create a collection from a Pandas DataFrame.\n\n    Args:\n        df: The Pandas DataFrame containing example data. The specific\n            column structure requirements depend on the concrete collection type.\n\n    Returns:\n        A new example collection populated with examples from the DataFrame.\n\n    Raises:\n        InvalidExampleCollectionError: If the DataFrame's structure doesn't match\n            the expected format for this collection type.\n    \"\"\"\n    polars_df = pl.from_pandas(data=df)\n    return cls.from_polars(polars_df)\n</code></pre>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.BaseExampleCollection.from_polars","title":"from_polars  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>from_polars(df: DataFrame) -&gt; BaseExampleCollection\n</code></pre> <p>Create a collection from a Polars DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>The Polars DataFrame containing example data. The specific column structure requirements depend on the concrete collection type.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>BaseExampleCollection</code>           \u2013            <p>A new example collection populated with examples from the DataFrame.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidExampleCollectionError</code>             \u2013            <p>If the DataFrame's structure doesn't match the expected format for this collection type.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_polars(cls, df: pl.DataFrame) -&gt; BaseExampleCollection:\n    \"\"\"Create a collection from a Polars DataFrame.\n\n    Args:\n        df: The Polars DataFrame containing example data. The specific\n            column structure requirements depend on the concrete collection type.\n\n    Returns:\n        A new example collection populated with examples from the DataFrame.\n\n    Raises:\n        InvalidExampleCollectionError: If the DataFrame's structure doesn't match\n            the expected format for this collection type.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.BaseExampleCollection.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; pd.DataFrame\n</code></pre> <p>Convert the collection to a Pandas DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>A Pandas DataFrame representing the collection's examples.</p> </li> <li> <code>DataFrame</code>           \u2013            <p>Returns an empty DataFrame if the collection contains no examples.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"Convert the collection to a Pandas DataFrame.\n\n    Returns:\n        A Pandas DataFrame representing the collection's examples.\n        Returns an empty DataFrame if the collection contains no examples.\n    \"\"\"\n    rows = self._as_df_input()\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.BaseExampleCollection.to_polars","title":"to_polars","text":"<pre><code>to_polars() -&gt; pl.DataFrame\n</code></pre> <p>Convert the collection to a Polars DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>A Polars DataFrame representing the collection's examples.</p> </li> <li> <code>DataFrame</code>           \u2013            <p>Returns an empty DataFrame if the collection contains no examples.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def to_polars(self) -&gt; pl.DataFrame:\n    \"\"\"Convert the collection to a Polars DataFrame.\n\n    Returns:\n        A Polars DataFrame representing the collection's examples.\n        Returns an empty DataFrame if the collection contains no examples.\n    \"\"\"\n    rows = self._as_df_input()\n    return pl.DataFrame(rows)\n</code></pre>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.ClassifyExample","title":"ClassifyExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single semantic example for classification operations.</p> <p>Classify examples demonstrate the classification of an input string into a specific category string, used in a semantic.classify operation.</p>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.ClassifyExampleCollection","title":"ClassifyExampleCollection","text":"<pre><code>ClassifyExampleCollection(examples: List[ExampleType] = None)\n</code></pre> <p>               Bases: <code>BaseExampleCollection[ClassifyExample]</code></p> <p>Collection of examples for semantic classification operations.</p> <p>Classification operations categorize input text into predefined classes. This collection manages examples that demonstrate the expected classification results for different inputs.</p> <p>Examples in this collection have a single input string and an output string representing the classification result.</p> <p>Methods:</p> <ul> <li> <code>from_polars</code>             \u2013              <p>Create collection from a Polars DataFrame. Must have an 'output' column and an 'input' column.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def __init__(self, examples: List[ExampleType] = None):\n    \"\"\"Initialize a collection of semantic examples.\n\n    Args:\n        examples: Optional list of examples to add to the collection. Each example\n            will be processed through create_example() to ensure proper formatting\n            and validation.\n\n    Note:\n        The examples list is initialized as empty if no examples are provided.\n        Each example in the provided list will be processed through create_example()\n        to ensure proper formatting and validation.\n    \"\"\"\n    self.examples: List[ExampleType] = []\n    if examples:\n        for example in examples:\n            self.create_example(example)\n</code></pre>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.ClassifyExampleCollection.from_polars","title":"from_polars  <code>classmethod</code>","text":"<pre><code>from_polars(df: DataFrame) -&gt; ClassifyExampleCollection\n</code></pre> <p>Create collection from a Polars DataFrame. Must have an 'output' column and an 'input' column.</p> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: pl.DataFrame) -&gt; ClassifyExampleCollection:\n    \"\"\"Create collection from a Polars DataFrame. Must have an 'output' column and an 'input' column.\"\"\"\n    collection = cls()\n\n    if EXAMPLE_INPUT_KEY not in df.columns:\n        raise InvalidExampleCollectionError(\n            f\"Classify Examples DataFrame missing required '{EXAMPLE_INPUT_KEY}' column\"\n        )\n    if EXAMPLE_OUTPUT_KEY not in df.columns:\n        raise InvalidExampleCollectionError(\n            f\"Classify Examples DataFrame missing required '{EXAMPLE_OUTPUT_KEY}' column\"\n        )\n\n    for row in df.iter_rows(named=True):\n        if row[EXAMPLE_INPUT_KEY] is None:\n            raise InvalidExampleCollectionError(\n                f\"Classify Examples DataFrame contains null values in '{EXAMPLE_INPUT_KEY}' column\"\n            )\n        if row[EXAMPLE_OUTPUT_KEY] is None:\n            raise InvalidExampleCollectionError(\n                f\"Classify Examples DataFrame contains null values in '{EXAMPLE_OUTPUT_KEY}' column\"\n            )\n\n        example = ClassifyExample(\n            input=row[EXAMPLE_INPUT_KEY],\n            output=row[EXAMPLE_OUTPUT_KEY],\n        )\n        collection.create_example(example)\n\n    return collection\n</code></pre>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.JoinExample","title":"JoinExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single semantic example for semantic join operations.</p> <p>Join examples demonstrate the evaluation of two input strings across different datasets against a specific condition, used in a semantic.join operation.</p>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.JoinExampleCollection","title":"JoinExampleCollection","text":"<pre><code>JoinExampleCollection(examples: List[ExampleType] = None)\n</code></pre> <p>               Bases: <code>BaseExampleCollection[JoinExample]</code></p> <p>Collection of examples for semantic join operations.</p> <p>Methods:</p> <ul> <li> <code>from_polars</code>             \u2013              <p>Create collection from a Polars DataFrame. Must have 'left', 'right', and 'output' columns.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def __init__(self, examples: List[ExampleType] = None):\n    \"\"\"Initialize a collection of semantic examples.\n\n    Args:\n        examples: Optional list of examples to add to the collection. Each example\n            will be processed through create_example() to ensure proper formatting\n            and validation.\n\n    Note:\n        The examples list is initialized as empty if no examples are provided.\n        Each example in the provided list will be processed through create_example()\n        to ensure proper formatting and validation.\n    \"\"\"\n    self.examples: List[ExampleType] = []\n    if examples:\n        for example in examples:\n            self.create_example(example)\n</code></pre>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.JoinExampleCollection.from_polars","title":"from_polars  <code>classmethod</code>","text":"<pre><code>from_polars(df: DataFrame) -&gt; JoinExampleCollection\n</code></pre> <p>Create collection from a Polars DataFrame. Must have 'left', 'right', and 'output' columns.</p> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: pl.DataFrame) -&gt; JoinExampleCollection:\n    \"\"\"Create collection from a Polars DataFrame. Must have 'left', 'right', and 'output' columns.\"\"\"\n    collection = cls()\n\n    required_columns = [\n        EXAMPLE_LEFT_KEY,\n        EXAMPLE_RIGHT_KEY,\n        EXAMPLE_OUTPUT_KEY,\n    ]\n    for col in required_columns:\n        if col not in df.columns:\n            raise InvalidExampleCollectionError(\n                f\"Join Examples DataFrame missing required '{col}' column\"\n            )\n\n    for row in df.iter_rows(named=True):\n        for col in required_columns:\n            if row[col] is None:\n                raise InvalidExampleCollectionError(\n                    f\"Join Examples DataFrame contains null values in '{col}' column\"\n                )\n\n        example = JoinExample(\n            left=row[EXAMPLE_LEFT_KEY],\n            right=row[EXAMPLE_RIGHT_KEY],\n            output=row[EXAMPLE_OUTPUT_KEY],\n        )\n        collection.create_example(example)\n\n    return collection\n</code></pre>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.MapExample","title":"MapExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single semantic example for semantic mapping operations.</p> <p>Map examples demonstrate the transformation of input variables to a specific output string used in a semantic.map operation.</p>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.MapExampleCollection","title":"MapExampleCollection","text":"<pre><code>MapExampleCollection(examples: List[ExampleType] = None)\n</code></pre> <p>               Bases: <code>BaseExampleCollection[MapExample]</code></p> <p>Collection of examples for semantic mapping operations.</p> <p>Map operations transform input variables into a text output according to specified instructions. This collection manages examples that demonstrate the expected transformations for different inputs.</p> <p>Examples in this collection can have multiple input variables, each mapped to their respective values, with a single output string representing the expected transformation result.</p> <p>Methods:</p> <ul> <li> <code>from_polars</code>             \u2013              <p>Create collection from a Polars DataFrame. Must have an 'output' column and at least one input column.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def __init__(self, examples: List[ExampleType] = None):\n    \"\"\"Initialize a collection of semantic examples.\n\n    Args:\n        examples: Optional list of examples to add to the collection. Each example\n            will be processed through create_example() to ensure proper formatting\n            and validation.\n\n    Note:\n        The examples list is initialized as empty if no examples are provided.\n        Each example in the provided list will be processed through create_example()\n        to ensure proper formatting and validation.\n    \"\"\"\n    self.examples: List[ExampleType] = []\n    if examples:\n        for example in examples:\n            self.create_example(example)\n</code></pre>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.MapExampleCollection.from_polars","title":"from_polars  <code>classmethod</code>","text":"<pre><code>from_polars(df: DataFrame) -&gt; MapExampleCollection\n</code></pre> <p>Create collection from a Polars DataFrame. Must have an 'output' column and at least one input column.</p> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: pl.DataFrame) -&gt; MapExampleCollection:\n    \"\"\"Create collection from a Polars DataFrame. Must have an 'output' column and at least one input column.\"\"\"\n    collection = cls()\n\n    if EXAMPLE_OUTPUT_KEY not in df.columns:\n        raise ValueError(\n            f\"Map Examples DataFrame missing required '{EXAMPLE_OUTPUT_KEY}' column\"\n        )\n\n    input_cols = [col for col in df.columns if col != EXAMPLE_OUTPUT_KEY]\n\n    if not input_cols:\n        raise ValueError(\n            \"Map Examples DataFrame must have at least one input column\"\n        )\n\n    for row in df.iter_rows(named=True):\n        if row[EXAMPLE_OUTPUT_KEY] is None:\n            raise InvalidExampleCollectionError(\n                f\"Map Examples DataFrame contains null values in '{EXAMPLE_OUTPUT_KEY}' column\"\n            )\n\n        input_dict = {\n            col: str(row[col]) for col in input_cols if row[col] is not None\n        }\n\n        example = MapExample(input=input_dict, output=row[EXAMPLE_OUTPUT_KEY])\n        collection.create_example(example)\n\n    return collection\n</code></pre>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.PredicateExample","title":"PredicateExample","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single semantic example for semantic predicate operations.</p> <p>Predicate examples demonstrate the evaluation of input variables against a specific condition, used in a semantic.predicate operation.</p>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.PredicateExampleCollection","title":"PredicateExampleCollection","text":"<pre><code>PredicateExampleCollection(examples: List[ExampleType] = None)\n</code></pre> <p>               Bases: <code>BaseExampleCollection[PredicateExample]</code></p> <p>Collection of examples for semantic predicate operations.</p> <p>Predicate operations evaluate conditions on input variables to produce boolean (True/False) results. This collection manages examples that demonstrate the expected boolean outcomes for different inputs.</p> <p>Examples in this collection can have multiple input variables, each mapped to their respective values, with a single boolean output representing the evaluation result of the predicate.</p> <p>Methods:</p> <ul> <li> <code>from_polars</code>             \u2013              <p>Create collection from a Polars DataFrame.</p> </li> </ul> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>def __init__(self, examples: List[ExampleType] = None):\n    \"\"\"Initialize a collection of semantic examples.\n\n    Args:\n        examples: Optional list of examples to add to the collection. Each example\n            will be processed through create_example() to ensure proper formatting\n            and validation.\n\n    Note:\n        The examples list is initialized as empty if no examples are provided.\n        Each example in the provided list will be processed through create_example()\n        to ensure proper formatting and validation.\n    \"\"\"\n    self.examples: List[ExampleType] = []\n    if examples:\n        for example in examples:\n            self.create_example(example)\n</code></pre>"},{"location":"reference/fenic/core/types/semantic_examples/#fenic.core.types.semantic_examples.PredicateExampleCollection.from_polars","title":"from_polars  <code>classmethod</code>","text":"<pre><code>from_polars(df: DataFrame) -&gt; PredicateExampleCollection\n</code></pre> <p>Create collection from a Polars DataFrame.</p> Source code in <code>src/fenic/core/types/semantic_examples.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: pl.DataFrame) -&gt; PredicateExampleCollection:\n    \"\"\"Create collection from a Polars DataFrame.\"\"\"\n    collection = cls()\n\n    # Validate output column exists\n    if EXAMPLE_OUTPUT_KEY not in df.columns:\n        raise InvalidExampleCollectionError(\n            f\"Predicate Examples DataFrame missing required '{EXAMPLE_OUTPUT_KEY}' column\"\n        )\n\n    input_cols = [col for col in df.columns if col != EXAMPLE_OUTPUT_KEY]\n\n    if not input_cols:\n        raise InvalidExampleCollectionError(\n            \"Predicate Examples DataFrame must have at least one input column\"\n        )\n\n    for row in df.iter_rows(named=True):\n        if row[EXAMPLE_OUTPUT_KEY] is None:\n            raise InvalidExampleCollectionError(\n                f\"Predicate Examples DataFrame contains null values in '{EXAMPLE_OUTPUT_KEY}' column\"\n            )\n\n        input_dict = {col: row[col] for col in input_cols if row[col] is not None}\n\n        example = PredicateExample(input=input_dict, output=row[EXAMPLE_OUTPUT_KEY])\n        collection.create_example(example)\n\n    return collection\n</code></pre>"},{"location":"reference/fenic/logging/","title":"fenic.logging","text":""},{"location":"reference/fenic/logging/#fenic.logging","title":"fenic.logging","text":"<p>Logging configuration utilities for Fenic.</p> <p>Functions:</p> <ul> <li> <code>configure_logging</code>             \u2013              <p>Configure logging for the library and root logger in interactive environments.</p> </li> </ul>"},{"location":"reference/fenic/logging/#fenic.logging.configure_logging","title":"configure_logging","text":"<pre><code>configure_logging(log_level: int = logging.INFO, log_format: str = '%(asctime)s [%(name)s] %(levelname)s: %(message)s', log_stream: Optional[TextIO] = None) -&gt; None\n</code></pre> <p>Configure logging for the library and root logger in interactive environments.</p> <p>This function ensures that logs from the library's modules appear in output by setting up a default handler on the root logger only if one does not already exist. This is especially useful in notebooks, scripts, or REPLs where logging is often unset. It configures the root logger and sets the library's top-level logger to propagate logs to the root.</p> <p>If the root logger has no handlers, this function sets up a default configuration and silences noisy dependencies like 'openai' and 'httpx'.</p> <p>In more complex applications or when integrating with existing logging configurations, you might prefer to manage logging setup externally. In such cases, you may not need to call this function.</p> Source code in <code>src/fenic/logging.py</code> <pre><code>def configure_logging(\n    log_level: int = logging.INFO,\n    log_format: str = \"%(asctime)s [%(name)s] %(levelname)s: %(message)s\",\n    log_stream: Optional[TextIO] = None,\n) -&gt; None:\n    \"\"\"Configure logging for the library and root logger in interactive environments.\n\n    This function ensures that logs from the library's modules appear in output by\n    setting up a default handler on the root logger *only if* one does not already\n    exist. This is especially useful in notebooks, scripts, or REPLs where logging\n    is often unset. It configures the root logger and sets the library's top-level\n    logger to propagate logs to the root.\n\n    If the root logger has no handlers, this function sets up a default configuration\n    and silences noisy dependencies like 'openai' and 'httpx'.\n\n    In more complex applications or when integrating with existing logging\n    configurations, you might prefer to manage logging setup externally. In such\n    cases, you may not need to call this function.\n    \"\"\"\n    stream = log_stream or sys.stderr\n    formatter = logging.Formatter(log_format)\n    handler = logging.StreamHandler(stream)\n    handler.setFormatter(formatter)\n\n    root_logger = logging.getLogger()\n    if not root_logger.hasHandlers():\n        # Set up root logger only if not already configured\n        root_logger.setLevel(log_level)\n        root_logger.addHandler(handler)\n\n        # Silence noisy dependencies\n        for noisy_logger_name in (\"openai\", \"httpx\"):\n            noisy_logger = logging.getLogger(noisy_logger_name)\n            noisy_logger.setLevel(logging.ERROR)\n\n    # Set the library logger level and enable propagation\n    library_root_name = __name__.split(\".\")[0]\n    library_logger = logging.getLogger(library_root_name)\n    library_logger.setLevel(log_level)\n    library_logger.propagate = True\n</code></pre>"}]}