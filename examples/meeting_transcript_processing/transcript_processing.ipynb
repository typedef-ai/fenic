{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meeting Transcript Processing with Fenic\n",
    "\n",
    "This notebook demonstrates how to work with transcripts in fenic using its native transcript processing capabilities, including format detection, parsing, and semantic extraction of structured information from unstructured meeting content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Define Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import fenic as fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Meeting Transcripts\n",
    "\n",
    "Let's define three sample meeting transcripts representing different types of engineering meetings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineering Architecture Review Transcript\n",
    "architecture_review_transcript = \"\"\"Sarah (00:02:15)\n",
    "  Alright, so we're here to discuss the user service redesign. Um, Mike can you walk us through the current bottlenecks?\n",
    "\n",
    "Mike (00:02:28)\n",
    "  Sure, so right now we're seeing about 2-second response times on the /users endpoint during peak hours. The main issue is we're hitting the PostgreSQL database directly for every request.\n",
    "\n",
    "David (00:02:45)\n",
    "  Right, and I think we discussed adding Redis caching last sprint but didn't get to it.\n",
    "\n",
    "Sarah (00:03:01)\n",
    "  OK so action item for Mike - investigate Redis implementation. What about the authentication service dependency?\n",
    "\n",
    "Mike (00:03:18)\n",
    "  Yeah that's another bottleneck. Every user request has to validate the JWT token with the auth service. We could cache those validations too.\n",
    "\n",
    "David (00:03:35)\n",
    "  Actually, we had that incident last week where auth service went down and took out the whole user flow. Incident #INC-2024-007.\n",
    "\n",
    "Sarah (00:03:48)\n",
    "  Good point. So we need resilience there. Mike, can you also look into circuit breaker patterns? I'm thinking we implement that by end of Q1.\n",
    "\n",
    "Mike (00:04:05)\n",
    "  Yep, I'll research both Redis caching and circuit breakers. Should have a design doc ready by next Friday.\n",
    "\n",
    "Sarah (00:04:15)\n",
    "  Perfect. David, anything else on the database side?\n",
    "\n",
    "David (00:04:22)\n",
    "  We should consider read replicas too. I've been seeing high CPU on the primary during reports generation.\n",
    "\n",
    "Sarah (00:04:35)\n",
    "  OK, let's add that to the backlog. Decision: we're moving forward with the caching and circuit breaker approach for user service optimization.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incident Post-Mortem Transcript\n",
    "incident_postmortem_transcript = \"\"\"Alex (00:01:05)\n",
    "  OK everyone, this is the post-mortem for yesterday's outage. Incident #INC-2024-012. We had approximately 45 minutes of downtime starting at 14:30 UTC.\n",
    "\n",
    "Jordan (00:01:20)\n",
    "  The root cause was the payment service running out of memory. We saw heap size spike to 8GB before the JVM crashed.\n",
    "\n",
    "Sam (00:01:35)\n",
    "  Yeah, I was monitoring the dashboards. CPU was normal but memory kept climbing. No garbage collection could keep up.\n",
    "\n",
    "Alex (00:01:48)\n",
    "  What triggered it? Any recent deployments?\n",
    "\n",
    "Jordan (00:01:55)\n",
    "  We deployed the new batch processing feature Tuesday morning. I think there's a memory leak in the transaction processing loop.\n",
    "\n",
    "Sam (00:02:10)\n",
    "  Action item for me - I'll review the batch processing code and look for leaked objects or unclosed resources.\n",
    "\n",
    "Alex (00:02:20)\n",
    "  Good. Jordan, can you increase the heap size as a temporary mitigation? Maybe 12GB instead of 8?\n",
    "\n",
    "Jordan (00:02:30)\n",
    "  Already done. I bumped it to 12GB and added memory alerts at 80% usage.\n",
    "\n",
    "Alex (00:02:40)\n",
    "  Perfect. Sam, when can you have the code review done?\n",
    "\n",
    "Sam (00:02:45)\n",
    "  I'll have findings by tomorrow EOD. If it's a simple fix, we can hotfix Friday.\n",
    "\n",
    "Alex (00:02:55)\n",
    "  Decision: temporary mitigation in place, code review by Thursday, hotfix target Friday if feasible.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprint Planning Transcript\n",
    "sprint_planning_transcript = \"\"\"Emma (00:00:30)\n",
    "  Alright team, let's plan Sprint 23. We have 40 story points available this sprint. What's our priorities?\n",
    "\n",
    "Ryan (00:00:45)\n",
    "  The user authentication refactor is our biggest priority. That's been blocking the mobile team for two weeks.\n",
    "\n",
    "Lisa (00:00:58)\n",
    "  Right, and we estimated that at 13 story points. We also need to address the API rate limiting issues.\n",
    "\n",
    "Emma (00:01:12)\n",
    "  Good point. Ryan, can you take the auth refactor? And Lisa, would you handle the rate limiting?\n",
    "\n",
    "Ryan (00:01:20)\n",
    "  Yeah, I can do the auth work. I'll need to coordinate with the mobile team on the JWT token format changes.\n",
    "\n",
    "Lisa (00:01:35)\n",
    "  Sure, I'll take rate limiting. I'm thinking we implement token bucket algorithm with Redis backend.\n",
    "\n",
    "Emma (00:01:50)\n",
    "  Perfect. What about the database migration for the user profiles table?\n",
    "\n",
    "Ryan (00:02:05)\n",
    "  That's risky. We're adding three new columns and need to backfill data for 2 million users.\n",
    "\n",
    "Lisa (00:02:18)\n",
    "  Action item - let's create a migration plan with zero-downtime strategy. I can draft that by Wednesday.\n",
    "\n",
    "Emma (00:02:35)\n",
    "  Great. Decision: Sprint 23 priorities are auth refactor, rate limiting, and migration planning. Ryan and Lisa are the leads.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Session and Create DataFrame\n",
    "\n",
    "Let's set up a fenic session with semantic capabilities and create a DataFrame with our meeting transcripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure session with semantic capabilities\n",
    "config = fc.SessionConfig(\n",
    "    app_name=\"meeting_transcript_processing\",\n",
    "    semantic=fc.SemanticConfig(\n",
    "        language_models={\n",
    "            \"mini\": fc.OpenAIModelConfig(model_name=\"gpt-4o-mini\", rpm=500, tpm=200_000)\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create session\n",
    "session = fc.Session.get_or_create(config)\n",
    "\n",
    "# Create DataFrame with meeting transcripts\n",
    "transcripts_data = [\n",
    "    {\n",
    "        \"meeting_id\": \"ARCH-2024-001\",\n",
    "        \"meeting_type\": \"Architecture Review\",\n",
    "        \"transcript\": architecture_review_transcript,\n",
    "    },\n",
    "    {\n",
    "        \"meeting_id\": \"INC-2024-012\",\n",
    "        \"meeting_type\": \"Incident Post-Mortem\",\n",
    "        \"transcript\": incident_postmortem_transcript,\n",
    "    },\n",
    "    {\n",
    "        \"meeting_id\": \"SPRINT-23\",\n",
    "        \"meeting_type\": \"Sprint Planning\",\n",
    "        \"transcript\": sprint_planning_transcript,\n",
    "    },\n",
    "]\n",
    "\n",
    "transcripts_df = session.create_dataframe(transcripts_data)\n",
    "\n",
    "print(\"Meeting transcripts loaded:\")\n",
    "transcripts_df.select(fc.col(\"meeting_id\"), fc.col(\"meeting_type\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Native Transcript Parsing\n",
    "\n",
    "Fenic provides built-in transcript processing capabilities. Let's parse the transcripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse transcripts into structured format\n",
    "# generic is a commonly found format for transcripts that follows the format:\n",
    "# speaker (00:00:00)\n",
    "# content\n",
    "# speaker (00:00:00)\n",
    "parsed_transcripts_df = transcripts_df.with_column(\n",
    "    \"structured_transcript\",\n",
    "    fc.text.parse_transcript(fc.col(\"transcript\"), \"generic\"),\n",
    ")\n",
    "\n",
    "print(\"Parsed transcript structure sample:\")\n",
    "sample_parsed = parsed_transcripts_df.select(\n",
    "    fc.col(\"meeting_id\"), fc.col(\"structured_transcript\")\n",
    ").limit(1)\n",
    "sample_parsed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Individual Speaking Segments\n",
    "\n",
    "Now let's explode the structured transcript data into individual speaking segments for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode structured transcript into individual segments\n",
    "segments_df = (\n",
    "    parsed_transcripts_df.explode(\"structured_transcript\")\n",
    "    .unnest(\"structured_transcript\")\n",
    "    .select(\n",
    "        fc.col(\"meeting_id\"),\n",
    "        fc.col(\"meeting_type\"),\n",
    "        fc.col(\"speaker\"),\n",
    "        fc.col(\"start_time\"),\n",
    "        fc.col(\"content\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Individual speaking segments:\")\n",
    "segments_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Semantic Extraction Schemas\n",
    "\n",
    "Let's define schemas to extract structured information from the unstructured transcript content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical entities schema\n",
    "technical_entities_schema = fc.ExtractSchema(\n",
    "    [\n",
    "        fc.ExtractSchemaField(\n",
    "            name=\"services\",\n",
    "            data_type=fc.ExtractSchemaList(element_type=fc.StringType),\n",
    "            description=\"Technical services or systems mentioned (e.g., user-service, auth-service, payment-service)\",\n",
    "        ),\n",
    "        fc.ExtractSchemaField(\n",
    "            name=\"technologies\",\n",
    "            data_type=fc.ExtractSchemaList(element_type=fc.StringType),\n",
    "            description=\"Technologies, databases, or tools mentioned (e.g., Redis, PostgreSQL, JWT, JVM)\",\n",
    "        ),\n",
    "        fc.ExtractSchemaField(\n",
    "            name=\"metrics\",\n",
    "            data_type=fc.ExtractSchemaList(element_type=fc.StringType),\n",
    "            description=\"Performance metrics, numbers, or measurements mentioned (e.g., response times, memory usage)\",\n",
    "        ),\n",
    "        fc.ExtractSchemaField(\n",
    "            name=\"incident_references\",\n",
    "            data_type=fc.ExtractSchemaList(element_type=fc.StringType),\n",
    "            description=\"Incident IDs, ticket numbers, or reference numbers mentioned\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Action items schema\n",
    "class ActionItemSchema(BaseModel):\n",
    "    \"\"\"Schema for extracting action items from meeting transcript segments.\"\"\"\n",
    "\n",
    "    has_action_item: str = Field(\n",
    "        description=\"Whether this segment contains an action item (yes/no)\"\n",
    "    )\n",
    "    assignee: str = Field(\n",
    "        default=None, description=\"Person assigned to the action item (if any)\"\n",
    "    )\n",
    "    task_description: str = Field(\n",
    "        description=\"Description of the task or action to be completed\"\n",
    "    )\n",
    "    deadline: str = Field(\n",
    "        default=None, description=\"When the task should be completed (if mentioned)\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Decisions schema\n",
    "class DecisionSchema(BaseModel):\n",
    "    \"\"\"Schema for extracting decisions from meeting transcript segments.\"\"\"\n",
    "\n",
    "    has_decision: str = Field(\n",
    "        description=\"Whether this segment contains a decision (yes/no)\"\n",
    "    )\n",
    "    decision_summary: str = Field(description=\"Summary of the decision made\")\n",
    "    decision_rationale: str = Field(\n",
    "        default=None, description=\"Why this decision was made (if mentioned)\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Created schemas for:\")\n",
    "print(\"- Technical entities (services, technologies, metrics, incidents)\")\n",
    "print(\"- Action items (assignee, task, deadline)\")\n",
    "print(\"- Decisions (summary, rationale)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply Semantic Extraction\n",
    "\n",
    "Now let's apply semantic extraction to each segment to extract technical entities, action items, and decisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract technical entities, action items, and decisions from each segment\n",
    "enriched_df = (\n",
    "    segments_df.with_column(\n",
    "        \"technical_entities\",\n",
    "        fc.semantic.extract(fc.col(\"content\"), technical_entities_schema),\n",
    "    )\n",
    "    .with_column(\n",
    "        \"action_items\", fc.semantic.extract(fc.col(\"content\"), ActionItemSchema)\n",
    "    )\n",
    "    .with_column(\"decisions\", fc.semantic.extract(fc.col(\"content\"), DecisionSchema))\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "print(\"Applied semantic extraction to all segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Extract and Structure Insights\n",
    "\n",
    "Let's unnest the extracted data to create a structured insights DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnest extracted data and create structured insights\n",
    "insights_df = (\n",
    "    enriched_df.unnest(\"technical_entities\")\n",
    "    .unnest(\"action_items\")\n",
    "    .unnest(\"decisions\")\n",
    "    .select(\n",
    "        fc.col(\"meeting_id\"),\n",
    "        fc.col(\"meeting_type\"),\n",
    "        fc.col(\"speaker\"),\n",
    "        fc.col(\"timestamp\"),\n",
    "        fc.col(\"content\"),\n",
    "        fc.col(\"services\"),\n",
    "        fc.col(\"technologies\"),\n",
    "        fc.col(\"metrics\"),\n",
    "        fc.col(\"incident_references\"),\n",
    "        fc.col(\"has_action_item\"),\n",
    "        fc.col(\"assignee\"),\n",
    "        fc.col(\"task_description\"),\n",
    "        fc.col(\"deadline\"),\n",
    "        fc.col(\"has_decision\"),\n",
    "        fc.col(\"decision_summary\"),\n",
    "        fc.col(\"decision_rationale\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Structured insights sample:\")\n",
    "insights_df.select(\"*\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Meeting-Level Analytics\n",
    "\n",
    "Now let's aggregate the insights to generate meeting-level analytics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract action items summary\n",
    "action_items_summary = insights_df.filter(fc.col(\"has_action_item\") == \"yes\").select(\n",
    "    fc.col(\"meeting_id\"),\n",
    "    fc.col(\"meeting_type\"),\n",
    "    fc.col(\"assignee\"),\n",
    "    fc.col(\"task_description\"),\n",
    "    fc.col(\"deadline\"),\n",
    ")\n",
    "print(\"Action Items Summary:\")\n",
    "action_items_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract decisions summary\n",
    "decisions_summary = insights_df.filter(fc.col(\"has_decision\") == \"yes\").select(\n",
    "    fc.col(\"meeting_id\"),\n",
    "    fc.col(\"meeting_type\"),\n",
    "    fc.col(\"decision_summary\"),\n",
    "    fc.col(\"decision_rationale\"),\n",
    ")\n",
    "\n",
    "print(\"Decisions Summary:\")\n",
    "decisions_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze most mentioned services\n",
    "all_services = (\n",
    "    insights_df.select(fc.col(\"meeting_id\"), fc.col(\"services\"))\n",
    "    .explode(\"services\")\n",
    "    .filter(fc.col(\"services\").is_not_null() & (fc.col(\"services\") != \"\"))\n",
    "    .group_by(\"services\")\n",
    "    .agg(fc.count(fc.col(\"meeting_id\")).alias(\"mention_count\"))\n",
    "    .sort(\"mention_count\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Most Mentioned Services:\")\n",
    "all_services.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze most mentioned technologies\n",
    "all_technologies = (\n",
    "    insights_df.select(fc.col(\"meeting_id\"), fc.col(\"technologies\"))\n",
    "    .explode(\"technologies\")\n",
    "    .filter(fc.col(\"technologies\").is_not_null() & (fc.col(\"technologies\") != \"\"))\n",
    "    .group_by(\"technologies\")\n",
    "    .agg(fc.count(fc.col(\"meeting_id\")).alias(\"mention_count\"))\n",
    "    .sort(\"mention_count\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Most Mentioned Technologies:\")\n",
    "all_technologies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate meeting productivity metrics\n",
    "meeting_stats = insights_df.group_by(\"meeting_id\", \"meeting_type\").agg(\n",
    "    fc.count(fc.col(\"speaker\")).alias(\"total_segments\"),\n",
    "    fc.sum(\n",
    "        fc.when(fc.col(\"has_action_item\") == \"yes\", fc.lit(1)).otherwise(fc.lit(0))\n",
    "    ).alias(\"action_items_count\"),\n",
    "    fc.sum(\n",
    "        fc.when(fc.col(\"has_decision\") == \"yes\", fc.lit(1)).otherwise(fc.lit(0))\n",
    "    ).alias(\"decisions_count\"),\n",
    "    fc.sum(\n",
    "        fc.when(fc.col(\"services\").is_not_null(), fc.lit(1)).otherwise(fc.lit(0))\n",
    "    ).alias(\"technical_mentions\"),\n",
    ")\n",
    "\n",
    "print(\"Meeting Productivity Metrics:\")\n",
    "meeting_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Actionable Outputs\n",
    "\n",
    "Finally, let's generate actionable insights from our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique assignees and their workload\n",
    "assignee_workload = (\n",
    "    insights_df.filter(fc.col(\"has_action_item\") == \"yes\")\n",
    "    .group_by(\"assignee\")\n",
    "    .agg(fc.count(\"*\").alias(\"assigned_tasks\"))\n",
    "    .order_by(fc.col(\"assigned_tasks\").desc())\n",
    ")\n",
    "\n",
    "print(\"Team Member Workload (Action Items):\")\n",
    "assignee_workload.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline of action items\n",
    "action_timeline = (\n",
    "    insights_df.filter(\n",
    "        (fc.col(\"has_action_item\") == \"yes\") & (fc.col(\"deadline\").is_not_null())\n",
    "    )\n",
    "    .select(\n",
    "        fc.col(\"meeting_id\"),\n",
    "        fc.col(\"assignee\"),\n",
    "        fc.col(\"task_description\"),\n",
    "        fc.col(\"deadline\"),\n",
    "    )\n",
    "    .order_by(\"deadline\")\n",
    ")\n",
    "\n",
    "print(\"Action Items Timeline:\")\n",
    "action_timeline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the power of fenic for processing meeting transcripts:\n",
    "\n",
    "✅ **Native transcript parsing** with fenic's built-in functions\n",
    "\n",
    "✅ **Semantic extraction** of technical entities, action items, and decisions\n",
    "\n",
    "✅ **Structured data processing** on unstructured meeting content\n",
    "\n",
    "✅ **Automated knowledge capture** for engineering teams\n",
    "\n",
    "✅ **Actionable insights** for project management and team coordination\n",
    "\n",
    "The pipeline shows how fenic enables you to transform unstructured meeting conversations into structured, queryable data that can drive better engineering decisions and team coordination."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
