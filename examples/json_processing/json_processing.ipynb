{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import fenic as fc\n",
    "\n",
    "config = fc.SessionConfig(\n",
    "    app_name=\"json_processing\",\n",
    "    semantic=fc.SemanticConfig(\n",
    "        language_models={\n",
    "            \"mini\": fc.OpenAIModelConfig(\n",
    "                model_name=\"gpt-4o-mini\",\n",
    "                rpm=500,\n",
    "                tpm=200_000\n",
    "            )\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create session\n",
    "session = fc.Session.get_or_create(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_path = Path(\"whisper-transcript.json\")\n",
    "\n",
    "with open(transcript_path, \"r\") as f:\n",
    "    json_content = f.read()\n",
    "\n",
    "# Create dataframe with the JSON string\n",
    "df = session.create_dataframe([{\"json_string\": json_content}])\n",
    "\n",
    "# Cast the JSON string to JSON type\n",
    "df_json = df.select(\n",
    "    fc.col(\"json_string\").cast(fc.JsonType).alias(\"json_data\")\n",
    ")\n",
    "\n",
    "df_json.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all words from all segments using JQ\n",
    "# This demonstrates nested array traversal and variable binding in JQ\n",
    "words_df = df_json.select(\n",
    "    fc.json.jq(\n",
    "        fc.col(\"json_data\"),\n",
    "        # JQ query explanation:\n",
    "        # - '.segments[] as $seg' iterates through segments, binding each to $seg\n",
    "        # - '$seg.words[]' iterates through words in each segment\n",
    "        # - Constructs object with both word-level and segment-level data\n",
    "        '.segments[] as $seg | $seg.words[] | {word: .word, speaker: .speaker, start: .start, end: .end, probability: .probability, segment_start: $seg.start, segment_end: $seg.end, segment_text: $seg.text}'\n",
    "    ).alias(\"word_data\")\n",
    ").explode(\"word_data\")  # Convert array of word objects into separate rows\n",
    "words_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scalar extracted fields:\n",
      "┌───────────┬────────────┬─────────────┬─────────────┬─────────────┬───────────────┬─────────────┐\n",
      "│ word_text ┆ speaker    ┆ start_time  ┆ end_time    ┆ probability ┆ segment_start ┆ segment_end │\n",
      "╞═══════════╪════════════╪═════════════╪═════════════╪═════════════╪═══════════════╪═════════════╡\n",
      "│  Let      ┆ SPEAKER_01 ┆ 2.94        ┆ 3.12        ┆ 0.693848    ┆ 2.94          ┆ 4.48        │\n",
      "│  me       ┆ SPEAKER_01 ┆ 3.12        ┆ 3.26        ┆ 0.999023    ┆ 2.94          ┆ 4.48        │\n",
      "│ …         ┆ …          ┆ …           ┆ …           ┆ …           ┆ …             ┆ …           │\n",
      "│  value.   ┆ SPEAKER_00 ┆ 1486.699951 ┆ 1486.959961 ┆ 1.0         ┆ 1482.060059   ┆ 1486.959961 │\n",
      "└───────────┴────────────┴─────────────┴─────────────┴─────────────┴───────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Extract scalar values using struct casting and unnest - more efficient than JQ + get_item(0)\n",
    "# Define schema for word-level data structure\n",
    "word_schema = fc.StructType([\n",
    "    fc.StructField(\"word\", fc.StringType),\n",
    "    fc.StructField(\"speaker\", fc.StringType),\n",
    "    fc.StructField(\"start\", fc.FloatType),\n",
    "    fc.StructField(\"end\", fc.FloatType),\n",
    "    fc.StructField(\"probability\", fc.FloatType),\n",
    "    fc.StructField(\"segment_start\", fc.FloatType),\n",
    "    fc.StructField(\"segment_end\", fc.FloatType)\n",
    "])\n",
    "\n",
    "# Cast to struct and unnest to automatically extract all fields\n",
    "words_clean_df = words_df.select(\n",
    "    fc.col(\"word_data\").cast(word_schema).alias(\"word_struct\")\n",
    ").unnest(\"word_struct\").select(\n",
    "    # Rename fields for clarity\n",
    "    fc.col(\"word\").alias(\"word_text\"),\n",
    "    fc.col(\"speaker\"),\n",
    "    fc.col(\"start\").alias(\"start_time\"),\n",
    "    fc.col(\"end\").alias(\"end_time\"),\n",
    "    fc.col(\"probability\"),\n",
    "    fc.col(\"segment_start\"),\n",
    "    fc.col(\"segment_end\")\n",
    ")\n",
    "\n",
    "print(\"\\nScalar extracted fields:\")\n",
    "words_clean_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Words DataFrame with calculated duration:\n",
      "┌────────────┬────────────┬────────────┬────────────┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ word_text  ┆ speaker    ┆ start_time ┆ end_time   ┆ probabili ┆ segment_s ┆ segment_e ┆ duration │\n",
      "│            ┆            ┆            ┆            ┆ ty        ┆ tart      ┆ nd        ┆          │\n",
      "╞════════════╪════════════╪════════════╪════════════╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│  Let       ┆ SPEAKER_01 ┆ 2.94       ┆ 3.12       ┆ 0.693848  ┆ 2.94      ┆ 4.48      ┆ 0.18     │\n",
      "│  me        ┆ SPEAKER_01 ┆ 3.12       ┆ 3.26       ┆ 0.999023  ┆ 2.94      ┆ 4.48      ┆ 0.14     │\n",
      "│  ask       ┆ SPEAKER_01 ┆ 3.26       ┆ 3.74       ┆ 0.998047  ┆ 2.94      ┆ 4.48      ┆ 0.48     │\n",
      "│  you       ┆ SPEAKER_01 ┆ 3.74       ┆ 3.86       ┆ 0.992676  ┆ 2.94      ┆ 4.48      ┆ 0.12     │\n",
      "│  about     ┆ SPEAKER_01 ┆ 3.86       ┆ 4.1        ┆ 0.999023  ┆ 2.94      ┆ 4.48      ┆ 0.24     │\n",
      "│ …          ┆ …          ┆ …          ┆ …          ┆ …         ┆ …         ┆ …         ┆ …        │\n",
      "│  ton       ┆ SPEAKER_00 ┆ 1485.61999 ┆ 1485.78002 ┆ 1.0       ┆ 1482.0600 ┆ 1486.9599 ┆ 0.160034 │\n",
      "│            ┆            ┆ 5          ┆ 9          ┆           ┆ 59        ┆ 61        ┆          │\n",
      "│  of        ┆ SPEAKER_00 ┆ 1485.78002 ┆ 1486.04003 ┆ 1.0       ┆ 1482.0600 ┆ 1486.9599 ┆ 0.26001  │\n",
      "│            ┆            ┆ 9          ┆ 9          ┆           ┆ 59        ┆ 61        ┆          │\n",
      "│ innovation ┆ SPEAKER_00 ┆ 1486.04003 ┆ 1486.5     ┆ 1.0       ┆ 1482.0600 ┆ 1486.9599 ┆ 0.459961 │\n",
      "│            ┆            ┆ 9          ┆            ┆           ┆ 59        ┆ 61        ┆          │\n",
      "│  and       ┆ SPEAKER_00 ┆ 1486.5     ┆ 1486.69995 ┆ 0.999512  ┆ 1482.0600 ┆ 1486.9599 ┆ 0.199951 │\n",
      "│            ┆            ┆            ┆ 1          ┆           ┆ 59        ┆ 61        ┆          │\n",
      "│  value.    ┆ SPEAKER_00 ┆ 1486.69995 ┆ 1486.95996 ┆ 1.0       ┆ 1482.0600 ┆ 1486.9599 ┆ 0.26001  │\n",
      "│            ┆            ┆ 1          ┆ 1          ┆           ┆ 59        ┆ 61        ┆          │\n",
      "└────────────┴────────────┴────────────┴────────────┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Add calculated fields - types are already correct from struct schema\n",
    "# This demonstrates arithmetic operations on struct-extracted data\n",
    "words_final_df = words_clean_df.select(\n",
    "    \"*\",\n",
    "    # Calculate duration: end_time - start_time (demonstrates arithmetic on struct data)\n",
    "    (fc.col(\"end_time\") - fc.col(\"start_time\")).alias(\"duration\")\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Words DataFrame with calculated duration:\")\n",
    "\n",
    "words_final_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Creating Segments DataFrame...\n",
      "Extracted 243 segments\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ segment_data                                                                                     │\n",
      "╞══════════════════════════════════════════════════════════════════════════════════════════════════╡\n",
      "│ {\"text\":\"Let me ask you about AI.\",\"start\":2.94,\"end\":4.48,\"words\":[{\"end\":3.12,\"probability\":0. │\n",
      "│ 69384765625,\"speaker\":\"SPEAKER_01\",\"start\":2.94,\"word\":\"                                         │\n",
      "│ Let\"},{\"end\":3.26,\"probability\":0.9990234375,\"speaker\":\"SPEAKER_01\",\"start\":3.12,\"word\":\"        │\n",
      "│ me\"},{\"end\":3.74,\"probability\":0.998046875,\"speaker\":\"SPEAKER_01\",\"start\":3.26,\"word\":\"          │\n",
      "│ ask\"},{\"end\":3.86,\"probability\":0.99267578125,\"speaker\":\"SPEAKER_01\",\"start\":3.74,\"word\":\"       │\n",
      "│ you\"},{\"end\":4.1,\"probability\":0.9990234375,\"speaker\":\"SPEAKER_01\",\"start\":3.86,\"word\":\"         │\n",
      "│ about\"},{\"end\":4.48,\"probability\":0.966796875,\"speaker\":\"SPEAKER_01\",\"start\":4.1,\"word\":\"        │\n",
      "│ AI.\"}]}                                                                                          │\n",
      "│ {\"text\":\"It seems like this year for the entirety of the human civilization is an interesting    │\n",
      "│ year for the development of artificial intelligence.\",\"start\":4.72,\"end\":11.66,\"words\":[{\"end\":4 │\n",
      "│ .74,\"probability\":0.99462890625,\"speaker\":\"SPEAKER_01\",\"start\":4.72,\"word\":\"                     │\n",
      "│ It\"},{\"end\":4.94,\"probability\":0.998046875,\"speaker\":\"SPEAKER_01\",\"start\":4.74,\"word\":\"          │\n",
      "│ seems\"},{\"end\":5.12,\"probability\":0.99462890625,\"speaker\":\"SPEAKER_01\",\"start\":4.94,\"word\":\"     │\n",
      "│ like\"},{\"end\":5.32,\"probability\":0.99853515625,\"speaker\":\"SPEAKER_01\",\"start\":5.12,\"word\":\"      │\n",
      "│ this\"},{\"end\":5.72,\"probability\":0.99951171875,\"speaker\":\"SPEAKER_01\",\"start\":5.32,\"word\":\"      │\n",
      "│ year\"},{\"end\":6.5,\"probability\":0.68603515625,\"speaker\":\"SPEAKER_01\",\"start\":5.72,\"word\":\"       │\n",
      "│ for\"},{\"end\":6.62,\"probability\":0.9990234375,\"speaker\":\"SPEAKER_01\",\"start\":6.5,\"word\":\"         │\n",
      "│ the\"},{\"end\":6.98,\"probability\":0.98828125,\"speaker\":\"SPEAKER_01\",\"start\":6.62,\"word\":\"          │\n",
      "│ entirety\"},{\"end\":7.16,\"probability\":0.99951171875,\"speaker\":\"SPEAKER_01\",\"start\":6.98,\"word\":\"  │\n",
      "│ of\"}…                                                                                            │\n",
      "│ …                                                                                                │\n",
      "│ {\"text\":\"It's going to create better experiences for people and just unlock a ton of innovation  │\n",
      "│ and value.\",\"start\":1482.06,\"end\":1486.96,\"words\":[{\"end\":1482.12,\"probability\":0.99951171875,\"s │\n",
      "│ peaker\":\"SPEAKER_00\",\"start\":1482.06,\"word\":\"                                                    │\n",
      "│ It's\"},{\"end\":1482.16,\"probability\":0.9990234375,\"speaker\":\"SPEAKER_00\",\"start\":1482.12,\"word\":\" │\n",
      "│ going\"},{\"end\":1482.22,\"probability\":0.99951171875,\"speaker\":\"SPEAKER_00\",\"start\":1482.16,\"word\" │\n",
      "│ :\" to\"},{\"end\":1482.38,\"probability\":1,\"speaker\":\"SPEAKER_00\",\"start\":1482.22,\"word\":\" create\"}, │\n",
      "│ {\"end\":1482.7,\"probability\":0.99951171875,\"speaker\":\"SPEAKER_00\",\"start\":1482.38,\"word\":\" better │\n",
      "│ \"},{\"end\":1484.12,\"probability\":0.99560546875,\"speaker\":\"SPEAKER_00\",\"start\":1482.92,\"word\":\"    │\n",
      "│ experiences\"},{\"end\":1484.42,\"probability\":1,\"speaker\":\"SPEAKER_00\",\"start\":1484.12,\"word\":\"     │\n",
      "│ for\"},{\"end\":1484.7,\"probability\":1,\"speaker\":\"SPEAKER_00\",\"start\":1484.42,\"word\":\" people\"},{\"e │\n",
      "│ nd\":1484.92,\"probability\":0.99853515625,\"speaker\":\"SPEAKER_00\",\"start\":1484.7,\"word\":\" and\"},{…  │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# 2. Create Segments DataFrame (Content-focused)\n",
    "print(\"\\n📝 Creating Segments DataFrame...\")\n",
    "\n",
    "# Extract segment-level data using JQ\n",
    "# This demonstrates extracting data at a different granularity level\n",
    "segments_df = df_json.select(\n",
    "    fc.json.jq(\n",
    "        fc.col(\"json_data\"),\n",
    "        # Extract segment objects with their text, timing, and nested words array\n",
    "        '.segments[] | {text: .text, start: .start, end: .end, words: .words}'\n",
    "    ).alias(\"segment_data\")\n",
    ").explode(\"segment_data\")  # Convert segments array into separate rows\n",
    "\n",
    "print(f\"Extracted {segments_df.count()} segments\")\n",
    "segments_df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Segments DataFrame with calculated metrics:\n",
      "┌─────────────────────────┬─────────────┬─────────────┬──────────┬────────────┬────────────────────┐\n",
      "│ segment_text            ┆ start_time  ┆ end_time    ┆ duration ┆ word_count ┆ average_confidence │\n",
      "╞═════════════════════════╪═════════════╪═════════════╪══════════╪════════════╪════════════════════╡\n",
      "│ Let me ask you about    ┆ 2.94        ┆ 4.48        ┆ 1.54     ┆ 6          ┆ 0.941569           │\n",
      "│ AI.                     ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ It seems like this year ┆ 4.72        ┆ 11.66       ┆ 6.94     ┆ 22         ┆ 0.981401           │\n",
      "│ for the entirety of the ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ human civilization is   ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ an interesting year for ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ the development of      ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ artificial              ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ intelligence.           ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ A lot of interesting    ┆ 12.2        ┆ 14.24       ┆ 2.04     ┆ 7          ┆ 0.998186           │\n",
      "│ stuff is happening.     ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ …                       ┆ …           ┆ …           ┆ …        ┆ …          ┆ …                  │\n",
      "│ now we're going to be   ┆ 1460.180054 ┆ 1481.920044 ┆ 21.73999 ┆ 62         ┆ 0.949474           │\n",
      "│ able to get help coding ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ the things that they    ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ need to go build things ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ or designing the things ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ that they need, will be ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ able to use these       ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ models to be able to do ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ customer support for    ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ the people that they're ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ serving over WhatsApp   ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ without having to, I    ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ think that this is all  ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ going to be super       ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ exciting.               ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ It's going to create    ┆ 1482.060059 ┆ 1486.959961 ┆ 4.899902 ┆ 17         ┆ 0.998851           │\n",
      "│ better experiences for  ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ people and just unlock  ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ a ton of innovation and ┆             ┆             ┆          ┆            ┆                    │\n",
      "│ value.                  ┆             ┆             ┆          ┆            ┆                    │\n",
      "└─────────────────────────┴─────────────┴─────────────┴──────────┴────────────┴────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Extract segment fields using hybrid approach: struct casting + JQ for complex aggregations\n",
    "# Define schema for basic segment fields (text, start, end)\n",
    "segment_basic_schema = fc.StructType([\n",
    "    fc.StructField(\"text\", fc.StringType),\n",
    "    fc.StructField(\"start\", fc.FloatType),\n",
    "    fc.StructField(\"end\", fc.FloatType)\n",
    "])\n",
    "\n",
    "# First extract basic fields using struct casting, then add complex JQ aggregations\n",
    "segments_clean_df = segments_df.select(\n",
    "    # Extract basic segment data using struct casting (more efficient)\n",
    "    fc.col(\"segment_data\").cast(segment_basic_schema).alias(\"segment_struct\"),\n",
    "    # Complex array aggregations still use JQ (best tool for this)\n",
    "    fc.json.jq(fc.col(\"segment_data\"), '.words | length').get_item(0).cast(fc.IntegerType).alias(\"word_count\"),\n",
    "    fc.json.jq(fc.col(\"segment_data\"), '[.words[].probability] | add / length').get_item(0).cast(fc.FloatType).alias(\"average_confidence\")\n",
    ").unnest(\"segment_struct\").select(\n",
    "    # Rename for clarity\n",
    "    fc.col(\"text\").alias(\"segment_text\"),\n",
    "    fc.col(\"start\").alias(\"start_time\"),\n",
    "    fc.col(\"end\").alias(\"end_time\"),\n",
    "    fc.col(\"word_count\"),\n",
    "    fc.col(\"average_confidence\")\n",
    ").select(\n",
    "    \"segment_text\",\n",
    "    \"start_time\",\n",
    "    \"end_time\",\n",
    "    # Calculate segment duration using DataFrame arithmetic\n",
    "    (fc.col(\"end_time\") - fc.col(\"start_time\")).alias(\"duration\"),\n",
    "    \"word_count\",\n",
    "    \"average_confidence\"\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Segments DataFrame with calculated metrics:\")\n",
    "segments_clean_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎤 Creating Speaker Summary DataFrame...\n",
      "\n",
      "📊 Speaker Summary DataFrame:\n",
      "┌────────────┬─────────────┬──────────────┬──────────────┬──────────────┬─────────────┬────────────┐\n",
      "│ speaker    ┆ total_words ┆ total_speaki ┆ average_conf ┆ first_speaki ┆ last_speaki ┆ word_rate  │\n",
      "│            ┆             ┆ ng_time      ┆ idence       ┆ ng_time      ┆ ng_time     ┆            │\n",
      "╞════════════╪═════════════╪══════════════╪══════════════╪══════════════╪═════════════╪════════════╡\n",
      "│ SPEAKER_00 ┆ 3124        ┆ 912.02002    ┆ 0.969939     ┆ 39.66        ┆ 1486.959961 ┆ 205.521792 │\n",
      "│ SPEAKER_01 ┆ 1011        ┆ 329.14093    ┆ 0.963907     ┆ 2.94         ┆ 1409.420044 ┆ 184.297943 │\n",
      "└────────────┴─────────────┴──────────────┴──────────────┴──────────────┴─────────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "# 3. Create Speaker Summary DataFrame (Aggregated)\n",
    "print(\"\\n🎤 Creating Speaker Summary DataFrame...\")\n",
    "\n",
    "# Use traditional DataFrame aggregations on JSON-extracted data\n",
    "# This demonstrates hybrid processing: JSON extraction + DataFrame analytics\n",
    "speaker_summary_df = words_final_df.group_by(\"speaker\").agg(\n",
    "    fc.count(\"*\").alias(\"total_words\"),                    # Count words per speaker\n",
    "    fc.avg(\"probability\").alias(\"average_confidence\"),     # Average speech confidence\n",
    "    fc.min(\"start_time\").alias(\"first_speaking_time\"),     # When speaker first appears\n",
    "    fc.max(\"end_time\").alias(\"last_speaking_time\"),        # When speaker last appears\n",
    "    fc.sum(\"duration\").alias(\"total_speaking_time\")        # Total time speaking\n",
    ").select(\n",
    "    \"speaker\",\n",
    "    \"total_words\", \n",
    "    \"total_speaking_time\",\n",
    "    \"average_confidence\",\n",
    "    \"first_speaking_time\",\n",
    "    \"last_speaking_time\",\n",
    "    # Calculate derived metric: words per minute\n",
    "    (fc.col(\"total_words\") / (fc.col(\"total_speaking_time\") / 60.0)).alias(\"word_rate\")\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Speaker Summary DataFrame:\")\n",
    "speaker_summary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 JSON Processing Pipeline Summary:\n",
      "============================================================\n",
      "📁 Input: Single JSON file (whisper-transcript.json)\n",
      "\n",
      "📊 Output: 3 structured DataFrames\n",
      "\n",
      "1. 🔤 Words DataFrame:\n",
      "   - 4135 individual words extracted\n",
      "   - Fields: word_text, speaker, timing, probability, duration\n",
      "   - Demonstrates: JQ nested array extraction, type casting\n",
      "\n",
      "2. 📝 Segments DataFrame:\n",
      "   - 243 conversation segments\n",
      "   - Fields: text, timing, word_count, average_confidence\n",
      "   - Demonstrates: JQ aggregations, array operations\n",
      "\n",
      "3. 🎤 Speaker Summary DataFrame:\n",
      "   - 2 speakers analyzed\n",
      "   - Fields: totals, averages, speaking patterns, word rates\n",
      "   - Demonstrates: DataFrame aggregations on JSON-extracted data\n",
      "\n",
      "🔧 Key Fenic JSON Features Used:\n",
      "   ✓ JSON type casting from strings\n",
      "   ✓ JQ queries for complex nested extraction\n",
      "   ✓ Array operations and aggregations\n",
      "   ✓ Type conversion and calculated fields\n",
      "   ✓ Traditional DataFrame operations on JSON data\n"
     ]
    }
   ],
   "source": [
    "# Summary of what we accomplished\n",
    "print(\"\\n🎯 JSON Processing Pipeline Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"📁 Input: Single JSON file (whisper-transcript.json)\\n\")\n",
    "print(\"📊 Output: 3 structured DataFrames\")\n",
    "print()\n",
    "print(\"1. 🔤 Words DataFrame:\")\n",
    "print(f\"   - {words_final_df.count()} individual words extracted\")\n",
    "print(\"   - Fields: word_text, speaker, timing, probability, duration\")\n",
    "print(\"   - Demonstrates: JQ nested array extraction, type casting\")\n",
    "print()\n",
    "print(\"2. 📝 Segments DataFrame:\")\n",
    "print(f\"   - {segments_clean_df.count()} conversation segments\")\n",
    "print(\"   - Fields: text, timing, word_count, average_confidence\")\n",
    "print(\"   - Demonstrates: JQ aggregations, array operations\")\n",
    "print()\n",
    "print(\"3. 🎤 Speaker Summary DataFrame:\")\n",
    "print(f\"   - {speaker_summary_df.count()} speakers analyzed\")\n",
    "print(\"   - Fields: totals, averages, speaking patterns, word rates\")\n",
    "print(\"   - Demonstrates: DataFrame aggregations on JSON-extracted data\")\n",
    "print()\n",
    "print(\"🔧 Key Fenic JSON Features Used:\")\n",
    "print(\"   ✓ JSON type casting from strings\")\n",
    "print(\"   ✓ JQ queries for complex nested extraction\")\n",
    "print(\"   ✓ Array operations and aggregations\")\n",
    "print(\"   ✓ Type conversion and calculated fields\")\n",
    "print(\"   ✓ Traditional DataFrame operations on JSON data\")\n",
    "\n",
    "# Clean up\n",
    "session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
