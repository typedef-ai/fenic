{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import fenic as fc\n",
    "\n",
    "config = fc.SessionConfig(\n",
    "    app_name=\"json_processing\",\n",
    "    semantic=fc.SemanticConfig(\n",
    "        language_models={\n",
    "            \"mini\": fc.OpenAIModelConfig(\n",
    "                model_name=\"gpt-4o-mini\",\n",
    "                rpm=500,\n",
    "                tpm=200_000\n",
    "            )\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create session\n",
    "session = fc.Session.get_or_create(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_path = Path(\"whisper-transcript.json\")\n",
    "\n",
    "with open(transcript_path, \"r\") as f:\n",
    "    json_content = f.read()\n",
    "\n",
    "# Create dataframe with the JSON string\n",
    "df = session.create_dataframe([{\"json_string\": json_content}])\n",
    "\n",
    "# Cast the JSON string to JSON type\n",
    "df_json = df.select(\n",
    "    fc.col(\"json_string\").cast(fc.JsonType).alias(\"json_data\")\n",
    ")\n",
    "\n",
    "df_json.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all words from all segments using JQ\n",
    "# This demonstrates nested array traversal and variable binding in JQ\n",
    "words_df = df_json.select(\n",
    "    fc.json.jq(\n",
    "        fc.col(\"json_data\"),\n",
    "        # JQ query explanation:\n",
    "        # - '.segments[] as $seg' iterates through segments, binding each to $seg\n",
    "        # - '$seg.words[]' iterates through words in each segment\n",
    "        # - Constructs object with both word-level and segment-level data\n",
    "        '.segments[] as $seg | $seg.words[] | {word: .word, speaker: .speaker, start: .start, end: .end, probability: .probability, segment_start: $seg.start, segment_end: $seg.end, segment_text: $seg.text}'\n",
    "    ).alias(\"word_data\")\n",
    ").explode(\"word_data\")  # Convert array of word objects into separate rows\n",
    "words_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scalar extracted fields:\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ word_text â”† speaker    â”† start_time  â”† end_time    â”† probability â”† segment_start â”† segment_end â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚  Let      â”† SPEAKER_01 â”† 2.94        â”† 3.12        â”† 0.693848    â”† 2.94          â”† 4.48        â”‚\n",
      "â”‚  me       â”† SPEAKER_01 â”† 3.12        â”† 3.26        â”† 0.999023    â”† 2.94          â”† 4.48        â”‚\n",
      "â”‚ â€¦         â”† â€¦          â”† â€¦           â”† â€¦           â”† â€¦           â”† â€¦             â”† â€¦           â”‚\n",
      "â”‚  value.   â”† SPEAKER_00 â”† 1486.699951 â”† 1486.959961 â”† 1.0         â”† 1482.060059   â”† 1486.959961 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# Extract scalar values using struct casting and unnest - more efficient than JQ + get_item(0)\n",
    "# Define schema for word-level data structure\n",
    "word_schema = fc.StructType([\n",
    "    fc.StructField(\"word\", fc.StringType),\n",
    "    fc.StructField(\"speaker\", fc.StringType),\n",
    "    fc.StructField(\"start\", fc.FloatType),\n",
    "    fc.StructField(\"end\", fc.FloatType),\n",
    "    fc.StructField(\"probability\", fc.FloatType),\n",
    "    fc.StructField(\"segment_start\", fc.FloatType),\n",
    "    fc.StructField(\"segment_end\", fc.FloatType)\n",
    "])\n",
    "\n",
    "# Cast to struct and unnest to automatically extract all fields\n",
    "words_clean_df = words_df.select(\n",
    "    fc.col(\"word_data\").cast(word_schema).alias(\"word_struct\")\n",
    ").unnest(\"word_struct\").select(\n",
    "    # Rename fields for clarity\n",
    "    fc.col(\"word\").alias(\"word_text\"),\n",
    "    fc.col(\"speaker\"),\n",
    "    fc.col(\"start\").alias(\"start_time\"),\n",
    "    fc.col(\"end\").alias(\"end_time\"),\n",
    "    fc.col(\"probability\"),\n",
    "    fc.col(\"segment_start\"),\n",
    "    fc.col(\"segment_end\")\n",
    ")\n",
    "\n",
    "print(\"\\nScalar extracted fields:\")\n",
    "words_clean_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Words DataFrame with calculated duration:\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ word_text  â”† speaker    â”† start_time â”† end_time   â”† probabili â”† segment_s â”† segment_e â”† duration â”‚\n",
      "â”‚            â”†            â”†            â”†            â”† ty        â”† tart      â”† nd        â”†          â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚  Let       â”† SPEAKER_01 â”† 2.94       â”† 3.12       â”† 0.693848  â”† 2.94      â”† 4.48      â”† 0.18     â”‚\n",
      "â”‚  me        â”† SPEAKER_01 â”† 3.12       â”† 3.26       â”† 0.999023  â”† 2.94      â”† 4.48      â”† 0.14     â”‚\n",
      "â”‚  ask       â”† SPEAKER_01 â”† 3.26       â”† 3.74       â”† 0.998047  â”† 2.94      â”† 4.48      â”† 0.48     â”‚\n",
      "â”‚  you       â”† SPEAKER_01 â”† 3.74       â”† 3.86       â”† 0.992676  â”† 2.94      â”† 4.48      â”† 0.12     â”‚\n",
      "â”‚  about     â”† SPEAKER_01 â”† 3.86       â”† 4.1        â”† 0.999023  â”† 2.94      â”† 4.48      â”† 0.24     â”‚\n",
      "â”‚ â€¦          â”† â€¦          â”† â€¦          â”† â€¦          â”† â€¦         â”† â€¦         â”† â€¦         â”† â€¦        â”‚\n",
      "â”‚  ton       â”† SPEAKER_00 â”† 1485.61999 â”† 1485.78002 â”† 1.0       â”† 1482.0600 â”† 1486.9599 â”† 0.160034 â”‚\n",
      "â”‚            â”†            â”† 5          â”† 9          â”†           â”† 59        â”† 61        â”†          â”‚\n",
      "â”‚  of        â”† SPEAKER_00 â”† 1485.78002 â”† 1486.04003 â”† 1.0       â”† 1482.0600 â”† 1486.9599 â”† 0.26001  â”‚\n",
      "â”‚            â”†            â”† 9          â”† 9          â”†           â”† 59        â”† 61        â”†          â”‚\n",
      "â”‚ innovation â”† SPEAKER_00 â”† 1486.04003 â”† 1486.5     â”† 1.0       â”† 1482.0600 â”† 1486.9599 â”† 0.459961 â”‚\n",
      "â”‚            â”†            â”† 9          â”†            â”†           â”† 59        â”† 61        â”†          â”‚\n",
      "â”‚  and       â”† SPEAKER_00 â”† 1486.5     â”† 1486.69995 â”† 0.999512  â”† 1482.0600 â”† 1486.9599 â”† 0.199951 â”‚\n",
      "â”‚            â”†            â”†            â”† 1          â”†           â”† 59        â”† 61        â”†          â”‚\n",
      "â”‚  value.    â”† SPEAKER_00 â”† 1486.69995 â”† 1486.95996 â”† 1.0       â”† 1482.0600 â”† 1486.9599 â”† 0.26001  â”‚\n",
      "â”‚            â”†            â”† 1          â”† 1          â”†           â”† 59        â”† 61        â”†          â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# Add calculated fields - types are already correct from struct schema\n",
    "# This demonstrates arithmetic operations on struct-extracted data\n",
    "words_final_df = words_clean_df.select(\n",
    "    \"*\",\n",
    "    # Calculate duration: end_time - start_time (demonstrates arithmetic on struct data)\n",
    "    (fc.col(\"end_time\") - fc.col(\"start_time\")).alias(\"duration\")\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“Š Words DataFrame with calculated duration:\")\n",
    "\n",
    "words_final_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Creating Segments DataFrame...\n",
      "Extracted 243 segments\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ segment_data                                                                                     â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ {\"text\":\"Let me ask you about AI.\",\"start\":2.94,\"end\":4.48,\"words\":[{\"end\":3.12,\"probability\":0. â”‚\n",
      "â”‚ 69384765625,\"speaker\":\"SPEAKER_01\",\"start\":2.94,\"word\":\"                                         â”‚\n",
      "â”‚ Let\"},{\"end\":3.26,\"probability\":0.9990234375,\"speaker\":\"SPEAKER_01\",\"start\":3.12,\"word\":\"        â”‚\n",
      "â”‚ me\"},{\"end\":3.74,\"probability\":0.998046875,\"speaker\":\"SPEAKER_01\",\"start\":3.26,\"word\":\"          â”‚\n",
      "â”‚ ask\"},{\"end\":3.86,\"probability\":0.99267578125,\"speaker\":\"SPEAKER_01\",\"start\":3.74,\"word\":\"       â”‚\n",
      "â”‚ you\"},{\"end\":4.1,\"probability\":0.9990234375,\"speaker\":\"SPEAKER_01\",\"start\":3.86,\"word\":\"         â”‚\n",
      "â”‚ about\"},{\"end\":4.48,\"probability\":0.966796875,\"speaker\":\"SPEAKER_01\",\"start\":4.1,\"word\":\"        â”‚\n",
      "â”‚ AI.\"}]}                                                                                          â”‚\n",
      "â”‚ {\"text\":\"It seems like this year for the entirety of the human civilization is an interesting    â”‚\n",
      "â”‚ year for the development of artificial intelligence.\",\"start\":4.72,\"end\":11.66,\"words\":[{\"end\":4 â”‚\n",
      "â”‚ .74,\"probability\":0.99462890625,\"speaker\":\"SPEAKER_01\",\"start\":4.72,\"word\":\"                     â”‚\n",
      "â”‚ It\"},{\"end\":4.94,\"probability\":0.998046875,\"speaker\":\"SPEAKER_01\",\"start\":4.74,\"word\":\"          â”‚\n",
      "â”‚ seems\"},{\"end\":5.12,\"probability\":0.99462890625,\"speaker\":\"SPEAKER_01\",\"start\":4.94,\"word\":\"     â”‚\n",
      "â”‚ like\"},{\"end\":5.32,\"probability\":0.99853515625,\"speaker\":\"SPEAKER_01\",\"start\":5.12,\"word\":\"      â”‚\n",
      "â”‚ this\"},{\"end\":5.72,\"probability\":0.99951171875,\"speaker\":\"SPEAKER_01\",\"start\":5.32,\"word\":\"      â”‚\n",
      "â”‚ year\"},{\"end\":6.5,\"probability\":0.68603515625,\"speaker\":\"SPEAKER_01\",\"start\":5.72,\"word\":\"       â”‚\n",
      "â”‚ for\"},{\"end\":6.62,\"probability\":0.9990234375,\"speaker\":\"SPEAKER_01\",\"start\":6.5,\"word\":\"         â”‚\n",
      "â”‚ the\"},{\"end\":6.98,\"probability\":0.98828125,\"speaker\":\"SPEAKER_01\",\"start\":6.62,\"word\":\"          â”‚\n",
      "â”‚ entirety\"},{\"end\":7.16,\"probability\":0.99951171875,\"speaker\":\"SPEAKER_01\",\"start\":6.98,\"word\":\"  â”‚\n",
      "â”‚ of\"}â€¦                                                                                            â”‚\n",
      "â”‚ â€¦                                                                                                â”‚\n",
      "â”‚ {\"text\":\"It's going to create better experiences for people and just unlock a ton of innovation  â”‚\n",
      "â”‚ and value.\",\"start\":1482.06,\"end\":1486.96,\"words\":[{\"end\":1482.12,\"probability\":0.99951171875,\"s â”‚\n",
      "â”‚ peaker\":\"SPEAKER_00\",\"start\":1482.06,\"word\":\"                                                    â”‚\n",
      "â”‚ It's\"},{\"end\":1482.16,\"probability\":0.9990234375,\"speaker\":\"SPEAKER_00\",\"start\":1482.12,\"word\":\" â”‚\n",
      "â”‚ going\"},{\"end\":1482.22,\"probability\":0.99951171875,\"speaker\":\"SPEAKER_00\",\"start\":1482.16,\"word\" â”‚\n",
      "â”‚ :\" to\"},{\"end\":1482.38,\"probability\":1,\"speaker\":\"SPEAKER_00\",\"start\":1482.22,\"word\":\" create\"}, â”‚\n",
      "â”‚ {\"end\":1482.7,\"probability\":0.99951171875,\"speaker\":\"SPEAKER_00\",\"start\":1482.38,\"word\":\" better â”‚\n",
      "â”‚ \"},{\"end\":1484.12,\"probability\":0.99560546875,\"speaker\":\"SPEAKER_00\",\"start\":1482.92,\"word\":\"    â”‚\n",
      "â”‚ experiences\"},{\"end\":1484.42,\"probability\":1,\"speaker\":\"SPEAKER_00\",\"start\":1484.12,\"word\":\"     â”‚\n",
      "â”‚ for\"},{\"end\":1484.7,\"probability\":1,\"speaker\":\"SPEAKER_00\",\"start\":1484.42,\"word\":\" people\"},{\"e â”‚\n",
      "â”‚ nd\":1484.92,\"probability\":0.99853515625,\"speaker\":\"SPEAKER_00\",\"start\":1484.7,\"word\":\" and\"},{â€¦  â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# 2. Create Segments DataFrame (Content-focused)\n",
    "print(\"\\nğŸ“ Creating Segments DataFrame...\")\n",
    "\n",
    "# Extract segment-level data using JQ\n",
    "# This demonstrates extracting data at a different granularity level\n",
    "segments_df = df_json.select(\n",
    "    fc.json.jq(\n",
    "        fc.col(\"json_data\"),\n",
    "        # Extract segment objects with their text, timing, and nested words array\n",
    "        '.segments[] | {text: .text, start: .start, end: .end, words: .words}'\n",
    "    ).alias(\"segment_data\")\n",
    ").explode(\"segment_data\")  # Convert segments array into separate rows\n",
    "\n",
    "print(f\"Extracted {segments_df.count()} segments\")\n",
    "segments_df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Segments DataFrame with calculated metrics:\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ segment_text            â”† start_time  â”† end_time    â”† duration â”† word_count â”† average_confidence â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ Let me ask you about    â”† 2.94        â”† 4.48        â”† 1.54     â”† 6          â”† 0.941569           â”‚\n",
      "â”‚ AI.                     â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ It seems like this year â”† 4.72        â”† 11.66       â”† 6.94     â”† 22         â”† 0.981401           â”‚\n",
      "â”‚ for the entirety of the â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ human civilization is   â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ an interesting year for â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ the development of      â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ artificial              â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ intelligence.           â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ A lot of interesting    â”† 12.2        â”† 14.24       â”† 2.04     â”† 7          â”† 0.998186           â”‚\n",
      "â”‚ stuff is happening.     â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ â€¦                       â”† â€¦           â”† â€¦           â”† â€¦        â”† â€¦          â”† â€¦                  â”‚\n",
      "â”‚ now we're going to be   â”† 1460.180054 â”† 1481.920044 â”† 21.73999 â”† 62         â”† 0.949474           â”‚\n",
      "â”‚ able to get help coding â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ the things that they    â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ need to go build things â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ or designing the things â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ that they need, will be â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ able to use these       â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ models to be able to do â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ customer support for    â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ the people that they're â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ serving over WhatsApp   â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ without having to, I    â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ think that this is all  â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ going to be super       â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ exciting.               â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ It's going to create    â”† 1482.060059 â”† 1486.959961 â”† 4.899902 â”† 17         â”† 0.998851           â”‚\n",
      "â”‚ better experiences for  â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ people and just unlock  â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ a ton of innovation and â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â”‚ value.                  â”†             â”†             â”†          â”†            â”†                    â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# Extract segment fields using hybrid approach: struct casting + JQ for complex aggregations\n",
    "# Define schema for basic segment fields (text, start, end)\n",
    "segment_basic_schema = fc.StructType([\n",
    "    fc.StructField(\"text\", fc.StringType),\n",
    "    fc.StructField(\"start\", fc.FloatType),\n",
    "    fc.StructField(\"end\", fc.FloatType)\n",
    "])\n",
    "\n",
    "# First extract basic fields using struct casting, then add complex JQ aggregations\n",
    "segments_clean_df = segments_df.select(\n",
    "    # Extract basic segment data using struct casting (more efficient)\n",
    "    fc.col(\"segment_data\").cast(segment_basic_schema).alias(\"segment_struct\"),\n",
    "    # Complex array aggregations still use JQ (best tool for this)\n",
    "    fc.json.jq(fc.col(\"segment_data\"), '.words | length').get_item(0).cast(fc.IntegerType).alias(\"word_count\"),\n",
    "    fc.json.jq(fc.col(\"segment_data\"), '[.words[].probability] | add / length').get_item(0).cast(fc.FloatType).alias(\"average_confidence\")\n",
    ").unnest(\"segment_struct\").select(\n",
    "    # Rename for clarity\n",
    "    fc.col(\"text\").alias(\"segment_text\"),\n",
    "    fc.col(\"start\").alias(\"start_time\"),\n",
    "    fc.col(\"end\").alias(\"end_time\"),\n",
    "    fc.col(\"word_count\"),\n",
    "    fc.col(\"average_confidence\")\n",
    ").select(\n",
    "    \"segment_text\",\n",
    "    \"start_time\",\n",
    "    \"end_time\",\n",
    "    # Calculate segment duration using DataFrame arithmetic\n",
    "    (fc.col(\"end_time\") - fc.col(\"start_time\")).alias(\"duration\"),\n",
    "    \"word_count\",\n",
    "    \"average_confidence\"\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“Š Segments DataFrame with calculated metrics:\")\n",
    "segments_clean_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¤ Creating Speaker Summary DataFrame...\n",
      "\n",
      "ğŸ“Š Speaker Summary DataFrame:\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ speaker    â”† total_words â”† total_speaki â”† average_conf â”† first_speaki â”† last_speaki â”† word_rate  â”‚\n",
      "â”‚            â”†             â”† ng_time      â”† idence       â”† ng_time      â”† ng_time     â”†            â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ SPEAKER_00 â”† 3124        â”† 912.02002    â”† 0.969939     â”† 39.66        â”† 1486.959961 â”† 205.521792 â”‚\n",
      "â”‚ SPEAKER_01 â”† 1011        â”† 329.14093    â”† 0.963907     â”† 2.94         â”† 1409.420044 â”† 184.297943 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# 3. Create Speaker Summary DataFrame (Aggregated)\n",
    "print(\"\\nğŸ¤ Creating Speaker Summary DataFrame...\")\n",
    "\n",
    "# Use traditional DataFrame aggregations on JSON-extracted data\n",
    "# This demonstrates hybrid processing: JSON extraction + DataFrame analytics\n",
    "speaker_summary_df = words_final_df.group_by(\"speaker\").agg(\n",
    "    fc.count(\"*\").alias(\"total_words\"),                    # Count words per speaker\n",
    "    fc.avg(\"probability\").alias(\"average_confidence\"),     # Average speech confidence\n",
    "    fc.min(\"start_time\").alias(\"first_speaking_time\"),     # When speaker first appears\n",
    "    fc.max(\"end_time\").alias(\"last_speaking_time\"),        # When speaker last appears\n",
    "    fc.sum(\"duration\").alias(\"total_speaking_time\")        # Total time speaking\n",
    ").select(\n",
    "    \"speaker\",\n",
    "    \"total_words\", \n",
    "    \"total_speaking_time\",\n",
    "    \"average_confidence\",\n",
    "    \"first_speaking_time\",\n",
    "    \"last_speaking_time\",\n",
    "    # Calculate derived metric: words per minute\n",
    "    (fc.col(\"total_words\") / (fc.col(\"total_speaking_time\") / 60.0)).alias(\"word_rate\")\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“Š Speaker Summary DataFrame:\")\n",
    "speaker_summary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ JSON Processing Pipeline Summary:\n",
      "============================================================\n",
      "ğŸ“ Input: Single JSON file (whisper-transcript.json)\n",
      "\n",
      "ğŸ“Š Output: 3 structured DataFrames\n",
      "\n",
      "1. ğŸ”¤ Words DataFrame:\n",
      "   - 4135 individual words extracted\n",
      "   - Fields: word_text, speaker, timing, probability, duration\n",
      "   - Demonstrates: JQ nested array extraction, type casting\n",
      "\n",
      "2. ğŸ“ Segments DataFrame:\n",
      "   - 243 conversation segments\n",
      "   - Fields: text, timing, word_count, average_confidence\n",
      "   - Demonstrates: JQ aggregations, array operations\n",
      "\n",
      "3. ğŸ¤ Speaker Summary DataFrame:\n",
      "   - 2 speakers analyzed\n",
      "   - Fields: totals, averages, speaking patterns, word rates\n",
      "   - Demonstrates: DataFrame aggregations on JSON-extracted data\n",
      "\n",
      "ğŸ”§ Key Fenic JSON Features Used:\n",
      "   âœ“ JSON type casting from strings\n",
      "   âœ“ JQ queries for complex nested extraction\n",
      "   âœ“ Array operations and aggregations\n",
      "   âœ“ Type conversion and calculated fields\n",
      "   âœ“ Traditional DataFrame operations on JSON data\n"
     ]
    }
   ],
   "source": [
    "# Summary of what we accomplished\n",
    "print(\"\\nğŸ¯ JSON Processing Pipeline Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“ Input: Single JSON file (whisper-transcript.json)\\n\")\n",
    "print(\"ğŸ“Š Output: 3 structured DataFrames\")\n",
    "print()\n",
    "print(\"1. ğŸ”¤ Words DataFrame:\")\n",
    "print(f\"   - {words_final_df.count()} individual words extracted\")\n",
    "print(\"   - Fields: word_text, speaker, timing, probability, duration\")\n",
    "print(\"   - Demonstrates: JQ nested array extraction, type casting\")\n",
    "print()\n",
    "print(\"2. ğŸ“ Segments DataFrame:\")\n",
    "print(f\"   - {segments_clean_df.count()} conversation segments\")\n",
    "print(\"   - Fields: text, timing, word_count, average_confidence\")\n",
    "print(\"   - Demonstrates: JQ aggregations, array operations\")\n",
    "print()\n",
    "print(\"3. ğŸ¤ Speaker Summary DataFrame:\")\n",
    "print(f\"   - {speaker_summary_df.count()} speakers analyzed\")\n",
    "print(\"   - Fields: totals, averages, speaking patterns, word rates\")\n",
    "print(\"   - Demonstrates: DataFrame aggregations on JSON-extracted data\")\n",
    "print()\n",
    "print(\"ğŸ”§ Key Fenic JSON Features Used:\")\n",
    "print(\"   âœ“ JSON type casting from strings\")\n",
    "print(\"   âœ“ JQ queries for complex nested extraction\")\n",
    "print(\"   âœ“ Array operations and aggregations\")\n",
    "print(\"   âœ“ Type conversion and calculated fields\")\n",
    "print(\"   âœ“ Traditional DataFrame operations on JSON data\")\n",
    "\n",
    "# Clean up\n",
    "session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
